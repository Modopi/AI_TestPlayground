{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” â€” FlashAttention ì‹¬ì¸µ ë¶„ì„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- í‘œì¤€ Attentionì˜ **IO ë³µì¡ë„**ë¥¼ ë¶„ì„í•˜ê³  HBM ì ‘ê·¼ì´ ë³‘ëª©ì„ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•œë‹¤\n",
    "- FlashAttentionì˜ **Tiling + Online Softmax + Recomputation** ì „ëµì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤\n",
    "- SRAM í¬ê¸° $M$ì— ë”°ë¥¸ **IO ë³µì¡ë„ $O(N^2d^2/M)$**ì„ ìœ ë„í•œë‹¤\n",
    "- FlashAttention v1 â†’ v2 â†’ v3ì˜ **ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸**ë¥¼ ë¹„êµ ë¶„ì„í•œë‹¤\n",
    "- ë¸”ë¡ ë‹¨ìœ„ Attention ê³„ì‚°ì„ **TensorFlowë¡œ ì§ì ‘ ì‹œë®¬ë ˆì´ì…˜**í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Attention IO ë³µì¡ë„](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [í‘œì¤€ Attentionì˜ ë©”ëª¨ë¦¬ ë³‘ëª©](#2.-í‘œì¤€-Attention)\n",
    "3. [FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜](#3.-Tiling-ì‹œë®¬ë ˆì´ì…˜)\n",
    "4. [IO ë³µì¡ë„ ë¹„êµ ì‹œê°í™”](#4.-IO-ë³µì¡ë„-ì‹œê°í™”)\n",
    "5. [v1 â†’ v2 â†’ v3 ë°œì „ì‚¬](#5.-FlashAttention-ë°œì „ì‚¬)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### í‘œì¤€ Attentionì˜ IO ë³µì¡ë„\n",
    "\n",
    "Attention ì—°ì‚°: $\\text{Attn}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V$\n",
    "\n",
    "ì—¬ê¸°ì„œ $Q, K, V \\in \\mathbb{R}^{N \\times d}$ì´ê³ , $N$ì€ ì‹œí€€ìŠ¤ ê¸¸ì´, $d$ëŠ” í—¤ë“œ ì°¨ì›ì…ë‹ˆë‹¤.\n",
    "\n",
    "**í‘œì¤€ êµ¬í˜„ì˜ HBM ì ‘ê·¼:**\n",
    "\n",
    "| ë‹¨ê³„ | ì½ê¸° | ì“°ê¸° | HBM ì ‘ê·¼ |\n",
    "|------|------|------|----------|\n",
    "| $S = QK^T/\\sqrt{d}$ | $Q, K$ ($2Nd$) | $S$ ($N^2$) | $O(Nd + N^2)$ |\n",
    "| $P = \\text{softmax}(S)$ | $S$ ($N^2$) | $P$ ($N^2$) | $O(N^2)$ |\n",
    "| $O = PV$ | $P$ ($N^2$), $V$ ($Nd$) | $O$ ($Nd$) | $O(N^2 + Nd)$ |\n",
    "\n",
    "$$\\text{Total HBM I/O} = O(Nd + N^2) \\quad \\text{bytes (ì›ì†Œ ë‹¨ìœ„)}$$\n",
    "\n",
    "í•µì‹¬ ë¬¸ì œ: **$N^2$ í¬ê¸°ì˜ ì¤‘ê°„ í–‰ë ¬ $S, P$ê°€ HBMì— ì €ì¥**ë¨!\n",
    "\n",
    "### FlashAttentionì˜ IO ë³µì¡ë„\n",
    "\n",
    "Tilingì„ ì ìš©í•˜ë©´ $S, P$ë¥¼ **SRAMì—ì„œë§Œ ì²˜ë¦¬**í•˜ê³  HBMì— ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{FlashAttention HBM I/O} = O\\left(\\frac{N^2 d^2}{M}\\right)$$\n",
    "\n",
    "- $M$: SRAM í¬ê¸° (A100: 192KB per SM, ì „ì²´ ~20MB)\n",
    "- ì¡°ê±´: $d^2 \\leq M$ (í—¤ë“œ ì°¨ì›ì˜ ì œê³±ì´ SRAMì— ë“¤ì–´ê°€ì•¼ í•¨)\n",
    "\n",
    "### Tiling ì „ëµ\n",
    "\n",
    "ë¸”ë¡ í¬ê¸°: $B_r = \\lceil M / (4d) \\rceil$, $B_c = \\min\\left(\\lceil M / (4d) \\rceil, d\\right)$\n",
    "\n",
    "ë‚´ë¶€ ë£¨í”„ì—ì„œ ë¸”ë¡ $(i, j)$ì˜ ì—°ì‚°:\n",
    "\n",
    "$$S_{ij} = Q_i K_j^T \\in \\mathbb{R}^{B_r \\times B_c}$$\n",
    "\n",
    "Online Softmax (Milakov & Gimelshein 2018):\n",
    "\n",
    "$$m_i^{(j)} = \\max(m_i^{(j-1)}, \\text{rowmax}(S_{ij}))$$\n",
    "\n",
    "$$\\ell_i^{(j)} = e^{m_i^{(j-1)} - m_i^{(j)}} \\ell_i^{(j-1)} + \\text{rowsum}(e^{S_{ij} - m_i^{(j)}})$$\n",
    "\n",
    "$$O_i^{(j)} = \\text{diag}(e^{m_i^{(j-1)} - m_i^{(j)}}) O_i^{(j-1)} + e^{S_{ij} - m_i^{(j)}} V_j$$\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| ë°©ë²• | HBM I/O | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì •í™•ë„ |\n",
    "|------|---------|------------|--------|\n",
    "| í‘œì¤€ Attention | $O(Nd + N^2)$ | $O(N^2)$ | Exact |\n",
    "| FlashAttention | $O(N^2d^2/M)$ | $O(N)$ | Exact |\n",
    "| Sparse Attention | $O(N\\sqrt{N})$ | $O(N\\sqrt{N})$ | Approximate |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ FlashAttention ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ Tiling(íƒ€ì¼ë§)ì´ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: í¼ì¦ì„ í’€ ë•Œ **ì „ì²´ ê·¸ë¦¼ì„ ì±…ìƒì— í¼ì¹˜ëŠ” ê²ƒ** vs **ì‘ì€ ì¡°ê°ì”© ë§ì¶”ëŠ” ê²ƒ**ì„ ë¹„êµí•´ ë´…ì‹œë‹¤!\n",
    "\n",
    "**í‘œì¤€ Attention**: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 4096ì´ë©´, 4096Ã—4096 = 1,677ë§Œ ê°œì˜ ìˆ«ìë¥¼ í•œêº¼ë²ˆì— \n",
    "ë©”ëª¨ì¥(HBM)ì— ì¨ì•¼ í•´ìš”. ë©”ëª¨ì¥ê¹Œì§€ ì™•ë³µí•˜ëŠë¼ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ìš”! ğŸ“\n",
    "\n",
    "**FlashAttention**: ì‘ì€ ë¸”ë¡(ì˜ˆ: 64Ã—64)ì”© ì˜ë¼ì„œ **ë¨¸ë¦¿ì† ì¹ íŒ(SRAM)**ì—ì„œ ë°”ë¡œ ê³„ì‚°í•´ìš”.\n",
    "ì¹ íŒì€ ì‘ì§€ë§Œ ì•„ì£¼ ë¹¨ë¼ì„œ, ë©”ëª¨ì¥ì— ì™•ë³µí•˜ì§€ ì•Šì•„ë„ ë¼ìš”! ğŸ§ \n",
    "\n",
    "#### ğŸ”„ Online SoftmaxëŠ” ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì ìˆ˜ì˜ í‰ê· ì„ êµ¬í•  ë•Œ, ëª¨ë“  ì ìˆ˜ë¥¼ ë‹¤ ëª¨ì•„ì•¼ í• ê¹Œìš”?\n",
    "\n",
    "ì•„ë‹ˆìš”! í•œ ëª…ì”© ì ìˆ˜ë¥¼ ë°›ìœ¼ë©´ì„œ \"ì§€ê¸ˆê¹Œì§€ì˜ ìµœëŒ€ê°’\"ê³¼ \"ëˆ„ì  í•©\"ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ë¼ìš”.\n",
    "FlashAttentionë„ ì „ì²´ í–‰ì„ ë³´ì§€ ì•Šê³ , **ë¸”ë¡ë³„ë¡œ softmaxë¥¼ ì ì§„ì ìœ¼ë¡œ ê³„ì‚°**í•©ë‹ˆë‹¤!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: HBM ì ‘ê·¼ëŸ‰ ë¹„êµ\n",
    "\n",
    "$N=2048$, $d=128$, SRAM $M = 100\\text{KB} = 100 \\times 1024 / 2 = 51200$ ì›ì†Œ (FP16)ì¼ ë•Œ:\n",
    "\n",
    "1. í‘œì¤€ Attentionì˜ HBM I/O (ì›ì†Œ ìˆ˜)\n",
    "2. FlashAttentionì˜ HBM I/O (ì›ì†Œ ìˆ˜)\n",
    "3. ì ˆê° ë¹„ìœ¨\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "**í‘œì¤€ Attention:**\n",
    "$$\\text{I/O} = Nd + N^2 + N^2 + Nd + N^2 = 3N^2 + 2Nd$$\n",
    "$$= 3 \\times 2048^2 + 2 \\times 2048 \\times 128 = 12,582,912 + 524,288 = 13,107,200$$\n",
    "\n",
    "**FlashAttention:**\n",
    "$$\\text{I/O} = O\\left(\\frac{N^2 d^2}{M}\\right) = \\frac{2048^2 \\times 128^2}{51200} = \\frac{68,719,476,736}{51200} \\approx 1,342,177$$\n",
    "\n",
    "**ì ˆê° ë¹„ìœ¨:** $13,107,200 / 1,342,177 \\approx 9.8\\times$ â†’ **ì•½ 10ë°° ì ˆê°!**\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: ë¸”ë¡ í¬ê¸° ê³„ì‚°\n",
    "\n",
    "SRAM $M = 192\\text{KB}$, $d = 128$, FP16 (2 bytes)ì¼ ë•Œ ì ì ˆí•œ ë¸”ë¡ í¬ê¸° $B_r$ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "SRAMì— $Q_i$ ($B_r \\times d$), $K_j$ ($B_c \\times d$), $S_{ij}$ ($B_r \\times B_c$), $O_i$ ($B_r \\times d$)ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$M \\geq (B_r \\cdot d + B_c \\cdot d + B_r \\cdot B_c + B_r \\cdot d) \\times 2\\;\\text{bytes}$$\n",
    "\n",
    "$B_r = B_c$ë¡œ ë†“ìœ¼ë©´: $M \\geq (3Bd + B^2) \\times 2$\n",
    "\n",
    "$$192 \\times 1024 \\geq (3B \\times 128 + B^2) \\times 2$$\n",
    "$$98304 \\geq 768B + 2B^2$$\n",
    "\n",
    "$B = 64$: $768 \\times 64 + 2 \\times 4096 = 49152 + 8192 = 57344 < 98304$ âœ“\n",
    "\n",
    "$B = 128$: $768 \\times 128 + 2 \\times 16384 = 98304 + 32768 = 131072 > 98304$ âœ—\n",
    "\n",
    "â†’ $B_r = B_c = 64$ê°€ ì ì ˆí•©ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í‘œì¤€ Attentionì˜ ë©”ëª¨ë¦¬ ë³‘ëª© <a name='2.-í‘œì¤€-Attention'></a>\n",
    "\n",
    "í‘œì¤€ Attentionê³¼ FlashAttentionì˜ **HBM ì ‘ê·¼ íšŸìˆ˜**ë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ë¡œ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í‘œì¤€ vs FlashAttention HBM ì ‘ê·¼ëŸ‰ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d = 128\n",
    "M_sram_bytes = 192 * 1024  # 192KB SRAM (A100 per SM)\n",
    "M_sram_elements = M_sram_bytes // 2  # FP16\n",
    "\n",
    "seq_lengths = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "\n",
    "print(f\"SRAM í¬ê¸°: {M_sram_bytes / 1024:.0f} KB ({M_sram_elements} FP16 ì›ì†Œ)\")\n",
    "print(f\"í—¤ë“œ ì°¨ì› d: {d}\")\n",
    "print(f\"\\n{'N':>7} | {'Standard I/O':>14} | {'Flash I/O':>14} | {'ì ˆê° ë°°ìœ¨':>10} | {'Standard Mem':>14} | {'Flash Mem':>12}\")\n",
    "print(f\"{'-'*82}\")\n",
    "\n",
    "standard_ios = []\n",
    "flash_ios = []\n",
    "standard_mems = []\n",
    "flash_mems = []\n",
    "\n",
    "for N in seq_lengths:\n",
    "    # í‘œì¤€ Attention\n",
    "    std_io = 3 * N * N + 2 * N * d  # S=QK^T, P=softmax, O=PV\n",
    "    std_mem = N * N  # S/P í–‰ë ¬ ì €ì¥\n",
    "\n",
    "    # FlashAttention\n",
    "    flash_io = (N * N * d * d) / M_sram_elements\n",
    "    flash_mem = N  # O(N) ì¶”ê°€ ë©”ëª¨ë¦¬\n",
    "\n",
    "    ratio = std_io / flash_io if flash_io > 0 else float('inf')\n",
    "\n",
    "    standard_ios.append(std_io)\n",
    "    flash_ios.append(flash_io)\n",
    "    standard_mems.append(std_mem * 2 / 1e6)  # MB\n",
    "    flash_mems.append(flash_mem * 2 / 1e6)\n",
    "\n",
    "    print(f\"{N:>7} | {std_io:>14.2e} | {flash_io:>14.2e} | {ratio:>9.1f}x | {std_mem * 2 / 1e6:>12.1f} MB | {flash_mem * 2 / 1e6:>10.4f} MB\")\n",
    "\n",
    "print(f\"\\ní•µì‹¬:\")\n",
    "print(f\"  í‘œì¤€ Attention: O(N^2) ë©”ëª¨ë¦¬ â†’ N=16384ì—ì„œ {standard_mems[-1]:.0f} MB!\")\n",
    "print(f\"  FlashAttention: O(N) ë©”ëª¨ë¦¬  â†’ N=16384ì—ì„œ {flash_mems[-1]:.4f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜ <a name='3.-Tiling-ì‹œë®¬ë ˆì´ì…˜'></a>\n",
    "\n",
    "ë¸”ë¡ ë‹¨ìœ„ë¡œ Attentionì„ ê³„ì‚°í•˜ëŠ” FlashAttention ì•Œê³ ë¦¬ì¦˜ì„ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "í•µì‹¬ì€ **Online Softmax**: ì „ì²´ í–‰ì„ ë³´ì§€ ì•Šê³ ë„ ì •í™•í•œ softmaxë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$O_i = \\frac{\\sum_j e^{S_{ij} - m_i} V_j}{\\sum_j e^{S_{ij} - m_i}} \\quad \\text{where } m_i = \\max_j S_{ij}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜ (ë¸”ë¡ ë‹¨ìœ„ ê³„ì‚°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def standard_attention(Q, K, V):\n",
    "    # í‘œì¤€ Attention: ì „ì²´ S í–‰ë ¬ì„ ë©”ëª¨ë¦¬ì— ìƒì„±\n",
    "    d_k = tf.cast(tf.shape(Q)[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d_k)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    output = tf.matmul(weights, V)\n",
    "    return output\n",
    "\n",
    "def flash_attention_sim(Q, K, V, block_size=64):\n",
    "    # FlashAttention ì‹œë®¬ë ˆì´ì…˜: ë¸”ë¡ ë‹¨ìœ„ + Online Softmax\n",
    "    N = Q.shape[0]\n",
    "    d = Q.shape[1]\n",
    "\n",
    "    O = tf.zeros_like(Q)\n",
    "    l = tf.zeros([N, 1])  # softmax ë¶„ëª¨\n",
    "    m = tf.fill([N, 1], -1e9)  # í–‰ë³„ ìµœëŒ€ê°’\n",
    "\n",
    "    n_blocks = (N + block_size - 1) // block_size\n",
    "    hbm_reads = 0\n",
    "\n",
    "    for j in range(n_blocks):\n",
    "        kj_start = j * block_size\n",
    "        kj_end = min((j + 1) * block_size, N)\n",
    "\n",
    "        Kj = K[kj_start:kj_end]  # [Bc, d]\n",
    "        Vj = V[kj_start:kj_end]  # [Bc, d]\n",
    "        hbm_reads += (kj_end - kj_start) * d * 2  # K, V ì½ê¸°\n",
    "\n",
    "        for i in range(n_blocks):\n",
    "            qi_start = i * block_size\n",
    "            qi_end = min((i + 1) * block_size, N)\n",
    "\n",
    "            Qi = Q[qi_start:qi_end]  # [Br, d]\n",
    "            hbm_reads += (qi_end - qi_start) * d  # Q ì½ê¸°\n",
    "\n",
    "            Sij = tf.matmul(Qi, Kj, transpose_b=True) / tf.sqrt(tf.cast(d, tf.float32))\n",
    "\n",
    "            m_old = m[qi_start:qi_end]\n",
    "            m_new_block = tf.reduce_max(Sij, axis=-1, keepdims=True)\n",
    "            m_new = tf.maximum(m_old, m_new_block)\n",
    "\n",
    "            exp_old = tf.exp(m_old - m_new)\n",
    "            exp_new = tf.exp(Sij - m_new)\n",
    "\n",
    "            l_old = l[qi_start:qi_end]\n",
    "            l_new = exp_old * l_old + tf.reduce_sum(exp_new, axis=-1, keepdims=True)\n",
    "\n",
    "            O_old = O[qi_start:qi_end]\n",
    "            O_new = exp_old * O_old + tf.matmul(exp_new, Vj)\n",
    "\n",
    "            # í…ì„œ ê°±ì‹  (scatter ëŒ€ì‹  ìŠ¬ë¼ì´ìŠ¤)\n",
    "            O = tf.concat([O[:qi_start], O_new, O[qi_end:]], axis=0)\n",
    "            l = tf.concat([l[:qi_start], l_new, l[qi_end:]], axis=0)\n",
    "            m = tf.concat([m[:qi_start], m_new, m[qi_end:]], axis=0)\n",
    "\n",
    "    O = O / l\n",
    "    return O, hbm_reads\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "N, d = 256, 64\n",
    "Q = tf.random.normal([N, d])\n",
    "K = tf.random.normal([N, d])\n",
    "V = tf.random.normal([N, d])\n",
    "\n",
    "std_out = standard_attention(Q, K, V)\n",
    "flash_out, hbm_reads = flash_attention_sim(Q, K, V, block_size=64)\n",
    "\n",
    "diff = tf.reduce_max(tf.abs(std_out - flash_out)).numpy()\n",
    "print(f\"ì‹œí€€ìŠ¤ ê¸¸ì´: {N}, í—¤ë“œ ì°¨ì›: {d}, ë¸”ë¡ í¬ê¸°: 64\")\n",
    "print(f\"í‘œì¤€ Attention vs FlashAttention ìµœëŒ€ ì˜¤ì°¨: {diff:.2e}\")\n",
    "print(f\"ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼ ì—¬ë¶€: {diff < 1e-4}\")\n",
    "print(f\"FlashAttention ì‹œë®¬ë ˆì´ì…˜ HBM ì½ê¸°: {hbm_reads:,} ì›ì†Œ\")\n",
    "print(f\"í‘œì¤€ Attention HBM ì½ê¸° (ì´ë¡ ): {3*N*N + 2*N*d:,} ì›ì†Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IO ë³µì¡ë„ ë¹„êµ ì‹œê°í™” <a name='4.-IO-ë³µì¡ë„-ì‹œê°í™”'></a>\n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ HBM I/OëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ IO ë³µì¡ë„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# HBM I/O ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(seq_lengths, standard_ios, 'r-o', lw=2.5, ms=8, label='Standard: $O(Nd + N^2)$')\n",
    "ax1.semilogy(seq_lengths, flash_ios, 'b-s', lw=2.5, ms=8, label='FlashAttn: $O(N^2d^2/M)$')\n",
    "ax1.fill_between(seq_lengths, flash_ios, standard_ios, alpha=0.1, color='green')\n",
    "ax1.set_xlabel('Sequence Length (N)', fontsize=11)\n",
    "ax1.set_ylabel('HBM I/O (elements, log scale)', fontsize=11)\n",
    "ax1.set_title('HBM I/O: Standard vs FlashAttention', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "ax2.semilogy(seq_lengths, standard_mems, 'r-o', lw=2.5, ms=8, label='Standard: $O(N^2)$')\n",
    "ax2.semilogy(seq_lengths, flash_mems, 'b-s', lw=2.5, ms=8, label='FlashAttn: $O(N)$')\n",
    "ax2.axhline(y=80*1024, color='gray', ls='--', lw=1.5, label='A100 80GB')\n",
    "ax2.set_xlabel('Sequence Length (N)', fontsize=11)\n",
    "ax2.set_ylabel('Peak Memory (MB, log scale)', fontsize=11)\n",
    "ax2.set_title('Memory Usage: Standard vs FlashAttention', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter14_extreme_inference/flash_attention_io_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/flash_attention_io_comparison.png\")\n",
    "\n",
    "# ì ˆê° ë¹„ìœ¨ í‘œ\n",
    "print(f\"\\nì‹œí€€ìŠ¤ ê¸¸ì´ë³„ I/O ì ˆê° ë¹„ìœ¨:\")\n",
    "for i, N in enumerate(seq_lengths):\n",
    "    ratio = standard_ios[i] / flash_ios[i]\n",
    "    print(f\"  N={N:>6}: {ratio:>6.1f}x ì ˆê°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. FlashAttention v1 â†’ v2 â†’ v3 ë°œì „ì‚¬ <a name='5.-FlashAttention-ë°œì „ì‚¬'></a>\n",
    "\n",
    "FlashAttentionì€ ì„¸ ë²„ì „ì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ë²„ì „ì˜ í•µì‹¬ ê¸°ì—¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ FlashAttention v1 â†’ v2 â†’ v3 ë¹„êµí‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"=\" * 90)\n",
    "print(\"  FlashAttention Version Comparison\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "headers = ['í•­ëª©', 'v1 (2022.05)', 'v2 (2023.07)', 'v3 (2024.07)']\n",
    "rows = [\n",
    "    ['ë…¼ë¬¸', 'Dao et al.', 'Dao (ë‹¨ë…)', 'Shah, Dao et al.'],\n",
    "    ['íƒ€ê²Ÿ GPU', 'A100', 'A100/H100', 'H100 (Hopper)'],\n",
    "    ['IO ë³µì¡ë„', 'O(NÂ²dÂ²/M)', 'O(NÂ²dÂ²/M) (ë™ì¼)', 'O(NÂ²dÂ²/M) (ë™ì¼)'],\n",
    "    ['í•µì‹¬ ê°œì„ ', 'Tiling+Online Softmax', 'ë£¨í”„ ìˆœì„œ ìµœì í™”', 'Warp íŠ¹í™”+ë¹„ë™ê¸°'],\n",
    "    ['Forward ì†ë„', '~2-4x vs PyTorch', '~2x vs v1', '~1.5-2x vs v2'],\n",
    "    ['Backward ì§€ì›', 'O (Recomputation)', 'O (ê°œì„ ëœ ë¶„í• )', 'O (ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸)'],\n",
    "    ['Causal Mask', 'ì§€ì›', 'ìµœì í™”ë¨ (~50% ì ˆê°)', 'ì¶”ê°€ ìµœì í™”'],\n",
    "    ['MQA/GQA', 'ë¯¸ì§€ì›', 'ì§€ì›', 'ì™„ì „ ì§€ì›'],\n",
    "    ['FP8 ì§€ì›', 'ë¯¸ì§€ì›', 'ë¯¸ì§€ì›', 'ì§€ì› (H100)'],\n",
    "    ['ì›Œí”„ ë³‘ë ¬í™”', 'ë°°ì¹˜+í—¤ë“œ', '+ì‹œí€€ìŠ¤ ë¶„í• ', '+warpgroup íŠ¹í™”'],\n",
    "    ['ë¹„ë™ê¸° ì—°ì‚°', 'ë¯¸ì‚¬ìš©', 'ë¯¸ì‚¬ìš©', 'TMA+WGMMA ì˜¤ë²„ë©'],\n",
    "    ['í”¼í¬ TFLOPS', '~125 (A100)', '~230 (H100)', '~740 (H100, FP8)'],\n",
    "]\n",
    "\n",
    "col_widths = [18, 22, 22, 22]\n",
    "header_line = ' | '.join(h.ljust(w) for h, w in zip(headers, col_widths))\n",
    "print(header_line)\n",
    "print('-' * len(header_line))\n",
    "\n",
    "for row in rows:\n",
    "    line = ' | '.join(val.ljust(w) for val, w in zip(row, col_widths))\n",
    "    print(line)\n",
    "\n",
    "print(f\"\\ní•µì‹¬ ë°œì „ ìš”ì•½:\")\n",
    "print(f\"  v1: Tiling + Online Softmax â†’ O(N^2) ë©”ëª¨ë¦¬ë¥¼ O(N)ìœ¼ë¡œ ì ˆê°\")\n",
    "print(f\"  v2: ë£¨í”„ ìˆœì„œ ì¬ë°°ì¹˜ + ì‹œí€€ìŠ¤ ë³‘ë ¬í™” â†’ 2x ì†ë„ í–¥ìƒ\")\n",
    "print(f\"  v3: Hopper ì „ìš© ìµœì í™” (TMA, WGMMA) â†’ FP8ê¹Œì§€ í™•ì¥, 740 TFLOPS\")\n",
    "\n",
    "# ì„±ëŠ¥ ì‹œê°í™”\n",
    "versions = ['v1\\n(A100)', 'v2\\n(A100)', 'v2\\n(H100)', 'v3\\n(H100 FP16)', 'v3\\n(H100 FP8)']\n",
    "tflops = [125, 190, 230, 420, 740]\n",
    "peak_util = [40, 61, 23, 43, 75]  # GPU utilization %\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "colors = ['#90CAF9', '#42A5F5', '#1E88E5', '#1565C0', '#0D47A1']\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(versions, tflops, color=colors, edgecolor='black', lw=0.5)\n",
    "for bar, val in zip(bars, tflops):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 15,\n",
    "             f'{val}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.set_ylabel('TFLOPS', fontsize=11)\n",
    "ax1.set_title('FlashAttention ë²„ì „ë³„ ì„±ëŠ¥', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "ax2 = axes[1]\n",
    "bars2 = ax2.bar(versions, peak_util, color=colors, edgecolor='black', lw=0.5)\n",
    "for bar, val in zip(bars2, peak_util):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,\n",
    "             f'{val}%', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.set_ylabel('GPU Utilization (%)', fontsize=11)\n",
    "ax2.set_title('FlashAttention ë²„ì „ë³„ GPU í™œìš©ë¥ ', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter14_extreme_inference/flash_attention_versions.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/flash_attention_versions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| í‘œì¤€ Attention I/O | $O(Nd + N^2)$ HBM ì ‘ê·¼, $O(N^2)$ ë©”ëª¨ë¦¬ | â­â­â­ |\n",
    "| FlashAttention I/O | $O(N^2d^2/M)$ HBM ì ‘ê·¼, $O(N)$ ë©”ëª¨ë¦¬ | â­â­â­ |\n",
    "| Tiling | Q, K, Vë¥¼ ë¸”ë¡ìœ¼ë¡œ ë¶„í• í•˜ì—¬ SRAMì—ì„œ ì²˜ë¦¬ | â­â­â­ |\n",
    "| Online Softmax | ë¸”ë¡ë³„ ì ì§„ì  softmax â€” ì „ì²´ í–‰ ì—†ì´ë„ ì •í™• | â­â­â­ |\n",
    "| Recomputation | Backwardì—ì„œ S, Pë¥¼ ì¬ê³„ì‚° (ì €ì¥ ì•ˆ í•¨) | â­â­ |\n",
    "| v1 â†’ v2 ê°œì„  | ë£¨í”„ ìˆœì„œ + ì‹œí€€ìŠ¤ ë³‘ë ¬í™” | â­â­ |\n",
    "| v3 (Hopper) | TMA + WGMMA ë¹„ë™ê¸°, FP8 ì§€ì› | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\text{Standard I/O} = O(Nd + N^2), \\quad \\text{Flash I/O} = O\\left(\\frac{N^2 d^2}{M}\\right)$$\n",
    "\n",
    "$$\\text{Online Softmax: } m_i^{new} = \\max(m_i^{old}, \\max(S_{ij})), \\quad \\ell_i^{new} = e^{m_i^{old}-m_i^{new}}\\ell_i^{old} + \\sum e^{S_{ij}-m_i^{new}}$$\n",
    "\n",
    "$$\\text{Block size: } B_r = \\left\\lceil \\frac{M}{4d} \\right\\rceil$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**03_speculative_decoding.ipynb** â€” Draft-Verify íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ Decode ë‹¨ê³„ë¥¼ ê°€ì†í•˜ëŠ” Speculative Decodingì˜ ìˆ˜í•™ì  ì›ë¦¬ì™€ Medusa/EAGLE ë“± ìµœì‹  ê¸°ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}