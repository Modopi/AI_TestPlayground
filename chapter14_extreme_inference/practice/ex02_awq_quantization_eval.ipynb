{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: AWQ 양자화 파이프라인 시뮬레이션\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: 균일 INT8 양자화](#q1)\n",
    "- [Q2: 양자화 오차 측정](#q2)\n",
    "- [Q3: Salient Channel 탐지](#q3)\n",
    "- [Q4: AWQ 스케일 팩터 최적화](#q4)\n",
    "- [종합 도전: AWQ 양자화 파이프라인](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 환경 설정 ──────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: 균일 INT8 양자화 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "가중치 벡터 $w = [-1.5, -0.3, 0.0, 0.7, 1.2]$를 INT8($b=8$)로 균일 양자화합니다.\n",
    "\n",
    "$$\\Delta = \\frac{x_{max} - x_{min}}{2^b - 1}, \\quad x_q = \\text{round}\\left(\\frac{x - x_{min}}{\\Delta}\\right)$$\n",
    "\n",
    "1. 양자화 스텝 $\\Delta = ?$\n",
    "2. 각 값의 양자화 정수 $x_q = ?$\n",
    "3. 역양자화 후 복원값 $\\hat{x} = ?$\n",
    "4. 최대 양자화 오차 $= ?$\n",
    "\n",
    "**여러분의 예측:**\n",
    "- $\\Delta = ?$\n",
    "- $x_q[-1.5] = ?$, $x_q[0.7] = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: 균일 INT8 양자화\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "w = np.array([-1.5, -0.3, 0.0, 0.7, 1.2])\n",
    "bits = 8\n",
    "n_levels = 2**bits - 1\n",
    "\n",
    "x_min, x_max = w.min(), w.max()\n",
    "delta = (x_max - x_min) / n_levels\n",
    "\n",
    "x_q = np.round((w - x_min) / delta).astype(int)\n",
    "x_q = np.clip(x_q, 0, n_levels)\n",
    "\n",
    "x_hat = x_q * delta + x_min\n",
    "error = np.abs(w - x_hat)\n",
    "\n",
    "print(f\"\\n원본 가중치: {w}\")\n",
    "print(f\"범위: [{x_min}, {x_max}]\")\n",
    "print(f\"비트폭: {bits}, 레벨 수: {n_levels + 1}\")\n",
    "print(f\"양자화 스텝 Δ = ({x_max} - ({x_min})) / {n_levels} = {delta:.6f}\")\n",
    "\n",
    "print(f\"\\n{'원본 w':>10} | {'x_q':>6} | {'복원 x_hat':>12} | {'오차':>10}\")\n",
    "print(\"-\" * 48)\n",
    "for i in range(len(w)):\n",
    "    print(f\"{w[i]:>10.4f} | {x_q[i]:>6} | {x_hat[i]:>12.6f} | {error[i]:>10.6f}\")\n",
    "\n",
    "print(f\"\\n최대 양자화 오차: {max(error):.6f}\")\n",
    "print(f\"이론적 최대 오차 (Δ/2): {delta/2:.6f}\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  INT8은 256개 레벨로 나누므로 스텝이 매우 작습니다.\")\n",
    "print(f\"  Δ = {delta:.6f} → 최대 오차 ≈ Δ/2 = {delta/2:.6f}\")\n",
    "print(f\"  실용적으로 INT8은 FP16과 거의 동일한 정밀도를 제공합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: 양자화 오차 측정 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "정규분포 $\\mathcal{N}(0, 0.02^2)$를 따르는 가중치 행렬 $W \\in \\mathbb{R}^{256 \\times 256}$에 대해:\n",
    "\n",
    "1. INT8과 INT4 양자화를 각각 적용하세요\n",
    "2. 각각의 MSE, SNR(dB), Cosine Similarity를 측정하세요\n",
    "3. INT4→INT8 전환 시 SNR이 이론값(약 24dB)에 근접하는지 확인하세요\n",
    "\n",
    "**여러분의 예측:** INT8 SNR `?` dB, INT4 SNR `?` dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: 양자화 오차 측정\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(256, 256) * 0.02\n",
    "\n",
    "def uniform_quantize(x, num_bits):\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    n_levels = 2**num_bits - 1\n",
    "    delta = (x_max - x_min) / n_levels\n",
    "    x_q = np.round((x - x_min) / delta).astype(int)\n",
    "    x_q = np.clip(x_q, 0, n_levels)\n",
    "    x_hat = x_q * delta + x_min\n",
    "    return x_hat, delta\n",
    "\n",
    "signal_power = np.var(W)\n",
    "W_flat = W.flatten()\n",
    "\n",
    "print(f\"가중치 행렬: {W.shape}, 표준편차={np.std(W):.4f}\")\n",
    "print(f\"신호 전력: {signal_power:.8f}\")\n",
    "\n",
    "print(f\"\\n{'비트폭':>8} | {'Δ':>12} | {'MSE':>14} | {'SNR(dB)':>10} | {'Cos Sim':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "snr_values = {}\n",
    "for bits in [2, 4, 6, 8, 16]:\n",
    "    W_hat, delta = uniform_quantize(W, bits)\n",
    "    mse = np.mean((W - W_hat)**2)\n",
    "    snr_db = 10 * np.log10(signal_power / mse) if mse > 0 else float('inf')\n",
    "    cos_sim = np.dot(W_flat, W_hat.flatten()) / (np.linalg.norm(W_flat) * np.linalg.norm(W_hat.flatten()))\n",
    "    snr_values[bits] = snr_db\n",
    "    print(f\"{bits:>8} | {delta:>12.8f} | {mse:>14.10f} | {snr_db:>10.2f} | {cos_sim:>10.8f}\")\n",
    "\n",
    "print(f\"\\nSNR 차이 검증:\")\n",
    "snr_diff = snr_values[8] - snr_values[4]\n",
    "print(f\"  INT4→INT8: {snr_diff:.2f} dB (이론: {6.02*4:.2f} dB)\")\n",
    "print(f\"  이론 대비 오차: {abs(snr_diff - 6.02*4):.2f} dB\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  비트당 약 6dB 증가 법칙이 근사적으로 성립합니다.\")\n",
    "print(f\"  완벽한 균일분포가 아닌 정규분포이므로 약간의 차이가 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Salient Channel 탐지 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Llama와 유사한 설정에서 활성화 기반으로 중요 채널을 탐지합니다:\n",
    "\n",
    "- 히든 차원: 512\n",
    "- 캘리브레이션 데이터: 32 시퀀스 × 512 차원\n",
    "- 5개 채널에 인위적으로 큰 활성화 삽입\n",
    "\n",
    "1. 상위 1% 활성화 크기를 기준으로 salient channel을 탐지하세요\n",
    "2. 탐지된 채널이 실제 salient channel과 일치하는지 확인하세요\n",
    "3. Salient channel만 INT8로, 나머지는 INT4로 양자화하면 정밀도가 어떻게 되나요?\n",
    "\n",
    "**여러분의 예측:** salient channel 탐지 정확도 `?`%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: Salient Channel 탐지\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "np.random.seed(42)\n",
    "hidden = 512\n",
    "cal_seqs = 32\n",
    "W = np.random.randn(hidden, hidden) * 0.02\n",
    "X = np.random.randn(cal_seqs, hidden) * 0.5\n",
    "\n",
    "true_salient = [25, 100, 250, 400, 480]\n",
    "for ch in true_salient:\n",
    "    X[:, ch] *= 20.0\n",
    "\n",
    "act_magnitude = np.max(np.abs(X), axis=0)\n",
    "threshold_1pct = np.percentile(act_magnitude, 99)\n",
    "detected = np.where(act_magnitude > threshold_1pct)[0]\n",
    "\n",
    "precision = len(set(detected) & set(true_salient)) / len(detected) if len(detected) > 0 else 0\n",
    "recall = len(set(detected) & set(true_salient)) / len(true_salient)\n",
    "\n",
    "print(f\"히든 차원: {hidden}\")\n",
    "print(f\"99th percentile 임계값: {threshold_1pct:.4f}\")\n",
    "print(f\"탐지된 salient 채널: {detected.tolist()}\")\n",
    "print(f\"실제 salient 채널: {true_salient}\")\n",
    "print(f\"Precision: {precision:.1%}, Recall: {recall:.1%}\")\n",
    "\n",
    "# 혼합 정밀도 양자화 실험\n",
    "Y_ref = X @ W.T\n",
    "\n",
    "# 방법 1: 전체 INT4\n",
    "W_q4, _ = uniform_quantize(W, 4)\n",
    "Y_int4 = X @ W_q4.T\n",
    "mse_int4 = np.mean((Y_ref - Y_int4)**2)\n",
    "\n",
    "# 방법 2: 전체 INT8\n",
    "W_q8, _ = uniform_quantize(W, 8)\n",
    "Y_int8 = X @ W_q8.T\n",
    "mse_int8 = np.mean((Y_ref - Y_int8)**2)\n",
    "\n",
    "# 방법 3: 혼합 (salient=INT8, 나머지=INT4)\n",
    "W_mixed = np.zeros_like(W)\n",
    "salient_mask = np.zeros(hidden, dtype=bool)\n",
    "salient_mask[detected] = True\n",
    "\n",
    "for j in range(hidden):\n",
    "    if salient_mask[j]:\n",
    "        col_q, _ = uniform_quantize(W[:, j], 8)\n",
    "    else:\n",
    "        col_q, _ = uniform_quantize(W[:, j], 4)\n",
    "    W_mixed[:, j] = col_q\n",
    "\n",
    "Y_mixed = X @ W_mixed.T\n",
    "mse_mixed = np.mean((Y_ref - Y_mixed)**2)\n",
    "\n",
    "print(f\"\\n{'방법':<25} | {'MSE':>15} | {'상대 오차':>12}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'전체 INT4':<25} | {mse_int4:>15.10f} | {1.0:>12.4f}x\")\n",
    "print(f\"{'혼합 (salient=8, else=4)':<25} | {mse_mixed:>15.10f} | {mse_mixed/mse_int4:>12.4f}x\")\n",
    "print(f\"{'전체 INT8':<25} | {mse_int8:>15.10f} | {mse_int8/mse_int4:>12.4f}x\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  Salient channel은 전체의 {len(detected)/hidden:.1%}에 불과하지만\")\n",
    "print(f\"  이들만 INT8로 보호해도 MSE가 {(1-mse_mixed/mse_int4)*100:.1f}% 감소합니다.\")\n",
    "print(f\"  AWQ는 이 원리를 스케일링으로 더 우아하게 구현합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: AWQ 스케일 팩터 최적화 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "AWQ의 채널별 스케일 팩터 $S_j$를 최적화합니다:\n",
    "\n",
    "$$S_j = \\left(\\frac{\\max|X_j|}{\\max|W_j|}\\right)^\\alpha, \\quad \\alpha \\in [0, 1]$$\n",
    "\n",
    "1. $\\alpha = 0, 0.25, 0.5, 0.75, 1.0$에 대해 양자화 후 출력 MSE를 측정하세요\n",
    "2. 최적 $\\alpha$를 찾으세요\n",
    "3. $\\alpha = 0$과 $\\alpha = 1$의 의미를 설명하세요\n",
    "\n",
    "**여러분의 예측:** 최적 $\\alpha \\approx ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: AWQ 스케일 팩터 최적화\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "np.random.seed(42)\n",
    "hidden = 256\n",
    "W = np.random.randn(hidden, hidden) * 0.02\n",
    "X = np.random.randn(64, hidden) * 0.5\n",
    "\n",
    "salient_chs = [15, 80, 150, 200]\n",
    "for ch in salient_chs:\n",
    "    X[:, ch] *= 15.0\n",
    "\n",
    "Y_ref = X @ W.T\n",
    "\n",
    "act_mag = np.max(np.abs(X), axis=0)\n",
    "w_mag = np.max(np.abs(W), axis=0)\n",
    "\n",
    "alphas = np.linspace(0, 1, 21)\n",
    "mse_per_alpha = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    S = np.where(w_mag > 1e-10, (act_mag / np.maximum(w_mag, 1e-10))**alpha, 1.0)\n",
    "    W_scaled = W * S[np.newaxis, :]\n",
    "    W_q, _ = uniform_quantize(W_scaled, 4)\n",
    "    X_descaled = X / S[np.newaxis, :]\n",
    "    Y_awq = X_descaled @ W_q.T\n",
    "    mse = np.mean((Y_ref - Y_awq)**2)\n",
    "    mse_per_alpha.append(mse)\n",
    "\n",
    "best_idx = np.argmin(mse_per_alpha)\n",
    "best_alpha = alphas[best_idx]\n",
    "best_mse = mse_per_alpha[best_idx]\n",
    "\n",
    "# Naive 양자화 (alpha=0 → S=1)\n",
    "W_q_naive, _ = uniform_quantize(W, 4)\n",
    "Y_naive = X @ W_q_naive.T\n",
    "mse_naive = np.mean((Y_ref - Y_naive)**2)\n",
    "\n",
    "print(f\"\\n{'alpha':>8} | {'MSE':>15} | {'상대 오차':>12}\")\n",
    "print(\"-\" * 42)\n",
    "for alpha_show in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    idx = int(alpha_show * 20)\n",
    "    mse_val = mse_per_alpha[idx]\n",
    "    print(f\"{alpha_show:>8.2f} | {mse_val:>15.10f} | {mse_val/mse_naive:>12.4f}x\")\n",
    "\n",
    "print(f\"\\n최적 alpha: {best_alpha:.2f} (MSE = {best_mse:.10f})\")\n",
    "print(f\"개선률: {(1 - best_mse/mse_naive)*100:.1f}% (Naive INT4 대비)\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  α=0: S_j=1 → 스케일링 없음 (Naive 양자화와 동일)\")\n",
    "print(f\"  α=1: S_j = max|X_j|/max|W_j| → 활성화에만 의존\")\n",
    "print(f\"  최적 α≈{best_alpha:.2f}: 활성화와 가중치를 균형 있게 고려\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(alphas, mse_per_alpha, 'b-o', lw=2, ms=5, label='AWQ INT4 MSE')\n",
    "ax.axhline(y=mse_naive, color='red', ls='--', lw=2, label=f'Naive INT4 (α=0): {mse_naive:.6f}')\n",
    "ax.axvline(x=best_alpha, color='green', ls=':', lw=2, label=f'최적 α={best_alpha:.2f}: {best_mse:.6f}')\n",
    "ax.scatter([best_alpha], [best_mse], color='green', s=100, zorder=5)\n",
    "ax.set_xlabel('α (스케일 팩터 지수)', fontsize=11)\n",
    "ax.set_ylabel('출력 MSE', fontsize=11)\n",
    "ax.set_title('AWQ α 하이퍼파라미터 vs 출력 MSE', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter14_extreme_inference/practice/q4_alpha_optimization.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter14_extreme_inference/practice/q4_alpha_optimization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 도전: AWQ 양자화 파이프라인 시뮬레이션 <a name='bonus'></a>\n",
    "\n",
    "### 미니 프로젝트\n",
    "\n",
    "Llama-like 모델의 가중치를 AWQ 방식으로 W4A16 양자화하고, Perplexity 및 속도를 평가하는 전체 파이프라인을 구현하세요:\n",
    "\n",
    "1. 캘리브레이션 데이터로 채널별 활성화 통계 수집\n",
    "2. Salient channel 탐지 및 스케일 팩터 계산\n",
    "3. 스케일 적용 후 INT4 양자화\n",
    "4. Perplexity 시뮬레이션 (출력 분포 왜곡 측정)\n",
    "5. 메모리 절약 및 속도 향상 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이: AWQ 양자화 파이프라인 ──────────────────────\n",
    "print(\"=\" * 50)\n",
    "print(\"종합 도전 풀이: AWQ W4A16 양자화 파이프라인\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. 모델 설정 (소형 Transformer 레이어)\n",
    "hidden_dim = 512\n",
    "ffn_dim = 2048\n",
    "num_layers = 4\n",
    "\n",
    "class MiniTransformerLayer:\n",
    "    def __init__(self, hidden, ffn):\n",
    "        self.W_q = np.random.randn(hidden, hidden) * 0.02\n",
    "        self.W_k = np.random.randn(hidden, hidden) * 0.02\n",
    "        self.W_v = np.random.randn(hidden, hidden) * 0.02\n",
    "        self.W_o = np.random.randn(hidden, hidden) * 0.02\n",
    "        self.W_up = np.random.randn(ffn, hidden) * 0.02\n",
    "        self.W_down = np.random.randn(hidden, ffn) * 0.02\n",
    "        self.weight_names = ['W_q', 'W_k', 'W_v', 'W_o', 'W_up', 'W_down']\n",
    "        self.weights = [self.W_q, self.W_k, self.W_v, self.W_o, self.W_up, self.W_down]\n",
    "\n",
    "layers = [MiniTransformerLayer(hidden_dim, ffn_dim) for _ in range(num_layers)]\n",
    "\n",
    "total_params = sum(w.size for layer in layers for w in layer.weights)\n",
    "print(f\"\\n1. 모델 설정\")\n",
    "print(f\"  히든: {hidden_dim}, FFN: {ffn_dim}, 레이어: {num_layers}\")\n",
    "print(f\"  총 파라미터: {total_params:,} ({total_params * 2 / 1e6:.1f} MB @ FP16)\")\n",
    "\n",
    "# 2. 캘리브레이션 데이터\n",
    "cal_data = np.random.randn(32, hidden_dim) * 0.5\n",
    "salient_real = [30, 100, 250, 400, 470]\n",
    "for ch in salient_real:\n",
    "    cal_data[:, ch] *= 20.0\n",
    "\n",
    "act_stats = np.max(np.abs(cal_data), axis=0)\n",
    "threshold = np.percentile(act_stats, 99)\n",
    "detected_salient = np.where(act_stats > threshold)[0]\n",
    "\n",
    "print(f\"\\n2. 캘리브레이션 분석\")\n",
    "print(f\"  샘플: {cal_data.shape[0]}개, 차원: {cal_data.shape[1]}\")\n",
    "print(f\"  탐지된 salient 채널: {len(detected_salient)}개\")\n",
    "\n",
    "# 3. AWQ 양자화 적용\n",
    "alpha_optimal = 0.5\n",
    "results_by_method = {\"FP16\": [], \"Naive INT4\": [], \"AWQ INT4\": []}\n",
    "\n",
    "for layer_idx, layer in enumerate(layers):\n",
    "    for w_name, W in zip(layer.weight_names, layer.weights):\n",
    "        # 입력 차원에 대한 스케일 계산\n",
    "        in_dim = W.shape[1]\n",
    "        if in_dim == hidden_dim:\n",
    "            act_for_scale = act_stats\n",
    "        else:\n",
    "            act_for_scale = np.random.rand(in_dim) * 2\n",
    "\n",
    "        w_mag = np.max(np.abs(W), axis=0)\n",
    "        S = np.where(w_mag > 1e-10, (act_for_scale[:in_dim] / np.maximum(w_mag, 1e-10))**alpha_optimal, 1.0)\n",
    "\n",
    "        # FP16 기준\n",
    "        Y_ref = cal_data[:, :in_dim] @ W.T if in_dim == hidden_dim else np.random.randn(32, W.shape[0])\n",
    "\n",
    "        # Naive INT4\n",
    "        W_q_naive, _ = uniform_quantize(W, 4)\n",
    "        Y_naive = cal_data[:, :in_dim] @ W_q_naive.T if in_dim == hidden_dim else np.random.randn(32, W.shape[0])\n",
    "        mse_naive = np.mean((Y_ref - Y_naive)**2) if in_dim == hidden_dim else 0\n",
    "\n",
    "        # AWQ INT4\n",
    "        W_scaled = W * S[np.newaxis, :]\n",
    "        W_q_awq, _ = uniform_quantize(W_scaled, 4)\n",
    "        if in_dim == hidden_dim:\n",
    "            X_descaled = cal_data[:, :in_dim] / S[np.newaxis, :]\n",
    "            Y_awq = X_descaled @ W_q_awq.T\n",
    "            mse_awq = np.mean((Y_ref - Y_awq)**2)\n",
    "        else:\n",
    "            mse_awq = 0\n",
    "\n",
    "        results_by_method[\"Naive INT4\"].append(mse_naive)\n",
    "        results_by_method[\"AWQ INT4\"].append(mse_awq)\n",
    "\n",
    "avg_mse_naive = np.mean(results_by_method[\"Naive INT4\"])\n",
    "avg_mse_awq = np.mean(results_by_method[\"AWQ INT4\"])\n",
    "\n",
    "print(f\"\\n3. 양자화 결과\")\n",
    "print(f\"  Naive INT4 평균 MSE: {avg_mse_naive:.8f}\")\n",
    "print(f\"  AWQ INT4 평균 MSE:   {avg_mse_awq:.8f}\")\n",
    "print(f\"  AWQ 개선률: {(1 - avg_mse_awq/avg_mse_naive)*100:.1f}%\")\n",
    "\n",
    "# 4. Perplexity 시뮬레이션\n",
    "ppl_fp16 = 5.50\n",
    "ppl_naive_int4 = ppl_fp16 * (1 + avg_mse_naive * 1e4)\n",
    "ppl_awq_int4 = ppl_fp16 * (1 + avg_mse_awq * 1e4)\n",
    "\n",
    "print(f\"\\n4. Perplexity 추정 (시뮬레이션)\")\n",
    "print(f\"  FP16:       {ppl_fp16:.2f}\")\n",
    "print(f\"  Naive INT4: {ppl_naive_int4:.2f} (+{ppl_naive_int4-ppl_fp16:.2f})\")\n",
    "print(f\"  AWQ INT4:   {ppl_awq_int4:.2f} (+{ppl_awq_int4-ppl_fp16:.2f})\")\n",
    "\n",
    "# 5. 메모리 및 속도 추정\n",
    "mem_fp16 = total_params * 2 / 1e9\n",
    "mem_int4 = total_params * 0.5 / 1e9\n",
    "speedup_est = mem_fp16 / mem_int4\n",
    "\n",
    "print(f\"\\n5. 메모리 및 속도\")\n",
    "print(f\"  FP16 메모리:  {mem_fp16:.3f} GB\")\n",
    "print(f\"  INT4 메모리:  {mem_int4:.3f} GB\")\n",
    "print(f\"  압축률: {mem_fp16/mem_int4:.1f}x\")\n",
    "print(f\"  추론 속도 향상 (메모리 바운드): ~{speedup_est:.1f}x\")\n",
    "\n",
    "# 종합 비교 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Perplexity 비교\n",
    "ax1 = axes[0]\n",
    "methods = ['FP16', 'Naive\\nINT4', 'AWQ\\nINT4']\n",
    "ppls = [ppl_fp16, ppl_naive_int4, ppl_awq_int4]\n",
    "colors = ['#4C72B0', '#C44E52', '#55A868']\n",
    "bars = ax1.bar(methods, ppls, color=colors, edgecolor='black', width=0.5)\n",
    "ax1.set_ylabel('Perplexity', fontsize=11)\n",
    "ax1.set_title('Perplexity 비교', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, ppl in zip(bars, ppls):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{ppl:.2f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 메모리 비교\n",
    "ax2 = axes[1]\n",
    "mems = [mem_fp16, mem_int4, mem_int4]\n",
    "bars2 = ax2.bar(methods, mems, color=colors, edgecolor='black', width=0.5)\n",
    "ax2.set_ylabel('메모리 (GB)', fontsize=11)\n",
    "ax2.set_title('메모리 사용량', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, m in zip(bars2, mems):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n",
    "             f'{m:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# 레이어별 MSE 분포\n",
    "ax3 = axes[2]\n",
    "ax3.boxplot([results_by_method[\"Naive INT4\"], results_by_method[\"AWQ INT4\"]],\n",
    "            labels=['Naive INT4', 'AWQ INT4'])\n",
    "ax3.set_ylabel('레이어 MSE', fontsize=11)\n",
    "ax3.set_title('레이어별 MSE 분포', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter14_extreme_inference/practice/bonus_awq_pipeline.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter14_extreme_inference/practice/bonus_awq_pipeline.png\")\n",
    "print(\"\\nAWQ 양자화 파이프라인 시뮬레이션 완료!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}