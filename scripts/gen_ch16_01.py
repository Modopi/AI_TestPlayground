"""Generate chapter16_sparse_attention/01_deepseek_v3_fp8_training.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ê³¼ DeepSeek-V3

## í•™ìŠµ ëª©í‘œ
- FP8 E4M3 / E5M2 ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹ì˜ ë¹„íŠ¸ ë ˆì´ì•„ì›ƒê³¼ ìˆ˜ì¹˜ ë²”ìœ„ë¥¼ ì´í•´í•œë‹¤
- Per-tensor / per-channel ìŠ¤ì¼€ì¼ë§ì„ í†µí•œ FP8 ì–‘ìí™” ìˆ˜ì‹ì„ ë„ì¶œí•œë‹¤
- FP8 í›ˆë ¨ ì‹œ ë°œìƒí•˜ëŠ” ì–‘ìí™” ì˜¤ì°¨ì™€ SNRì„ ë¶„ì„í•œë‹¤
- DeepSeek-V3ì˜ Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ê¸°ë²•ì„ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤
- Multi-Token Prediction(MTP) ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  í•™ìŠµ íš¨ìœ¨ì„±ì„ ë¶„ì„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: FP8 ë¶€ë™ì†Œìˆ˜ì ê³¼ ì–‘ìí™”](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ](#2.-ìˆ˜ì¹˜-ë²”ìœ„-ë¹„êµ)
3. [FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ê³¼ ì˜¤ì°¨ ë¶„ì„](#3.-FP8-ì–‘ìí™”-ì‹œë®¬ë ˆì´ì…˜)
4. [FP8 E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„](#4.-E4M3-vs-E5M2)
5. [FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ë¹„êµ](#5.-í•™ìŠµ-ì•ˆì •ì„±-ë¹„êµ)
6. [Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±](#6.-Auxiliary-Loss-Free-ë¡œë“œë°¸ëŸ°ì‹±)
7. [Multi-Token Prediction (MTP)](#7.-Multi-Token-Prediction)
8. [ì •ë¦¬](#8.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### FP8 ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹

FP8ì€ 8ë¹„íŠ¸ë¡œ ë¶€ë™ì†Œìˆ˜ì ì„ í‘œí˜„í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ, ë‘ ê°€ì§€ ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤:

**E4M3 (4-bit exponent, 3-bit mantissa):**

$$x = (-1)^s \times 2^{e - 7} \times (1 + m \cdot 2^{-3})$$

- $s$: ë¶€í˜¸ ë¹„íŠ¸ (1ë¹„íŠ¸)
- $e$: ì§€ìˆ˜ (4ë¹„íŠ¸, bias = 7)
- $m$: ê°€ìˆ˜ (3ë¹„íŠ¸)
- í‘œí˜„ ë²”ìœ„: $[-448, 448]$

**E5M2 (5-bit exponent, 2-bit mantissa):**

$$x = (-1)^s \times 2^{e - 15} \times (1 + m \cdot 2^{-2})$$

- í‘œí˜„ ë²”ìœ„: $[-57344, 57344]$
- ë” ë„“ì€ ë²”ìœ„, ë” ë‚®ì€ ì •ë°€ë„

### FP8 ìŠ¤ì¼€ì¼ë§ ì–‘ìí™”

ì…ë ¥ í…ì„œ $x$ë¥¼ FP8ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ìˆ˜ì‹:

$$Q(x) = \text{clamp}\!\left(\left\lfloor \frac{x}{s} \right\rceil,\; -q_{max},\; q_{max}\right) \cdot s$$

- $s = \frac{\max(|x|)}{q_{max}}$: ìŠ¤ì¼€ì¼ íŒ©í„°
- $q_{max} = 448$ (E4M3) ë˜ëŠ” $q_{max} = 57344$ (E5M2)

**Per-tensor vs Per-channel ìŠ¤ì¼€ì¼ë§:**

| ë°©ì‹ | ìŠ¤ì¼€ì¼ ê³„ì‚° | ì¥ì  | ë‹¨ì  |
|------|-------------|------|------|
| Per-tensor | $s = \frac{\max(\|x\|)}{q_{max}}$ | ë‹¨ìˆœ, ë¹ ë¦„ | ì•„ì›ƒë¼ì´ì–´ì— ì·¨ì•½ |
| Per-channel | $s_c = \frac{\max(\|x_c\|)}{q_{max}}$ | ì±„ë„ë³„ ìµœì í™” | ì˜¤ë²„í—¤ë“œ ì¦ê°€ |

### ì–‘ìí™” ì˜¤ì°¨ì™€ SNR

$$\text{MSE} = \mathbb{E}\left[(x - Q(x))^2\right]$$

$$\text{SNR} = 10 \log_{10}\!\left(\frac{\text{Var}(x)}{\text{MSE}}\right) \propto 2^{2b}$$

- $b$: ë¹„íŠ¸ ìˆ˜ (FP8 â†’ $b \approx 3\sim4$ ìœ íš¨ ë¹„íŠ¸)

### Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±

DeepSeek-V3ëŠ” ë³´ì¡° ì†ì‹¤ ì—†ì´ ì „ë¬¸ê°€ ë¶€í•˜ë¥¼ ê· í˜•ì‹œí‚µë‹ˆë‹¤:

$$g_i' = g_i + b_i, \quad b_i \leftarrow b_i + \alpha \cdot \text{sign}(\bar{f} - f_i)$$

- $g_i$: ì „ë¬¸ê°€ $i$ì˜ ê²Œì´íŠ¸ ê°’
- $b_i$: í¸í–¥ ë³´ì • í•­ (í•™ìŠµ ê°€ëŠ¥)
- $f_i$: ì „ë¬¸ê°€ $i$ì— í• ë‹¹ëœ í† í° ë¹„ìœ¨
- $\bar{f} = 1/N_E$: ëª©í‘œ ê· ë“± ë¹„ìœ¨
- $\alpha$: ì—…ë°ì´íŠ¸ ìŠ¤í… í¬ê¸°

### Multi-Token Prediction (MTP)

$$\mathcal{L}_{MTP} = \sum_{k=1}^{K} \lambda_k \cdot \mathbb{E}\left[-\log p_\theta(x_{t+k} \mid x_{\leq t})\right]$$

- $K$: ì˜ˆì¸¡ í† í° ìˆ˜ (DeepSeek-V3: $K = 2$)
- $\lambda_k$: ê° í† í° ìœ„ì¹˜ì˜ ê°€ì¤‘ì¹˜

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| FP8 E4M3 ë²”ìœ„ | $[-448, 448]$ | ë†’ì€ ì •ë°€ë„, ì¢ì€ ë²”ìœ„ |
| FP8 E5M2 ë²”ìœ„ | $[-57344, 57344]$ | ë‚®ì€ ì •ë°€ë„, ë„“ì€ ë²”ìœ„ |
| ìŠ¤ì¼€ì¼ë§ ì–‘ìí™” | $Q(x) = \lfloor x/s \rceil \cdot s$ | ë™ì  ë²”ìœ„ ì¡°ì • |
| SNR | $\propto 2^{2b}$ | ë¹„íŠ¸ ìˆ˜ì— ì§€ìˆ˜ì  ë¹„ë¡€ |
| í¸í–¥ ë³´ì • | $b_i \leftarrow b_i + \alpha \cdot \text{sign}(\bar{f} - f_i)$ | ë³´ì¡° ì†ì‹¤ ì—†ëŠ” ê· í˜• |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ FP8 ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ FP8ì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìˆ«ìë¥¼ ì ì„ ë•Œ, A4 ì¢…ì´ í•œ ì¥(FP32)ì— ì“°ë©´ ì•„ì£¼ ì„¸ë°€í•˜ê²Œ ì“¸ ìˆ˜ ìˆì§€ë§Œ, 
> í¬ìŠ¤íŠ¸ì‡(FP8)ì— ì“°ë©´ ê³µê°„ì´ ì‘ì•„ì„œ ëŒ€ëµì ìœ¼ë¡œë§Œ ì ì„ ìˆ˜ ìˆì–´ìš”!

FP8ì€ ìˆ«ìë¥¼ **8ë¹„íŠ¸**ë¡œ í‘œí˜„í•˜ëŠ” ì•„ì£¼ ì‘ì€ í˜•ì‹ì´ì—ìš”:
- **E4M3**: "í° ìˆ«ìëŠ” ëª» ì“°ì§€ë§Œ, ì‘ì€ ìˆ«ìëŠ” ê½¤ ì •í™•í•´ìš”" â†’ ê°€ì¤‘ì¹˜(weight) ì €ì¥ì— ì¢‹ì•„ìš”
- **E5M2**: "í° ìˆ«ìë„ ì“¸ ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ê°€ ì¢€ ë–¨ì–´ì ¸ìš”" â†’ ê¸°ìš¸ê¸°(gradient) ì €ì¥ì— ì¢‹ì•„ìš”

#### âš–ï¸ ì™œ FP8ë¡œ í›ˆë ¨í•˜ë‚˜ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìˆ˜í•™ ì‹œí—˜ì„ ë³¼ ë•Œ, ì •ë°€í•œ ê³„ì‚°ê¸°(FP32) ëŒ€ì‹  
> ì£¼íŒ(FP8)ìœ¼ë¡œ í’€ë©´ **2ë°° ë¹ ë¥´ê²Œ** í’€ ìˆ˜ ìˆì–´ìš”! ì•½ê°„ì˜ ì˜¤ì°¨ê°€ ìˆì§€ë§Œ, ì •ë‹µì— ê±°ì˜ ë§ì•„ìš”.

DeepSeek-V3ëŠ” FP8ë¡œ í›ˆë ¨í•´ì„œ:
- ë©”ëª¨ë¦¬ë¥¼ **ì ˆë°˜**ìœ¼ë¡œ ì¤„ì˜€ì–´ìš”
- GPU ì—°ì‚°ì„ **2ë°°** ë¹ ë¥´ê²Œ í–ˆì–´ìš”
- ê·¸ëŸ°ë°ë„ ì •í™•ë„ëŠ” ê±°ì˜ ë–¨ì–´ì§€ì§€ ì•Šì•˜ì–´ìš”!

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: FP8 E4M3 í‘œí˜„ ë²”ìœ„

E4M3ì—ì„œ ê°€ìˆ˜(mantissa)ê°€ 3ë¹„íŠ¸ì´ê³  ì§€ìˆ˜(exponent)ê°€ 4ë¹„íŠ¸ì¼ ë•Œ, 
ìµœëŒ€ ì–‘ìˆ˜ ê°’ì„ ê³„ì‚°í•˜ì„¸ìš”. (bias = 7, íŠ¹ìˆ˜ê°’ ì œì™¸)

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$x_{max} = 2^{(15-7)} \times (1 + (2^3-1) \cdot 2^{-3})$$
$$= 2^{8} \times (1 + 7/8) = 256 \times 1.875 = 480$$

ì‹¤ì œ FP8 E4M3 í‘œì¤€ì—ì„œëŠ” NaN í‘œí˜„ì„ ìœ„í•´ $e=15, m=7$ì„ ì˜ˆì•½í•˜ë¯€ë¡œ:
$$x_{max} = 2^{(14-7)} \times (1 + 7/8) = 128 \times 1.875 = 240$$

â†’ ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” $448$ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (NVIDIA FP8 ì‚¬ì–‘ ê¸°ì¤€, $e=15$ì˜ ì¼ë¶€ë¥¼ ìœ íš¨ê°’ìœ¼ë¡œ í™œìš©).
</details>

#### ë¬¸ì œ 2: ì–‘ìí™” SNR ë¹„êµ

FP32(23ë¹„íŠ¸ ê°€ìˆ˜)ì™€ FP8 E4M3(3ë¹„íŠ¸ ê°€ìˆ˜)ì˜ ì´ë¡ ì  SNR ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\frac{\text{SNR}_{FP32}}{\text{SNR}_{FP8}} \approx \frac{2^{2 \times 23}}{2^{2 \times 3}} = 2^{40} \approx 10^{12}$$

â†’ FP32ëŠ” FP8ë³´ë‹¤ ì•½ $10^{12}$ ë°° ë” ë†’ì€ SNRì„ ê°€ì§‘ë‹ˆë‹¤. í•˜ì§€ë§Œ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ë…¸ì´ì¦ˆì— ê°•ê±´í•˜ë¯€ë¡œ FP8ë¡œë„ ì¶©ë¶„í•œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import struct

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 - Numeric range comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ <a name='2.-ìˆ˜ì¹˜-ë²”ìœ„-ë¹„êµ'></a>

ë‹¤ì–‘í•œ ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹ì˜ ë¹„íŠ¸ êµ¬ì„±ê³¼ ìˆ˜ì¹˜ ë²”ìœ„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

| í˜•ì‹ | ì´ ë¹„íŠ¸ | ë¶€í˜¸ | ì§€ìˆ˜ | ê°€ìˆ˜ | ìµœëŒ€ê°’ | ìµœì†Œ ì •ê·œê°’ |
|------|---------|------|------|------|--------|-------------|
| FP32 | 32 | 1 | 8 | 23 | $\sim3.4\times10^{38}$ | $\sim1.2\times10^{-38}$ |
| FP16 | 16 | 1 | 5 | 10 | $65504$ | $\sim6.1\times10^{-5}$ |
| BF16 | 16 | 1 | 8 | 7 | $\sim3.4\times10^{38}$ | $\sim1.2\times10^{-38}$ |
| FP8 E4M3 | 8 | 1 | 4 | 3 | $448$ | $\sim0.015625$ |
| FP8 E5M2 | 8 | 1 | 5 | 2 | $57344$ | $\sim6.1\times10^{-5}$ |"""))

# â”€â”€ Cell 7: Range comparison code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
formats = {
    'FP32':     {'bits': 32, 'sign': 1, 'exp': 8,  'man': 23, 'max': 3.4028235e+38,  'min_normal': 1.175494e-38},
    'FP16':     {'bits': 16, 'sign': 1, 'exp': 5,  'man': 10, 'max': 65504.0,        'min_normal': 6.1035e-5},
    'BF16':     {'bits': 16, 'sign': 1, 'exp': 8,  'man': 7,  'max': 3.3895314e+38,  'min_normal': 1.175494e-38},
    'FP8 E4M3': {'bits': 8,  'sign': 1, 'exp': 4,  'man': 3,  'max': 448.0,          'min_normal': 0.015625},
    'FP8 E5M2': {'bits': 8,  'sign': 1, 'exp': 5,  'man': 2,  'max': 57344.0,        'min_normal': 6.1035e-5},
}

print("=" * 85)
print(f"{'í˜•ì‹':<12} | {'ë¹„íŠ¸':>4} | {'ë¶€í˜¸':>4} | {'ì§€ìˆ˜':>4} | {'ê°€ìˆ˜':>4} | {'ìµœëŒ€ê°’':>15} | {'ìµœì†Œ ì •ê·œê°’':>15}")
print("=" * 85)
for name, f in formats.items():
    print(f"{name:<12} | {f['bits']:>4} | {f['sign']:>4} | {f['exp']:>4} | {f['man']:>4} | "
          f"{f['max']:>15.4e} | {f['min_normal']:>15.4e}")
print("=" * 85)

# ë™ì  ë²”ìœ„ (dB) ê³„ì‚°
print(f"\\në™ì  ë²”ìœ„ ë¹„êµ (dB):")
print(f"{'í˜•ì‹':<12} | {'ë™ì  ë²”ìœ„ (dB)':>15} | {'ìœ íš¨ ì†Œìˆ˜ ìë¦¿ìˆ˜':>15}")
print("-" * 50)
for name, f in formats.items():
    dynamic_range_db = 20 * np.log10(f['max'] / f['min_normal'])
    decimal_digits = np.log10(2**f['man'])
    print(f"{name:<12} | {dynamic_range_db:>15.1f} | {decimal_digits:>15.2f}")

# ë©”ëª¨ë¦¬ ì ˆì•½ë¥ 
print(f"\\në©”ëª¨ë¦¬ ì ˆì•½ë¥  (FP32 ëŒ€ë¹„):")
for name, f in formats.items():
    savings = (1 - f['bits'] / 32) * 100
    print(f"  {name}: {savings:.0f}% ì ˆì•½ ({f['bits']}ë¹„íŠ¸ / 32ë¹„íŠ¸)")"""))

# â”€â”€ Cell 8: Section 3 - FP8 quantization simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ê³¼ ì˜¤ì°¨ ë¶„ì„ <a name='3.-FP8-ì–‘ìí™”-ì‹œë®¬ë ˆì´ì…˜'></a>

ì‹¤ì œ í…ì„œì— FP8 ì–‘ìí™”ë¥¼ ì ìš©í•˜ê³  ë°œìƒí•˜ëŠ” ì˜¤ì°¨ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

$$Q_{E4M3}(x) = \text{clamp}\!\left(\text{round}\!\left(\frac{x}{s}\right), -448, 448\right) \cdot s$$

$$\text{MSE} = \frac{1}{N}\sum_{i=1}^{N}(x_i - Q(x_i))^2$$"""))

# â”€â”€ Cell 9: FP8 quantization code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def simulate_fp8_quantization(x, fmt='e4m3'):
    # FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ (ì†Œí”„íŠ¸ì›¨ì–´ ì—ë®¬ë ˆì´ì…˜)
    if fmt == 'e4m3':
        q_max = 448.0
        mantissa_bits = 3
    else:  # e5m2
        q_max = 57344.0
        mantissa_bits = 2

    # per-tensor ìŠ¤ì¼€ì¼ë§
    abs_max = np.max(np.abs(x)) + 1e-12
    scale = abs_max / q_max

    # ì–‘ìí™”: ìŠ¤ì¼€ì¼ë§ â†’ ë¼ìš´ë”© â†’ í´ë¨í•‘
    x_scaled = x / scale
    n_levels = 2 ** mantissa_bits
    x_quantized = np.round(x_scaled * n_levels) / n_levels
    x_quantized = np.clip(x_quantized, -q_max, q_max)

    # ì—­ì–‘ìí™”
    x_dequant = x_quantized * scale
    return x_dequant, scale

# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ë‹¤ì–‘í•œ ë¶„í¬ì˜ í…ì„œ
np.random.seed(42)
n_elements = 10000

# ì •ê·œ ë¶„í¬ (ëª¨ë¸ ê°€ì¤‘ì¹˜ ê·¼ì‚¬)
x_normal = np.random.randn(n_elements).astype(np.float32)
# ê· ë“± ë¶„í¬
x_uniform = np.random.uniform(-2, 2, n_elements).astype(np.float32)
# ì•„ì›ƒë¼ì´ì–´ê°€ ìˆëŠ” ë¶„í¬
x_outlier = np.random.randn(n_elements).astype(np.float32)
x_outlier[np.random.choice(n_elements, 50)] *= 100  # 0.5% ì•„ì›ƒë¼ì´ì–´

distributions = {'ì •ê·œ ë¶„í¬': x_normal, 'ê· ë“± ë¶„í¬': x_uniform, 'ì•„ì›ƒë¼ì´ì–´ ë¶„í¬': x_outlier}

print(f"FP8 ì–‘ìí™” ì˜¤ì°¨ ë¶„ì„ (ì›ì†Œ ìˆ˜: {n_elements})")
print(f"{'ë¶„í¬':<16} | {'í˜•ì‹':<8} | {'MSE':>12} | {'SNR (dB)':>10} | {'ìµœëŒ€ ì˜¤ì°¨':>10} | {'ìŠ¤ì¼€ì¼':>10}")
print("-" * 78)

for dist_name, x in distributions.items():
    for fmt in ['e4m3', 'e5m2']:
        x_q, scale = simulate_fp8_quantization(x, fmt=fmt)
        mse = np.mean((x - x_q) ** 2)
        var_x = np.var(x)
        snr = 10 * np.log10(var_x / (mse + 1e-12))
        max_err = np.max(np.abs(x - x_q))
        fmt_label = 'E4M3' if fmt == 'e4m3' else 'E5M2'
        print(f"{dist_name:<16} | {fmt_label:<8} | {mse:>12.6f} | {snr:>10.2f} | {max_err:>10.4f} | {scale:>10.6f}")

print(f"\\nê²°ë¡ : E4M3ëŠ” ì¼ë°˜ ë¶„í¬ì—ì„œ ë” ë†’ì€ SNR (ë†’ì€ ì •ë°€ë„)")
print(f"      E5M2ëŠ” ì•„ì›ƒë¼ì´ì–´ ë¶„í¬ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì  (ë„“ì€ ë²”ìœ„)")"""))

# â”€â”€ Cell 10: Section 4 - E4M3 vs E5M2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. FP8 E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„ <a name='4.-E4M3-vs-E5M2'></a>

DeepSeek-V3ì—ì„œì˜ ì‚¬ìš© ì „ëµ:
- **Forward pass (ê°€ì¤‘ì¹˜, í™œì„±í™”)**: E4M3 â€” ë†’ì€ ì •ë°€ë„ í•„ìš”
- **Backward pass (ê¸°ìš¸ê¸°)**: E5M2 â€” í° ë™ì  ë²”ìœ„ í•„ìš”

$$\text{Forward}: W_{E4M3} \times X_{E4M3} \rightarrow Y_{FP32}$$
$$\text{Backward}: \frac{\partial L}{\partial Y}_{E5M2} \times X_{E4M3}^T \rightarrow \frac{\partial L}{\partial W}_{FP32}$$"""))

# â”€â”€ Cell 11: E4M3 vs E5M2 visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ì–‘ìí™” ì˜¤ì°¨ ë¶„í¬ ë¹„êµ
ax1 = axes[0]
x_test = np.random.randn(5000).astype(np.float32) * 2
x_e4m3, _ = simulate_fp8_quantization(x_test, 'e4m3')
x_e5m2, _ = simulate_fp8_quantization(x_test, 'e5m2')

err_e4m3 = x_test - x_e4m3
err_e5m2 = x_test - x_e5m2

ax1.hist(err_e4m3, bins=80, alpha=0.6, color='blue', label=f'E4M3 (std={np.std(err_e4m3):.4f})', density=True)
ax1.hist(err_e5m2, bins=80, alpha=0.6, color='red', label=f'E5M2 (std={np.std(err_e5m2):.4f})', density=True)
ax1.set_xlabel('ì–‘ìí™” ì˜¤ì°¨', fontsize=11)
ax1.set_ylabel('ë°€ë„', fontsize=11)
ax1.set_title('E4M3 vs E5M2 ì–‘ìí™” ì˜¤ì°¨ ë¶„í¬', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) í¬ê¸°ë³„ SNR ë¹„êµ
ax2 = axes[1]
scale_factors = np.logspace(-2, 2, 20)
snr_e4m3_list = []
snr_e5m2_list = []

for sf in scale_factors:
    x_scaled = x_test * sf
    for fmt, snr_list in [('e4m3', snr_e4m3_list), ('e5m2', snr_e5m2_list)]:
        x_q, _ = simulate_fp8_quantization(x_scaled, fmt)
        mse = np.mean((x_scaled - x_q) ** 2) + 1e-12
        snr_val = 10 * np.log10(np.var(x_scaled) / mse)
        snr_list.append(snr_val)

ax2.semilogx(scale_factors, snr_e4m3_list, 'b-o', lw=2, ms=6, label='E4M3 (ê°€ì¤‘ì¹˜ìš©)')
ax2.semilogx(scale_factors, snr_e5m2_list, 'r-s', lw=2, ms=6, label='E5M2 (ê¸°ìš¸ê¸°ìš©)')
ax2.set_xlabel('í…ì„œ ìŠ¤ì¼€ì¼ íŒ©í„°', fontsize=11)
ax2.set_ylabel('SNR (dB)', fontsize=11)
ax2.set_title('ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ FP8 SNR ë¹„êµ', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/fp8_e4m3_vs_e5m2.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/fp8_e4m3_vs_e5m2.png")
print(f"E4M3 í‰ê·  SNR: {np.mean(snr_e4m3_list):.2f} dB")
print(f"E5M2 í‰ê·  SNR: {np.mean(snr_e5m2_list):.2f} dB")"""))

# â”€â”€ Cell 12: Section 5 - Training stability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ë¹„êµ <a name='5.-í•™ìŠµ-ì•ˆì •ì„±-ë¹„êµ'></a>

FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ê³¼ FP16 í›ˆë ¨ì˜ í•™ìŠµ ê³¡ì„ ì„ ë¹„êµí•©ë‹ˆë‹¤.

**í˜¼í•© ì •ë°€ë„ í›ˆë ¨ ì „ëµ (DeepSeek-V3):**
1. Forward: $W_{E4M3} \times X_{E4M3}$ (GEMM ì—°ì‚°)
2. Backward: $\nabla_{E5M2}$ (ê¸°ìš¸ê¸° ê³„ì‚°)
3. Update: $W_{FP32} \leftarrow W_{FP32} - \eta \nabla_{FP32}$ (ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸)"""))

# â”€â”€ Cell 13: Training stability simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ë¡œ ì •ë°€ë„ë³„ í•™ìŠµ ê³¡ì„  ë¹„êµ
np.random.seed(42)

# í•©ì„± ë°ì´í„° ìƒì„±
n_data = 200
X_data = np.random.randn(n_data, 10).astype(np.float32)
true_w = np.random.randn(10, 1).astype(np.float32)
Y_data = X_data @ true_w + np.random.randn(n_data, 1).astype(np.float32) * 0.1

X_tf = tf.constant(X_data)
Y_tf = tf.constant(Y_data)

def train_with_precision(precision_name, n_epochs=100, lr=0.01):
    # ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ëŠ” í•­ìƒ FP32
    W = tf.Variable(tf.random.normal([10, 1], seed=42))
    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)
    losses = []

    for epoch in range(n_epochs):
        with tf.GradientTape() as tape:
            if precision_name == 'FP8':
                # FP8 ì‹œë®¬ë ˆì´ì…˜: forwardì—ì„œ ì–‘ìí™” ë…¸ì´ì¦ˆ ì¶”ê°€
                W_noisy = W + tf.random.normal(W.shape, stddev=0.005)
                X_noisy = X_tf + tf.random.normal(X_tf.shape, stddev=0.002)
                pred = tf.matmul(X_noisy, W_noisy)
            elif precision_name == 'FP16':
                W_fp16 = tf.cast(tf.cast(W, tf.float16), tf.float32)
                pred = tf.matmul(X_tf, W_fp16)
            else:  # FP32
                pred = tf.matmul(X_tf, W)

            loss = tf.reduce_mean((pred - Y_tf) ** 2)

        grads = tape.gradient(loss, [W])
        optimizer.apply_gradients(zip(grads, [W]))
        losses.append(loss.numpy())

    return losses

# ê° ì •ë°€ë„ë¡œ í•™ìŠµ
losses_fp32 = train_with_precision('FP32')
losses_fp16 = train_with_precision('FP16')
losses_fp8 = train_with_precision('FP8')

# ê²°ê³¼ ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

ax1 = axes[0]
ax1.plot(losses_fp32, 'g-', lw=2.5, label='FP32 (ê¸°ì¤€)')
ax1.plot(losses_fp16, 'b--', lw=2, label='FP16')
ax1.plot(losses_fp8, 'r-.', lw=2, label='FP8 (ì‹œë®¬ë ˆì´ì…˜)')
ax1.set_xlabel('ì—í­', fontsize=11)
ax1.set_ylabel('MSE Loss', fontsize=11)
ax1.set_title('ì •ë°€ë„ë³„ í•™ìŠµ ê³¡ì„ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.set_yscale('log')

ax2 = axes[1]
fp8_gap = np.array(losses_fp8) - np.array(losses_fp32)
fp16_gap = np.array(losses_fp16) - np.array(losses_fp32)
ax2.plot(fp16_gap, 'b-', lw=2, label='FP16 - FP32')
ax2.plot(fp8_gap, 'r-', lw=2, label='FP8 - FP32')
ax2.axhline(y=0, color='gray', ls='--', lw=1)
ax2.set_xlabel('ì—í­', fontsize=11)
ax2.set_ylabel('Loss ì°¨ì´', fontsize=11)
ax2.set_title('FP32 ëŒ€ë¹„ Loss ê°­', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/fp8_vs_fp16_training.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/fp8_vs_fp16_training.png")
print(f"\\nìµœì¢… Loss ë¹„êµ:")
print(f"  FP32: {losses_fp32[-1]:.6f}")
print(f"  FP16: {losses_fp16[-1]:.6f} (FP32 ëŒ€ë¹„ ì°¨ì´: {losses_fp16[-1]-losses_fp32[-1]:+.6f})")
print(f"  FP8:  {losses_fp8[-1]:.6f} (FP32 ëŒ€ë¹„ ì°¨ì´: {losses_fp8[-1]-losses_fp32[-1]:+.6f})")
print(f"\\nâ†’ FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ì€ ì•½ê°„ì˜ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ë˜ì§€ë§Œ,")
print(f"  regularization íš¨ê³¼ë¡œ ìµœì¢… ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ")"""))

# â”€â”€ Cell 14: Section 6 - Auxiliary-Loss-Free Load Balancing â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± <a name='6.-Auxiliary-Loss-Free-ë¡œë“œë°¸ëŸ°ì‹±'></a>

ê¸°ì¡´ MoEì˜ ë³´ì¡° ì†ì‹¤(Auxiliary Loss)ì€ í•™ìŠµ ëª©ì í•¨ìˆ˜ë¥¼ ì™œê³¡ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
DeepSeek-V3ëŠ” **í¸í–¥ ë³´ì •(bias correction)** ë°©ì‹ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤:

$$g_i^{(\text{topk})} = \begin{cases} g_i & \text{if } g_i + b_i \in \text{TopK}(g_1+b_1, \ldots, g_N+b_N) \\ 0 & \text{otherwise} \end{cases}$$

í¸í–¥ ì—…ë°ì´íŠ¸ ê·œì¹™:

$$b_i \leftarrow b_i + \alpha \cdot \text{sign}(\bar{f} - f_i)$$

- ê³¼ë¶€í•˜ëœ ì „ë¬¸ê°€: $f_i > \bar{f}$ â†’ $b_i$ ê°ì†Œ â†’ ì„ íƒ í™•ë¥  â†“
- ê³¼ì†Œ í™œìš© ì „ë¬¸ê°€: $f_i < \bar{f}$ â†’ $b_i$ ì¦ê°€ â†’ ì„ íƒ í™•ë¥  â†‘"""))

cells.append(code("""\
# â”€â”€ Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)

n_experts = 8
n_tokens = 1000
n_steps = 200
top_k = 2
alpha = 0.01  # í¸í–¥ ì—…ë°ì´íŠ¸ ìŠ¤í… í¬ê¸°
target_freq = 1.0 / n_experts

# ì´ˆê¸° ê²Œì´íŠ¸ ê°’: ë¶ˆê· í˜•í•˜ê²Œ ì„¤ì • (ì¼ë¶€ ì „ë¬¸ê°€ì— í¸í–¥)
gate_bias = np.array([0.5, 0.3, 0.1, -0.1, -0.2, -0.3, 0.2, 0.0])

# í¸í–¥ ë³´ì • í•­ ì´ˆê¸°í™”
bias_correction = np.zeros(n_experts)

freq_history = []
bias_history = []

for step in range(n_steps):
    # ëœë¤ í† í° ê²Œì´íŠ¸ ê°’ ìƒì„±
    raw_gates = np.random.randn(n_tokens, n_experts) + gate_bias

    # í¸í–¥ ë³´ì • ì ìš©
    adjusted_gates = raw_gates + bias_correction

    # Top-K ì„ íƒ
    expert_counts = np.zeros(n_experts)
    for t in range(n_tokens):
        top_indices = np.argsort(adjusted_gates[t])[-top_k:]
        expert_counts[top_indices] += 1

    # ë¹ˆë„ ê³„ì‚°
    freqs = expert_counts / (n_tokens * top_k)
    freq_history.append(freqs.copy())
    bias_history.append(bias_correction.copy())

    # í¸í–¥ ì—…ë°ì´íŠ¸: ê³¼ë¶€í•˜ â†’ ê°ì†Œ, ê³¼ì†Œí™œìš© â†’ ì¦ê°€
    bias_correction += alpha * np.sign(target_freq - freqs)

freq_history = np.array(freq_history)
bias_history = np.array(bias_history)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

ax1 = axes[0]
for i in range(n_experts):
    ax1.plot(freq_history[:, i], lw=1.5, label=f'Expert {i}')
ax1.axhline(y=target_freq, color='black', ls='--', lw=2, label=f'ëª©í‘œ ({target_freq:.3f})')
ax1.set_xlabel('ìŠ¤í…', fontsize=11)
ax1.set_ylabel('í† í° í• ë‹¹ ë¹„ìœ¨', fontsize=11)
ax1.set_title('Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±', fontweight='bold')
ax1.legend(fontsize=8, ncol=3, loc='upper right')
ax1.grid(True, alpha=0.3)

ax2 = axes[1]
for i in range(n_experts):
    ax2.plot(bias_history[:, i], lw=1.5, label=f'Expert {i}')
ax2.axhline(y=0, color='black', ls='--', lw=1)
ax2.set_xlabel('ìŠ¤í…', fontsize=11)
ax2.set_ylabel('í¸í–¥ ë³´ì • ê°’ ($b_i$)', fontsize=11)
ax2.set_title('í¸í–¥ ë³´ì • í•­ ë³€í™”', fontweight='bold')
ax2.legend(fontsize=8, ncol=3)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/aux_loss_free_balancing.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/aux_loss_free_balancing.png")

print(f"\\në¡œë“œë°¸ëŸ°ì‹± ê²°ê³¼:")
print(f"  ì´ˆê¸° ë¶ˆê· í˜• (step 0):  std = {freq_history[0].std():.4f}")
print(f"  ìµœì¢… ê· í˜• (step {n_steps-1}): std = {freq_history[-1].std():.4f}")
print(f"  ê· í˜• ê°œì„  ë¹„ìœ¨: {freq_history[0].std()/freq_history[-1].std():.1f}x")
print(f"\\nìµœì¢… ì „ë¬¸ê°€ë³„ í† í° ë¹„ìœ¨:")
for i in range(n_experts):
    bar = 'â–ˆ' * int(freq_history[-1, i] * 200)
    print(f"  Expert {i}: {freq_history[-1, i]:.4f} {bar}")"""))

# â”€â”€ Cell 15: Section 7 - MTP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 7. Multi-Token Prediction (MTP) <a name='7.-Multi-Token-Prediction'></a>

DeepSeek-V3ì˜ MTPëŠ” í•œ ë²ˆì˜ forward passì—ì„œ ì—¬ëŸ¬ ë¯¸ë˜ í† í°ì„ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤:

$$\mathcal{L}_{total} = \mathcal{L}_{main} + \sum_{k=1}^{K} \lambda_k \cdot \mathcal{L}_{MTP}^{(k)}$$

$$\mathcal{L}_{MTP}^{(k)} = -\frac{1}{T} \sum_{t=1}^{T} \log p_\theta(x_{t+k} \mid x_{\leq t})$$

MTPì˜ ì´ì :
1. í•™ìŠµ ë°ì´í„° í™œìš© íš¨ìœ¨ ì¦ê°€ ($K$ë°° ë” ë§ì€ ì‹ í˜¸)
2. ì¶”ë¡  ì‹œ Speculative Decodingê³¼ ê²°í•© ê°€ëŠ¥
3. ë‚´ë¶€ í‘œí˜„ì˜ ì˜ˆì¸¡ ëŠ¥ë ¥ ê°•í™”"""))

cells.append(code("""\
# â”€â”€ Multi-Token Prediction (MTP) ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)

vocab_size = 100
seq_len = 32
batch_size = 16
embed_dim = 64
K_predict = 3  # ì˜ˆì¸¡ í† í° ìˆ˜

# ê°„ë‹¨í•œ MTP ëª¨ë¸: ê³µìœ  ì¸ì½”ë” + Kê°œì˜ ì˜ˆì¸¡ í—¤ë“œ
shared_encoder = tf.keras.layers.Dense(embed_dim, activation='relu')
mtp_heads = [tf.keras.layers.Dense(vocab_size) for _ in range(K_predict)]

# í•©ì„± ì‹œí€€ìŠ¤ ë°ì´í„°
input_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))
embeddings = tf.one_hot(input_ids, vocab_size)
embeddings = tf.cast(embeddings, tf.float32)

# Forward pass
hidden = shared_encoder(tf.reshape(embeddings, [-1, vocab_size]))
hidden = tf.reshape(hidden, [batch_size, seq_len, embed_dim])

print(f"MTP ì„¤ì •:")
print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
print(f"  ì˜ˆì¸¡ í† í° ìˆ˜ (K): {K_predict}")
print(f"  ì–´íœ˜ í¬ê¸°: {vocab_size}")
print(f"  ì¸ì½”ë” ì¶œë ¥: {hidden.shape}")

# ê° ì˜ˆì¸¡ í—¤ë“œë³„ ì†ì‹¤ ê³„ì‚°
total_loss = 0.0
lambda_weights = [1.0, 0.5, 0.25]  # ê°€ê¹Œìš´ í† í°ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ

print(f"\\nê° ì˜ˆì¸¡ í—¤ë“œë³„ ì†ì‹¤:")
print(f"{'í—¤ë“œ (k)':>10} | {'Lambda':>8} | {'Loss':>12} | {'ê°€ì¤‘ Loss':>12}")
print("-" * 50)

for k in range(K_predict):
    logits_k = mtp_heads[k](tf.reshape(hidden[:, :seq_len-K_predict, :], [-1, embed_dim]))
    targets_k = input_ids[:, k+1:seq_len-K_predict+k+1].flatten()
    targets_k = tf.constant(targets_k, dtype=tf.int32)

    loss_k = tf.keras.losses.sparse_categorical_crossentropy(
        targets_k, logits_k, from_logits=True
    )
    loss_k_mean = tf.reduce_mean(loss_k).numpy()
    weighted_loss = lambda_weights[k] * loss_k_mean
    total_loss += weighted_loss

    print(f"  k={k+1:>5} | {lambda_weights[k]:>8.2f} | {loss_k_mean:>12.4f} | {weighted_loss:>12.4f}")

print(f"{'':>10} {'':>8}   {'í•©ê³„':>12}   {total_loss:>12.4f}")

# MTP í•™ìŠµ íš¨ìœ¨ ë¶„ì„
print(f"\\nMTP í•™ìŠµ íš¨ìœ¨:")
standard_signals = seq_len - 1  # í‘œì¤€ next-token prediction
mtp_signals = (seq_len - K_predict) * K_predict + (seq_len - 1)
print(f"  í‘œì¤€ NTP í•™ìŠµ ì‹ í˜¸: {standard_signals}ê°œ/ì‹œí€€ìŠ¤")
print(f"  MTP í•™ìŠµ ì‹ í˜¸: {mtp_signals}ê°œ/ì‹œí€€ìŠ¤")
print(f"  íš¨ìœ¨ ì¦ê°€: {mtp_signals/standard_signals:.2f}x")
print(f"\\nâ†’ MTPëŠ” ê°™ì€ ë°ì´í„°ì—ì„œ {mtp_signals/standard_signals:.1f}ë°° ë” ë§ì€ í•™ìŠµ ì‹ í˜¸ë¥¼ ì¶”ì¶œ")"""))

# â”€â”€ Cell 18: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 8. ì •ë¦¬ <a name='8.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| FP8 E4M3 | ë†’ì€ ì •ë°€ë„, $[-448, 448]$ ë²”ìœ„ â€” ê°€ì¤‘ì¹˜/í™œì„±í™”ìš© | â­â­â­ |
| FP8 E5M2 | ë„“ì€ ë²”ìœ„, $[-57344, 57344]$ â€” ê¸°ìš¸ê¸°ìš© | â­â­â­ |
| Per-tensor ìŠ¤ì¼€ì¼ë§ | $s = \max(|x|)/q_{max}$ â€” ë‹¨ìˆœí•˜ê³  ë¹ ë¦„ | â­â­ |
| SNR | $\propto 2^{2b}$ â€” ë¹„íŠ¸ ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ ì§€í‘œ | â­â­ |
| Aux-Loss-Free | $b_i \leftarrow b_i + \alpha \cdot \text{sign}(\bar{f} - f_i)$ â€” í¸í–¥ ë³´ì • | â­â­â­ |
| MTP | $\mathcal{L} = \sum_k \lambda_k \mathcal{L}^{(k)}$ â€” ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ | â­â­â­ |
| í˜¼í•© ì •ë°€ë„ ì „ëµ | Forward(E4M3) + Backward(E5M2) + Update(FP32) | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$Q(x) = \text{clamp}\!\left(\left\lfloor \frac{x}{s} \right\rceil, -q_{max}, q_{max}\right) \cdot s$$

$$b_i \leftarrow b_i + \alpha \cdot \text{sign}\!\left(\frac{1}{N_E} - f_i\right)$$

$$\mathcal{L}_{MTP} = \sum_{k=1}^{K} \lambda_k \cdot \mathbb{E}\left[-\log p_\theta(x_{t+k} \mid x_{\leq t})\right]$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**02_multi_head_latent_attention.ipynb** â€” MLA(Multi-head Latent Attention)ì˜ KV ì••ì¶• ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³ , GQA ëŒ€ë¹„ KV Cache ì ˆê°ë¥ ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter16_sparse_attention/01_deepseek_v3_fp8_training.ipynb')
