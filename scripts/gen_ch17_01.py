"""Generate chapter17_diffusion_transformers/01_from_unet_to_dit.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 17: ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT â€” U-Netì—ì„œ DiTë¡œì˜ ì „í™˜

## í•™ìŠµ ëª©í‘œ
- U-Net ê¸°ë°˜ Diffusion ëª¨ë¸ì˜ ìŠ¤ì¼€ì¼ë§ í•œê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•œë‹¤
- DiT(Diffusion Transformer)ì˜ íŒ¨ì¹˜ ì„ë² ë”© ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  êµ¬í˜„í•œë‹¤
- 2D(ì´ë¯¸ì§€)ì™€ 3D(ë¹„ë””ì˜¤) íŒ¨ì¹˜ í† í¬ë‚˜ì´ì§•ì˜ ì°¨ì´ë¥¼ ë¹„êµí•œë‹¤
- DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™(FID vs Compute)ì„ ì‹œê°í™”í•˜ê³  ë¶„ì„í•œë‹¤
- íŒ¨ì¹˜ í¬ê¸°ê°€ ì‹œí€€ìŠ¤ ê¸¸ì´Â·ë©”ëª¨ë¦¬Â·í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: íŒ¨ì¹˜ ì„ë² ë”©ê³¼ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •](#2.-í™˜ê²½-ì„¤ì •)
3. [2D íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„](#3.-2D-íŒ¨ì¹˜-ì„ë² ë”©)
4. [U-Net vs DiT íŒŒë¼ë¯¸í„°Â·FLOPs ë¹„êµ](#4.-UNet-vs-DiT-ë¹„êµ)
5. [2D(ì´ë¯¸ì§€) vs 3D(ë¹„ë””ì˜¤) íŒ¨ì¹˜ ë¹„êµ](#5.-2D-vs-3D-íŒ¨ì¹˜)
6. [DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ì‹œê°í™”](#6.-ìŠ¤ì¼€ì¼ë§-ë²•ì¹™)
7. [íŒ¨ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´Â·ë©”ëª¨ë¦¬ ë¶„ì„](#7.-íŒ¨ì¹˜-í¬ê¸°-ë¶„ì„)
8. [ì •ë¦¬](#8.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### íŒ¨ì¹˜ ì„ë² ë”© (Patch Embedding)

ì…ë ¥ ì´ë¯¸ì§€ $x \in \mathbb{R}^{H \times W \times C}$ë¥¼ íŒ¨ì¹˜ í¬ê¸° $p$ë¡œ ë¶„í• í•˜ë©´:

$$x_{\text{patches}} \in \mathbb{R}^{N \times (p^2 \cdot C)}, \quad N = \frac{H}{p} \cdot \frac{W}{p}$$

- $H, W$: ì´ë¯¸ì§€ ë†’ì´, ë„ˆë¹„
- $C$: ì±„ë„ ìˆ˜ (RGB=3, latent=4 ë“±)
- $p$: íŒ¨ì¹˜ í¬ê¸° (ì˜ˆ: 2, 4, 8)
- $N$: ì‹œí€€ìŠ¤ ê¸¸ì´ (Transformerì˜ í† í° ìˆ˜)

ê° íŒ¨ì¹˜ëŠ” ì„ í˜• ë³€í™˜ìœ¼ë¡œ $d$ì°¨ì› ì„ë² ë”©ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤:

$$z_i = W_E \cdot \text{flatten}(\text{patch}_i) + b_E, \quad W_E \in \mathbb{R}^{d \times (p^2 C)}$$

### 3D íŒ¨ì¹˜ ì„ë² ë”© (ë¹„ë””ì˜¤)

ë¹„ë””ì˜¤ ì…ë ¥ $x \in \mathbb{R}^{T \times H \times W \times C}$ì— ëŒ€í•´ ì‹œê³µê°„ íŒ¨ì¹˜ $(p_t, p_h, p_w)$ë¥¼ ì ìš©í•˜ë©´:

$$N_{3D} = \frac{T}{p_t} \cdot \frac{H}{p_h} \cdot \frac{W}{p_w}, \quad \text{patch dim} = p_t \cdot p_h \cdot p_w \cdot C$$

### DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™

Peebles & Xie (2023)ì´ ë°œê²¬í•œ í•µì‹¬ ê´€ê³„:

$$\text{FID} \propto -a \cdot \log(\text{GFLOPs}) + b$$

- FID(Frechet Inception Distance)ëŠ” ê³„ì‚°ëŸ‰ì˜ ë¡œê·¸ì— ë¹„ë¡€í•˜ì—¬ ê°œì„ ë©ë‹ˆë‹¤
- U-Netì€ ê¹Šì´ë¥¼ ëŠ˜ë ¤ë„ ì„±ëŠ¥ì´ í¬í™”ë˜ì§€ë§Œ, DiTëŠ” ViTì²˜ëŸ¼ ìŠ¤ì¼€ì¼ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤

### U-Net vs DiT íŒŒë¼ë¯¸í„° ë¹„êµ

| ëª¨ë¸ | íŒŒë¼ë¯¸í„° ìˆ˜ | FLOPs (256Ã—256) | FID-256 |
|------|------------|-----------------|---------|
| U-Net (ADM) | ~554M | ~1050 GFLOPs | 10.94 |
| DiT-XL/2 | ~675M | ~119 GFLOPs | 9.62 |
| DiT-XL/2 (cfg) | ~675M | ~238 GFLOPs | 2.27 |

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| 2D ì‹œí€€ìŠ¤ ê¸¸ì´ | $N = (H/p)(W/p)$ | ì´ë¯¸ì§€ â†’ í† í° ìˆ˜ |
| 3D ì‹œí€€ìŠ¤ ê¸¸ì´ | $N = (T/p_t)(H/p_h)(W/p_w)$ | ë¹„ë””ì˜¤ â†’ í† í° ìˆ˜ |
| íŒ¨ì¹˜ ì°¨ì› | $d_{patch} = p^2 \cdot C$ (2D) | ê° íŒ¨ì¹˜ì˜ ë²¡í„° í¬ê¸° |
| Self-Attention ë¹„ìš© | $O(N^2 \cdot d)$ | ì‹œí€€ìŠ¤ ê¸¸ì´ì— 2ì°¨ ì˜ì¡´ |

---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ DiT íŒ¨ì¹˜ ì„ë² ë”© ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ íŒ¨ì¹˜ ì„ë² ë”©ì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: í° ì‚¬ì§„ì„ ì‘ì€ í¼ì¦ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ìƒê°í•´ë³´ì„¸ìš”!

í° ì‚¬ì§„(ì´ë¯¸ì§€)ì„ í•œ ë²ˆì— ì´í•´í•˜ê¸°ëŠ” ì–´ë µìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì‚¬ì§„ì„ ì‘ì€ ì •ì‚¬ê°í˜• ì¡°ê°(íŒ¨ì¹˜)ìœ¼ë¡œ ì˜ë¼ì„œ,
ê° ì¡°ê°ì„ ìˆ«ì ë¦¬ìŠ¤íŠ¸(ë²¡í„°)ë¡œ ë°”ê¿‰ë‹ˆë‹¤. ì´ëŸ¬ë©´ Transformerê°€ ê° ì¡°ê°ì„ "ë‹¨ì–´"ì²˜ëŸ¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ìš”!

- ğŸ–¼ï¸ **ì´ë¯¸ì§€ í¼ì¦**: 256Ã—256 ì‚¬ì§„ì„ 8Ã—8 ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ë©´ â†’ 32Ã—32 = 1,024ê°œì˜ í¼ì¦ ì¡°ê°
- ğŸ¬ **ë¹„ë””ì˜¤ í¼ì¦**: ë¹„ë””ì˜¤ëŠ” ì‹œê°„ë„ ìˆìœ¼ë‹ˆ 3ì°¨ì›ìœ¼ë¡œ ìë¦…ë‹ˆë‹¤ â†’ ì‹œê°„ ì¡°ê° + ê³µê°„ ì¡°ê°

#### ğŸ¤” ì™œ U-Net ëŒ€ì‹  Transformerë¥¼ ì“°ë‚˜ìš”?

> ğŸ’¡ **ë¹„ìœ **: U-Netì€ "ê³ ì •ëœ í¬ê¸°ì˜ ê¹”ë•Œê¸°"ì´ê³ , DiTëŠ” "ì›í•˜ëŠ” ë§Œí¼ ë„“í ìˆ˜ ìˆëŠ” ê³µì¥"ì…ë‹ˆë‹¤

U-Netì€ êµ¬ì¡°ê°€ ê³ ì •ë˜ì–´ ìˆì–´ì„œ ëª¨ë¸ì„ í‚¤ìš°ê¸° ì–´ë µì§€ë§Œ,
Transformer(DiT)ëŠ” ì¸µì„ ìŒ“ê¸°ë§Œ í•˜ë©´ ë˜ë‹ˆê¹Œ ë” í¬ê³  ë” ì¢‹ì€ ëª¨ë¸ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”!

---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: íŒ¨ì¹˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°

512Ã—512 ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ í¬ê¸° $p=16$ìœ¼ë¡œ ë¶„í• í•  ë•Œ ì‹œí€€ìŠ¤ ê¸¸ì´ $N$ì€?

$$N = \frac{H}{p} \cdot \frac{W}{p} = ?$$

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$N = \frac{512}{16} \times \frac{512}{16} = 32 \times 32 = 1024$$

ì‹œí€€ìŠ¤ ê¸¸ì´ëŠ” **1,024 í† í°**ì…ë‹ˆë‹¤. ë¹„êµ: $p=8$ì´ë©´ $N = 64 \times 64 = 4096$ìœ¼ë¡œ 4ë°° ì¦ê°€í•©ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: 3D ë¹„ë””ì˜¤ íŒ¨ì¹˜

16í”„ë ˆì„, 256Ã—256 ë¹„ë””ì˜¤ë¥¼ $(p_t, p_h, p_w) = (4, 16, 16)$ìœ¼ë¡œ íŒ¨ì¹˜í•  ë•Œ í† í° ìˆ˜ëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$N = \frac{16}{4} \times \frac{256}{16} \times \frac{256}{16} = 4 \times 16 \times 16 = 1024$$

3D íŒ¨ì¹˜ ê²°ê³¼ë„ **1,024 í† í°**ì…ë‹ˆë‹¤. ì‹œê°„ ì••ì¶• $p_t$ê°€ í´ìˆ˜ë¡ í† í° ìˆ˜ê°€ ì¤„ì–´ íš¨ìœ¨ì ì´ì§€ë§Œ, ì‹œê°„ í•´ìƒë„ê°€ ê°ì†Œí•©ë‹ˆë‹¤.
</details>"""))

# â”€â”€ Cell 3: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}")"""))

# â”€â”€ Cell 4: Section 3 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 3. 2D íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„ <a name='3.-2D-íŒ¨ì¹˜-ì„ë² ë”©'></a>

DiTì˜ í•µì‹¬ì€ ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
`tf.image.extract_patches` ë˜ëŠ” Conv2Dë¥¼ í™œìš©í•˜ì—¬ 2D íŒ¨ì¹˜ ì„ë² ë”©ì„ êµ¬í˜„í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 5: 2D patch embedding implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ 2D íŒ¨ì¹˜ ì„ë² ë”© êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PatchEmbedding2D(tf.keras.layers.Layer):
    # ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©í•˜ëŠ” ë ˆì´ì–´
    def __init__(self, patch_size, embed_dim, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.proj = tf.keras.layers.Conv2D(
            embed_dim, kernel_size=patch_size, strides=patch_size,
            padding='valid', use_bias=True
        )

    def call(self, x):
        # x: (B, H, W, C)
        patches = self.proj(x)  # (B, H/p, W/p, embed_dim)
        B = tf.shape(patches)[0]
        patches = tf.reshape(patches, [B, -1, self.embed_dim])  # (B, N, D)
        return patches

# í…ŒìŠ¤íŠ¸: 256x256 RGB ì´ë¯¸ì§€, íŒ¨ì¹˜ í¬ê¸° 8
image = tf.random.normal([2, 256, 256, 3])
patch_embed = PatchEmbedding2D(patch_size=8, embed_dim=384)
tokens = patch_embed(image)

H, W, p = 256, 256, 8
expected_N = (H // p) * (W // p)

print("=== 2D íŒ¨ì¹˜ ì„ë² ë”© ê²°ê³¼ ===")
print(f"ì…ë ¥ ì´ë¯¸ì§€ shape: {image.shape}")
print(f"íŒ¨ì¹˜ í¬ê¸°: {p}x{p}")
print(f"ì¶œë ¥ í† í° shape: {tokens.shape}")
print(f"ì‹œí€€ìŠ¤ ê¸¸ì´ N = ({H}/{p}) x ({W}/{p}) = {expected_N}")
print(f"ê° í† í° ì°¨ì›: {tokens.shape[-1]}")
print(f"íŒŒë¼ë¯¸í„° ìˆ˜: {patch_embed.count_params():,}")

# ë‹¤ì–‘í•œ íŒ¨ì¹˜ í¬ê¸° ë¹„êµ
print("\\n=== íŒ¨ì¹˜ í¬ê¸°ë³„ ì‹œí€€ìŠ¤ ê¸¸ì´ ===")
print(f"{'íŒ¨ì¹˜ í¬ê¸°':<12} | {'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'íŒ¨ì¹˜ ì°¨ì›':>10}")
print("-" * 42)
for ps in [2, 4, 8, 16, 32]:
    seq_len = (256 // ps) * (256 // ps)
    patch_dim = ps * ps * 3
    print(f"p={ps:<10} | {seq_len:>12,} | {patch_dim:>10}")"""))

# â”€â”€ Cell 6: Section 4 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 4. U-Net vs DiT íŒŒë¼ë¯¸í„°Â·FLOPs ë¹„êµ <a name='4.-UNet-vs-DiT-ë¹„êµ'></a>

U-Net(ADM)ê³¼ DiTì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì™€ FLOPsë¥¼ ë™ì¼ í•´ìƒë„ì—ì„œ ë¹„êµí•©ë‹ˆë‹¤.
DiTëŠ” ë” ì ì€ FLOPsë¡œ ë” ì¢‹ì€ FIDë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 7: UNet vs DiT comparison table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ U-Net vs DiT íŒŒë¼ë¯¸í„°/FLOPs ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Peebles & Xie (2023), "Scalable Diffusion Models with Transformers"

models = {
    'DiT-S/2': {'params': 33e6, 'gflops': 6.06, 'fid': 68.4, 'type': 'DiT'},
    'DiT-S/4': {'params': 33e6, 'gflops': 1.55, 'fid': 122.0, 'type': 'DiT'},
    'DiT-B/2': {'params': 130e6, 'gflops': 23.0, 'fid': 43.5, 'type': 'DiT'},
    'DiT-B/4': {'params': 130e6, 'gflops': 5.97, 'fid': 68.4, 'type': 'DiT'},
    'DiT-L/2': {'params': 458e6, 'gflops': 80.7, 'fid': 23.3, 'type': 'DiT'},
    'DiT-L/4': {'params': 458e6, 'gflops': 20.5, 'fid': 44.4, 'type': 'DiT'},
    'DiT-XL/2': {'params': 675e6, 'gflops': 119.0, 'fid': 9.62, 'type': 'DiT'},
    'DiT-XL/4': {'params': 675e6, 'gflops': 29.9, 'fid': 27.0, 'type': 'DiT'},
    'ADM (U-Net)': {'params': 554e6, 'gflops': 1050.0, 'fid': 10.94, 'type': 'UNet'},
    'ADM-U': {'params': 730e6, 'gflops': 742.0, 'fid': 7.49, 'type': 'UNet'},
    'LDM-4 (U-Net)': {'params': 400e6, 'gflops': 103.0, 'fid': 10.56, 'type': 'UNet'},
}

print("=" * 78)
print("U-Net vs DiT ëª¨ë¸ ë¹„êµ (ImageNet 256x256, class-conditional)")
print("=" * 78)
print(f"{'ëª¨ë¸':<18} | {'íŒŒë¼ë¯¸í„°':>10} | {'GFLOPs':>10} | {'FID-256':>8} | {'íƒ€ì…':>6}")
print("-" * 78)
for name, m in models.items():
    p_str = f"{m['params']/1e6:.0f}M"
    print(f"{name:<18} | {p_str:>10} | {m['gflops']:>10.1f} | {m['fid']:>8.2f} | {m['type']:>6}")

# í•µì‹¬ ë¹„êµ ìš”ì•½
dit_xl = models['DiT-XL/2']
adm = models['ADM (U-Net)']
flops_ratio = adm['gflops'] / dit_xl['gflops']
print(f"\\ní•µì‹¬ ë¹„êµ: DiT-XL/2 vs ADM (U-Net)")
print(f"  FLOPs ì ˆê°: {flops_ratio:.1f}x ì ì€ ì—°ì‚°ëŸ‰")
print(f"  FID ê°œì„ : {adm['fid']:.2f} â†’ {dit_xl['fid']:.2f}")
print(f"  DiTëŠ” {flops_ratio:.1f}ë°° ì ì€ FLOPsë¡œ ë” ì¢‹ì€ FIDë¥¼ ë‹¬ì„±!")"""))

# â”€â”€ Cell 8: Section 5 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. 2D(ì´ë¯¸ì§€) vs 3D(ë¹„ë””ì˜¤) íŒ¨ì¹˜ ë¹„êµ <a name='5.-2D-vs-3D-íŒ¨ì¹˜'></a>

DiTë¥¼ ë¹„ë””ì˜¤ë¡œ í™•ì¥í•  ë•Œ í•µì‹¬ì€ **ì‹œê³µê°„ 3D íŒ¨ì¹˜**ì…ë‹ˆë‹¤.

| êµ¬ë¶„ | 2D íŒ¨ì¹˜ (ì´ë¯¸ì§€) | 3D íŒ¨ì¹˜ (ë¹„ë””ì˜¤) |
|------|-----------------|-----------------|
| ì…ë ¥ | $\mathbb{R}^{H \times W \times C}$ | $\mathbb{R}^{T \times H \times W \times C}$ |
| íŒ¨ì¹˜ í¬ê¸° | $(p_h, p_w)$ | $(p_t, p_h, p_w)$ |
| ì‹œí€€ìŠ¤ ê¸¸ì´ | $(H/p_h)(W/p_w)$ | $(T/p_t)(H/p_h)(W/p_w)$ |
| íŒ¨ì¹˜ ì°¨ì› | $p_h \cdot p_w \cdot C$ | $p_t \cdot p_h \cdot p_w \cdot C$ |

Sora/HunyuanVideo ë“± ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì€ 3D íŒ¨ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°„ ì¶•ê¹Œì§€ ì••ì¶•í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 9: 2D vs 3D patch comparison code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ 2D(ì´ë¯¸ì§€) vs 3D(ë¹„ë””ì˜¤) íŒ¨ì¹˜ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class PatchEmbedding3D(tf.keras.layers.Layer):
    # ë¹„ë””ì˜¤ë¥¼ ì‹œê³µê°„ íŒ¨ì¹˜ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©í•˜ëŠ” ë ˆì´ì–´
    def __init__(self, patch_size_t, patch_size_h, patch_size_w, embed_dim, in_channels=4, **kwargs):
        super().__init__(**kwargs)
        self.pt, self.ph, self.pw = patch_size_t, patch_size_h, patch_size_w
        self.embed_dim = embed_dim
        self.in_channels = in_channels
        patch_dim = patch_size_t * patch_size_h * patch_size_w * in_channels
        self.linear_proj = tf.keras.layers.Dense(embed_dim)

    def call(self, x):
        # x: (B, T, H, W, C) - ë¹„ë””ì˜¤ í…ì„œ
        B = tf.shape(x)[0]
        T, H, W, C = x.shape[1], x.shape[2], x.shape[3], x.shape[4]
        nt, nh, nw = T // self.pt, H // self.ph, W // self.pw

        # ì‹œê³µê°„ íŒ¨ì¹˜ ì¶”ì¶œ (reshape ê¸°ë°˜)
        x = tf.reshape(x, [B, nt, self.pt, nh, self.ph, nw, self.pw, C])
        x = tf.transpose(x, [0, 1, 3, 5, 2, 4, 6, 7])  # (B, nt, nh, nw, pt, ph, pw, C)
        x = tf.reshape(x, [B, nt * nh * nw, self.pt * self.ph * self.pw * C])

        # ì„ í˜• íˆ¬ì˜
        tokens = self.linear_proj(x)  # (B, N, embed_dim)
        return tokens

# 2D íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸
image = tf.random.normal([1, 256, 256, 4])
patch_2d = PatchEmbedding2D(patch_size=2, embed_dim=1152)
tokens_2d = patch_2d(image)

# 3D íŒ¨ì¹˜ í…ŒìŠ¤íŠ¸ (ë¹„ë””ì˜¤: 16 í”„ë ˆì„)
video = tf.random.normal([1, 16, 256, 256, 4])
patch_3d = PatchEmbedding3D(
    patch_size_t=2, patch_size_h=2, patch_size_w=2,
    embed_dim=1152, in_channels=4
)
tokens_3d = patch_3d(video)

print("=" * 65)
print("2D(ì´ë¯¸ì§€) vs 3D(ë¹„ë””ì˜¤) íŒ¨ì¹˜ ì„ë² ë”© ë¹„êµ")
print("=" * 65)
print(f"{'êµ¬ë¶„':<18} | {'2D (ì´ë¯¸ì§€)':>20} | {'3D (ë¹„ë””ì˜¤)':>20}")
print("-" * 65)
print(f"{'ì…ë ¥ shape':<18} | {str(image.shape):>20} | {str(video.shape):>20}")
print(f"{'íŒ¨ì¹˜ í¬ê¸°':<18} | {'(2, 2)':>20} | {'(2, 2, 2)':>20}")
print(f"{'ì‹œí€€ìŠ¤ ê¸¸ì´ N':<18} | {tokens_2d.shape[1]:>20,} | {tokens_3d.shape[1]:>20,}")
print(f"{'í† í° ì°¨ì› D':<18} | {tokens_2d.shape[2]:>20} | {tokens_3d.shape[2]:>20}")
mem_2d = tokens_2d.shape[1] * 1152 * 4 / 1024
mem_3d = tokens_3d.shape[1] * 1152 * 4 / 1024
print(f"{'ë©”ëª¨ë¦¬ (float32)':<18} | {mem_2d:>17.0f} KB | {mem_3d:>17.0f} KB")

# ë‹¤ì–‘í•œ 3D íŒ¨ì¹˜ í¬ê¸° ë¹„êµ
print("\\n=== 3D íŒ¨ì¹˜ í¬ê¸°ë³„ ì‹œí€€ìŠ¤ ê¸¸ì´ (16í”„ë ˆì„, 256x256, 4ch) ===")
print(f"{'(pt, ph, pw)':<15} | {'í† í° ìˆ˜':>10} | {'íŒ¨ì¹˜ ì°¨ì›':>10} | {'ë¹„ê³ ':>20}")
print("-" * 62)
configs = [(1,2,2), (2,2,2), (4,4,4), (1,8,8), (2,4,4)]
for pt, ph, pw in configs:
    T, H, W, C = 16, 256, 256, 4
    n_tok = (T//pt) * (H//ph) * (W//pw)
    p_dim = pt * ph * pw * C
    note = ""
    if pt == 1:
        note = "ì‹œê°„ ì••ì¶• ì—†ìŒ"
    elif pt == 4:
        note = "ë†’ì€ ì‹œê°„ ì••ì¶•"
    else:
        note = "ì‹œê°„+ê³µê°„ ê· í˜•"
    print(f"({pt},{ph},{pw})" + " " * (14-len(f"({pt},{ph},{pw})")) + f"| {n_tok:>10,} | {p_dim:>10} | {note:>20}")"""))

# â”€â”€ Cell 10: Section 6 - Scaling law â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 6. DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ì‹œê°í™” <a name='6.-ìŠ¤ì¼€ì¼ë§-ë²•ì¹™'></a>

Peebles & Xie (2023)ì€ DiTê°€ Transformerì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ì„ ë”°ë¦„ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
FIDëŠ” GFLOPsì˜ ë¡œê·¸ì— ë¹„ë¡€í•˜ì—¬ ê°œì„ ë©ë‹ˆë‹¤."""))

# â”€â”€ Cell 11: Scaling law visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë…¼ë¬¸ Table 1, Figure 4 ê¸°ë°˜ ë°ì´í„° (Peebles & Xie, 2023)

dit_data = {
    'DiT-S/8': (0.4, 177.0), 'DiT-S/4': (1.6, 122.0), 'DiT-S/2': (6.1, 68.4),
    'DiT-B/8': (1.4, 131.0), 'DiT-B/4': (6.0, 68.4), 'DiT-B/2': (23.0, 43.5),
    'DiT-L/8': (5.0, 99.4), 'DiT-L/4': (20.5, 44.4), 'DiT-L/2': (80.7, 23.3),
    'DiT-XL/8': (7.4, 80.1), 'DiT-XL/4': (29.9, 27.0), 'DiT-XL/2': (119.0, 9.62),
}

sizes = {'S': ('o', '#2196F3', 7), 'B': ('s', '#4CAF50', 8),
         'L': ('^', '#FF9800', 9), 'XL': ('D', '#F44336', 10)}

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# ì¢Œ: GFLOPs vs FID (ëª¨ë“  ëª¨ë¸)
ax1 = axes[0]
for name, (gflops, fid) in dit_data.items():
    size_key = name.split('-')[1].split('/')[0]
    marker, color, ms = sizes[size_key]
    ax1.scatter(gflops, fid, marker=marker, color=color, s=ms**2,
                zorder=5, edgecolors='black', linewidth=0.5)

# ì¶”ì„¸ì„  (ë¡œê·¸ ì„ í˜•)
gflops_arr = np.array([v[0] for v in dit_data.values()])
fid_arr = np.array([v[1] for v in dit_data.values()])
log_gf = np.log10(gflops_arr)
coeffs = np.polyfit(log_gf, fid_arr, 1)
x_fit = np.linspace(np.log10(0.3), np.log10(150), 100)
y_fit = np.polyval(coeffs, x_fit)
ax1.plot(10**x_fit, y_fit, 'k--', lw=1.5, alpha=0.5, label='log-linear ì¶”ì„¸')

ax1.set_xscale('log')
ax1.set_xlabel('GFLOPs (log scale)', fontsize=11)
ax1.set_ylabel('FID-50K â†“', fontsize=11)
ax1.set_title('DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™: FID vs Compute', fontweight='bold')

# ë²”ë¡€ (í¬ê¸°ë³„)
for size_key, (marker, color, ms) in sizes.items():
    ax1.scatter([], [], marker=marker, color=color, s=ms**2,
                label=f'DiT-{size_key}', edgecolors='black', linewidth=0.5)
ax1.legend(fontsize=9, loc='upper right')
ax1.grid(True, alpha=0.3)

# ìš°: íŒ¨ì¹˜ í¬ê¸°ë³„ FID ë¹„êµ (ê°™ì€ ëª¨ë¸ í¬ê¸°)
ax2 = axes[1]
patch_sizes = [8, 4, 2]
for size_key, (marker, color, ms) in sizes.items():
    fids = []
    for ps in patch_sizes:
        key = f'DiT-{size_key}/{ps}'
        if key in dit_data:
            fids.append(dit_data[key][1])
    if len(fids) == 3:
        ax2.plot(patch_sizes, fids, f'-{marker}', color=color, lw=2,
                 ms=ms, label=f'DiT-{size_key}')

ax2.set_xlabel('íŒ¨ì¹˜ í¬ê¸° p', fontsize=11)
ax2.set_ylabel('FID-50K â†“', fontsize=11)
ax2.set_title('íŒ¨ì¹˜ í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡ FID ê°œì„ ', fontweight='bold')
ax2.set_xticks([2, 4, 8])
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.invert_xaxis()

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/dit_scaling_law.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/dit_scaling_law.png")
print(f"\\nìŠ¤ì¼€ì¼ë§ ì¶”ì„¸ ê³„ìˆ˜: FID â‰ˆ {coeffs[0]:.1f} * log10(GFLOPs) + {coeffs[1]:.1f}")
print("â†’ GFLOPsê°€ 10ë°° ì¦ê°€í•  ë•Œ FIDê°€ ì•½ {:.1f} ê°ì†Œ".format(abs(coeffs[0])))"""))

# â”€â”€ Cell 12: Section 7 - Patch size analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 7. íŒ¨ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´Â·ë©”ëª¨ë¦¬ ë¶„ì„ <a name='7.-íŒ¨ì¹˜-í¬ê¸°-ë¶„ì„'></a>

íŒ¨ì¹˜ í¬ê¸° $p$ëŠ” í’ˆì§ˆê³¼ íš¨ìœ¨ì„±ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤:
- $p$ ì‘ìœ¼ë©´: ë†’ì€ í’ˆì§ˆ (FID â†“), ê¸´ ì‹œí€€ìŠ¤, ë†’ì€ ë©”ëª¨ë¦¬ ($O(N^2)$)
- $p$ í¬ë©´: ë‚®ì€ í’ˆì§ˆ (FID â†‘), ì§§ì€ ì‹œí€€ìŠ¤, ë‚®ì€ ë©”ëª¨ë¦¬"""))

# â”€â”€ Cell 13: Patch size analysis visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ íŒ¨ì¹˜ í¬ê¸° â†’ ì‹œí€€ìŠ¤ ê¸¸ì´ Â· ë©”ëª¨ë¦¬ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

patch_sizes = np.array([1, 2, 4, 8, 16, 32])
H_img, W_img, C_img = 256, 256, 4
d_model = 1152

# ì‹œí€€ìŠ¤ ê¸¸ì´
seq_lens = (H_img // patch_sizes) * (W_img // patch_sizes)

# Self-Attention ë©”ëª¨ë¦¬ (float32, ë‹¨ì¼ í—¤ë“œ, ë°°ì¹˜ 1)
attn_mem_mb = (seq_lens.astype(np.float64) ** 2 * 4) / (1024**2)

# FLOPs (self-attention only)
flops_giga = 2 * seq_lens.astype(np.float64) ** 2 * d_model / 1e9

ax1 = axes[0]
ax1.bar(range(len(patch_sizes)), seq_lens, color='#2196F3', alpha=0.8, edgecolor='black')
ax1.set_xticks(range(len(patch_sizes)))
ax1.set_xticklabels([f'p={p}' for p in patch_sizes])
ax1.set_ylabel('ì‹œí€€ìŠ¤ ê¸¸ì´ N', fontsize=11)
ax1.set_title('íŒ¨ì¹˜ í¬ê¸°ë³„ ì‹œí€€ìŠ¤ ê¸¸ì´', fontweight='bold')
ax1.set_yscale('log')
for i, v in enumerate(seq_lens):
    ax1.text(i, v * 1.3, f'{v:,}', ha='center', fontsize=8)
ax1.grid(True, alpha=0.3, axis='y')

ax2 = axes[1]
ax2.bar(range(len(patch_sizes)), attn_mem_mb, color='#F44336', alpha=0.8, edgecolor='black')
ax2.set_xticks(range(len(patch_sizes)))
ax2.set_xticklabels([f'p={p}' for p in patch_sizes])
ax2.set_ylabel('Attention ë©”ëª¨ë¦¬ (MB)', fontsize=11)
ax2.set_title('Self-Attention ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontweight='bold')
ax2.set_yscale('log')
for i, v in enumerate(attn_mem_mb):
    label = f'{v:.0f}MB' if v >= 1 else f'{v*1024:.0f}KB'
    ax2.text(i, v * 1.3, label, ha='center', fontsize=8)
ax2.grid(True, alpha=0.3, axis='y')

ax3 = axes[2]
ax3.bar(range(len(patch_sizes)), flops_giga, color='#4CAF50', alpha=0.8, edgecolor='black')
ax3.set_xticks(range(len(patch_sizes)))
ax3.set_xticklabels([f'p={p}' for p in patch_sizes])
ax3.set_ylabel('Self-Attention FLOPs (GFLOPs)', fontsize=11)
ax3.set_title('Attention ì—°ì‚° ë¹„ìš©', fontweight='bold')
ax3.set_yscale('log')
for i, v in enumerate(flops_giga):
    ax3.text(i, v * 1.3, f'{v:.1f}G', ha='center', fontsize=8)
ax3.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/patch_size_analysis.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/patch_size_analysis.png")

# ìˆ˜ì¹˜ ìš”ì•½ í‘œ
print(f"\\n{'íŒ¨ì¹˜ í¬ê¸°':<10} | {'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'Attn ë©”ëª¨ë¦¬':>12} | {'Attn FLOPs':>12}")
print("-" * 55)
for i, p in enumerate(patch_sizes):
    mem_str = f"{attn_mem_mb[i]:.1f} MB" if attn_mem_mb[i] >= 1 else f"{attn_mem_mb[i]*1024:.0f} KB"
    print(f"p={p:<8} | {seq_lens[i]:>12,} | {mem_str:>12} | {flops_giga[i]:>10.1f} G")

print("\\nê²°ë¡ : p=2 (DiT-XL/2)ê°€ í’ˆì§ˆ-íš¨ìœ¨ ê· í˜•ì ì´ë©°, ëŒ€ë¶€ë¶„ì˜ DiT ë…¼ë¬¸ì—ì„œ ì±„íƒ")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 8. ì •ë¦¬ <a name='8.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| 2D íŒ¨ì¹˜ ì„ë² ë”© | ì´ë¯¸ì§€ â†’ $(H/p)(W/p)$ í† í° ì‹œí€€ìŠ¤ | â­â­â­ |
| 3D íŒ¨ì¹˜ ì„ë² ë”© | ë¹„ë””ì˜¤ â†’ $(T/p_t)(H/p_h)(W/p_w)$ í† í° ì‹œí€€ìŠ¤ | â­â­â­ |
| DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ | FID âˆ âˆ’log(GFLOPs), U-Net ëŒ€ë¹„ íš¨ìœ¨ì  ìŠ¤ì¼€ì¼ë§ | â­â­â­ |
| íŒ¨ì¹˜ í¬ê¸° íŠ¸ë ˆì´ë“œì˜¤í”„ | p ì‘ì„ìˆ˜ë¡ í’ˆì§ˆ â†‘, ë©”ëª¨ë¦¬/ì—°ì‚° â†‘ ($O(N^2)$) | â­â­ |
| U-Net â†’ DiT ì „í™˜ | ê³ ì • êµ¬ì¡° â†’ Transformer ìŠ¤ì¼€ì¼ë§, ~9x FLOPs ì ˆê° | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$N_{2D} = \frac{H}{p} \cdot \frac{W}{p}, \quad N_{3D} = \frac{T}{p_t} \cdot \frac{H}{p_h} \cdot \frac{W}{p_w}$$

$$\text{FID} \approx -a \cdot \log_{10}(\text{GFLOPs}) + b \quad \text{(DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™)}$$

$$\text{Self-Attention Cost} = O(N^2 \cdot d) \quad \text{â†’ íŒ¨ì¹˜ í¬ê¸°ê°€ í•µì‹¬}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**02_spatiotemporal_vae.ipynb** â€” 3D Causal VAEë¥¼ í†µí•œ ë¹„ë””ì˜¤ ì ì¬ ê³µê°„ ì••ì¶•ê³¼ ì‹œê°„ì  ì¼ê´€ì„± ìœ ì§€ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."""))

# â”€â”€ ë…¸íŠ¸ë¶ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
create_notebook(cells, 'chapter17_diffusion_transformers/01_from_unet_to_dit.ipynb')
