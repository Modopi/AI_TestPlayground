"""Generate chapter16_sparse_attention/04_long_context_and_sparse_attn.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Long Contextì™€ Sparse Attention

## í•™ìŠµ ëª©í‘œ
- YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì •ì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ì°½ í™•ì¥ ì›ë¦¬ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤
- Sliding Window Attention(SWA)ì˜ ë§ˆìŠ¤í¬ êµ¬ì¡°ì™€ ë©”ëª¨ë¦¬ ì ˆì•½ ì›ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤
- DeepSeek Sparse Attention(DSA)ì˜ ì„ íƒì  í† í° íŒ¨í„´ì„ ë¶„ì„í•œë‹¤
- Full, Sliding Window, Sparse Attentionì˜ ë§ˆìŠ¤í¬ë¥¼ ì‹œê°í™”í•˜ì—¬ ë¹„êµí•œë‹¤
- 50% ì´ìƒì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ëŠ” í†µí•© ë°©ë²•ë¡ ì„ ì´í•´í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ê³¼ Sparse Attention](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Attention ë§ˆìŠ¤í¬ ì‹œê°í™”](#2.-Attention-ë§ˆìŠ¤í¬-ì‹œê°í™”)
3. [ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ](#3.-ë©”ëª¨ë¦¬-ì ˆì•½-ë¹„êµ)
4. [Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜](#4.-Long-Context-Perplexity)
5. [50% ì´ìƒ ë¹„ìš© ì ˆê° ë¶„ì„](#5.-ë¹„ìš©-ì ˆê°-ë¶„ì„)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### YaRN (Yet another RoPE extensioN)

RoPEì˜ ì£¼íŒŒìˆ˜ë¥¼ ì¬ì¡°ì •í•˜ì—¬ í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸ ì°½ì„ ë„˜ì–´ì„œëŠ” ìœ„ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:

$$\theta_i' = \theta_i \cdot \begin{cases} 1 & \text{if } \lambda_i < 1 \text{ (ê³ ì£¼íŒŒ â€” ë³€ê²½ ì—†ìŒ)} \\ 1/s & \text{if } \lambda_i > 1 \text{ (ì €ì£¼íŒŒ â€” ìŠ¤ì¼€ì¼ ë‹¤ìš´)} \\ (1-\gamma) \cdot 1 + \gamma / s & \text{otherwise (ì„ í˜• ë³´ê°„)} \end{cases}$$

- $\theta_i = 10000^{-2i/d}$: ì›ë˜ RoPE ì£¼íŒŒìˆ˜
- $s$: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨ (ì˜ˆ: 4x â†’ $s=4$)
- $\lambda_i = 2\pi / \theta_i$: íŒŒì¥
- $\gamma$: ë³´ê°„ ë¹„ìœ¨

### Sliding Window Attention (SWA)

ê° í† í°ì€ ìì‹ ì˜ ìœˆë„ìš° ë‚´ í† í°ë§Œ attendí•©ë‹ˆë‹¤:

$$A_{ij} = \begin{cases} \text{softmax}(q_i k_j^T / \sqrt{d}) & \text{if } |i - j| \leq w/2 \text{ and } j \leq i \\ 0 & \text{otherwise} \end{cases}$$

- $w$: ìœˆë„ìš° í¬ê¸°
- ë©”ëª¨ë¦¬ ë³µì¡ë„: $O(Nw)$ vs Fullì˜ $O(N^2)$

### DeepSeek Sparse Attention (DSA)

í† í°ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ì¤‘ìš”í•œ ë¸”ë¡ë§Œ ì„ íƒì ìœ¼ë¡œ attend:

1. **ë¸”ë¡ ë¶„í• **: ì‹œí€€ìŠ¤ë¥¼ $B$ê°œ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ” ($b = N/B$)
2. **ì¤‘ìš”ë„ ì ìˆ˜**: $\text{score}_j = \max_{k \in \text{block}_j} q_i^T k / \sqrt{d}$
3. **Top-K ë¸”ë¡ ì„ íƒ**: ìƒìœ„ $K$ê°œ ë¸”ë¡ë§Œ attend

$$\text{Sparsity} = 1 - \frac{K}{B}, \quad \text{ë¹„ìš© ì ˆê°} = 1 - \frac{Kb + w}{N}$$

### ë¹„ìš© ì ˆê° ë¶„ì„

| ë°©ë²• | Attention ë¹„ìš© | Full ëŒ€ë¹„ ì ˆê°ë¥  |
|------|---------------|-----------------|
| Full | $N^2$ | 0% |
| SWA ($w$) | $Nw$ | $1 - w/N$ |
| DSA ($K$ ë¸”ë¡, $b$ í¬ê¸°) | $NKb$ | $1 - Kb/N$ |
| SWA + DSA | $N(w + Kb)$ | $1 - (w + Kb)/N$ |

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| YaRN ìŠ¤ì¼€ì¼ë§ | $\theta_i' \propto \theta_i / s$ | ì €ì£¼íŒŒ ì£¼íŒŒìˆ˜ ì••ì¶• |
| SWA ë§ˆìŠ¤í¬ | $\|i-j\| \leq w/2$ | ì§€ì—­ ìœˆë„ìš° ì œí•œ |
| DSA ë¸”ë¡ ì„ íƒ | $\text{TopK}(\max_{k} q^Tk)$ | ì¤‘ìš” ë¸”ë¡ë§Œ attend |
| ë¹„ìš© ì ˆê° | $> 50\%$ ê°€ëŠ¥ | SWA + DSA ê²°í•© |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Long Context ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ ì™œ ê¸´ ë¬¸ì¥ì„ ì½ê¸° ì–´ë ¤ìš´ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: 1000í˜ì´ì§€ì§œë¦¬ ì±…ì„ ì½ì„ ë•Œ, ëª¨ë“  í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë™ì‹œì— ê¸°ì–µí•˜ë ¤ë©´ 
> ì—„ì²­ë‚œ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•´ìš”! AIë„ ë§ˆì°¬ê°€ì§€ì˜ˆìš”.

- **Full Attention**: ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì°¸ì¡° â†’ í˜ì´ì§€ê°€ ë§ìœ¼ë©´ $N^2$ë°° ëŠë ¤ì§
- **Sliding Window**: í˜„ì¬ ì½ëŠ” í˜ì´ì§€ ì£¼ë³€ 10í˜ì´ì§€ë§Œ ì°¸ì¡° â†’ ë¹ ë¥´ì§€ë§Œ ë¨¼ ë‚´ìš©ì„ ëª» ë´„
- **Sparse Attention**: **ì¤‘ìš”í•œ í˜ì´ì§€ë§Œ** ê³¨ë¼ì„œ ì°¸ì¡° â†’ ë¹ ë¥´ë©´ì„œë„ í•µì‹¬ì€ ë†“ì¹˜ì§€ ì•ŠìŒ!

#### ğŸ“ YaRNì€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: 30cm ì(í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸)ë¡œ 1më¥¼ ì¬ê³  ì‹¶ì„ ë•Œ, 
> ìì˜ ëˆˆê¸ˆ ê°„ê²©ì„ ì¤„ì—¬ì„œ(ì£¼íŒŒìˆ˜ ì¬ì¡°ì •) ë” ê¸´ ê±°ë¦¬ë¥¼ ì´ ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê±°ì˜ˆìš”!

YaRNì€ í•™ìŠµ ì‹œ 4K í† í°ê¹Œì§€ë§Œ ë°°ìš´ ëª¨ë¸ì´ 64K í† í°ê¹Œì§€ ì½ì„ ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”.

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: SWA ì ˆê°ë¥ 

ì‹œí€€ìŠ¤ ê¸¸ì´ $N = 8192$, ìœˆë„ìš° í¬ê¸° $w = 512$ì¼ ë•Œ SWAì˜ Full ëŒ€ë¹„ ë¹„ìš© ì ˆê°ë¥ ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{ì ˆê°ë¥ } = 1 - \frac{w}{N} = 1 - \frac{512}{8192} = 1 - 0.0625 = 93.75\%$$

â†’ ìœˆë„ìš°ë¥¼ ì œí•œí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ **93.75%** ë¹„ìš© ì ˆê°!
ë‹¤ë§Œ 512 í† í° ë°–ì˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ ë†“ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: DSA + SWA ê²°í•©

$N=8192$, SWA $w=512$, DSA ë¸”ë¡ í¬ê¸° $b=256$, ì„ íƒ ë¸”ë¡ $K=4$ì¼ ë•Œ ì´ ë¹„ìš© ì ˆê°ë¥ ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{SWA ë¹„ìš©} = N \times w = 8192 \times 512$$
$$\text{DSA ë¹„ìš©} = N \times K \times b = 8192 \times 4 \times 256$$
$$\text{ì´ ë¹„ìš©} = N(w + Kb) = 8192 \times (512 + 1024) = 8192 \times 1536$$
$$\text{Full ë¹„ìš©} = N^2 = 8192^2 = 8192 \times 8192$$

$$\text{ì ˆê°ë¥ } = 1 - \frac{1536}{8192} = 1 - 0.1875 = 81.25\%$$

â†’ SWA + DSA ê²°í•©ìœ¼ë¡œ **81.25%** ë¹„ìš© ì ˆê° ë‹¬ì„±!
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 - Attention mask visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. Attention ë§ˆìŠ¤í¬ ì‹œê°í™” <a name='2.-Attention-ë§ˆìŠ¤í¬-ì‹œê°í™”'></a>

Full, Sliding Window, DeepSeek Sparse Attentionì˜ ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ë¹„êµí•©ë‹ˆë‹¤."""))

cells.append(code("""\
# â”€â”€ Attention ë§ˆìŠ¤í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
N = 64  # ì‹œê°í™”ë¥¼ ìœ„í•œ ì‘ì€ ì‹œí€€ìŠ¤

def create_causal_mask(N):
    # í‘œì¤€ causal mask (lower triangular)
    return np.tril(np.ones((N, N)))

def create_sliding_window_mask(N, window_size=16):
    # Sliding window + causal
    mask = np.zeros((N, N))
    for i in range(N):
        start = max(0, i - window_size + 1)
        mask[i, start:i+1] = 1.0
    return mask

def create_sparse_block_mask(N, block_size=8, top_k_blocks=2, window_size=8):
    # Sparse attention: ìœˆë„ìš° + Top-K ë¸”ë¡
    mask = np.zeros((N, N))
    n_blocks = N // block_size

    for i in range(N):
        # (1) ë¡œì»¬ ìœˆë„ìš°
        start = max(0, i - window_size + 1)
        mask[i, start:i+1] = 1.0

        # (2) Top-K ë¸”ë¡ (ëœë¤ ì‹œë®¬ë ˆì´ì…˜)
        current_block = i // block_size
        available_blocks = list(range(0, current_block + 1))
        if len(available_blocks) > top_k_blocks:
            np.random.seed(i * 7 + 3)
            selected = np.random.choice(available_blocks, top_k_blocks, replace=False)
        else:
            selected = available_blocks

        for b in selected:
            b_start = b * block_size
            b_end = min(b_start + block_size, i + 1)
            mask[i, b_start:b_end] = 1.0

    return mask

# ë§ˆìŠ¤í¬ ìƒì„±
mask_full = create_causal_mask(N)
mask_swa = create_sliding_window_mask(N, window_size=16)
mask_sparse = create_sparse_block_mask(N, block_size=8, top_k_blocks=2, window_size=8)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

titles = ['Full Causal Attention', 'Sliding Window (w=16)', 'Sparse Block (k=2, b=8, w=8)']
masks = [mask_full, mask_swa, mask_sparse]
cmaps = ['Blues', 'Oranges', 'Greens']

for ax, title, mask, cmap in zip(axes, titles, masks, cmaps):
    ax.imshow(mask, cmap=cmap, aspect='equal', interpolation='nearest')
    ax.set_xlabel('Key ìœ„ì¹˜', fontsize=11)
    ax.set_ylabel('Query ìœ„ì¹˜', fontsize=11)
    ax.set_title(title, fontweight='bold')

    # ë°€ë„(density) í‘œì‹œ
    density = mask.sum() / mask.size * 100
    ax.text(N*0.95, N*0.05, f'ë°€ë„: {density:.1f}%',
            ha='right', va='top', fontsize=10,
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/attention_masks_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/attention_masks_comparison.png")

# ë°€ë„ ë¹„êµ
print(f"\\nAttention ë§ˆìŠ¤í¬ ë°€ë„ ë¹„êµ (N={N}):")
for title, mask in zip(titles, masks):
    density = mask.sum() / mask.size * 100
    active = int(mask.sum())
    total = mask.size
    print(f"  {title}: {density:.1f}% ({active}/{total} ì›ì†Œ í™œì„±)")"""))

# â”€â”€ Cell 8: Section 3 - Memory savings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ <a name='3.-ë©”ëª¨ë¦¬-ì ˆì•½-ë¹„êµ'></a>

ë‹¤ì–‘í•œ Attention ë°©ì‹ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¹„êµí•©ë‹ˆë‹¤.

| ë°©ì‹ | Attention ë©”ëª¨ë¦¬ | KV Cache (ì¶”ë¡ ) |
|------|-----------------|-----------------|
| Full Causal | $O(N^2)$ | $O(Nd)$ |
| SWA | $O(Nw)$ | $O(wd)$ |
| Sparse Block | $O(NKb)$ | $O(Kbd)$ |"""))

cells.append(code("""\
# â”€â”€ ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
seq_lengths = np.array([1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072])
window_size = 1024
block_size = 512
top_k_blocks = 4
d = 128

# Attention í–‰ë ¬ ë©”ëª¨ë¦¬ (ì›ì†Œ ìˆ˜)
mem_full = seq_lengths ** 2
mem_swa = seq_lengths * window_size
mem_sparse = seq_lengths * (top_k_blocks * block_size + window_size)

# KV Cache ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸, FP16, ë‹¨ì¼ ë ˆì´ì–´)
kv_full = seq_lengths * 2 * d * 2  # ëª¨ë“  í† í°ì˜ KV
kv_swa = np.minimum(seq_lengths, window_size) * 2 * d * 2  # ìœˆë„ìš°ë§Œ
kv_sparse = (top_k_blocks * block_size + window_size) * 2 * d * 2  # ë¸”ë¡ + ìœˆë„ìš°

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) Attention ì—°ì‚° ë©”ëª¨ë¦¬
ax1 = axes[0]
ax1.loglog(seq_lengths / 1000, mem_full / 1e9, 'r-o', lw=2.5, ms=7, label='Full ($O(N^2)$)')
ax1.loglog(seq_lengths / 1000, mem_swa / 1e9, 'b-s', lw=2, ms=7, label=f'SWA ($w={window_size}$)')
ax1.loglog(seq_lengths / 1000, mem_sparse / 1e9, 'g-^', lw=2, ms=7,
           label=f'Sparse ($K={top_k_blocks}, b={block_size}$)')

ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)
ax1.set_ylabel('Attention ë©”ëª¨ë¦¬ (G ì›ì†Œ)', fontsize=11)
ax1.set_title('Attention ì—°ì‚° ë©”ëª¨ë¦¬ ë¹„êµ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ì ˆê°ë¥ 
ax2 = axes[1]
savings_swa = (1 - mem_swa / mem_full) * 100
savings_sparse = (1 - mem_sparse / mem_full) * 100

ax2.plot(seq_lengths / 1000, savings_swa, 'b-s', lw=2.5, ms=7, label='SWA')
ax2.plot(seq_lengths / 1000, savings_sparse, 'g-^', lw=2.5, ms=7, label='Sparse')
ax2.axhline(y=50, color='red', ls='--', lw=1.5, label='50% ê¸°ì¤€')
ax2.fill_between(seq_lengths / 1000, 50, 100, alpha=0.05, color='green')
ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)
ax2.set_ylabel('Full ëŒ€ë¹„ ì ˆê°ë¥  (%)', fontsize=11)
ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë¹„ìš© ì ˆê°ë¥ ', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0, 100)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/memory_savings_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/memory_savings_comparison.png")

# ìˆ˜ì¹˜ í‘œ
print(f"\\në©”ëª¨ë¦¬ ì ˆê°ë¥  ìˆ˜ì¹˜ ë¹„êµ:")
print(f"{'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'SWA ì ˆê°':>10} | {'Sparse ì ˆê°':>12} | {'>50% ë‹¬ì„±':>10}")
print("-" * 52)
for i, N in enumerate(seq_lengths):
    swa_s = savings_swa[i]
    sp_s = savings_sparse[i]
    check = 'âœ…' if sp_s > 50 else 'âŒ'
    print(f"{N:>12,} | {swa_s:>9.1f}% | {sp_s:>11.1f}% | {check:>10}")"""))

# â”€â”€ Cell 10: Section 4 - Long Context Perplexity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜ <a name='4.-Long-Context-Perplexity'></a>

YaRNì„ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì‹œ perplexity ë³€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.

$$\text{PPL}(s) = \text{PPL}_{base} \cdot \left(1 + \alpha \cdot \max(0, s - 1)\right)$$

- $s$: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨
- $\alpha$: í’ˆì§ˆ ì €í•˜ ê³„ìˆ˜ (YaRNì´ ì‘ì„ìˆ˜ë¡ ìš°ìˆ˜)"""))

cells.append(code("""\
# â”€â”€ Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)

# ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨
extension_ratios = np.array([1, 2, 4, 8, 16, 32])
base_ctx = 4096  # ê¸°ë³¸ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸

# ê° ë°©ë²•ì˜ perplexity ëª¨ë¸ë§
ppl_base = 5.0  # ê¸°ë³¸ perplexity

# NTK-aware (ë‹¨ìˆœ ìŠ¤ì¼€ì¼ë§)
alpha_ntk = 0.15
ppl_ntk = ppl_base * (1 + alpha_ntk * np.maximum(0, extension_ratios - 1))

# YaRN (ê°œì„ ëœ ìŠ¤ì¼€ì¼ë§)
alpha_yarn = 0.03
ppl_yarn = ppl_base * (1 + alpha_yarn * np.maximum(0, extension_ratios - 1))

# í•™ìŠµ ì—†ì´ ì§ì ‘ í™•ì¥ (PI: Position Interpolation)
alpha_pi = 0.08
ppl_pi = ppl_base * (1 + alpha_pi * np.maximum(0, extension_ratios - 1))

# í•™ìŠµ ì—†ì´ RoPE (ì™¸ì‚½ - ê¸‰ê²©íˆ ë‚˜ë¹ ì§)
ppl_no_ext = ppl_base * np.exp(0.2 * np.maximum(0, extension_ratios - 1))

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) PPL vs í™•ì¥ ë¹„ìœ¨
ax1 = axes[0]
effective_ctx = base_ctx * extension_ratios
ax1.plot(effective_ctx / 1000, ppl_no_ext, 'r-x', lw=2, ms=8, label='RoPE ì™¸ì‚½ (í•™ìŠµ ì—†ìŒ)')
ax1.plot(effective_ctx / 1000, ppl_ntk, 'orange', lw=2, marker='D', ms=7, label='NTK-aware')
ax1.plot(effective_ctx / 1000, ppl_pi, 'b-s', lw=2, ms=7, label='Position Interpolation')
ax1.plot(effective_ctx / 1000, ppl_yarn, 'g-o', lw=2.5, ms=8, label='YaRN')

ax1.set_xlabel('ìœ íš¨ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (K)', fontsize=11)
ax1.set_ylabel('Perplexity', fontsize=11)
ax1.set_title('ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë°©ë²•ë³„ Perplexity', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.set_xscale('log')
ax1.set_ylim(4, 30)

# (2) YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì •
ax2 = axes[1]
d_model = 128
n_dims = d_model // 2
freqs = 10000 ** (-2 * np.arange(n_dims) / d_model)
wavelengths = 2 * np.pi / freqs

scale = 4  # 4x í™•ì¥
threshold_low = base_ctx
threshold_high = base_ctx * scale

freqs_yarn = np.copy(freqs)
for i in range(n_dims):
    wl = wavelengths[i]
    if wl < threshold_low:
        freqs_yarn[i] = freqs[i]  # ê³ ì£¼íŒŒ: ë³€ê²½ ì—†ìŒ
    elif wl > threshold_high:
        freqs_yarn[i] = freqs[i] / scale  # ì €ì£¼íŒŒ: ìŠ¤ì¼€ì¼ ë‹¤ìš´
    else:
        gamma = (wl - threshold_low) / (threshold_high - threshold_low)
        freqs_yarn[i] = freqs[i] * ((1 - gamma) + gamma / scale)

ax2.plot(range(n_dims), freqs, 'b-', lw=2, alpha=0.5, label='ì›ë˜ RoPE')
ax2.plot(range(n_dims), freqs_yarn, 'g-', lw=2.5, label='YaRN (4x)')
ax2.set_xlabel('ì°¨ì› ì¸ë±ìŠ¤', fontsize=11)
ax2.set_ylabel('ì£¼íŒŒìˆ˜', fontsize=11)
ax2.set_title('YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì • (4x í™•ì¥)', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_yscale('log')

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/yarn_context_extension.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/yarn_context_extension.png")

print(f"\\nPerplexity ë¹„êµ (ê¸°ë³¸ = {ppl_base}):")
print(f"{'í™•ì¥ ë¹„ìœ¨':>10} | {'ìœ íš¨ ê¸¸ì´':>10} | {'RoPE ì™¸ì‚½':>10} | {'NTK':>8} | {'PI':>8} | {'YaRN':>8}")
print("-" * 65)
for i, ratio in enumerate(extension_ratios):
    ctx = base_ctx * ratio
    print(f"{ratio:>10}x | {ctx:>10,} | {ppl_no_ext[i]:>10.2f} | {ppl_ntk[i]:>8.2f} | "
          f"{ppl_pi[i]:>8.2f} | {ppl_yarn[i]:>8.2f}")
print(f"\\nâ†’ YaRNì€ 32x í™•ì¥ì—ì„œë„ PPL ì¦ê°€ê°€ {(ppl_yarn[-1]/ppl_base - 1)*100:.1f}%ì— ë¶ˆê³¼")"""))

# â”€â”€ Cell 12: Section 5 - Cost reduction analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. 50% ì´ìƒ ë¹„ìš© ì ˆê° ë¶„ì„ <a name='5.-ë¹„ìš©-ì ˆê°-ë¶„ì„'></a>

SWA + Sparse Attentionì„ ê²°í•©í•œ ì‹¤ì „ ë¹„ìš© ì ˆê° ë¶„ì„ì…ë‹ˆë‹¤.

**DeepSeek-V3 ì‹¤ì œ ì„¤ì •:**
- ì¼ë¶€ ë ˆì´ì–´: Full Attention (ì „ì—­ ì˜ì¡´ì„±)
- ëŒ€ë¶€ë¶„ ë ˆì´ì–´: SWA ($w=4096$) + Sparse Block ($K=4, b=512$)
- ì´ ë¹„ìš© ì ˆê°: $> 50\%$"""))

cells.append(code("""\
# â”€â”€ ë¹„ìš© ì ˆê° ì¢…í•© ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤ì „ ëª¨ë¸ ì„¤ì • ê¸°ë°˜ ë¹„ìš© ë¶„ì„
N_values = [4096, 8192, 16384, 32768, 65536, 131072]

# ëª¨ë¸ ì„¤ì •
n_total_layers = 61  # DeepSeek-V3
n_full_layers = 4  # Full attention ë ˆì´ì–´
n_swa_layers = n_total_layers - n_full_layers  # SWA + sparse ë ˆì´ì–´
w = 4096  # SWA ìœˆë„ìš°
K_blocks = 4  # Top-K ë¸”ë¡
b = 512  # ë¸”ë¡ í¬ê¸°

print(f"DeepSeek-V3 ìŠ¤íƒ€ì¼ ë¹„ìš© ë¶„ì„:")
print(f"  ì´ ë ˆì´ì–´: {n_total_layers} (Full: {n_full_layers}, SWA+Sparse: {n_swa_layers})")
print(f"  SWA ìœˆë„ìš°: {w}, Top-K ë¸”ë¡: {K_blocks}, ë¸”ë¡ í¬ê¸°: {b}")
print()

print(f"{'ì‹œí€€ìŠ¤':>8} | {'Full Only':>12} | {'Hybrid':>12} | {'ì ˆê°ë¥ ':>8} | {'>50%':>5}")
print("-" * 55)

savings_list = []

for N in N_values:
    # Full attention ë¹„ìš© (ëª¨ë“  ë ˆì´ì–´)
    full_cost = n_total_layers * N * N

    # Hybrid ë¹„ìš©
    full_layer_cost = n_full_layers * N * N
    swa_sparse_cost = n_swa_layers * N * min(w + K_blocks * b, N)
    hybrid_cost = full_layer_cost + swa_sparse_cost

    saving = (1 - hybrid_cost / full_cost) * 100
    savings_list.append(saving)
    check = 'âœ…' if saving > 50 else 'âŒ'
    print(f"{N:>8,} | {full_cost/1e12:>10.2f}T | {hybrid_cost/1e12:>10.2f}T | {saving:>7.1f}% | {check:>5}")

# ë¹„ìš© ë¶„í•´ ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ë¹„ìš© êµ¬ì„± (ìŠ¤íƒ ë°” ì°¨íŠ¸)
ax1 = axes[0]
x_pos = range(len(N_values))
full_attn_cost = [n_full_layers * N * N / 1e12 for N in N_values]
swa_cost = [n_swa_layers * N * min(w, N) / 1e12 for N in N_values]
sparse_cost = [n_swa_layers * N * min(K_blocks * b, N) / 1e12 for N in N_values]

ax1.bar(x_pos, full_attn_cost, color='red', alpha=0.7, label='Full Attn ë ˆì´ì–´')
ax1.bar(x_pos, swa_cost, bottom=full_attn_cost, color='blue', alpha=0.7, label='SWA')
bottoms = [f + s for f, s in zip(full_attn_cost, swa_cost)]
ax1.bar(x_pos, sparse_cost, bottom=bottoms, color='green', alpha=0.7, label='Sparse Block')

ax1.set_xticks(x_pos)
ax1.set_xticklabels([f'{N//1000}K' for N in N_values], fontsize=9)
ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´', fontsize=11)
ax1.set_ylabel('ì—°ì‚° ë¹„ìš© (T ì—°ì‚°)', fontsize=11)
ax1.set_title('Hybrid Attention ë¹„ìš© êµ¬ì„±', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')

# (2) ì ˆê°ë¥ 
ax2 = axes[1]
ax2.bar(x_pos, savings_list, color=['orange' if s < 50 else 'green' for s in savings_list],
        alpha=0.7, edgecolor='black')
ax2.axhline(y=50, color='red', ls='--', lw=2, label='50% ê¸°ì¤€')
for i, s in enumerate(savings_list):
    ax2.text(i, s + 1, f'{s:.1f}%', ha='center', fontsize=10, fontweight='bold')
ax2.set_xticks(x_pos)
ax2.set_xticklabels([f'{N//1000}K' for N in N_values], fontsize=9)
ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´', fontsize=11)
ax2.set_ylabel('ë¹„ìš© ì ˆê°ë¥  (%)', fontsize=11)
ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë¹„ìš© ì ˆê°ë¥ ', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')
ax2.set_ylim(0, 100)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/cost_reduction_analysis.png', dpi=100, bbox_inches='tight')
plt.close()
print("\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/cost_reduction_analysis.png")

# ìµœì¢… ê²°ë¡ 
print(f"\\nê²°ë¡ :")
print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´ 4K: ì ˆê°ë¥  = {savings_list[0]:.1f}% (ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œëŠ” íš¨ê³¼ ì œí•œì )")
over_50 = sum(1 for s in savings_list if s > 50)
print(f"  50% ì´ìƒ ì ˆê° ë‹¬ì„±: {over_50}/{len(N_values)} ì„¤ì •")
print(f"  ì‹œí€€ìŠ¤ ê¸¸ì´ 128K: ì ˆê°ë¥  = {savings_list[-1]:.1f}%")
print(f"  â†’ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ SWA + Sparse Attentionì˜ íš¨ê³¼ê°€ ê·¹ëŒ€í™”ë¨")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| YaRN | ì£¼íŒŒìˆ˜ ì¬ì¡°ì •ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì°½ í™•ì¥ | â­â­â­ |
| Sliding Window (SWA) | $\|i-j\| \leq w/2$ ë§ˆìŠ¤í¬ë¡œ ì§€ì—­ íŒ¨í„´ í¬ì°© | â­â­â­ |
| DeepSeek Sparse Attn | Top-K ë¸”ë¡ ì„ íƒìœ¼ë¡œ ì¤‘ìš” ì •ë³´ë§Œ attend | â­â­â­ |
| Hybrid ì „ëµ | Full + SWA + Sparse ë ˆì´ì–´ë³„ í˜¼í•© | â­â­â­ |
| ë¹„ìš© ì ˆê° | ê¸´ ì‹œí€€ìŠ¤ì—ì„œ $>50\%$ ì ˆê° ë‹¬ì„± | â­â­â­ |
| ë§ˆìŠ¤í¬ ë°€ë„ | Full(50%) â†’ SWA(~6%) â†’ Sparse(~20%) | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\theta_i' = \theta_i / s \quad \text{(YaRN ì €ì£¼íŒŒ ì¬ì¡°ì •)}$$

$$\text{SWA}: A_{ij} \neq 0 \iff |i-j| \leq w/2 \text{ and } j \leq i$$

$$\text{ë¹„ìš© ì ˆê°} = 1 - \frac{n_{full} \cdot N + n_{swa} \cdot (w + Kb)}{n_{total} \cdot N}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**Chapter 17: Diffusion Transformers** â€” DiT ì•„í‚¤í…ì²˜ì˜ adaLN-Zero ìˆ˜ì‹ê³¼ Flow Matching, HunyuanVideoì˜ 3D ë¹„ë””ì˜¤ ì¸ì½”ë”© ì•„í‚¤í…ì²˜ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter16_sparse_attention/04_long_context_and_sparse_attn.ipynb')
