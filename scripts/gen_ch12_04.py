"""Generate Chapter 12-04: MoE Routing and Load Balancing."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

cells = [
md("""# Chapter 12: ìµœì‹  LLM ì•„í‚¤í…ì²˜ â€” MoE ë¼ìš°íŒ…ê³¼ ë¶€í•˜ ê· í˜•

## í•™ìŠµ ëª©í‘œ
- Mixture of Experts(MoE)ì˜ **Top-k ê²Œì´íŒ… ìˆ˜ì‹**ê³¼ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- **Softmax ê²Œì´íŒ…**ê³¼ **Linear ê²Œì´íŒ…**ì˜ ì°¨ì´ë¥¼ ë¹„êµí•˜ê³  êµ¬í˜„í•œë‹¤
- **Auxiliary Loss (ë³´ì¡° ì†ì‹¤)**ì˜ ìˆ˜í•™ì  ë„ì¶œê³¼ ë¶€í•˜ ê· í˜• íš¨ê³¼ë¥¼ ê²€ì¦í•œë‹¤
- **Expert Capacity Factor**ê°€ MoE ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: MoE ê²Œì´íŒ…ê³¼ Auxiliary Loss](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Top-k ë¼ìš°í„° êµ¬í˜„](#2.-Top-k-ë¼ìš°í„°)
3. [Auxiliary Lossì™€ ë¶€í•˜ ê· í˜•](#3.-Auxiliary-Loss)
4. [Expert Capacity Factor ì‹¤í—˜](#4.-Expert-Capacity)
5. [ì •ë¦¬](#5.-ì •ë¦¬)"""),

md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### MoE ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜

MoE ë ˆì´ì–´ëŠ” $N$ê°œì˜ ì „ë¬¸ê°€(Expert) ì¤‘ **Top-kê°œë§Œ í™œì„±í™”**í•©ë‹ˆë‹¤:

$$y = \sum_{i \in \text{Top-k}} g_i \cdot E_i(x)$$

**ê²Œì´íŒ… ê°€ì¤‘ì¹˜ ê³„ì‚°:**

$$\mathbf{h} = x \cdot W_g, \quad W_g \in \mathbb{R}^{d_{model} \times N}$$

$$g_i = \frac{e^{h_i}}{\sum_{j \in \text{Top-k}} e^{h_j}} \quad (\text{Top-k ì„ íƒ í›„ ì¬ì •ê·œí™”})$$

- $x$: ì…ë ¥ í† í° íˆë“  ë²¡í„°
- $W_g$: ê²Œì´íŒ… ê°€ì¤‘ì¹˜ (ë¼ìš°í„°)
- $E_i$: $i$ë²ˆì§¸ ì „ë¬¸ê°€ FFN
- $g_i$: $i$ë²ˆì§¸ ì „ë¬¸ê°€ì˜ ê¸°ì—¬ ê°€ì¤‘ì¹˜

### Auxiliary Loss (ë¶€í•˜ ê· í˜• ë³´ì¡° ì†ì‹¤)

ì „ë¬¸ê°€ì—ê²Œ í† í°ì´ **ê· ë“±í•˜ê²Œ ë¶„ë°°**ë˜ë„ë¡ ìœ ë„í•˜ëŠ” ì†ì‹¤:

$$L_{aux} = \alpha \cdot N \sum_{i=1}^{N} f_i \cdot P_i$$

- $f_i = \frac{\text{expert } i\text{ì— ë¼ìš°íŒ…ëœ í† í° ìˆ˜}}{\text{ì „ì²´ í† í° ìˆ˜}}$ (ì‹¤ì œ ë¶„ë°° ë¹„ìœ¨)
- $P_i = \frac{1}{T}\sum_{x \in \mathcal{B}} p_i(x)$ (ë¼ìš°íŒ… í™•ë¥  í‰ê· , $p_i(x) = \text{softmax}(h)_i$)
- $\alpha$: ë³´ì¡° ì†ì‹¤ ê³„ìˆ˜ (ë³´í†µ $10^{-2}$)
- $N$: ì „ë¬¸ê°€ ìˆ˜

**ê· ë“± ë¶„ë°° ì‹œ ìµœì†Ÿê°’:** $f_i = P_i = 1/N$ â†’ $L_{aux} = \alpha \cdot N \cdot N \cdot (1/N)^2 = \alpha$

**ìš”ì•½ í‘œ:**

| í•­ëª© | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| ê²Œì´íŒ… | $g_i = \text{softmax}(\text{Top-k}(xW_g))_i$ | kê°œ ì „ë¬¸ê°€ ì„ íƒ |
| Expert ì¶œë ¥ | $y = \sum_{i \in \text{Top-k}} g_i \cdot E_i(x)$ | ê°€ì¤‘ í•©ì‚° |
| Aux Loss | $L_{aux} = \alpha N \sum f_i P_i$ | ë¶€í•˜ ê· í˜• |
| Capacity Factor | $C = \frac{k \cdot T}{N}$ | ì „ë¬¸ê°€ë‹¹ ìµœëŒ€ í† í° |

---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ MoE ì¹œì ˆ ì„¤ëª…!

#### ğŸ³ MoEê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: **ë·”í˜ ë ˆìŠ¤í† ë‘**ì„ ìƒìƒí•´ë³´ì„¸ìš”!
> - ìš”ë¦¬ì‚¬(Expert) 8ëª…ì´ ê°ê° ë‹¤ë¥¸ ìš”ë¦¬ë¥¼ ì „ë¬¸ìœ¼ë¡œ í•´ìš”
> - ì†ë‹˜(Token)ì´ ì˜¤ë©´ **2ëª…ì˜ ìš”ë¦¬ì‚¬ë§Œ ê³¨ë¼ì„œ** ìŒì‹ì„ ë°›ì•„ìš” (Top-2)
> - ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  ìš”ë¦¬ì‚¬ê°€ ë™ì‹œì— ì¼í•˜ì§€ ì•Šì•„ë„ ë˜ë‹ˆê¹Œ ë¹ ë¥´ê³  íš¨ìœ¨ì !

#### âš–ï¸ Auxiliary LossëŠ” ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ë§Œì•½ ëª¨ë“  ì†ë‹˜ì´ **í”¼ì ìš”ë¦¬ì‚¬ë§Œ ì°¾ì•„ê°€ë©´** ë‹¤ë¥¸ ìš”ë¦¬ì‚¬ëŠ” ë†€ê³  ìˆì–ì•„ìš”!
> Auxiliary LossëŠ” "ì†ë‹˜ì„ ê³¨ê³ ë£¨ ë‚˜ëˆ ì£¼ì„¸ìš”!"ë¼ê³  ì•Œë ¤ì£¼ëŠ” **ê³µì • ë¶„ë°° ê·œì¹™**ì´ì—ìš”.

| ìƒí™© | ë¹„ìœ  | ê²°ê³¼ |
|------|------|------|
| Aux Loss ì—†ìŒ | ì¸ê¸° ìˆëŠ” ìš”ë¦¬ì‚¬ë§Œ ë°”ì¨ | ë¶ˆê· í˜• â†’ ì„±ëŠ¥â†“ |
| Aux Loss ìˆìŒ | ì†ë‹˜ì„ ê³¨ê³ ë£¨ ë°°ë¶„ | ê· í˜• â†’ íš¨ìœ¨â†‘ |"""),

md(r"""---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Top-2 ê²Œì´íŒ… ê³„ì‚°

ì…ë ¥ $h = [2.0, 1.0, 0.5, 3.0]$ (ì „ë¬¸ê°€ 4ëª…)ì—ì„œ Top-2ë¥¼ ì„ íƒí•˜ê³  ê²Œì´íŒ… ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

Top-2 ì„ íƒ: Expert 3 ($h_3=3.0$), Expert 0 ($h_0=2.0$)

ì¬ì •ê·œí™”: $g_3 = \frac{e^{3.0}}{e^{3.0}+e^{2.0}} = \frac{20.09}{20.09+7.39} = 0.731$

$g_0 = \frac{e^{2.0}}{e^{3.0}+e^{2.0}} = \frac{7.39}{27.48} = 0.269$

ì¶œë ¥: $y = 0.731 \cdot E_3(x) + 0.269 \cdot E_0(x)$
</details>

#### ë¬¸ì œ 2: Auxiliary Loss ê³„ì‚°

ì „ë¬¸ê°€ 4ëª…, $\alpha=0.01$, ë°°ì¹˜ ë‚´ ë¶„ë°°: $f=[0.5, 0.1, 0.1, 0.3]$, $P=[0.4, 0.2, 0.1, 0.3]$ì¼ ë•Œ $L_{aux}$ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$L_{aux} = 0.01 \times 4 \times (0.5 \times 0.4 + 0.1 \times 0.2 + 0.1 \times 0.1 + 0.3 \times 0.3)$$
$$= 0.04 \times (0.20 + 0.02 + 0.01 + 0.09) = 0.04 \times 0.32 = 0.0128$$

ê· ë“± ë¶„ë°° ì‹œ: $0.01 \times 4 \times 4 \times (1/4)^2 = 0.01$ â†’ í˜„ì¬ ë¶ˆê· í˜•!
</details>"""),

code("""import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")"""),

md("""## 2. Top-k ë¼ìš°í„° êµ¬í˜„ <a name='2.-Top-k-ë¼ìš°í„°'></a>"""),

code(r"""# â”€â”€ Top-k MoE ë¼ìš°í„° êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Top-k ê²Œì´íŒ…ìœ¼ë¡œ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•˜ê³  ì¶œë ¥ì„ í•©ì‚°í•©ë‹ˆë‹¤

class MoERouter(tf.keras.layers.Layer):
    # Top-k MoE Router with gating
    def __init__(self, d_model, n_experts, top_k=2):
        super().__init__()
        self.n_experts = n_experts
        self.top_k = top_k
        self.gate = tf.keras.layers.Dense(n_experts, use_bias=False, name='gate')

    def call(self, x):
        # x: [B, S, d_model]
        logits = self.gate(x)  # [B, S, n_experts]
        
        # Top-k ì„ íƒ
        top_k_logits, top_k_indices = tf.math.top_k(logits, k=self.top_k)
        
        # Top-kì— ëŒ€í•´ì„œë§Œ softmax (ì¬ì •ê·œí™”)
        top_k_gates = tf.nn.softmax(top_k_logits, axis=-1)  # [B, S, k]
        
        # ë¼ìš°íŒ… í™•ë¥  (ì „ì²´ softmax, Aux Loss ê³„ì‚°ìš©)
        routing_probs = tf.nn.softmax(logits, axis=-1)  # [B, S, N]
        
        return top_k_gates, top_k_indices, routing_probs


class MoELayer(tf.keras.layers.Layer):
    # Mixture of Experts Layer
    def __init__(self, d_model, d_ff, n_experts, top_k=2):
        super().__init__()
        self.n_experts = n_experts
        self.top_k = top_k
        self.router = MoERouter(d_model, n_experts, top_k)
        # ê° ExpertëŠ” ê°„ë‹¨í•œ FFN
        self.experts = [
            tf.keras.Sequential([
                tf.keras.layers.Dense(d_ff, activation='relu'),
                tf.keras.layers.Dense(d_model)
            ], name=f'expert_{i}')
            for i in range(n_experts)
        ]

    def call(self, x):
        B, S, D = x.shape
        gates, indices, routing_probs = self.router(x)
        
        # ê° í† í°ì— ëŒ€í•´ ì„ íƒëœ Expert ì¶œë ¥ì„ ê°€ì¤‘ í•©ì‚°
        # (ì‹¤ì œ êµ¬í˜„ì€ scatter/gather ìµœì í™”, ì—¬ê¸°ì„œëŠ” ëª…ì‹œì  ë£¨í”„)
        output = tf.zeros_like(x)
        
        for k_idx in range(self.top_k):
            expert_indices = indices[:, :, k_idx]   # [B, S]
            expert_gates = gates[:, :, k_idx:k_idx+1]  # [B, S, 1]
            
            for e_idx in range(self.n_experts):
                mask = tf.cast(tf.equal(expert_indices, e_idx), tf.float32)
                mask = mask[:, :, tf.newaxis]  # [B, S, 1]
                
                if tf.reduce_sum(mask) > 0:
                    expert_out = self.experts[e_idx](x)  # [B, S, D]
                    output += expert_out * mask * expert_gates
        
        return output, routing_probs


# í…ŒìŠ¤íŠ¸
d_model, d_ff, n_experts, top_k = 256, 512, 8, 2
moe = MoELayer(d_model, d_ff, n_experts, top_k)

x_test = tf.random.normal((2, 32, d_model))
output, probs = moe(x_test)

print("=" * 60)
print(f"MoE Layer (N={n_experts} experts, Top-{top_k})")
print("=" * 60)
print(f"ì…ë ¥ shape:  {x_test.shape}")
print(f"ì¶œë ¥ shape:  {output.shape}")
print()

# ë¼ìš°íŒ… í†µê³„
gates, indices, _ = moe.router(x_test)
flat_indices = tf.reshape(indices, [-1]).numpy()

print("ë¼ìš°íŒ… ë¶„í¬:")
for i in range(n_experts):
    count = np.sum(flat_indices == i)
    pct = count / len(flat_indices) * 100
    bar = '#' * int(pct / 2)
    print(f"  Expert {i}: {count:>4} tokens ({pct:>5.1f}%) {bar}")"""),

code(r"""# â”€â”€ ê²Œì´íŒ… ê°€ì¤‘ì¹˜ ë¶„í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# ì™¼ìª½: Expertë³„ ë¼ìš°íŒ… ë¹ˆë„
ax1 = axes[0]
expert_counts = [np.sum(flat_indices == i) for i in range(n_experts)]
colors_exp = plt.cm.Set3(np.linspace(0, 1, n_experts))
bars = ax1.bar(range(n_experts), expert_counts, color=colors_exp, edgecolor='black', lw=1)
ax1.axhline(y=len(flat_indices) / n_experts, color='red', ls='--', lw=2, label='ê· ë“± ë¶„ë°°')
ax1.set_xlabel('Expert ì¸ë±ìŠ¤', fontsize=11)
ax1.set_ylabel('ë¼ìš°íŒ…ëœ í† í° ìˆ˜', fontsize=11)
ax1.set_title(f'Expertë³„ ë¼ìš°íŒ… ë¹ˆë„ (Top-{top_k})', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')

# ì˜¤ë¥¸ìª½: ê²Œì´íŒ… ê°€ì¤‘ì¹˜ ë¶„í¬
ax2 = axes[1]
gate_values = gates.numpy().flatten()
ax2.hist(gate_values, bins=30, color='#1E88E5', edgecolor='black', alpha=0.7)
ax2.axvline(x=0.5, color='red', ls='--', lw=2, label='ê· ë“± (0.5)')
ax2.set_xlabel('ê²Œì´íŒ… ê°€ì¤‘ì¹˜ g_i', fontsize=11)
ax2.set_ylabel('ë¹ˆë„', fontsize=11)
ax2.set_title('Top-2 ê²Œì´íŒ… ê°€ì¤‘ì¹˜ ë¶„í¬', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter12_modern_llms/moe_routing.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/moe_routing.png")"""),

md("""## 3. Auxiliary Lossì™€ ë¶€í•˜ ê· í˜• <a name='3.-Auxiliary-Loss'></a>"""),

code(r"""# â”€â”€ Auxiliary Loss êµ¬í˜„ ë° íš¨ê³¼ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Aux Lossê°€ Expert ë¶€í•˜ ê· í˜•ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤

def compute_aux_loss(routing_probs, indices, n_experts, alpha=0.01):
    # routing_probs: [B, S, N] - ì „ì²´ softmax í™•ë¥ 
    # indices: [B, S, k] - ì„ íƒëœ expert ì¸ë±ìŠ¤
    B, S, N = routing_probs.shape
    T = B * S  # ì „ì²´ í† í° ìˆ˜
    
    # f_i: expert iì— ì‹¤ì œ ë¼ìš°íŒ…ëœ í† í° ë¹„ìœ¨
    flat_indices = tf.reshape(indices, [-1])
    f = tf.zeros(N)
    for i in range(N):
        f_i = tf.reduce_sum(tf.cast(tf.equal(flat_indices, i), tf.float32))
        f = tf.tensor_scatter_nd_update(f, [[i]], [f_i / tf.cast(T, tf.float32)])
    
    # P_i: expert iì˜ í‰ê·  ë¼ìš°íŒ… í™•ë¥ 
    P = tf.reduce_mean(routing_probs, axis=[0, 1])  # [N]
    
    # L_aux = alpha * N * sum(f_i * P_i)
    loss = alpha * tf.cast(N, tf.float32) * tf.reduce_sum(f * P)
    return loss, f, P

# ë¶ˆê· í˜• vs ê· í˜• ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ
print("=" * 65)
print("Auxiliary Loss íš¨ê³¼ ë¶„ì„")
print("=" * 65)

# ì‹œë‚˜ë¦¬ì˜¤ 1: Aux Loss ì—†ì´ í•™ìŠµ (ë¶ˆê· í˜• ë°œìƒ)
# ì¸ìœ„ì ìœ¼ë¡œ ë¶ˆê· í˜•í•œ ë¼ìš°íŒ… ìƒì„±
logits_biased = tf.constant([[
    [5.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],  # Expert 0 ì„ í˜¸
    [4.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [4.5, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [3.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
] * 8])  # [1, 32, 8]
probs_biased = tf.nn.softmax(logits_biased, axis=-1)
_, indices_biased = tf.math.top_k(logits_biased, k=2)
loss_biased, f_biased, P_biased = compute_aux_loss(probs_biased, indices_biased, n_experts=8)

# ì‹œë‚˜ë¦¬ì˜¤ 2: ê· í˜• ì¡íŒ ë¼ìš°íŒ…
logits_balanced = tf.random.normal((1, 32, 8))
probs_balanced = tf.nn.softmax(logits_balanced, axis=-1)
_, indices_balanced = tf.math.top_k(logits_balanced, k=2)
loss_balanced, f_balanced, P_balanced = compute_aux_loss(probs_balanced, indices_balanced, n_experts=8)

print(f"{'ì‹œë‚˜ë¦¬ì˜¤':<20} | {'Aux Loss':>12} | {'f ìµœëŒ€/ìµœì†Œ':>15} | {'ë¶ˆê· í˜•ë„':>10}")
print("-" * 65)
f_b = f_biased.numpy()
f_bl = f_balanced.numpy()
print(f"{'ë¶ˆê· í˜• (í¸í–¥)':<20} | {loss_biased.numpy():>12.4f} | {f_b.max():.3f}/{f_b.min():.3f} | {'ë†’ìŒ':>10}")
print(f"{'ê· í˜• (ëœë¤)':<20} | {loss_balanced.numpy():>12.4f} | {f_bl.max():.3f}/{f_bl.min():.3f} | {'ë‚®ìŒ':>10}")
print()
print("Aux Lossê°€ ë†’ì„ìˆ˜ë¡ ë¶ˆê· í˜• â†’ Lossë¥¼ ìµœì†Œí™”í•˜ë©´ ìë™ìœ¼ë¡œ ê· í˜• ìœ ë„!")
print()

# Expertë³„ ë¶„ë°° ë¹„êµ
print("Expertë³„ í† í° ë¶„ë°° ë¹„ìœ¨ (f_i):")
print(f"  {'Expert':<10}", end='')
for i in range(8):
    print(f" | {i:>6}", end='')
print()
print(f"  {'ë¶ˆê· í˜•':<10}", end='')
for v in f_b:
    print(f" | {v:>5.1%}", end='')
print()
print(f"  {'ê· í˜•':<10}", end='')
for v in f_bl:
    print(f" | {v:>5.1%}", end='')
print()"""),

md("""## 4. Expert Capacity Factor ì‹¤í—˜ <a name='4.-Expert-Capacity'></a>"""),

code(r"""# â”€â”€ Expert Capacity Factor ì‹¤í—˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Capacityê°€ Expert í™œìš©ë¥ ê³¼ í† í° ë“œë¡­ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤

def simulate_capacity(n_tokens, n_experts, top_k, capacity_factor):
    # Capacity: ê° Expertê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜
    capacity = int(capacity_factor * top_k * n_tokens / n_experts)
    
    # ëœë¤ ë¼ìš°íŒ… (ì•½ê°„ì˜ í¸í–¥ í¬í•¨)
    logits = np.random.randn(n_tokens, n_experts)
    logits[:, 0] += 1.0  # Expert 0ì— ì•½ê°„ì˜ í¸í–¥
    
    top_k_indices = np.argsort(-logits, axis=-1)[:, :top_k]
    
    # Expertë³„ í• ë‹¹ (Capacity ì œí•œ)
    expert_counts = np.zeros(n_experts, dtype=int)
    assigned = 0
    dropped = 0
    
    for token in range(n_tokens):
        for k in range(top_k):
            e = top_k_indices[token, k]
            if expert_counts[e] < capacity:
                expert_counts[e] += 1
                assigned += 1
            else:
                dropped += 1
    
    total_assignments = n_tokens * top_k
    utilization = assigned / total_assignments
    drop_rate = dropped / total_assignments
    
    return capacity, utilization, drop_rate, expert_counts

n_tokens = 1024
n_experts = 8
top_k = 2

capacity_factors = [0.5, 0.75, 1.0, 1.25, 1.5, 2.0]
results = []

print("=" * 70)
print(f"Expert Capacity Factor ì‹¤í—˜ (T={n_tokens}, N={n_experts}, k={top_k})")
print("=" * 70)
print(f"{'CF':>5} | {'Capacity':>10} | {'í™œìš©ë¥ ':>8} | {'ë“œë¡­ë¥ ':>8} | {'Expert ë¶„í¬ í¸ì°¨':>15}")
print("-" * 70)

for cf in capacity_factors:
    cap, util, drop, counts = simulate_capacity(n_tokens, n_experts, top_k, cf)
    std = np.std(counts)
    results.append((cf, cap, util, drop, std))
    print(f"{cf:>5.2f} | {cap:>10} | {util:>7.1%} | {drop:>7.1%} | {std:>15.1f}")

print()
print("í•µì‹¬ ê´€ì°°:")
print("  â€¢ CF < 1.0: ìš©ëŸ‰ ë¶€ì¡± â†’ í† í° ë“œë¡­ ë°œìƒ (ì •ë³´ ì†ì‹¤)")
print("  â€¢ CF = 1.0: ì´ë¡ ìƒ ë”± ë§ì§€ë§Œ, ë¶ˆê· í˜• ì‹œ ì¼ë¶€ ë“œë¡­")
print("  â€¢ CF > 1.0: ì—¬ìœ  ìˆì§€ë§Œ, ë©”ëª¨ë¦¬/ì—°ì‚° ë‚­ë¹„ ì¦ê°€")
print("  â€¢ ì‹¤ì „ ê¶Œì¥: CF = 1.0~1.25")"""),

code(r"""# â”€â”€ Capacity Factor ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# ì™¼ìª½: CFë³„ í™œìš©ë¥ /ë“œë¡­ë¥ 
ax1 = axes[0]
cfs = [r[0] for r in results]
utils = [r[2] for r in results]
drops = [r[3] for r in results]

ax1.plot(cfs, utils, 'b-o', lw=2.5, ms=8, label='í™œìš©ë¥ ')
ax1.plot(cfs, drops, 'r-s', lw=2.5, ms=8, label='ë“œë¡­ë¥ ')
ax1.axvline(x=1.0, color='gray', ls='--', lw=1.5, label='CF=1.0')
ax1.set_xlabel('Capacity Factor', fontsize=11)
ax1.set_ylabel('ë¹„ìœ¨', fontsize=11)
ax1.set_title('CFë³„ Expert í™œìš©ë¥  vs ë“œë¡­ë¥ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: Expertë³„ í† í° ë¶„ë°° (CF=1.0 vs CF=1.5)
ax2 = axes[1]
_, _, _, counts_1 = simulate_capacity(n_tokens, n_experts, top_k, 1.0)
_, _, _, counts_15 = simulate_capacity(n_tokens, n_experts, top_k, 1.5)

x_pos = np.arange(n_experts)
width = 0.35
ax2.bar(x_pos - width/2, counts_1, width, label='CF=1.0', color='#1E88E5', edgecolor='black')
ax2.bar(x_pos + width/2, counts_15, width, label='CF=1.5', color='#43A047', edgecolor='black')
ax2.axhline(y=n_tokens * top_k / n_experts, color='red', ls='--', lw=2, label='ì´ìƒì  ê· ë“±')
ax2.set_xlabel('Expert ì¸ë±ìŠ¤', fontsize=11)
ax2.set_ylabel('í• ë‹¹ëœ í† í° ìˆ˜', fontsize=11)
ax2.set_title('CFë³„ Expert í† í° ë¶„ë°°', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('chapter12_modern_llms/moe_capacity.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/moe_capacity.png")"""),

md(r"""## 5. ì •ë¦¬ <a name='5.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Top-k ê²Œì´íŒ… | $N$ê°œ Expert ì¤‘ $k$ê°œë§Œ í™œì„±í™” â†’ ì—°ì‚° íš¨ìœ¨ | â­â­â­ |
| Auxiliary Loss | $L_{aux} = \alpha N \sum f_i P_i$ â†’ Expert ë¶€í•˜ ê· í˜• ìœ ë„ | â­â­â­ |
| Capacity Factor | Expertë‹¹ ìµœëŒ€ í† í° ìˆ˜ ì œí•œ â†’ ë©”ëª¨ë¦¬/ë“œë¡­ íŠ¸ë ˆì´ë“œì˜¤í”„ | â­â­ |
| Softmax ê²Œì´íŒ… | Top-k ì„ íƒ í›„ ì¬ì •ê·œí™” â†’ ë¶€ë“œëŸ¬ìš´ ê°€ì¤‘ì¹˜ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$y = \sum_{i \in \text{Top-k}} g_i \cdot E_i(x), \quad g_i = \frac{e^{h_i}}{\sum_{j \in \text{Top-k}} e^{h_j}}$$

$$L_{aux} = \alpha \cdot N \sum_{i=1}^{N} f_i \cdot P_i$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**Chapter 12-05: DeepSeek-V3 MoE ì•„í‚¤í…ì²˜** â€” Shared Expert, Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±, Multi-Token Prediction ë“± ìµœì‹  MoE í˜ì‹ ì„ ë‹¤ë£¹ë‹ˆë‹¤."""),
]

create_notebook(cells, 'chapter12_modern_llms/04_moe_routing_and_load_balancing.ipynb')
