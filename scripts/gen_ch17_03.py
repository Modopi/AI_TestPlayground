"""Generate chapter17_diffusion_transformers/03_dit_conditioning_and_adaln.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 17: ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT â€” adaLN-Zero ì¡°ê±´ ì£¼ì…ê³¼ CFG

## í•™ìŠµ ëª©í‘œ
- adaLN-Zeroì˜ ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  í‘œì¤€ LayerNormê³¼ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤
- Zero-initializationì´ í•™ìŠµ ì•ˆì •ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œë‹¤
- ì‹œê°„ $t$, í´ë˜ìŠ¤, í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ ì£¼ì… ë°©ì‹ì„ ë¹„êµí•œë‹¤
- DiTì—ì„œì˜ Classifier-Free Guidance(CFG) ì„¤ê³„ë¥¼ ì´í•´í•œë‹¤
- TensorFlowë¡œ adaLN-Zero ë ˆì´ì–´ë¥¼ ì§ì ‘ êµ¬í˜„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: adaLN-Zeroì™€ ì¡°ê±´ ì£¼ì…](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •](#2.-í™˜ê²½-ì„¤ì •)
3. [adaLN-Zero ë ˆì´ì–´ êµ¬í˜„](#3.-adaLN-Zero-êµ¬í˜„)
4. [í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ](#4.-LayerNorm-vs-adaLN)
5. [Zero-Init í•™ìŠµ ì•ˆì •ì„± ì‹¤í—˜](#5.-Zero-Init-ì•ˆì •ì„±)
6. [ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨](#6.-ë‹¤ì¤‘-ì¡°ê±´-ì£¼ì…)
7. [DiTì—ì„œì˜ CFG ì„¤ê³„](#7.-CFG-ì„¤ê³„)
8. [ì •ë¦¬](#8.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### í‘œì¤€ LayerNorm

$$\text{LN}(x) = \gamma \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta$$

- $\gamma, \beta$: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼/ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„°
- $\mu, \sigma^2$: ì…ë ¥ì˜ í‰ê· , ë¶„ì‚° (ì±„ë„ ì¶•)

### Adaptive LayerNorm (adaLN)

ì¡°ê±´ ë²¡í„° $c$ (ì‹œê°„ $t$, í´ë˜ìŠ¤, í…ìŠ¤íŠ¸ ì„ë² ë”©)ë¡œë¶€í„° $\gamma, \beta$ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±:

$$\text{adaLN}(x, c) = \gamma_c \odot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta_c$$

$$(\gamma_c, \beta_c) = \text{MLP}(c)$$

### adaLN-Zero (DiTì˜ í•µì‹¬ ê¸°ë²•)

adaLNì— **ê²Œì´íŒ… íŒŒë¼ë¯¸í„°** $\alpha$ë¥¼ ì¶”ê°€í•˜ê³ , ì´ˆê¸°ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •:

$$h = x + \alpha_1 \cdot \text{Attn}\!\left((1 + \gamma_1) \odot \text{LN}(x) + \beta_1\right)$$

$$\text{output} = h + \alpha_2 \cdot \text{FFN}\!\left((1 + \gamma_2) \odot \text{LN}(h) + \beta_2\right)$$

ì—¬ê¸°ì„œ:

$$(\gamma_1, \beta_1, \alpha_1, \gamma_2, \beta_2, \alpha_2) = \text{MLP}(c), \quad c = \text{Embed}(t) + \text{Embed}(y)$$

- $t$: diffusion íƒ€ì„ìŠ¤í…
- $y$: í´ë˜ìŠ¤ ë ˆì´ë¸” ë˜ëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©
- $\alpha_1, \alpha_2$: ê²Œì´íŒ… ìŠ¤ì¼€ì¼ (ì´ˆê¸°ê°’ = 0)

### Zero-Initializationì˜ íš¨ê³¼

ì´ˆê¸° ìƒíƒœì—ì„œ $\alpha = 0$ì´ë©´:

$$h = x + 0 \cdot \text{Attn}(\cdots) = x \quad \text{(í•­ë“± í•¨ìˆ˜)}$$

- í•™ìŠµ ì‹œì‘ ì‹œ ê° DiT ë¸”ë¡ì´ **í•­ë“± í•¨ìˆ˜**ë¡œ ë™ì‘
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ì—†ì´** ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ ì‹œì‘
- ViT/ResNetì˜ ì”ì°¨ ì—°ê²°ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë” ê°•ë ¥í•œ ì´ˆê¸°í™” ì „ëµ

### Classifier-Free Guidance (CFG) in DiT

$$\tilde{\epsilon}_\theta(x_t, c) = (1 + w) \cdot \epsilon_\theta(x_t, c) - w \cdot \epsilon_\theta(x_t, \varnothing)$$

- $w$: guidance scale (ë†’ì„ìˆ˜ë¡ ì¡°ê±´ì— ì¶©ì‹¤, ë‹¤ì–‘ì„± ê°ì†Œ)
- $\varnothing$: null ì¡°ê±´ (í•™ìŠµ ì‹œ ì¼ì • ë¹„ìœ¨ë¡œ ì¡°ê±´ì„ drop)
- DiT í•™ìŠµ ì‹œ 10~20% í™•ë¥ ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ $\varnothing$ìœ¼ë¡œ ëŒ€ì²´

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| í‘œì¤€ LN | $\gamma \odot \text{norm}(x) + \beta$ | ê³ ì • íŒŒë¼ë¯¸í„° |
| adaLN | $\gamma_c \odot \text{norm}(x) + \beta_c$ | ì¡°ê±´ë¶€ íŒŒë¼ë¯¸í„° |
| adaLN-Zero | $x + \alpha \cdot f(\gamma_c \odot \text{norm}(x) + \beta_c)$ | ê²Œì´íŒ… + ì˜ì  ì´ˆê¸°í™” |
| CFG | $(1+w)\epsilon(x,c) - w\epsilon(x,\varnothing)$ | ì¡°ê±´ë¶€ ê°€ì´ë˜ìŠ¤ |

---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ adaLN-Zero ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ adaLN-Zeroê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìš”ë¦¬ì‚¬(ëª¨ë¸)ì—ê²Œ "ì§€ê¸ˆì€ ê²¨ìš¸ì´ê³ , ë§¤ìš´ë§›ì„ ì›í•´ìš”"ë¼ëŠ” ì£¼ë¬¸(ì¡°ê±´)ì„ ë°›ì•„ì„œ ì–‘ë…(íŒŒë¼ë¯¸í„°)ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ê²ƒ!

ë³´í†µ LayerNormì€ í•­ìƒ ê°™ì€ ì–‘ë…ì„ ì“°ì§€ë§Œ, adaLNì€ ì£¼ë¬¸ì— ë”°ë¼ ì–‘ë…ì„ ë°”ê¿‰ë‹ˆë‹¤.
ê·¸ë¦¬ê³  "Zero"ëŠ” ì²˜ìŒì— **ì•„ë¬´ê²ƒë„ ì•ˆ ë„£ê² ë‹¤**ëŠ” ëœ»ì´ì—ìš”.

#### ğŸ¤” ì™œ ì²˜ìŒì— 0ìœ¼ë¡œ ì‹œì‘í•˜ë‚˜ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìƒˆ ìš”ë¦¬ì‚¬ê°€ ì²˜ìŒ ì¼í•  ë•Œ, ì¼ë‹¨ ì›ë˜ ë ˆì‹œí”¼(ì…ë ¥)ë¥¼ ê·¸ëŒ€ë¡œ ë‚´ë³´ë‚´ë©´ì„œ ì²œì²œíˆ ìê¸°ë§Œì˜ ì–‘ë…ì„ ë°°ìš°ëŠ” ê±°ì˜ˆìš”!

ì²˜ìŒë¶€í„° ì´ìƒí•œ ì–‘ë…ì„ ë„£ìœ¼ë©´ ìš”ë¦¬ê°€ ë§í•˜ë“¯ì´, ì²˜ìŒì— Î±=0ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì•ˆì „í•˜ê²Œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: adaLN-Zero ì´ˆê¸° ì¶œë ¥

DiT ë¸”ë¡ì˜ ì…ë ¥ì´ $x = [1, 2, 3]$ì´ê³  ì´ˆê¸° ìƒíƒœì—ì„œ $\alpha_1 = 0$ì¼ ë•Œ, Attention ì´í›„ ì¶œë ¥ $h$ëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$h = x + \alpha_1 \cdot \text{Attn}(\cdots) = x + 0 \cdot \text{Attn}(\cdots) = [1, 2, 3]$$

$\alpha = 0$ì´ë¯€ë¡œ **í•­ë“± í•¨ìˆ˜**ê°€ ë©ë‹ˆë‹¤. ì…ë ¥ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.
ê¹Šì€ ë„¤íŠ¸ì›Œí¬(DiT-XL: 28ë¸”ë¡)ì—ì„œë„ ì´ˆê¸°ì— ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” í•µì‹¬ íŠ¸ë¦­ì…ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: CFG ì¶œë ¥ ê³„ì‚°

$\epsilon_\theta(x_t, c) = 0.8$, $\epsilon_\theta(x_t, \varnothing) = 0.3$ì´ê³  guidance scale $w=4.0$ì¼ ë•Œ CFG ì¶œë ¥ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\tilde{\epsilon} = (1 + w) \cdot \epsilon(x_t, c) - w \cdot \epsilon(x_t, \varnothing)$$
$$= (1 + 4) \times 0.8 - 4 \times 0.3 = 4.0 - 1.2 = 2.8$$

CFGëŠ” ì¡°ê±´ë¶€ ì˜ˆì¸¡ì„ **ì¦í­**í•©ë‹ˆë‹¤. $w$ê°€ í´ìˆ˜ë¡ ì¡°ê±´ì— ë” ì¶©ì‹¤í•´ì§€ì§€ë§Œ ë‹¤ì–‘ì„±ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.
</details>"""))

# â”€â”€ Cell 3: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • <a name='2.-í™˜ê²½-ì„¤ì •'></a>

TensorFlowë¥¼ ì‚¬ìš©í•˜ì—¬ adaLN-Zero ë ˆì´ì–´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 4: Imports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}")"""))

# â”€â”€ Cell 5: Section 3 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. adaLN-Zero ë ˆì´ì–´ êµ¬í˜„ <a name='3.-adaLN-Zero-êµ¬í˜„'></a>

DiTì˜ í•µì‹¬ì¸ adaLN-Zeroë¥¼ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
ì¡°ê±´ ë²¡í„° $c$ë¡œë¶€í„° $(\gamma, \beta, \alpha)$ 6ê°œ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 6: adaLN-Zero implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ adaLN-Zero ë ˆì´ì–´ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class AdaLNZero(tf.keras.layers.Layer):
    # Adaptive Layer Normalization with Zero-initialization (Peebles & Xie, 2023)
    def __init__(self, hidden_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden_dim = hidden_dim
        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        # ì¡°ê±´ â†’ (gamma, beta, alpha) ë§¤í•‘
        self.adaLN_modulation = tf.keras.Sequential([
            tf.keras.layers.Dense(hidden_dim, activation='silu'),
            tf.keras.layers.Dense(3 * hidden_dim)
        ])

    def build(self, input_shape):
        # Zero-initialization: ë§ˆì§€ë§‰ Denseì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        last_layer = self.adaLN_modulation.layers[-1]
        last_layer.build((None, self.hidden_dim))
        last_layer.kernel.assign(tf.zeros_like(last_layer.kernel))
        last_layer.bias.assign(tf.zeros_like(last_layer.bias))
        super().build(input_shape)

    def call(self, x, condition):
        # x: (B, N, D), condition: (B, D_cond)
        modulation = self.adaLN_modulation(condition)  # (B, 3*D)
        gamma, beta, alpha = tf.split(modulation, 3, axis=-1)  # ê° (B, D)

        # adaLN: ì¡°ê±´ë¶€ ì •ê·œí™”
        x_norm = self.norm(x)  # (B, N, D)
        # gamma, betaë¥¼ ì‹œí€€ìŠ¤ ì¶•ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        gamma = tf.expand_dims(gamma, 1)  # (B, 1, D)
        beta = tf.expand_dims(beta, 1)
        alpha = tf.expand_dims(alpha, 1)

        x_modulated = (1 + gamma) * x_norm + beta
        return x_modulated, alpha

# í…ŒìŠ¤íŠ¸
B, N, D = 2, 16, 384
x_test = tf.random.normal([B, N, D])
cond_test = tf.random.normal([B, D])

adaln = AdaLNZero(hidden_dim=D)
x_mod, alpha = adaln(x_test, cond_test)

print("=" * 55)
print("adaLN-Zero ë ˆì´ì–´ í…ŒìŠ¤íŠ¸")
print("=" * 55)
print(f"ì…ë ¥ x shape: {x_test.shape}")
print(f"ì¡°ê±´ c shape: {cond_test.shape}")
print(f"ì¶œë ¥ x_modulated shape: {x_mod.shape}")
print(f"ê²Œì´íŒ… alpha shape: {alpha.shape}")

# Zero-init ê²€ì¦
print(f"\\nZero-initialization ê²€ì¦:")
print(f"  alpha í‰ê· : {tf.reduce_mean(alpha).numpy():.6f} (ê¸°ëŒ€ê°’: 0.0)")
print(f"  alpha í‘œì¤€í¸ì°¨: {tf.math.reduce_std(alpha).numpy():.6f} (ê¸°ëŒ€ê°’: 0.0)")
print(f"  alpha ìµœëŒ€ ì ˆëŒ€ê°’: {tf.reduce_max(tf.abs(alpha)).numpy():.6f}")

# ì´ˆê¸° ìƒíƒœì—ì„œ x + alpha * f(x) = x í™•ì¸
residual = x_test + alpha * x_mod  # ì´ˆê¸°ì— alpha=0
diff = tf.reduce_mean(tf.abs(residual - x_test)).numpy()
print(f"  |x + alpha*f(x) - x| í‰ê· : {diff:.8f} (0ì´ë©´ í•­ë“±í•¨ìˆ˜)")
print(f"  â†’ ì´ˆê¸° ìƒíƒœì—ì„œ í•­ë“± í•¨ìˆ˜ {'í™•ì¸' if diff < 1e-6 else 'ì‹¤íŒ¨'}!")

print(f"\\níŒŒë¼ë¯¸í„° ìˆ˜: {adaln.count_params():,}")"""))

# â”€â”€ Cell 7: Section 4 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 4. í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ <a name='4.-LayerNorm-vs-adaLN'></a>

í‘œì¤€ LayerNormì€ ì¡°ê±´ê³¼ ë¬´ê´€í•˜ê²Œ ê³ ì •ëœ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ì§€ë§Œ,
adaLNì€ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 8: Standard LN vs adaLN comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë™ì¼ ì…ë ¥ì— ë‹¤ë¥¸ ì¡°ê±´ì„ ì£¼ì—ˆì„ ë•Œì˜ ì¶œë ¥ ì°¨ì´ë¥¼ ë¹„êµ

x_input = tf.random.normal([1, 8, 128])

# í‘œì¤€ LayerNorm
standard_ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)
y_standard = standard_ln(x_input)

# adaLN-Zero (ì„œë¡œ ë‹¤ë¥¸ ì¡°ê±´)
adaln_layer = AdaLNZero(hidden_dim=128)

# ì¡°ê±´ì„ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì°¨ì´ë¥¼ ê´€ì°° (í•™ìŠµ í›„ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)
cond_a = tf.constant([[1.0] * 64 + [-1.0] * 64])
cond_b = tf.constant([[-1.0] * 64 + [1.0] * 64])

# ìˆ˜ë™ìœ¼ë¡œ modulation ê°€ì¤‘ì¹˜ ì„¤ì • (í•™ìŠµ í›„ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)
adaln_layer(x_input, cond_a)  # build
for layer in adaln_layer.adaLN_modulation.layers:
    if hasattr(layer, 'kernel'):
        layer.kernel.assign(tf.random.normal(layer.kernel.shape, stddev=0.1))
        layer.bias.assign(tf.random.normal(layer.bias.shape, stddev=0.01))

y_ada_a, alpha_a = adaln_layer(x_input, cond_a)
y_ada_b, alpha_b = adaln_layer(x_input, cond_b)

print("=" * 60)
print("í‘œì¤€ LayerNorm vs adaLN ë¹„êµ")
print("=" * 60)

# ë‘ ì¡°ê±´ ê°„ adaLN ì¶œë ¥ ì°¨ì´
diff_ada = tf.reduce_mean(tf.abs(y_ada_a - y_ada_b)).numpy()
diff_std = tf.reduce_mean(tf.abs(y_standard - y_standard)).numpy()  # í•­ìƒ 0

print(f"ì…ë ¥ x shape: {x_input.shape}")
print(f"\\ní‘œì¤€ LayerNorm:")
print(f"  ì¡°ê±´ A ì¶œë ¥ == ì¡°ê±´ B ì¶œë ¥: í•­ìƒ ë™ì¼ (ì°¨ì´: {diff_std:.6f})")
print(f"  â†’ ì¡°ê±´ì— ë¬´ê´€í•œ ê³ ì • ì •ê·œí™”")
print(f"\\nadaLN-Zero:")
print(f"  ì¡°ê±´ Aì™€ ì¡°ê±´ B ì¶œë ¥ ì°¨ì´: {diff_ada:.6f}")
print(f"  â†’ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì •ê·œí™” ìˆ˜í–‰")
print(f"  alpha í‰ê·  (ì¡°ê±´ A): {tf.reduce_mean(alpha_a).numpy():.4f}")
print(f"  alpha í‰ê·  (ì¡°ê±´ B): {tf.reduce_mean(alpha_b).numpy():.4f}")

# ì¡°ê±´ë³„ ì •ê·œí™” íš¨ê³¼ ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

ax1 = axes[0]
token_idx = 0
x_vals = x_input[0, token_idx, :64].numpy()
y_std_vals = y_standard[0, token_idx, :64].numpy()
y_ada_a_vals = y_ada_a[0, token_idx, :64].numpy()
y_ada_b_vals = y_ada_b[0, token_idx, :64].numpy()

ax1.plot(x_vals, 'gray', alpha=0.5, lw=1, label='ì…ë ¥ x')
ax1.plot(y_std_vals, 'b-', lw=2, label='í‘œì¤€ LN')
ax1.plot(y_ada_a_vals, 'r-', lw=2, alpha=0.7, label='adaLN (ì¡°ê±´ A)')
ax1.plot(y_ada_b_vals, 'g-', lw=2, alpha=0.7, label='adaLN (ì¡°ê±´ B)')
ax1.set_xlabel('ì°¨ì›', fontsize=11)
ax1.set_ylabel('ê°’', fontsize=11)
ax1.set_title('ì •ê·œí™” ì¶œë ¥ ë¹„êµ (ì²« ë²ˆì§¸ í† í°)', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# ìš°: ì¡°ê±´ì— ë”°ë¥¸ gamma, beta ë¶„í¬
ax2 = axes[1]
modulation_a = adaln_layer.adaLN_modulation(cond_a).numpy()[0]
modulation_b = adaln_layer.adaLN_modulation(cond_b).numpy()[0]
D_h = 128
gamma_a, beta_a, alpha_vals_a = modulation_a[:D_h], modulation_a[D_h:2*D_h], modulation_a[2*D_h:]
gamma_b, beta_b, alpha_vals_b = modulation_b[:D_h], modulation_b[D_h:2*D_h], modulation_b[2*D_h:]

x_pos = np.arange(D_h)
ax2.scatter(gamma_a, beta_a, c='red', alpha=0.5, s=15, label='ì¡°ê±´ A (gamma vs beta)')
ax2.scatter(gamma_b, beta_b, c='blue', alpha=0.5, s=15, label='ì¡°ê±´ B (gamma vs beta)')
ax2.axhline(y=0, color='gray', ls='--', lw=1)
ax2.axvline(x=0, color='gray', ls='--', lw=1)
ax2.set_xlabel('$\\gamma_c$ (ìŠ¤ì¼€ì¼)', fontsize=11)
ax2.set_ylabel('$\\beta_c$ (ì‹œí”„íŠ¸)', fontsize=11)
ax2.set_title('ì¡°ê±´ë³„ adaLN íŒŒë¼ë¯¸í„° ë¶„í¬', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/ln_vs_adaln_comparison.png',
            dpi=100, bbox_inches='tight')
plt.close()
print("\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/ln_vs_adaln_comparison.png")"""))

# â”€â”€ Cell 9: Section 5 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. Zero-Init í•™ìŠµ ì•ˆì •ì„± ì‹¤í—˜ <a name='5.-Zero-Init-ì•ˆì •ì„±'></a>

Zero-initialization($\alpha=0$) vs ëœë¤ ì´ˆê¸°í™”ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.
ê°„ë‹¨í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê³¼ì œì—ì„œ ë‘ ì´ˆê¸°í™” ë°©ì‹ì˜ ì´ˆê¸° ì†ì‹¤ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 10: Zero-init stability demonstration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Zero-Init vs ëœë¤ ì´ˆê¸°í™” í•™ìŠµ ì•ˆì •ì„± ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SimpleDiTBlock(tf.keras.layers.Layer):
    # ê°„ì†Œí™”ëœ DiT ë¸”ë¡ (adaLN-Zero í¬í•¨)
    def __init__(self, dim, zero_init=True, **kwargs):
        super().__init__(**kwargs)
        self.dim = dim
        self.zero_init = zero_init
        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.attn_proj = tf.keras.layers.Dense(dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(dim * 4, activation='gelu'),
            tf.keras.layers.Dense(dim)
        ])
        self.adaln = tf.keras.Sequential([
            tf.keras.layers.Dense(dim, activation='silu'),
            tf.keras.layers.Dense(6 * dim)
        ])

    def build(self, input_shape):
        dummy_cond = tf.zeros([1, self.dim])
        self.adaln(dummy_cond)
        if self.zero_init:
            last = self.adaln.layers[-1]
            last.kernel.assign(tf.zeros_like(last.kernel))
            last.bias.assign(tf.zeros_like(last.bias))
        super().build(input_shape)

    def call(self, x, cond):
        mod = self.adaln(cond)
        g1, b1, a1, g2, b2, a2 = tf.split(mod, 6, axis=-1)
        g1, b1, a1 = [tf.expand_dims(v, 1) for v in [g1, b1, a1]]
        g2, b2, a2 = [tf.expand_dims(v, 1) for v in [g2, b2, a2]]

        # Attention branch
        h = (1 + g1) * self.norm(x) + b1
        h = self.attn_proj(h)
        x = x + a1 * h

        # FFN branch
        h2 = (1 + g2) * self.norm(x) + b2
        h2 = self.ffn(h2)
        x = x + a2 * h2
        return x

# ë‹¤ì¸µ DiT ì‹œë®¬ë ˆì´ì…˜ (8ë¸”ë¡ ìŠ¤íƒ)
dim = 64
n_blocks = 8

def build_stacked_dit(zero_init):
    blocks = [SimpleDiTBlock(dim, zero_init=zero_init) for _ in range(n_blocks)]
    return blocks

x_input = tf.random.normal([4, 16, dim])
cond = tf.random.normal([4, dim])
target = tf.random.normal([4, 16, dim])

results = {}
for name, zero_init in [("Zero-Init", True), ("Random-Init", False)]:
    tf.random.set_seed(42)
    blocks = build_stacked_dit(zero_init)

    # Forward passë¡œ ì´ˆê¸° ì¶œë ¥ í™•ì¸
    h = x_input
    for block in blocks:
        h = block(h, cond)

    initial_output_std = tf.math.reduce_std(h).numpy()

    # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ì¸¡ì •
    trainable_vars = []
    for block in blocks:
        trainable_vars.extend(block.trainable_variables)

    with tf.GradientTape() as tape:
        h = x_input
        for block in blocks:
            h = block(h, cond)
        loss = tf.reduce_mean((h - target) ** 2)

    grads = tape.gradient(loss, trainable_vars)
    grad_norms = [tf.norm(g).numpy() for g in grads if g is not None]

    results[name] = {
        'output_std': initial_output_std,
        'loss': loss.numpy(),
        'grad_mean': np.mean(grad_norms),
        'grad_max': np.max(grad_norms),
        'grad_min': np.min(grad_norms),
    }

print("=" * 65)
print(f"Zero-Init vs Random-Init ì•ˆì •ì„± ë¹„êµ ({n_blocks}ë¸”ë¡ DiT)")
print("=" * 65)
print(f"{'í•­ëª©':<25} | {'Zero-Init':>15} | {'Random-Init':>15}")
print("-" * 60)
for key in ['output_std', 'loss', 'grad_mean', 'grad_max']:
    labels = {
        'output_std': 'ì´ˆê¸° ì¶œë ¥ í‘œì¤€í¸ì°¨',
        'loss': 'ì´ˆê¸° ì†ì‹¤',
        'grad_mean': 'ê·¸ë˜ë””ì–¸íŠ¸ í‰ê·  norm',
        'grad_max': 'ê·¸ë˜ë””ì–¸íŠ¸ ìµœëŒ€ norm',
    }
    v_zero = results['Zero-Init'][key]
    v_rand = results['Random-Init'][key]
    print(f"{labels[key]:<25} | {v_zero:>15.4f} | {v_rand:>15.4f}")

print(f"\\në¶„ì„:")
if results['Zero-Init']['output_std'] < results['Random-Init']['output_std']:
    print(f"  Zero-Init: ì´ˆê¸° ì¶œë ¥ì´ ì…ë ¥ì— ê°€ê¹Œì›€ (í•­ë“± í•¨ìˆ˜ ê·¼ì‚¬)")
else:
    print(f"  Zero-Init: ì´ˆê¸° ì¶œë ¥ í‘œì¤€í¸ì°¨ ë¶„ì„ ì™„ë£Œ")
print(f"  Random-Init: ì´ˆê¸°ë¶€í„° ì¶œë ¥ ë¶„í¬ê°€ í¬ê²Œ ë³€í˜•ë¨")
print(f"  â†’ Zero-Initì´ ê¹Šì€ DiTì—ì„œ ë” ì•ˆì •ì ì¸ í•™ìŠµ ì‹œì‘ì ì„ ì œê³µ")"""))

# â”€â”€ Cell 11: Section 6 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨ <a name='6.-ë‹¤ì¤‘-ì¡°ê±´-ì£¼ì…'></a>

DiTëŠ” ì—¬ëŸ¬ ì¡°ê±´ì„ ë™ì‹œì— ì£¼ì…í•©ë‹ˆë‹¤:
- **íƒ€ì„ìŠ¤í… $t$**: Sinusoidal ì„ë² ë”© â†’ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ ì •ë³´
- **í´ë˜ìŠ¤ ë ˆì´ë¸” $y$**: Embedding í…Œì´ë¸” â†’ ìƒì„± ëŒ€ìƒ ì¹´í…Œê³ ë¦¬
- **í…ìŠ¤íŠ¸**: CLIP/T5 ì¸ì½”ë” ì¶œë ¥ â†’ Cross-Attention ë˜ëŠ” adaLN

$$c = \text{MLP}(\text{SinEmbed}(t)) + \text{Embed}(y)$$"""))

# â”€â”€ Cell 12: Multi-condition injection demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TimestepEmbedding(tf.keras.layers.Layer):
    # Sinusoidal timestep embedding (DDPM ìŠ¤íƒ€ì¼)
    def __init__(self, dim, **kwargs):
        super().__init__(**kwargs)
        self.dim = dim
        self.mlp = tf.keras.Sequential([
            tf.keras.layers.Dense(dim, activation='silu'),
            tf.keras.layers.Dense(dim),
        ])

    def call(self, t):
        half_dim = self.dim // 2
        freqs = tf.exp(-tf.math.log(10000.0) * tf.range(0, half_dim, dtype=tf.float32) / half_dim)
        args = tf.cast(t[:, None], tf.float32) * freqs[None, :]
        embedding = tf.concat([tf.cos(args), tf.sin(args)], axis=-1)
        return self.mlp(embedding)


class ClassEmbedding(tf.keras.layers.Layer):
    # í´ë˜ìŠ¤ ë ˆì´ë¸” ì„ë² ë”©
    def __init__(self, num_classes, dim, **kwargs):
        super().__init__(**kwargs)
        self.embed = tf.keras.layers.Embedding(num_classes + 1, dim)  # +1 for null class
        self.null_class = num_classes  # null class for CFG

    def call(self, y, drop_prob=0.0):
        # CFG: drop_prob í™•ë¥ ë¡œ null classë¡œ ëŒ€ì²´
        if drop_prob > 0:
            mask = tf.random.uniform(tf.shape(y)) < drop_prob
            y = tf.where(mask, self.null_class, y)
        return self.embed(y)


class DiTConditioner(tf.keras.layers.Layer):
    # ì‹œê°„ + í´ë˜ìŠ¤ ì¡°ê±´ ê²°í•©
    def __init__(self, dim, num_classes, **kwargs):
        super().__init__(**kwargs)
        self.time_embed = TimestepEmbedding(dim)
        self.class_embed = ClassEmbedding(num_classes, dim)

    def call(self, t, y, cfg_drop_prob=0.0):
        c_time = self.time_embed(t)
        c_class = self.class_embed(y, drop_prob=cfg_drop_prob)
        return c_time + c_class


# í…ŒìŠ¤íŠ¸
dim = 256
num_classes = 1000
conditioner = DiTConditioner(dim, num_classes)

batch_size = 8
timesteps = tf.constant([0, 100, 250, 500, 750, 900, 950, 999])
class_labels = tf.constant([1, 42, 100, 207, 404, 555, 888, 999])

# ì¼ë°˜ ì¡°ê±´ (CFG off)
c_normal = conditioner(timesteps, class_labels, cfg_drop_prob=0.0)

# CFG í•™ìŠµ ëª¨ë“œ (10% drop)
c_cfg = conditioner(timesteps, class_labels, cfg_drop_prob=0.1)

# ë¬´ì¡°ê±´ ìƒì„± (null condition)
null_labels = tf.fill([batch_size], num_classes)
c_uncond = conditioner(timesteps, null_labels, cfg_drop_prob=0.0)

print("=" * 60)
print("ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
print("=" * 60)
print(f"ì„ë² ë”© ì°¨ì›: {dim}")
print(f"í´ë˜ìŠ¤ ìˆ˜: {num_classes}")
print(f"\\nì¡°ê±´ ë²¡í„° shape: {c_normal.shape}")
print(f"\\nì‹œê°„ë³„ ì¡°ê±´ ë²¡í„° í†µê³„:")
print(f"{'íƒ€ì„ìŠ¤í…':>10} | {'í´ë˜ìŠ¤':>8} | {'ì¡°ê±´ norm':>12} | {'ì¡°ê±´ mean':>12}")
print("-" * 50)
for i in range(batch_size):
    t_val = timesteps[i].numpy()
    y_val = class_labels[i].numpy()
    c_norm = tf.norm(c_normal[i]).numpy()
    c_mean = tf.reduce_mean(c_normal[i]).numpy()
    print(f"{t_val:>10} | {y_val:>8} | {c_norm:>12.4f} | {c_mean:>12.4f}")

# CFG: ì¡°ê±´ë¶€ vs ë¬´ì¡°ê±´ ì°¨ì´
cfg_diff = tf.reduce_mean(tf.abs(c_normal - c_uncond)).numpy()
print(f"\\nì¡°ê±´ë¶€ vs ë¬´ì¡°ê±´ ë²¡í„° ì°¨ì´: {cfg_diff:.4f}")
print(f"â†’ CFG ì¶”ë¡  ì‹œ ì´ ì°¨ì´ë¥¼ guidance scaleë¡œ ì¦í­")"""))

# â”€â”€ Cell 13: CFG design markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 7. DiTì—ì„œì˜ CFG ì„¤ê³„ <a name='7.-CFG-ì„¤ê³„'></a>

DiTëŠ” Classifier-Free Guidanceë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì ìš©í•©ë‹ˆë‹¤:

**í•™ìŠµ ì‹œ:**
- ì¼ì • ë¹„ìœ¨(ë³´í†µ 10%)ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ $\varnothing$(null class)ë¡œ ëŒ€ì²´
- ëª¨ë¸ì´ ì¡°ê±´ë¶€/ë¬´ì¡°ê±´ ìƒì„±ì„ ëª¨ë‘ í•™ìŠµ

**ì¶”ë¡  ì‹œ:**
$$\tilde{\epsilon}_\theta(x_t, c) = (1 + w) \cdot \epsilon_\theta(x_t, c) - w \cdot \epsilon_\theta(x_t, \varnothing)$$

| guidance scale $w$ | íš¨ê³¼ | ì ìš© |
|-------|------|------|
| 0.0 | ë¬´ì¡°ê±´ ìƒì„± (ì¡°ê±´ ë¬´ì‹œ) | ë‹¤ì–‘ì„± ìµœëŒ€ |
| 1.0~2.0 | ì•½í•œ ê°€ì´ë˜ìŠ¤ | ê· í˜• |
| 4.0~7.5 | ê°•í•œ ê°€ì´ë˜ìŠ¤ (DiT ê¸°ë³¸ê°’ 4.0) | í’ˆì§ˆ ìµœëŒ€ |
| >10.0 | ê³¼ë„í•œ ê°€ì´ë˜ìŠ¤ | ì•„í‹°íŒ©íŠ¸ ë°œìƒ |"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 8. ì •ë¦¬ <a name='8.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| adaLN-Zero | ì¡°ê±´ë¶€ ì •ê·œí™” + ê²Œì´íŒ…($\alpha=0$) ì´ˆê¸°í™” | â­â­â­ |
| Zero-Initialization | ì´ˆê¸° ìƒíƒœì—ì„œ í•­ë“± í•¨ìˆ˜ â†’ ì•ˆì •ì  í•™ìŠµ ì‹œì‘ | â­â­â­ |
| ì¡°ê±´ ì£¼ì… | timestep(sinusoidal) + class(embedding) â†’ adaLN | â­â­â­ |
| CFG in DiT | $(1+w)\epsilon(c) - w\epsilon(\varnothing)$, í•™ìŠµ ì‹œ 10% drop | â­â­â­ |
| í‘œì¤€ LN vs adaLN | ê³ ì • íŒŒë¼ë¯¸í„° vs ì¡°ê±´ë¶€ ë™ì  íŒŒë¼ë¯¸í„° | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$h = x + \alpha \cdot \text{Attn}\!\left((1 + \gamma_c) \odot \text{LN}(x) + \beta_c\right)$$

$$(\gamma, \beta, \alpha) \leftarrow \text{MLP}(c), \quad \alpha_{\text{init}} = 0 \;\text{(í•­ë“± í•¨ìˆ˜)}$$

$$\tilde{\epsilon}_\theta = (1 + w) \cdot \epsilon_\theta(x_t, c) - w \cdot \epsilon_\theta(x_t, \varnothing)$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**04_flow_matching_and_rectified_flow.ipynb** â€” Flow Matching ODE ìˆ˜ì‹ê³¼ Rectified Flowì˜ ì§ì„  ê²½ë¡œë¥¼ DDPMê³¼ ë¹„êµí•˜ë©°, SD3/Flux ë“± ìµœì‹  ëª¨ë¸ì˜ í›ˆë ¨ ë°©ì‹ì„ ë‹¤ë£¹ë‹ˆë‹¤."""))

# â”€â”€ ë…¸íŠ¸ë¶ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
create_notebook(cells, 'chapter17_diffusion_transformers/03_dit_conditioning_and_adaln.ipynb')
