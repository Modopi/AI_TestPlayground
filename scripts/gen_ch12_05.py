"""Generate Chapter 12-05: DeepSeek MoE Architecture."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

cells = [
md("""# Chapter 12: ìµœì‹  LLM ì•„í‚¤í…ì²˜ â€” DeepSeek-V3 MoE ì•„í‚¤í…ì²˜

## í•™ìŠµ ëª©í‘œ
- DeepSeek-V3ì˜ **Shared Expert + Routed Expert** ë¶„ë¦¬ ì„¤ê³„ì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- **Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±**(í¸í–¥ ë³´ì •) ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œë‹¤
- **Multi-Token Prediction(MTP)** ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  ì‹œë®¬ë ˆì´ì…˜í•œë‹¤
- DeepSeek-V3ì˜ ì‹¤ì œ ì•„í‚¤í…ì²˜ ìŠ¤í™(671B, 256 Experts)ì„ ë¶„ì„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: DeepSeekMoEì™€ MTP](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [DeepSeekMoE ë ˆì´ì–´ êµ¬í˜„](#2.-DeepSeekMoE)
3. [Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±](#3.-Aux-Free-Balance)
4. [Multi-Token Prediction (MTP)](#4.-MTP)
5. [ì •ë¦¬](#5.-ì •ë¦¬)"""),

md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### DeepSeekMoE: Shared Expert + Routed Expert

DeepSeek-V3ëŠ” ê¸°ì¡´ MoEì™€ ë‹¬ë¦¬ **ê³µìœ  ì „ë¬¸ê°€(Shared Expert)**ë¥¼ ë„ì…í•©ë‹ˆë‹¤:

$$y = \underbrace{E_s(x)}_{\text{Shared Expert}} + \sum_{i \in \text{Top-k}} g_i \cdot E_i^r(x)$$

- $E_s$: Shared Expert â€” **ëª¨ë“  í† í°**ì´ ë°˜ë“œì‹œ ê±°ì¹˜ëŠ” ì „ë¬¸ê°€ (ê³µí†µ ì§€ì‹)
- $E_i^r$: Routed Expert â€” Top-kë¡œ **ì„ íƒëœ** ì „ë¬¸ê°€ (ì „ë¬¸ ì§€ì‹)
- $g_i$: ë¼ìš°íŒ… ê²Œì´íŠ¸ ê°€ì¤‘ì¹˜

**DeepSeek-V3 ì‹¤ì œ ìŠ¤í™** (arxiv:2412.19437):

| í•­ëª© | ê°’ |
|------|-----|
| ì´ íŒŒë¼ë¯¸í„° | 671B |
| í™œì„± íŒŒë¼ë¯¸í„°/í† í° | 37B |
| ë ˆì´ì–´ ìˆ˜ | 61 |
| Shared Expert | 1ê°œ |
| Routed Expert | 256ê°œ |
| Top-k | 8 |
| íˆë“  ì°¨ì› | 7,168 |

### Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±

ê¸°ì¡´ Aux Loss($L_{aux}$)ëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¤ëŠ” ë¶€ì‘ìš©ì´ ìˆì—ˆìŠµë‹ˆë‹¤. DeepSeek-V3ëŠ” **í¸í–¥(bias) í•­**ìœ¼ë¡œ ëŒ€ì²´:

$$g_i' = g_i + b_i$$

- $b_i$: Expert $i$ì˜ í¸í–¥ í•­ (í•™ìŠµí•˜ì§€ ì•ŠìŒ!)
- **ì—…ë°ì´íŠ¸ ê·œì¹™**: Expert $i$ê°€ ê³¼ë„í•˜ê²Œ ì‚¬ìš©ë˜ë©´ $b_i$ ê°ì†Œ, ì ê²Œ ì‚¬ìš©ë˜ë©´ $b_i$ ì¦ê°€
- í•˜ì´í¼íŒŒë¼ë¯¸í„° $\gamma$ê°€ í¸í–¥ ì—…ë°ì´íŠ¸ ì†ë„ë¥¼ ì œì–´

$$b_i \leftarrow b_i + \gamma \cdot (\bar{f} - f_i)$$

ì—¬ê¸°ì„œ $\bar{f} = 1/N$ (ì´ìƒì  ê· ë“± ë¹„ìœ¨), $f_i$ = ì‹¤ì œ ë¹„ìœ¨

### Multi-Token Prediction (MTP)

ê¸°ì¡´ LLMì€ ë‹¤ìŒ 1ê°œ í† í°ë§Œ ì˜ˆì¸¡í•˜ì§€ë§Œ, MTPëŠ” **ì—¬ëŸ¬ ë¯¸ë˜ í† í°ì„ ë™ì‹œ ì˜ˆì¸¡**:

$$\mathcal{L}_{MTP} = -\frac{1}{D} \sum_{k=1}^{D} \sum_{t=1}^{T-k} \log P_\theta(x_{t+k} | x_{\leq t})$$

- $D$: ì˜ˆì¸¡ ê¹Šì´ (depth) â€” DeepSeek-V3ëŠ” $D=1$ (2í† í° ë™ì‹œ)
- í•™ìŠµ ì‹œ ì¶”ê°€ ì˜ˆì¸¡ í—¤ë“œ ì‚¬ìš©, ì¶”ë¡  ì‹œ Speculative Decodingì— í™œìš©

**ìš”ì•½ í‘œ:**

| í˜ì‹  | ìˆ˜ì‹ | íš¨ê³¼ |
|------|------|------|
| Shared Expert | $y = E_s(x) + \sum g_i E_i^r(x)$ | ê³µí†µ ì§€ì‹ ë³´ì¡´ |
| Aux-Free Balance | $g_i' = g_i + b_i$ | ì„±ëŠ¥ ì €í•˜ ì—†ëŠ” ê· í˜• |
| MTP | $\mathcal{L} = \sum_{k=1}^{D} \mathcal{L}_k$ | í‘œí˜„ë ¥ í–¥ìƒ + Spec. Decoding |

---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ DeepSeek MoE ì¹œì ˆ ì„¤ëª…!

#### ğŸ« Shared Expertê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: **í•™êµ ìˆ˜ì—…**ì„ ìƒê°í•´ë³´ì„¸ìš”!
> - **ê³µí†µ ìˆ˜ì—…**(êµ­ì–´, ìˆ˜í•™) = Shared Expert â†’ ëª¨ë“  í•™ìƒì´ ë“¤ì–´ì•¼ í•¨
> - **ì„ íƒ ê³¼ëª©**(ë¯¸ìˆ , ìŒì•…, ì½”ë”©) = Routed Expert â†’ ê°ì ì›í•˜ëŠ” 2ê°œë§Œ ì„ íƒ
>
> ê³µí†µ ì§€ì‹ì€ ëª¨ë‘ê°€ ë°°ìš°ê³ , ì „ë¬¸ ì§€ì‹ì€ í•„ìš”í•œ í•™ìƒë§Œ!

#### âš–ï¸ Auxiliary-Loss-Freeê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ê¸°ì¡´ ë°©ë²•ì€ "ë°˜ì¥ì´ ë²Œì ì„ ì¤˜ì„œ" ê³¨ê³ ë£¨ ë‚˜ëˆ´ëŠ”ë°, DeepSeek ë°©ë²•ì€ "ê° ì„ íƒ ê³¼ëª©ì˜ ì¸ê¸°ë„ë¥¼ ë³´ê³  ìë™ìœ¼ë¡œ ì‹œê°„í‘œë¥¼ ì¡°ì •"í•´ìš”. ë²Œì (Loss)ì´ í•„ìš” ì—†ì–´ì„œ í•™ìŠµì´ ë” ì˜ ë©ë‹ˆë‹¤!"""),

md(r"""---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: DeepSeek-V3 í™œì„± íŒŒë¼ë¯¸í„°

DeepSeek-V3ì—ì„œ 1ê°œ Shared Expert + Top-8 Routed Expertê°€ í™œì„±í™”ë©ë‹ˆë‹¤. ì „ì²´ 256ê°œ Routed Expert ì¤‘ í™œì„±í™” ë¹„ìœ¨ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

ì´ í™œì„± Expert: $1 (\text{shared}) + 8 (\text{routed}) = 9$

Routed Expert í™œì„± ë¹„ìœ¨: $8 / 256 = 3.125\%$

ì „ì²´ Expert í™œì„± ë¹„ìœ¨: $9 / 257 = 3.5\%$

â†’ ì „ì²´ 671B ì¤‘ 37Bë§Œ í™œì„±í™” = $37/671 = 5.5\%$

ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” ì„ë² ë”©, Attention ë“±ì— ì‚¬ìš©
</details>

#### ë¬¸ì œ 2: í¸í–¥ ë³´ì • ì‹œë®¬ë ˆì´ì…˜

Expert 4ê°œ, í˜„ì¬ ë¶„ë°° $f = [0.4, 0.3, 0.2, 0.1]$, $\gamma=0.1$ì¼ ë•Œ í¸í–¥ ì—…ë°ì´íŠ¸ í›„ ìƒˆ $b$ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤ (ì´ˆê¸° $b=[0,0,0,0]$).

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$\bar{f} = 1/4 = 0.25$

$b_0 = 0 + 0.1 \times (0.25 - 0.4) = -0.015$ (ê³¼ë‹¤ â†’ ê°ì†Œ)

$b_1 = 0 + 0.1 \times (0.25 - 0.3) = -0.005$

$b_2 = 0 + 0.1 \times (0.25 - 0.2) = +0.005$ (ê³¼ì†Œ â†’ ì¦ê°€)

$b_3 = 0 + 0.1 \times (0.25 - 0.1) = +0.015$

í¸í–¥ìœ¼ë¡œ ì¸í•´ Expert 0,1ì€ ì„ íƒ í™•ë¥  ê°ì†Œ, Expert 2,3ì€ ì¦ê°€ â†’ ê· í˜•ìœ¼ë¡œ ìˆ˜ë ´!
</details>"""),

code("""import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")"""),

md("""## 2. DeepSeekMoE ë ˆì´ì–´ êµ¬í˜„ <a name='2.-DeepSeekMoE'></a>"""),

code(r"""# â”€â”€ DeepSeekMoE ë ˆì´ì–´ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Shared Expert 1ê°œ + Routed Expert Nê°œì˜ DeepSeek ìŠ¤íƒ€ì¼ MoE

class DeepSeekMoELayer(tf.keras.layers.Layer):
    # DeepSeek-V3 ìŠ¤íƒ€ì¼ MoE: Shared Expert + Routed Experts
    def __init__(self, d_model, d_ff, n_routed_experts, top_k=2, n_shared=1):
        super().__init__()
        self.n_routed = n_routed_experts
        self.n_shared = n_shared
        self.top_k = top_k
        
        # Shared Expert(s)
        self.shared_experts = [
            tf.keras.Sequential([
                tf.keras.layers.Dense(d_ff, activation='relu'),
                tf.keras.layers.Dense(d_model)
            ], name=f'shared_{i}')
            for i in range(n_shared)
        ]
        
        # Routed Experts
        self.routed_experts = [
            tf.keras.Sequential([
                tf.keras.layers.Dense(d_ff, activation='relu'),
                tf.keras.layers.Dense(d_model)
            ], name=f'routed_{i}')
            for i in range(n_routed_experts)
        ]
        
        # Router (Routed Expertsì— ëŒ€í•´ì„œë§Œ)
        self.gate = tf.keras.layers.Dense(n_routed_experts, use_bias=False, name='router')
        
        # Bias terms for Aux-Free balancing (non-trainable)
        self.expert_bias = tf.Variable(
            tf.zeros(n_routed_experts), trainable=False, name='expert_bias'
        )
    
    def call(self, x, training=False):
        B, S, D = x.shape
        
        # 1. Shared Expert ì¶œë ¥ (ëª¨ë“  í† í°)
        shared_out = tf.zeros_like(x)
        for expert in self.shared_experts:
            shared_out += expert(x)
        
        # 2. Router ë¡œì§“ + í¸í–¥
        logits = self.gate(x)  # [B, S, N_routed]
        if not training:
            logits_biased = logits + self.expert_bias
        else:
            logits_biased = logits + self.expert_bias
        
        # Top-k ì„ íƒ
        top_k_logits, top_k_indices = tf.math.top_k(logits_biased, k=self.top_k)
        top_k_gates = tf.nn.softmax(top_k_logits, axis=-1)  # [B, S, k]
        
        # 3. Routed Expert ì¶œë ¥
        routed_out = tf.zeros_like(x)
        for k_idx in range(self.top_k):
            expert_indices = top_k_indices[:, :, k_idx]
            expert_gates = top_k_gates[:, :, k_idx:k_idx+1]
            
            for e_idx in range(self.n_routed):
                mask = tf.cast(tf.equal(expert_indices, e_idx), tf.float32)
                mask = mask[:, :, tf.newaxis]
                if tf.reduce_sum(mask) > 0:
                    e_out = self.routed_experts[e_idx](x)
                    routed_out += e_out * mask * expert_gates
        
        # 4. í•©ì‚°: Shared + Routed
        output = shared_out + routed_out
        
        return output, top_k_indices, tf.nn.softmax(logits, axis=-1)


# ë°ëª¨ (ì¶•ì†Œ ë²„ì „)
d_model, d_ff = 256, 512
n_routed, top_k = 8, 2

moe = DeepSeekMoELayer(d_model, d_ff, n_routed, top_k, n_shared=1)
x_test = tf.random.normal((2, 16, d_model))
output, indices, probs = moe(x_test)

print("=" * 60)
print(f"DeepSeekMoE Layer")
print(f"  Shared Experts: 1, Routed Experts: {n_routed}, Top-{top_k}")
print("=" * 60)
print(f"ì…ë ¥ shape:  {x_test.shape}")
print(f"ì¶œë ¥ shape:  {output.shape}")
print()

# íŒŒë¼ë¯¸í„° ë¶„ì„
shared_params = sum(tf.size(v).numpy() for exp in moe.shared_experts for v in exp.trainable_variables)
routed_params = sum(tf.size(v).numpy() for exp in moe.routed_experts for v in exp.trainable_variables)
router_params = sum(tf.size(v).numpy() for v in moe.gate.trainable_variables)

print(f"{'êµ¬ì„± ìš”ì†Œ':<25} | {'íŒŒë¼ë¯¸í„°':>12}")
print("-" * 42)
print(f"{'Shared Expert (1ê°œ)':<25} | {shared_params:>12,}")
print(f"{'Routed Experts (8ê°œ)':<25} | {routed_params:>12,}")
print(f"{'Router':<25} | {router_params:>12,}")
print(f"{'ì´ê³„':<25} | {shared_params+routed_params+router_params:>12,}")
print(f"{'í™œì„± íŒŒë¼ë¯¸í„° (1+2)':<25} | {shared_params + routed_params//n_routed*top_k + router_params:>12,}")"""),

md("""## 3. Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± <a name='3.-Aux-Free-Balance'></a>"""),

code(r"""# â”€â”€ Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í¸í–¥ ë³´ì • ë°©ì‹ìœ¼ë¡œ Expert ë¶€í•˜ë¥¼ ê· í˜•ì‹œí‚¤ëŠ” ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤

def simulate_aux_free_balancing(n_experts=8, n_tokens=256, n_steps=50, gamma=0.1, top_k=2):
    # í¸í–¥ì´ ì ì§„ì ìœ¼ë¡œ ê· í˜•ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •
    biases = np.zeros(n_experts)
    
    # Expert 0, 1ì— í¸í–¥ëœ ì´ˆê¸° ë¼ìš°í„° ê°€ì¤‘ì¹˜
    router_w = np.random.randn(64, n_experts) * 0.1
    router_w[:, 0] += 0.5  # Expert 0 ì„ í˜¸
    router_w[:, 1] += 0.3  # Expert 1 ì•½ê°„ ì„ í˜¸
    
    history = {'step': [], 'balance': [], 'max_load': [], 'min_load': []}
    load_history = []
    
    for step in range(n_steps):
        # í† í° ë¼ìš°íŒ… ì‹œë®¬ë ˆì´ì…˜
        x = np.random.randn(n_tokens, 64)
        logits = x @ router_w + biases  # í¸í–¥ ì¶”ê°€
        
        # Top-k ì„ íƒ
        top_k_idx = np.argsort(-logits, axis=-1)[:, :top_k]
        
        # Expertë³„ ë¡œë“œ ê³„ì‚°
        loads = np.zeros(n_experts)
        for e in range(n_experts):
            loads[e] = np.sum(top_k_idx == e) / (n_tokens * top_k)
        
        # í¸í–¥ ì—…ë°ì´íŠ¸: f_bar - f_i
        f_bar = 1.0 / n_experts
        biases += gamma * (f_bar - loads)
        
        history['step'].append(step)
        history['balance'].append(np.std(loads))
        history['max_load'].append(np.max(loads))
        history['min_load'].append(np.min(loads))
        load_history.append(loads.copy())
    
    return history, load_history, biases

history, load_hist, final_biases = simulate_aux_free_balancing()

print("=" * 65)
print("Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜")
print("=" * 65)
print(f"{'Step':<8} | {'ë¶€í•˜ í‘œì¤€í¸ì°¨':>12} | {'ìµœëŒ€ ë¡œë“œ':>10} | {'ìµœì†Œ ë¡œë“œ':>10}")
print("-" * 50)
for i in [0, 5, 10, 20, 49]:
    print(f"{i:<8} | {history['balance'][i]:>12.4f} | "
          f"{history['max_load'][i]:>9.1%} | {history['min_load'][i]:>9.1%}")
print()
print("ìµœì¢… í¸í–¥ ê°’:")
for i, b in enumerate(final_biases):
    print(f"  Expert {i}: b={b:+.4f}")"""),

code(r"""# â”€â”€ Aux-Free ë¡œë“œë°¸ëŸ°ì‹± ìˆ˜ë ´ ê³¼ì • ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# ì™¼ìª½: ë¶€í•˜ í¸ì°¨ ìˆ˜ë ´
ax1 = axes[0]
ax1.plot(history['step'], history['balance'], 'b-', lw=2.5, label='ë¶€í•˜ í‘œì¤€í¸ì°¨')
ax1.fill_between(history['step'], history['min_load'], history['max_load'],
                  alpha=0.2, color='orange', label='ìµœëŒ€-ìµœì†Œ ë¡œë“œ ë²”ìœ„')
ax1.axhline(y=0, color='gray', ls='--', lw=1)
ax1.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)
ax1.set_ylabel('ë¶€í•˜ í¸ì°¨ / ë¡œë“œ', fontsize=11)
ax1.set_title('Aux-Free ë¡œë“œë°¸ëŸ°ì‹± ìˆ˜ë ´', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: Expertë³„ ë¡œë“œ ë³€í™”
ax2 = axes[1]
load_arr = np.array(load_hist)
for e in range(8):
    ax2.plot(range(len(load_hist)), load_arr[:, e], lw=1.5, 
             label=f'Expert {e}', alpha=0.7)
ax2.axhline(y=1/8, color='red', ls='--', lw=2, label='ì´ìƒì  (12.5%)')
ax2.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)
ax2.set_ylabel('Expert ë¡œë“œ ë¹„ìœ¨', fontsize=11)
ax2.set_title('Expertë³„ ë¶€í•˜ ë³€í™”', fontweight='bold')
ax2.legend(fontsize=8, ncol=3)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter12_modern_llms/deepseek_auxfree.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/deepseek_auxfree.png")"""),

md("""## 4. Multi-Token Prediction (MTP) <a name='4.-MTP'></a>"""),

code(r"""# â”€â”€ Multi-Token Prediction ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¤ìŒ 1ê°œ í† í° ì˜ˆì¸¡(NTP) vs ë‹¤ì¤‘ í† í° ì˜ˆì¸¡(MTP)ì˜ í•™ìŠµ ì‹ í˜¸ ë¹„êµ

# ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ì˜ˆì¸¡ ì‹œë‚˜ë¦¬ì˜¤
vocab_size = 100
seq_len = 32
d_model = 128

# ëœë¤ ì‹œí€€ìŠ¤
sequence = np.random.randint(0, vocab_size, size=seq_len)

# NTP: ê° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ 1í† í° ì˜ˆì¸¡
ntp_targets = sequence[1:]  # T-1ê°œì˜ íƒ€ê²Ÿ
ntp_loss_terms = len(ntp_targets)

# MTP (depth=2): ê° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒ 2í† í° ì˜ˆì¸¡
mtp_depth = 2
mtp_loss_terms = 0
for d in range(1, mtp_depth + 1):
    mtp_loss_terms += max(0, seq_len - d)

print("=" * 60)
print("Multi-Token Prediction (MTP) vs Next-Token Prediction (NTP)")
print("=" * 60)
print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}")
print()
print(f"{'ë°©ë²•':<20} | {'ì˜ˆì¸¡ ê¹Šì´':>10} | {'Loss í•­ ìˆ˜':>12} | {'í•™ìŠµ ì‹ í˜¸':>10}")
print("-" * 60)
print(f"{'NTP (ê¸°ì¡´)':<20} | {'D=1':>10} | {ntp_loss_terms:>12} | {'ê¸°ì¤€':>10}")
print(f"{'MTP (DeepSeek)':<20} | {f'D={mtp_depth}':>10} | {mtp_loss_terms:>12} | {f'{mtp_loss_terms/ntp_loss_terms:.1f}x':>10}")
print()

# MTP êµ¬ì¡° ì‹œê°í™” (í…ìŠ¤íŠ¸)
print("MTP ë™ì‘ ì˜ˆì‹œ (D=2):")
print("=" * 50)
example_seq = "ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤".split()
for t in range(min(4, len(example_seq)-2)):
    context = " ".join(example_seq[:t+1])
    target1 = example_seq[t+1] if t+1 < len(example_seq) else "?"
    target2 = example_seq[t+2] if t+2 < len(example_seq) else "?"
    print(f"  ì…ë ¥: [{context}]")
    print(f"    â†’ NTP ì˜ˆì¸¡: {target1}")
    print(f"    â†’ MTP ì˜ˆì¸¡: {target1}, {target2}")
    print()

print("MTP ì¥ì :")
print("  1. í•™ìŠµ ì‹ í˜¸ ì¦ê°€ â†’ ë™ì¼ ë°ì´í„°ë¡œ ë” ë§ì€ í•™ìŠµ")
print("  2. ì¶”ë¡  ì‹œ MTP í—¤ë“œë¥¼ Draft Modelë¡œ í™œìš© â†’ Speculative Decoding")
print("  3. DeepSeek-V3: D=1 (2í† í° ë™ì‹œ ì˜ˆì¸¡)ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ í™•ì¸")"""),

md(r"""## 5. ì •ë¦¬ <a name='5.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Shared Expert | ëª¨ë“  í† í°ì´ ê±°ì¹˜ëŠ” ê³µìœ  ì „ë¬¸ê°€ â†’ ê³µí†µ ì§€ì‹ ë³´ì¡´ | â­â­â­ |
| Aux-Free Balance | í¸í–¥ í•­ $b_i$ë¡œ ë¶€í•˜ ê· í˜• â†’ ì„±ëŠ¥ ì €í•˜ ì—†ìŒ | â­â­â­ |
| MTP | ë‹¤ì¤‘ ë¯¸ë˜ í† í° ë™ì‹œ ì˜ˆì¸¡ â†’ í•™ìŠµ íš¨ìœ¨ + Speculative Decoding | â­â­â­ |
| DeepSeek-V3 ìŠ¤ì¼€ì¼ | 671B (37B active), 256 Routed + 1 Shared, Top-8 | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$y = E_s(x) + \sum_{i \in \text{Top-k}} g_i \cdot E_i^r(x)$$

$$b_i \leftarrow b_i + \gamma(\bar{f} - f_i) \quad \text{(Aux-Free Balance)}$$

$$\mathcal{L}_{MTP} = -\frac{1}{D}\sum_{k=1}^{D}\sum_{t} \log P(x_{t+k}|x_{\leq t})$$

### DeepSeek-V3 í•µì‹¬ ìŠ¤í™

| í•­ëª© | ê°’ |
|------|-----|
| ì´ íŒŒë¼ë¯¸í„° | 671B |
| í™œì„±/í† í° | 37B |
| Shared Expert | 1 |
| Routed Expert | 256 |
| Top-k | 8 |
| í•™ìŠµ ë°ì´í„° | 14.8T í† í° |
| í•™ìŠµ ë¹„ìš© | 2.788M H800 GPU-hours |
| ì •ë°€ë„ | FP8 í˜¼í•© ì •ë°€ë„ |

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**Chapter 13: ìƒì„± AI ì‹¬í™” â€” í™•ì‚° ëª¨ë¸ê³¼ SDE** â€” DDPMì˜ ìˆ˜í•™ì  ê¸°ì´ˆë¶€í„° ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„, DDIM ìƒ˜í”ŒëŸ¬, CFG, Score Matchingê¹Œì§€ í™•ì‚° ëª¨ë¸ì˜ ì „ ì´ë¡  ì²´ê³„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."""),
]

create_notebook(cells, 'chapter12_modern_llms/05_deepseek_moe_architecture.ipynb')
