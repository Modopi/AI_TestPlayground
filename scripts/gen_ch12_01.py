"""Generate Chapter 12-01: Llama Architecture Deep Dive."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

cells = [
# â”€â”€â”€ Cell 0: í—¤ë” â”€â”€â”€
md("""# Chapter 12: ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì•„í‚¤í…ì²˜ â€” Llama ì‹¬ì¸µ ë¶„ì„

## í•™ìŠµ ëª©í‘œ
- Llama 3 ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ(RMSNorm, SwiGLU, GQA)ì˜ **ìˆ˜í•™ì  ì›ë¦¬**ë¥¼ ì´í•´í•œë‹¤
- RMSNormì´ LayerNorm ëŒ€ë¹„ ê°–ëŠ” **ì—°ì‚° íš¨ìœ¨ ì´ì **ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•˜ê³  êµ¬í˜„í•œë‹¤
- SwiGLU í™œì„±í™” í•¨ìˆ˜ì˜ **ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜**ì„ êµ¬í˜„í•˜ê³  GELU ëŒ€ë¹„ ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤
- MHA â†’ MQA â†’ GQAì˜ ë°œì „ ê³¼ì •ì„ ì´í•´í•˜ê³ , **KV í—¤ë“œ ìˆ˜ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ì ˆê°ë¥ **ì„ ê³„ì‚°í•œë‹¤
- RMSNorm + SwiGLU + GQAë¥¼ ê²°í•©í•œ **ì†Œí˜• Llama Block**ì„ ë°‘ë°”ë‹¥ë¶€í„° êµ¬í˜„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: RMSNorm, SwiGLU, GQA](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [RMSNorm vs LayerNorm ë¹„êµ êµ¬í˜„](#2.-RMSNorm-vs-LayerNorm)
3. [SwiGLU FFN êµ¬í˜„](#3.-SwiGLU-FFN)
4. [MHA â†’ MQA â†’ GQA ë°œì „ê³¼ êµ¬í˜„](#4.-GQA-êµ¬í˜„)
5. [ì†Œí˜• Llama Block ì¡°ë¦½](#5.-Llama-Block)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""),

# â”€â”€â”€ Cell 1: ìˆ˜í•™ì  ê¸°ì´ˆ â”€â”€â”€
md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### RMSNorm (Root Mean Square Layer Normalization)

ê¸°ì¡´ LayerNormì€ í‰ê· ê³¼ ë¶„ì‚°ì„ ëª¨ë‘ ê³„ì‚°í•˜ì§€ë§Œ, RMSNormì€ **í‰ê·  ì œê±° ì—†ì´** RMS(ì œê³±í‰ê· ì œê³±ê·¼)ë§Œìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤:

$$\text{RMSNorm}(a_i) = \frac{a_i}{\text{RMS}(\mathbf{a})} \cdot g_i, \quad \text{RMS}(\mathbf{a}) = \sqrt{\frac{1}{n}\sum_{j=1}^{n} a_j^2 + \epsilon}$$

- $a_i$: ì…ë ¥ ë²¡í„°ì˜ $i$ë²ˆì§¸ ì›ì†Œ
- $g_i$: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° (gain)
- $n$: íˆë“  ì°¨ì› í¬ê¸°
- $\epsilon$: ìˆ˜ì¹˜ ì•ˆì •í™” ìƒìˆ˜ (ë³´í†µ $10^{-6}$)

**LayerNormê³¼ ë¹„êµ:**

| êµ¬ë¶„ | LayerNorm | RMSNorm |
|------|-----------|---------|
| ìˆ˜ì‹ | $\frac{a_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma_i + \beta_i$ | $\frac{a_i}{\text{RMS}(\mathbf{a})} \cdot g_i$ |
| íŒŒë¼ë¯¸í„° | $\gamma, \beta$ (2nê°œ) | $g$ (nê°œ) |
| ì—°ì‚° | í‰ê·  + ë¶„ì‚° (2-pass) | RMSë§Œ (1-pass) |
| FLOPs | $\sim 5n$ | $\sim 3n$ |

### SwiGLU (Swish-Gated Linear Unit)

Llamaì˜ FFNì€ í‘œì¤€ 2-layer MLP ëŒ€ì‹  **ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

$$\text{SwiGLU}(x) = \text{Swish}_\beta(xW_1) \otimes (xW_2)$$

$$\text{Swish}_\beta(x) = x \cdot \sigma(\beta x), \quad \sigma(x) = \frac{1}{1+e^{-x}}$$

- $x \in \mathbb{R}^{d_{model}}$: ì…ë ¥
- $W_1, W_2 \in \mathbb{R}^{d_{model} \times d_{ff}}$: ê²Œì´íŠ¸/ê°’ í”„ë¡œì ì…˜ ($\beta=1$ for Llama)
- $W_3 \in \mathbb{R}^{d_{ff} \times d_{model}}$: ì¶œë ¥ í”„ë¡œì ì…˜
- $\otimes$: ì›ì†Œë³„ ê³± (element-wise product)
- $d_{ff}$: Llama 3 8Bì—ì„œ 14,336 ($= \frac{8}{3} \times d_{model}$ì˜ 256 ë°°ìˆ˜ ì˜¬ë¦¼)

**ìµœì¢… FFN ì¶œë ¥:**

$$\text{FFN}(x) = \text{SwiGLU}(x) \cdot W_3 = [\text{Swish}(xW_1) \otimes (xW_2)] W_3$$

### Grouped Query Attention (GQA)

MHAì—ì„œ GQAë¡œì˜ ë°œì „ì€ **KV í—¤ë“œ ìˆ˜ë¥¼ ì¤„ì—¬** ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

| ë°©ì‹ | Q í—¤ë“œ ìˆ˜ | KV í—¤ë“œ ìˆ˜ | KV íŒŒë¼ë¯¸í„° ë¹„ìœ¨ |
|------|----------|-----------|----------------|
| MHA | $H$ | $H$ | $1.0$ |
| MQA | $H$ | $1$ | $1/H$ |
| GQA | $H$ | $G$ | $G/H$ |

Llama 3 8B: $H = 32$, $G = 8$ â†’ KV íŒŒë¼ë¯¸í„°ê°€ MHA ëŒ€ë¹„ $8/32 = 25\%$ë¡œ ì ˆê°

$$\text{KV ë©”ëª¨ë¦¬ ì ˆê°ë¥ } = 1 - \frac{G}{H} = 1 - \frac{n_{kv}}{n_q}$$

**ìš”ì•½ í‘œ:**

| êµ¬ì„± ìš”ì†Œ | ìˆ˜ì‹ | Llama 3 8B ê°’ |
|-----------|------|--------------|
| RMSNorm | $a_i / \text{RMS}(\mathbf{a}) \cdot g_i$ | $\epsilon = 10^{-5}$ |
| SwiGLU | $\text{Swish}(xW_1) \otimes (xW_2)$ | $d_{ff} = 14336$ |
| GQA | $H_Q=32, H_{KV}=8$ | 75% KV ì ˆê° |
| íˆë“  ì°¨ì› | $d_{model}$ | 4096 |
| ë ˆì´ì–´ ìˆ˜ | $L$ | 32 |
| ì–´íœ˜ í¬ê¸° | $V$ | 128,000 |"""),

# â”€â”€â”€ Cell 2: ğŸ£ ì¹œì ˆ ì„¤ëª… â”€â”€â”€
md(r"""---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Llama ì•„í‚¤í…ì²˜ ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ RMSNormì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ë°˜ ì•„ì´ë“¤ì˜ í‚¤ë¥¼ ë¹„êµí•  ë•Œ, **í‰ê·  í‚¤ë¥¼ ë¹¼ê³  ë‚˜ëˆ„ëŠ” ë°©ë²•**(LayerNorm)ê³¼ **ê·¸ëƒ¥ í‚¤ì˜ í¬ê¸°ë¡œë§Œ ë‚˜ëˆ„ëŠ” ë°©ë²•**(RMSNorm)ì´ ìˆì–´ìš”. ë‘ ë²ˆì§¸ ë°©ë²•ì´ ê³„ì‚°ì´ ë” ë¹¨ë¼ìš”!

RMSNormì€ ìˆ«ìë“¤ì„ "ì ë‹¹í•œ í¬ê¸°"ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”. ìˆ«ìê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ AIê°€ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ë°, RMSNormì´ ë”± ë§ê²Œ ì¡°ì ˆí•´ì¤ë‹ˆë‹¤.

#### ğŸšª SwiGLUëŠ” ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: **ë¬¸ ë‘ ê°œê°€ ë‹¬ë¦° ë³µë„**ë¥¼ ìƒìƒí•´ë³´ì„¸ìš”. ì²« ë²ˆì§¸ ë¬¸(Swish)ì€ "ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€" ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ë‘ ë²ˆì§¸ ë¬¸ì€ "ì‹¤ì œ ì •ë³´"ë¥¼ í†µê³¼ì‹œì¼œìš”. ë‘ ë¬¸ì˜ ê²°ê³¼ë¥¼ í•©ì³ì„œ ì •ë§ ì¤‘ìš”í•œ ì •ë³´ë§Œ í†µê³¼í•©ë‹ˆë‹¤!

#### ğŸ‘¥ GQAëŠ” ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ì„ ë³¼ ë•Œ, **ì§ˆë¬¸ì§€(Q)ëŠ” 32ëª…**ì´ ê°ì ë‹¤ë¥´ê²Œ ê°–ê³  ìˆì§€ë§Œ, **ë‹µì•ˆì§€(K,V)ëŠ” 8ê°œ ê·¸ë£¹ì´ ê³µìœ **í•´ìš”. ë‹µì•ˆì§€ë¥¼ ëœ ë§Œë“¤ì–´ë„ ë˜ë‹ˆê¹Œ ì¢…ì´(ë©”ëª¨ë¦¬)ê°€ ì ˆì•½ë©ë‹ˆë‹¤!

| ë°©ì‹ | ë¹„ìœ  | ë©”ëª¨ë¦¬ |
|------|------|--------|
| MHA | í•™ìƒ 32ëª…ì´ ê°ì ë‹µì•ˆì§€ 32ì¥ | ğŸ’¸ğŸ’¸ğŸ’¸ |
| MQA | í•™ìƒ 32ëª…ì´ ë‹µì•ˆì§€ 1ì¥ ê³µìœ  | ğŸ’¸ (ë„ˆë¬´ ì ì–´ í’ˆì§ˆâ†“) |
| GQA | í•™ìƒ 32ëª…ì´ 8ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  8ì¥ ê³µìœ  | ğŸ’¸ğŸ’¸ (ì ì ˆ!) |"""),

# â”€â”€â”€ Cell 3: ğŸ“ ì—°ìŠµ ë¬¸ì œ â”€â”€â”€
md(r"""---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: RMSNorm ìˆ˜ë™ ê³„ì‚°

ì…ë ¥ ë²¡í„° $\mathbf{a} = [3, 4, 0]$, ê²Œì¸ $\mathbf{g} = [1, 1, 1]$, $\epsilon = 0$ì¼ ë•Œ RMSNormì˜ ì¶œë ¥ì„ êµ¬í•˜ì‹œì˜¤.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{RMS}(\mathbf{a}) = \sqrt{\frac{3^2 + 4^2 + 0^2}{3}} = \sqrt{\frac{25}{3}} \approx 2.887$$

$$\text{RMSNorm}(\mathbf{a}) = \left[\frac{3}{2.887}, \frac{4}{2.887}, \frac{0}{2.887}\right] \approx [1.039, 1.386, 0]$$

ë²¡í„°ì˜ "í¬ê¸°"ë§Œìœ¼ë¡œ ì •ê·œí™”ë˜ì–´, ì›ë˜ ë°©í–¥ì€ ìœ ì§€í•˜ë©´ì„œ ìŠ¤ì¼€ì¼ë§Œ ì¡°ì ˆë¨.
</details>

#### ë¬¸ì œ 2: GQA ë©”ëª¨ë¦¬ ì ˆê°ë¥  ê³„ì‚°

Llama 3 70BëŠ” $H_Q = 64$, $H_{KV} = 8$ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. MHA ëŒ€ë¹„ GQAì˜ KV íŒŒë¼ë¯¸í„° ì ˆê°ë¥ ê³¼, ì‹œí€€ìŠ¤ ê¸¸ì´ $S = 4096$ì¼ ë•Œ FP16 KV ìºì‹œ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤. ($L=80, d_{head}=128$)

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

**ì ˆê°ë¥ **: $1 - G/H = 1 - 8/64 = 87.5\%$

**KV ìºì‹œ í¬ê¸°**:
$$M_{KV} = 2 \times L \times H_{KV} \times d_{head} \times S \times 2\text{B}$$
$$= 2 \times 80 \times 8 \times 128 \times 4096 \times 2 = 2 \times 80 \times 8 \times 128 \times 4096 \times 2$$
$$= 1,073,741,824 \text{ bytes} = 1.07 \text{ GB (ë°°ì¹˜ 1 ê¸°ì¤€)}$$

MHAì˜€ë‹¤ë©´: $2 \times 80 \times 64 \times 128 \times 4096 \times 2 = 8.59$ GB â†’ GQAë¡œ **7.52 GB ì ˆì•½!**
</details>"""),

# â”€â”€â”€ Cell 4: ì„í¬íŠ¸ â”€â”€â”€
code("""import numpy as np
import matplotlib
matplotlib.use('Agg')  # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ í•„ìˆ˜
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""),

# â”€â”€â”€ Cell 5: ì„¹ì…˜2 ë§ˆí¬ë‹¤ìš´ â”€â”€â”€
md("""## 2. RMSNorm vs LayerNorm ë¹„êµ êµ¬í˜„ <a name='2.-RMSNorm-vs-LayerNorm'></a>"""),

# â”€â”€â”€ Cell 6: RMSNorm êµ¬í˜„ â”€â”€â”€
code(r"""# â”€â”€ RMSNorm vs LayerNorm êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RMSNormê³¼ LayerNormì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ìˆ˜ì¹˜ì  ì°¨ì´ì™€ ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤

class RMSNorm(tf.keras.layers.Layer):
    # Root Mean Square Layer Normalization
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.g = self.add_weight(name='gain', shape=(dim,),
                                 initializer='ones', trainable=True)

    def call(self, x):
        # RMS ê³„ì‚°: sqrt(mean(x^2) + eps)
        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + self.eps)
        return (x / rms) * self.g


# í…ŒìŠ¤íŠ¸ ë°ì´í„°
d_model = 4096
batch_seq = (2, 128)  # (batch, seq_len)
x = tf.random.normal((*batch_seq, d_model))

# RMSNorm
rms_norm = RMSNorm(d_model)
rms_out = rms_norm(x)

# LayerNorm (TF ë‚´ì¥)
layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)
ln_out = layer_norm(x)

print("=" * 55)
print("RMSNorm vs LayerNorm ìˆ˜ì¹˜ ë¹„êµ")
print("=" * 55)
print(f"ì…ë ¥ shape: {x.shape}")
print(f"ì…ë ¥ í‰ê· : {tf.reduce_mean(x).numpy():.4f}")
print(f"ì…ë ¥ í‘œì¤€í¸ì°¨: {tf.math.reduce_std(x).numpy():.4f}")
print()
print(f"RMSNorm ì¶œë ¥ í‰ê· : {tf.reduce_mean(rms_out).numpy():.6f}")
print(f"RMSNorm ì¶œë ¥ std:  {tf.math.reduce_std(rms_out).numpy():.6f}")
print(f"LayerNorm ì¶œë ¥ í‰ê· : {tf.reduce_mean(ln_out).numpy():.6f}")
print(f"LayerNorm ì¶œë ¥ std:  {tf.math.reduce_std(ln_out).numpy():.6f}")
print()

# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
rms_params = sum(tf.size(v).numpy() for v in rms_norm.trainable_variables)
ln_params = sum(tf.size(v).numpy() for v in layer_norm.trainable_variables)
print(f"RMSNorm íŒŒë¼ë¯¸í„°: {rms_params:,} (gainë§Œ)")
print(f"LayerNorm íŒŒë¼ë¯¸í„°: {ln_params:,} (gamma + beta)")
print(f"íŒŒë¼ë¯¸í„° ì ˆê°: {(1 - rms_params/ln_params)*100:.0f}%")"""),

# â”€â”€â”€ Cell 7: RMSNorm ì†ë„ ë²¤ì¹˜ë§ˆí¬ â”€â”€â”€
code(r"""# â”€â”€ RMSNorm vs LayerNorm ì†ë„ ë²¤ì¹˜ë§ˆí¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì‹œê°„ ì¸¡ì •

n_warmup = 10
n_runs = 100

# ì›Œë°ì—…
for _ in range(n_warmup):
    _ = rms_norm(x)
    _ = layer_norm(x)

# RMSNorm ë²¤ì¹˜ë§ˆí¬
start = time.perf_counter()
for _ in range(n_runs):
    _ = rms_norm(x)
rms_time = (time.perf_counter() - start) / n_runs * 1000

# LayerNorm ë²¤ì¹˜ë§ˆí¬
start = time.perf_counter()
for _ in range(n_runs):
    _ = layer_norm(x)
ln_time = (time.perf_counter() - start) / n_runs * 1000

print("=" * 55)
print("ì†ë„ ë²¤ì¹˜ë§ˆí¬ (CPU, d_model=4096, batch=2, seq=128)")
print("=" * 55)
print(f"{'ë°©ë²•':<20} | {'ì‹œê°„ (ms)':>12} | {'ìƒëŒ€ ì†ë„':>10}")
print("-" * 55)
print(f"{'RMSNorm':<20} | {rms_time:>12.3f} | {'ê¸°ì¤€':>10}")
print(f"{'LayerNorm':<20} | {ln_time:>12.3f} | {ln_time/rms_time:>9.2f}x")
print()
print(f"RMSNormì´ LayerNorm ëŒ€ë¹„ ì•½ {(1-rms_time/ln_time)*100:.1f}% ë¹ ë¦„ (CPU ê¸°ì¤€)")"""),

# â”€â”€â”€ Cell 8: ì„¹ì…˜3 ë§ˆí¬ë‹¤ìš´ â”€â”€â”€
md("""## 3. SwiGLU FFN êµ¬í˜„ <a name='3.-SwiGLU-FFN'></a>"""),

# â”€â”€â”€ Cell 9: SwiGLU êµ¬í˜„ â”€â”€â”€
code(r"""# â”€â”€ SwiGLU FFN êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Llama 3 ìŠ¤íƒ€ì¼ì˜ SwiGLU FFNì„ êµ¬í˜„í•˜ê³ , í‘œì¤€ GELU FFNê³¼ ë¹„êµí•©ë‹ˆë‹¤

class SwiGLUFFN(tf.keras.layers.Layer):
    # SwiGLU Feed-Forward Network (Llama 3 style)
    def __init__(self, d_model, d_ff):
        super().__init__()
        # W1: gate projection, W2: up projection, W3: down projection
        self.w1 = tf.keras.layers.Dense(d_ff, use_bias=False, name='gate')
        self.w2 = tf.keras.layers.Dense(d_ff, use_bias=False, name='up')
        self.w3 = tf.keras.layers.Dense(d_model, use_bias=False, name='down')

    def call(self, x):
        # SwiGLU: Swish(x @ W1) * (x @ W2) @ W3
        gate = tf.nn.silu(self.w1(x))    # Swish = SiLU
        up = self.w2(x)
        return self.w3(gate * up)         # element-wise product â†’ down projection


class StandardFFN(tf.keras.layers.Layer):
    # í‘œì¤€ GELU FFN (GPT-2 ìŠ¤íƒ€ì¼)
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.fc1 = tf.keras.layers.Dense(d_ff, use_bias=False)
        self.fc2 = tf.keras.layers.Dense(d_model, use_bias=False)

    def call(self, x):
        return self.fc2(tf.nn.gelu(self.fc1(x)))


# Llama 3 8B ê¸°ì¤€ FFN ì°¨ì›
d_model = 4096
d_ff_swiglu = 14336   # Llama 3: 8/3 * 4096 â†’ 256ì˜ ë°°ìˆ˜ ì˜¬ë¦¼
d_ff_standard = 11008  # ìœ ì‚¬ íŒŒë¼ë¯¸í„° ìˆ˜ì˜ í‘œì¤€ FFN

swiglu_ffn = SwiGLUFFN(d_model, d_ff_swiglu)
std_ffn = StandardFFN(d_model, d_ff_standard)

# í…ŒìŠ¤íŠ¸
x_test = tf.random.normal((1, 16, d_model))
y_swiglu = swiglu_ffn(x_test)
y_std = std_ffn(x_test)

# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
swiglu_params = sum(tf.size(v).numpy() for v in swiglu_ffn.trainable_variables)
std_params = sum(tf.size(v).numpy() for v in std_ffn.trainable_variables)

print("=" * 60)
print("SwiGLU FFN vs í‘œì¤€ GELU FFN ë¹„êµ")
print("=" * 60)
print(f"{'í•­ëª©':<25} | {'SwiGLU':>15} | {'í‘œì¤€ GELU':>15}")
print("-" * 60)
print(f"{'d_ff':<25} | {d_ff_swiglu:>15,} | {d_ff_standard:>15,}")
print(f"{'íŒŒë¼ë¯¸í„° ìˆ˜':<25} | {swiglu_params:>15,} | {std_params:>15,}")
print(f"{'ì¶œë ¥ shape':<25} | {str(y_swiglu.shape):>15} | {str(y_std.shape):>15}")
print(f"{'ê²Œì´íŒ… ë°©ì‹':<25} | {'Swish gate':>15} | {'ì—†ìŒ':>15}")
print()
print("SwiGLUëŠ” ê²Œì´íŠ¸ í”„ë¡œì ì…˜(W1)ì„ ì¶”ê°€í•˜ì—¬ 3ê°œì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì‚¬ìš©")
print(f"  W1(gate): {d_model}Ã—{d_ff_swiglu} = {d_model*d_ff_swiglu:,}")
print(f"  W2(up):   {d_model}Ã—{d_ff_swiglu} = {d_model*d_ff_swiglu:,}")
print(f"  W3(down): {d_ff_swiglu}Ã—{d_model} = {d_ff_swiglu*d_model:,}")"""),

# â”€â”€â”€ Cell 10: SwiGLU ì‹œê°í™” â”€â”€â”€
code(r"""# â”€â”€ SwiGLU í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Swish, GELU, ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ SwiGLUì˜ íŠ¹ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤

x_range = np.linspace(-4, 4, 500)
x_tf = tf.constant(x_range, dtype=tf.float32)

activations = {
    'Swish (SiLU)': tf.nn.silu(x_tf).numpy(),
    'GELU': tf.nn.gelu(x_tf).numpy(),
    'ReLU': tf.nn.relu(x_tf).numpy(),
}

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# ì™¼ìª½: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ
ax1 = axes[0]
colors = ['#1E88E5', '#43A047', '#E53935']
for (name, vals), c in zip(activations.items(), colors):
    ax1.plot(x_range, vals, lw=2.5, label=name, color=c)
ax1.axhline(y=0, color='gray', ls='--', lw=1)
ax1.axvline(x=0, color='gray', ls='--', lw=1)
ax1.set_xlabel('x', fontsize=11)
ax1.set_ylabel('f(x)', fontsize=11)
ax1.set_title('í™œì„±í™” í•¨ìˆ˜ ë¹„êµ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: SwiGLU ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”
ax2 = axes[1]
gate_signal = tf.nn.silu(x_tf).numpy()
value_signal = np.tanh(x_range * 0.5)  # ì˜ˆì‹œ ê°’ ì‹ í˜¸
swiglu_out = gate_signal * value_signal

ax2.plot(x_range, gate_signal, 'b-', lw=2, label='Gate: Swish(xWâ‚)', alpha=0.7)
ax2.plot(x_range, value_signal, 'g-', lw=2, label='Value: xWâ‚‚', alpha=0.7)
ax2.plot(x_range, swiglu_out, 'r-', lw=2.5, label='SwiGLU: Gate âŠ— Value')
ax2.axhline(y=0, color='gray', ls='--', lw=1)
ax2.set_xlabel('x', fontsize=11)
ax2.set_ylabel('ì¶œë ¥', fontsize=11)
ax2.set_title('SwiGLU ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter12_modern_llms/swiglu_activation.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/swiglu_activation.png")
print()
print("í•µì‹¬ ê´€ì°°:")
print("  â€¢ SwishëŠ” ìŒìˆ˜ ì˜ì—­ì—ì„œ ì‘ì€ ìŒìˆ˜ê°’ì„ í—ˆìš© (ReLUì™€ ì°¨ì´)")
print("  â€¢ SwiGLUëŠ” Gate ì‹ í˜¸ë¡œ Value ì‹ í˜¸ë¥¼ 'í•„í„°ë§'í•˜ì—¬ ì¤‘ìš” ì •ë³´ë§Œ í†µê³¼")
print("  â€¢ ì´ ê²Œì´íŒ…ì´ í‘œì¤€ FFN ëŒ€ë¹„ í‘œí˜„ë ¥ì„ ë†’ì—¬ì¤Œ")"""),

# â”€â”€â”€ Cell 11: ì„¹ì…˜4 ë§ˆí¬ë‹¤ìš´ â”€â”€â”€
md("""## 4. MHA â†’ MQA â†’ GQA ë°œì „ê³¼ êµ¬í˜„ <a name='4.-GQA-êµ¬í˜„'></a>"""),

# â”€â”€â”€ Cell 12: GQA êµ¬í˜„ â”€â”€â”€
code(r"""# â”€â”€ MHA / MQA / GQA êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¸ ê°€ì§€ Attention ë°©ì‹ì„ êµ¬í˜„í•˜ì—¬ ë©”ëª¨ë¦¬ ë° íŒŒë¼ë¯¸í„° ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤

class GroupedQueryAttention(tf.keras.layers.Layer):
    # Grouped Query Attention (GQA) - MHA/MQAë¥¼ ëª¨ë‘ í¬ê´„
    def __init__(self, d_model, n_q_heads, n_kv_heads):
        super().__init__()
        self.n_q_heads = n_q_heads
        self.n_kv_heads = n_kv_heads
        self.d_head = d_model // n_q_heads
        self.n_groups = n_q_heads // n_kv_heads  # Q í—¤ë“œë¥¼ KV ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°

        # Q í”„ë¡œì ì…˜: ì „ì²´ Q í—¤ë“œ ìˆ˜ ì‚¬ìš©
        self.wq = tf.keras.layers.Dense(n_q_heads * self.d_head, use_bias=False)
        # K, V í”„ë¡œì ì…˜: KV í—¤ë“œ ìˆ˜ë§Œ ì‚¬ìš© (ì—¬ê¸°ê°€ í•µì‹¬!)
        self.wk = tf.keras.layers.Dense(n_kv_heads * self.d_head, use_bias=False)
        self.wv = tf.keras.layers.Dense(n_kv_heads * self.d_head, use_bias=False)
        self.wo = tf.keras.layers.Dense(d_model, use_bias=False)

    def call(self, x, mask=None):
        B, S, _ = x.shape

        # í”„ë¡œì ì…˜
        q = tf.reshape(self.wq(x), (B, S, self.n_q_heads, self.d_head))
        k = tf.reshape(self.wk(x), (B, S, self.n_kv_heads, self.d_head))
        v = tf.reshape(self.wv(x), (B, S, self.n_kv_heads, self.d_head))

        # KV í—¤ë“œë¥¼ Q ê·¸ë£¹ ìˆ˜ë§Œí¼ ë°˜ë³µ (repeat_kv)
        # [B, S, n_kv, d] â†’ [B, S, n_kv, n_groups, d] â†’ [B, S, n_q, d]
        k = tf.repeat(k, repeats=self.n_groups, axis=2)
        v = tf.repeat(v, repeats=self.n_groups, axis=2)

        # [B, n_heads, S, d_head] í˜•íƒœë¡œ ì „ì¹˜
        q = tf.transpose(q, [0, 2, 1, 3])  # [B, n_q, S, d]
        k = tf.transpose(k, [0, 2, 1, 3])
        v = tf.transpose(v, [0, 2, 1, 3])

        # Scaled Dot-Product Attention
        scale = tf.math.sqrt(tf.cast(self.d_head, tf.float32))
        attn = tf.matmul(q, k, transpose_b=True) / scale  # [B, n_q, S, S]

        if mask is not None:
            attn += mask

        attn_weights = tf.nn.softmax(attn, axis=-1)
        out = tf.matmul(attn_weights, v)  # [B, n_q, S, d]

        # Concat heads
        out = tf.transpose(out, [0, 2, 1, 3])
        out = tf.reshape(out, (B, S, self.n_q_heads * self.d_head))
        return self.wo(out)


# Llama 3 8B ìŠ¤ì¼€ì¼ (ì¶•ì†Œ ë²„ì „)
d_model = 256  # ë°ëª¨ìš© ì¶•ì†Œ

configs = {
    'MHA (H=8, G=8)': (8, 8),
    'MQA (H=8, G=1)': (8, 1),
    'GQA (H=8, G=2)': (8, 2),  # Llama ìŠ¤íƒ€ì¼: G = H/4
}

x_test = tf.random.normal((1, 32, d_model))

print("=" * 65)
print("MHA / MQA / GQA íŒŒë¼ë¯¸í„° ë° KV ë©”ëª¨ë¦¬ ë¹„êµ")
print("=" * 65)
print(f"{'ë°©ì‹':<20} | {'Q í—¤ë“œ':>7} | {'KV í—¤ë“œ':>7} | {'íŒŒë¼ë¯¸í„°':>12} | {'KV ë¹„ìœ¨':>8}")
print("-" * 65)

kv_sizes = {}
for name, (n_q, n_kv) in configs.items():
    attn = GroupedQueryAttention(d_model, n_q, n_kv)
    _ = attn(x_test)  # ë¹Œë“œ
    total_params = sum(tf.size(v).numpy() for v in attn.trainable_variables)
    kv_ratio = n_kv / n_q
    kv_sizes[name] = kv_ratio
    print(f"{name:<20} | {n_q:>7} | {n_kv:>7} | {total_params:>12,} | {kv_ratio:>7.0%}")

print()
print("ğŸ’¡ GQAëŠ” MHAì˜ í’ˆì§ˆì„ ìœ ì§€í•˜ë©´ì„œ MQA ìˆ˜ì¤€ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë‹¬ì„±!")"""),

# â”€â”€â”€ Cell 13: GQA ì‹œê°í™” â”€â”€â”€
code(r"""# â”€â”€ MHA / MQA / GQA êµ¬ì¡° ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Q-K-V í—¤ë“œ ë§¤í•‘ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ë‹¤ì´ì–´ê·¸ë¨

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

configs_viz = [
    ('MHA\n(Multi-Head)', 8, 8),
    ('MQA\n(Multi-Query)', 8, 1),
    ('GQA\n(Grouped-Query)', 8, 2),
]

for ax, (title, n_q, n_kv) in zip(axes, configs_viz):
    n_groups = n_q // n_kv

    # Q í—¤ë“œ (ìƒë‹¨)
    for i in range(n_q):
        color_idx = i // n_groups
        color = plt.cm.Set3(color_idx / max(n_kv, 1))
        ax.add_patch(plt.Rectangle((i * 1.1, 2.5), 0.9, 0.8,
                                    facecolor=color, edgecolor='black', lw=1.5))
        ax.text(i * 1.1 + 0.45, 2.9, f'Q{i}', ha='center', va='center', fontsize=7)

    # KV í—¤ë“œ (í•˜ë‹¨)
    kv_width = (n_q * 1.1) / n_kv
    for j in range(n_kv):
        color = plt.cm.Set3(j / max(n_kv, 1))
        # K
        ax.add_patch(plt.Rectangle((j * kv_width, 1.2), kv_width - 0.2, 0.8,
                                    facecolor=color, edgecolor='black', lw=1.5, alpha=0.7))
        ax.text(j * kv_width + (kv_width - 0.2) / 2, 1.6, f'K{j}',
                ha='center', va='center', fontsize=7)
        # V
        ax.add_patch(plt.Rectangle((j * kv_width, 0.0), kv_width - 0.2, 0.8,
                                    facecolor=color, edgecolor='black', lw=1.5, alpha=0.5))
        ax.text(j * kv_width + (kv_width - 0.2) / 2, 0.4, f'V{j}',
                ha='center', va='center', fontsize=7)

    # ì—°ê²°ì„ 
    for i in range(n_q):
        kv_idx = i // n_groups
        x_q = i * 1.1 + 0.45
        x_kv = kv_idx * kv_width + (kv_width - 0.2) / 2
        ax.plot([x_q, x_kv], [2.5, 2.0], 'k-', alpha=0.3, lw=0.8)

    ax.set_xlim(-0.5, n_q * 1.1 + 0.5)
    ax.set_ylim(-0.5, 4.0)
    ax.set_title(title, fontweight='bold', fontsize=11)
    ax.set_ylabel('Q â†’ K,V ë§¤í•‘', fontsize=9)
    ax.axis('off')

    # KV ë©”ëª¨ë¦¬ ë¹„ìœ¨ í‘œì‹œ
    ratio = n_kv / n_q * 100
    ax.text(n_q * 1.1 / 2, -0.3, f'KV ë©”ëª¨ë¦¬: {ratio:.0f}%', ha='center',
            fontsize=10, fontweight='bold', color='#D32F2F')

plt.suptitle('Attention ë°©ì‹ë³„ Q-KV í—¤ë“œ ë§¤í•‘ ë¹„êµ', fontsize=13, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('chapter12_modern_llms/gqa_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/gqa_comparison.png")
print()
print("GQA (Llama 3 ìŠ¤íƒ€ì¼):")
print(f"  â€¢ Q í—¤ë“œ 32ê°œê°€ KV í—¤ë“œ 8ê°œë¥¼ 4:1ë¡œ ê³µìœ ")
print(f"  â€¢ MHA ëŒ€ë¹„ KV íŒŒë¼ë¯¸í„° 75% ì ˆê°")
print(f"  â€¢ MQAë³´ë‹¤ í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ì„œ MHAë³´ë‹¤ ë¹ ë¦„ â†’ ìµœì  ê· í˜•ì ")"""),

# â”€â”€â”€ Cell 14: ì„¹ì…˜5 ë§ˆí¬ë‹¤ìš´ â”€â”€â”€
md("""## 5. ì†Œí˜• Llama Block ì¡°ë¦½ <a name='5.-Llama-Block'></a>"""),

# â”€â”€â”€ Cell 15: Llama Block ì¡°ë¦½ â”€â”€â”€
code(r"""# â”€â”€ ì†Œí˜• Llama Decoder Block ì¡°ë¦½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RMSNorm + GQA + SwiGLUë¥¼ ê²°í•©í•˜ì—¬ Llama 3 ìŠ¤íƒ€ì¼ ë¸”ë¡ì„ ì™„ì„±í•©ë‹ˆë‹¤

class LlamaDecoderBlock(tf.keras.layers.Layer):
    # Llama 3 ìŠ¤íƒ€ì¼ Decoder Block: Pre-Norm + GQA + SwiGLU
    def __init__(self, d_model, n_q_heads, n_kv_heads, d_ff, eps=1e-6):
        super().__init__()
        # Pre-Norm: Attention ì „ RMSNorm
        self.attn_norm = RMSNorm(d_model, eps)
        self.attn = GroupedQueryAttention(d_model, n_q_heads, n_kv_heads)

        # Pre-Norm: FFN ì „ RMSNorm
        self.ffn_norm = RMSNorm(d_model, eps)
        self.ffn = SwiGLUFFN(d_model, d_ff)

    def call(self, x, mask=None):
        # Pre-Norm â†’ Attention â†’ Residual
        h = x + self.attn(self.attn_norm(x), mask=mask)
        # Pre-Norm â†’ SwiGLU FFN â†’ Residual
        out = h + self.ffn(self.ffn_norm(h))
        return out


# ì†Œí˜• Llama 3 ì„¤ì • (ë°ëª¨ìš© ì¶•ì†Œ)
config = {
    'd_model': 256,
    'n_q_heads': 8,
    'n_kv_heads': 2,     # GQA: 4:1
    'd_ff': 688,          # ì•½ 8/3 * 256 â†’ 256ì˜ ë°°ìˆ˜ì— ê°€ê¹ê²Œ
}

block = LlamaDecoderBlock(**config)

# Causal mask ìƒì„±
seq_len = 32
causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
causal_mask = (1.0 - causal_mask) * -1e9  # ë¯¸ë˜ í† í° ë§ˆìŠ¤í‚¹
causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # [1, 1, S, S]

# í¬ì›Œë“œ íŒ¨ìŠ¤
x_input = tf.random.normal((2, seq_len, config['d_model']))
output = block(x_input, mask=causal_mask)

print("=" * 60)
print("ì†Œí˜• Llama Decoder Block êµ¬ì¡°")
print("=" * 60)
print(f"ì„¤ì •:")
print(f"  d_model:    {config['d_model']}")
print(f"  n_q_heads:  {config['n_q_heads']}")
print(f"  n_kv_heads: {config['n_kv_heads']} (GQA ratio: {config['n_q_heads']//config['n_kv_heads']}:1)")
print(f"  d_ff:       {config['d_ff']}")
print()
print(f"ì…ë ¥ shape:  {x_input.shape}")
print(f"ì¶œë ¥ shape:  {output.shape}")
print(f"shape ë³´ì¡´:  {x_input.shape == output.shape}")
print()

# ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜
total_params = sum(tf.size(v).numpy() for v in block.trainable_variables)
print(f"ë¸”ë¡ ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
print()
print("ë¸”ë¡ êµ¬ì¡° (Pre-Norm Transformer):")
print("  x â†’ RMSNorm â†’ GQA â†’ + (residual)")
print("    â†’ RMSNorm â†’ SwiGLU FFN â†’ + (residual) â†’ output")"""),

# â”€â”€â”€ Cell 16: Llama 3 ì‹¤ì œ ê·œëª¨ ë¶„ì„ â”€â”€â”€
code(r"""# â”€â”€ Llama 3 8B ì‹¤ì œ ê·œëª¨ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤ì œ Llama 3 8B íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ê·œëª¨ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤

print("=" * 65)
print("Llama 3 8B ì•„í‚¤í…ì²˜ ë¶„ì„")
print("=" * 65)

# Llama 3 8B ê³µì‹ ìŠ¤í™ (Meta AI, 2024)
specs = {
    'd_model': 4096,
    'n_layers': 32,
    'n_q_heads': 32,
    'n_kv_heads': 8,
    'd_head': 128,       # 4096 / 32
    'd_ff': 14336,       # 8/3 * 4096, rounded to 256 multiple
    'vocab_size': 128000,
    'max_seq_len': 8192,
    'rope_base': 500000,
}

for k, v in specs.items():
    print(f"  {k:<15} = {v:>10,}")

print()

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
d = specs['d_model']
L = specs['n_layers']
Hq = specs['n_q_heads']
Hkv = specs['n_kv_heads']
dh = specs['d_head']
dff = specs['d_ff']
V = specs['vocab_size']

# ê° ë¸”ë¡ íŒŒë¼ë¯¸í„°
attn_params = (Hq * dh * d) + 2 * (Hkv * dh * d) + (d * d)  # Wq + Wk + Wv + Wo
ffn_params = 3 * d * dff  # W1 + W2 + W3 (SwiGLU)
norm_params = 2 * d  # 2x RMSNorm per block
block_params = attn_params + ffn_params + norm_params

# ì „ì²´ ëª¨ë¸
embed_params = V * d  # í† í° ì„ë² ë”©
final_norm = d
head_params = d * V  # lm_head (ë³´í†µ ì„ë² ë”©ê³¼ ê³µìœ )
total = L * block_params + embed_params + final_norm

print(f"{'êµ¬ì„± ìš”ì†Œ':<30} | {'íŒŒë¼ë¯¸í„° ìˆ˜':>15} | {'ë¹„ìœ¨':>8}")
print("-" * 60)
print(f"{'Attention (per block)':<30} | {attn_params:>15,} | {attn_params/block_params*100:>6.1f}%")
print(f"{'SwiGLU FFN (per block)':<30} | {ffn_params:>15,} | {ffn_params/block_params*100:>6.1f}%")
print(f"{'RMSNorm (per block)':<30} | {norm_params:>15,} | {norm_params/block_params*100:>6.1f}%")
print(f"{'ë¸”ë¡ í•©ê³„':<30} | {block_params:>15,} |")
print(f"{'32 ë¸”ë¡ í•©ê³„':<30} | {L*block_params:>15,} |")
print(f"{'í† í° ì„ë² ë”©':<30} | {embed_params:>15,} |")
print(f"{'Final RMSNorm':<30} | {final_norm:>15,} |")
print("-" * 60)
print(f"{'ì´ê³„ (lm_head ê³µìœ  ê°€ì •)':<30} | {total:>15,} |")
print(f"{'â‰ˆ':<30} | {total/1e9:>14.2f}B |")"""),

# â”€â”€â”€ Cell 17: ì •ë¦¬ â”€â”€â”€
md(r"""## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| RMSNorm | í‰ê·  ì œê±° ì—†ì´ RMSë§Œìœ¼ë¡œ ì •ê·œí™” â†’ LayerNorm ëŒ€ë¹„ ~40% ì—°ì‚° ì ˆê° | â­â­â­ |
| SwiGLU | Swish ê²Œì´íŒ…ìœ¼ë¡œ ì •ë³´ í•„í„°ë§ â†’ GELU FFN ëŒ€ë¹„ í‘œí˜„ë ¥ í–¥ìƒ | â­â­â­ |
| GQA | KV í—¤ë“œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ê³µìœ  â†’ MHA í’ˆì§ˆ + MQA íš¨ìœ¨ì˜ ìµœì  ê· í˜• | â­â­â­ |
| Pre-Norm | Attention/FFN ì „ì— ì •ê·œí™” â†’ Post-Norm ëŒ€ë¹„ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ | â­â­ |
| Residual Connection | ì…ë ¥ì„ ì¶œë ¥ì— ë”í•´ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë°©ì§€ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\text{RMSNorm}(a_i) = \frac{a_i}{\sqrt{\frac{1}{n}\sum_j a_j^2 + \epsilon}} \cdot g_i$$

$$\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_2)$$

$$\text{GQA: } H_Q = 32, \; H_{KV} = 8 \;\Rightarrow\; \text{KV ì ˆê°ë¥ } = 1 - \frac{8}{32} = 75\%$$

### Llama 3 8B í•µì‹¬ ìŠ¤í™

| í•­ëª© | ê°’ |
|------|-----|
| ë ˆì´ì–´ ìˆ˜ | 32 |
| íˆë“  ì°¨ì› | 4096 |
| Q í—¤ë“œ | 32 |
| KV í—¤ë“œ | 8 (GQA) |
| FFN ì°¨ì› | 14,336 (SwiGLU) |
| ì–´íœ˜ í¬ê¸° | 128,000 |
| ì´ íŒŒë¼ë¯¸í„° | ~8B |

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**Chapter 12-02: KV Cacheì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬** â€” Autoregressive ìƒì„±ì—ì„œ KV Cacheì˜ ë©”ëª¨ë¦¬ ê³„ì‚°, Rolling Buffer, Prefix Caching ë“± ì‹¤ì „ ì„œë¹™ì— í•„ìˆ˜ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."""),
]

create_notebook(cells, 'chapter12_modern_llms/01_llama_architecture_deepdive.ipynb')
