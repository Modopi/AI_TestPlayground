"""Generate chapter16_sparse_attention/03_linear_attention_and_hybrids.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Linear Attentionê³¼ Hybrid êµ¬ì¡°

## í•™ìŠµ ëª©í‘œ
- í‘œì¤€ Softmax Attentionê³¼ Linear Attentionì˜ ë³µì¡ë„ ì°¨ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤
- GLA(Gated Linear Attention)ì˜ ìˆœí™˜(recurrence) êµ¬ì¡°ë¥¼ êµ¬í˜„í•œë‹¤
- RetNetê³¼ Mambaì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ë¹„êµ ë¶„ì„í•œë‹¤
- Qwenì˜ SWA+Full+Linear Hybrid ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤
- ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ìŠ¤ì¼€ì¼ë§ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Linear Attentionê³¼ ì»¤ë„ í•¨ìˆ˜](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Standard vs Linear Attention ë³µì¡ë„ ë¹„êµ](#2.-ë³µì¡ë„-ë¹„êµ)
3. [GLA ìˆœí™˜ ì‹œë®¬ë ˆì´ì…˜](#3.-GLA-ìˆœí™˜-ì‹œë®¬ë ˆì´ì…˜)
4. [Qwen Hybrid ì•„í‚¤í…ì²˜](#4.-Qwen-Hybrid-ì•„í‚¤í…ì²˜)
5. [ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ë¹„êµ](#5.-ë©”ëª¨ë¦¬-ì†ë„-ë¹„êµ)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### í‘œì¤€ Softmax Attention

$$\text{Attn}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d}}\right) V$$

- ì‹œê°„ ë³µì¡ë„: $O(N^2 d)$, ê³µê°„ ë³µì¡ë„: $O(N^2 + Nd)$
- $N$: ì‹œí€€ìŠ¤ ê¸¸ì´, $d$: ì°¨ì›

### Linear Attention

ì»¤ë„ í•¨ìˆ˜ $\phi$ë¥¼ ì‚¬ìš©í•˜ì—¬ softmaxë¥¼ ê·¼ì‚¬:

$$O_i = \frac{\phi(Q_i) \sum_{j=1}^{N} \phi(K_j)^T V_j}{\phi(Q_i) \sum_{j=1}^{N} \phi(K_j)^T}$$

**í•µì‹¬ íŠ¸ë¦­**: ê²°í•© ë²•ì¹™ì„ ì´ìš©í•œ ì—°ì‚° ìˆœì„œ ë³€ê²½

$$O = \phi(Q) \underbrace{\left(\phi(K)^T V\right)}_{S \in \mathbb{R}^{d \times d}} \quad \text{vs} \quad O = \underbrace{\left(\phi(Q) \phi(K)^T\right)}_{A \in \mathbb{R}^{N \times N}} V$$

- ì¢Œì¸¡: $O(Nd^2)$ â€” Linear Attention (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì„ í˜•)
- ìš°ì¸¡: $O(N^2d)$ â€” Standard Attention (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì´ì°¨)

### Causal Linear Attention (ìˆœí™˜ í˜•íƒœ)

$$s_t = s_{t-1} + \phi(k_t)^T v_t, \quad o_t = \frac{\phi(q_t) s_t}{\phi(q_t) z_t}$$

- $s_t \in \mathbb{R}^{d \times d}$: ìˆ¨ê²¨ì§„ ìƒíƒœ (ëˆ„ì  KV)
- $z_t = z_{t-1} + \phi(k_t)$: ì •ê·œí™” í•­
- í† í°ë‹¹ ë©”ëª¨ë¦¬: $O(d^2)$ â€” ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€!

### Gated Linear Attention (GLA)

$$s_t = G_t \odot s_{t-1} + k_t^T v_t$$

$$o_t = q_t \cdot s_t$$

- $G_t \in \mathbb{R}^{d \times d}$: ê²Œì´íŠ¸ í–‰ë ¬ (ë§ê° ë©”ì»¤ë‹ˆì¦˜)
- $G_t$ê°€ ì‘ìœ¼ë©´ ê³¼ê±° ì •ë³´ ìŠê¸°, í¬ë©´ ìœ ì§€

### RetNetì˜ ìˆœí™˜ ê³µì‹

$$s_t = \gamma \cdot s_{t-1} + k_t^T v_t, \quad o_t = q_t \cdot s_t$$

- $\gamma \in (0, 1)$: ê³ ì • ê°ì‡  ë¹„ìœ¨
- GLAì™€ ìœ ì‚¬í•˜ì§€ë§Œ ê²Œì´íŠ¸ê°€ ìŠ¤ì¹¼ë¼ ìƒìˆ˜

**ìš”ì•½ í‘œ:**

| ëª¨ë¸ | ìˆœí™˜ ê³µì‹ | ë³µì¡ë„ (ì¶”ë¡ ) | ì¥ì  |
|------|-----------|---------------|------|
| Standard Attention | $\text{softmax}(QK^T/\sqrt{d})V$ | $O(N^2d)$ | ë†’ì€ í‘œí˜„ë ¥ |
| Linear Attention | $\phi(Q)(\phi(K)^TV)$ | $O(Nd^2)$ | ê¸´ ì‹œí€€ìŠ¤ íš¨ìœ¨ |
| GLA | $s_t = G_t \odot s_{t-1} + k_t^T v_t$ | $O(d^2)$/í† í° | ì„ íƒì  ë§ê° |
| RetNet | $s_t = \gamma s_{t-1} + k_t^T v_t$ | $O(d^2)$/í† í° | ë‹¨ìˆœí•œ ê°ì‡  |
| Mamba | SSM ê¸°ë°˜ ì„ íƒì  ìŠ¤ìº” | $O(d)$/í† í° | ì…ë ¥ ì˜ì¡´ ê²Œì´íŒ… |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Linear Attention ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ ì™œ Linear Attentionì´ í•„ìš”í•œê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: êµì‹¤ì—ì„œ ëª¨ë“  í•™ìƒ(Nëª…)ì´ ì„œë¡œ ëŒ€í™”í•˜ë ¤ë©´ $N \\times N$ë²ˆ ëŒ€í™”í•´ì•¼ í•´ìš”. 
> í•™ìƒì´ 100ëª…ì´ë©´ 10,000ë²ˆ! í•˜ì§€ë§Œ **ë°˜ì¥ì„ í†µí•´** ëŒ€í™”í•˜ë©´ ê°ì ë°˜ì¥ê³¼ë§Œ ëŒ€í™”í•˜ë©´ ë¼ì„œ 
> 200ë²ˆì´ë©´ ì¶©ë¶„í•´ìš”!

Linear Attentionì€ **ë°˜ì¥(ìˆ¨ê²¨ì§„ ìƒíƒœ $s_t$)** ì„ ë‘ëŠ” ê±°ì˜ˆìš”:
- í‘œì¤€ Attention: ëª¨ë“  í† í°ì´ ì„œë¡œ ì§ì ‘ ëŒ€í™” â†’ $N^2$ë²ˆ ê³„ì‚°
- Linear Attention: ê° í† í°ì´ "ìš”ì•½ ë…¸íŠ¸($s_t$)"ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ì½ê¸° â†’ $N$ë²ˆ ê³„ì‚°

#### ğŸ§  GLAëŠ” ë­ê°€ ë‹¤ë¥¸ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: "ìš”ì•½ ë…¸íŠ¸"ì— **ì§€ìš°ê°œ(ê²Œì´íŠ¸ $G_t$)** ê°€ ìˆì–´ì„œ, 
> ì¤‘ìš”í•˜ì§€ ì•Šì€ ê³¼ê±° ì •ë³´ëŠ” ì§€ìš¸ ìˆ˜ ìˆì–´ìš”!

ì¼ë°˜ Linear Attentionì€ ëª¨ë“  ê³¼ê±° ì •ë³´ë¥¼ ê³„ì† ìŒ“ê¸°ë§Œ í•˜ì§€ë§Œ, 
GLAëŠ” ì„ íƒì ìœ¼ë¡œ ìŠì„ ìˆ˜ ìˆì–´ì„œ **ë” ë˜‘ë˜‘í•˜ê²Œ** ì •ë³´ë¥¼ ê´€ë¦¬í•´ìš”.

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: ë³µì¡ë„ ë¹„êµ

ì‹œí€€ìŠ¤ ê¸¸ì´ $N=4096$, ì°¨ì› $d=128$ì¼ ë•Œ, Standard Attentionê³¼ Linear Attentionì˜ ì—°ì‚°ëŸ‰(FLOPs)ì„ ë¹„êµí•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

**Standard Attention:**
$$\text{FLOPs} = O(N^2 d) = 4096^2 \times 128 \approx 2.15 \times 10^9$$

**Linear Attention:**
$$\text{FLOPs} = O(N d^2) = 4096 \times 128^2 \approx 6.71 \times 10^7$$

$$\text{ë¹„ìœ¨} = \frac{N^2 d}{N d^2} = \frac{N}{d} = \frac{4096}{128} = 32$$

â†’ Linear Attentionì´ **32ë°°** ë” ë¹ ë¦…ë‹ˆë‹¤! ($N > d$ì¼ ë•Œ í•­ìƒ ìœ ë¦¬)
</details>

#### ë¬¸ì œ 2: GLA ìƒíƒœ ì—…ë°ì´íŠ¸

$s_0 = \mathbf{0}$, $G_1 = 0.9I$, $k_1 = [1, 0]^T$, $v_1 = [2, 3]^T$ì¼ ë•Œ $s_1$ì„ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$s_1 = G_1 \odot s_0 + k_1^T v_1 = 0.9 \cdot \mathbf{0} + \begin{bmatrix} 1 \\ 0 \end{bmatrix} [2, 3] = \begin{bmatrix} 2 & 3 \\ 0 & 0 \end{bmatrix}$$

â†’ ì²« ë²ˆì§¸ í† í°ì˜ KV ì •ë³´ê°€ ìƒíƒœ í–‰ë ¬ì— ì €ì¥ë©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 - Complexity comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. Standard vs Linear Attention ë³µì¡ë„ ë¹„êµ <a name='2.-ë³µì¡ë„-ë¹„êµ'></a>

ë‘ ë°©ì‹ì˜ ì‹œê°„/ê³µê°„ ë³µì¡ë„ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë¹„êµí•˜ê³ , ì‹¤ì œ ì—°ì‚° ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

| ì¸¡ë©´ | Standard Attention | Linear Attention |
|------|-------------------|------------------|
| ì‹œê°„ ë³µì¡ë„ | $O(N^2 d)$ | $O(Nd^2)$ |
| ê³µê°„ ë³µì¡ë„ | $O(N^2)$ | $O(d^2)$ |
| ìœ ë¦¬í•œ ì¡°ê±´ | $N < d$ | $N > d$ (ê¸´ ì‹œí€€ìŠ¤) |"""))

cells.append(code("""\
# â”€â”€ Standard vs Linear Attention êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def standard_attention(Q, K, V):
    # Q, K, V: [batch, seq, dim]
    d = tf.cast(tf.shape(K)[-1], tf.float32)
    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d)
    weights = tf.nn.softmax(scores, axis=-1)
    return tf.matmul(weights, V)

def linear_attention(Q, K, V, kernel_fn=None):
    # ì»¤ë„ í•¨ìˆ˜: elu(x) + 1 (ì–‘ìˆ˜ ë³´ì¥)
    if kernel_fn is None:
        kernel_fn = lambda x: tf.nn.elu(x) + 1

    Q_prime = kernel_fn(Q)  # [B, N, d]
    K_prime = kernel_fn(K)  # [B, N, d]

    # S = K'^T V : [B, d, d]
    S = tf.matmul(K_prime, V, transpose_a=True)

    # O = Q' S : [B, N, d]
    numerator = tf.matmul(Q_prime, S)

    # ì •ê·œí™”
    Z = tf.matmul(Q_prime, tf.reduce_sum(K_prime, axis=1, keepdims=True),
                  transpose_b=True)
    Z = tf.maximum(Z, 1e-6)

    return numerator / Z

# ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ ì—°ì‚° ì‹œê°„ ë¹„êµ
dims = 64
batch = 4
seq_lengths_test = [64, 128, 256, 512, 1024, 2048]

std_times = []
lin_times = []

print(f"Standard vs Linear Attention ì‹¤í–‰ ì‹œê°„ ë¹„êµ:")
print(f"  d = {dims}, batch = {batch}")
print(f"{'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'Standard (ms)':>14} | {'Linear (ms)':>14} | {'ì†ë„ ë¹„ìœ¨':>10}")
print("-" * 58)

for seq_len in seq_lengths_test:
    Q = tf.random.normal([batch, seq_len, dims])
    K = tf.random.normal([batch, seq_len, dims])
    V = tf.random.normal([batch, seq_len, dims])

    # ì›Œë°ì—…
    _ = standard_attention(Q, K, V)
    _ = linear_attention(Q, K, V)

    # ì‹œê°„ ì¸¡ì •
    n_runs = 5
    t0 = time.time()
    for _ in range(n_runs):
        _ = standard_attention(Q, K, V)
    std_time = (time.time() - t0) / n_runs * 1000

    t0 = time.time()
    for _ in range(n_runs):
        _ = linear_attention(Q, K, V)
    lin_time = (time.time() - t0) / n_runs * 1000

    std_times.append(std_time)
    lin_times.append(lin_time)
    ratio = std_time / max(lin_time, 1e-6)
    print(f"{seq_len:>12} | {std_time:>14.2f} | {lin_time:>14.2f} | {ratio:>9.2f}x")

print(f"\\nâ†’ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ Linear Attentionì˜ ì´ì ì´ ì»¤ì§")"""))

# â”€â”€ Cell 8: Section 3 - GLA recurrence â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. GLA ìˆœí™˜ ì‹œë®¬ë ˆì´ì…˜ <a name='3.-GLA-ìˆœí™˜-ì‹œë®¬ë ˆì´ì…˜'></a>

GLA(Gated Linear Attention)ì˜ ìˆœí™˜ í˜•íƒœë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:

$$s_t = G_t \odot s_{t-1} + k_t v_t^T$$

$$o_t = s_t^T q_t$$

ì—¬ê¸°ì„œ $G_t = \sigma(W_g h_t)$ëŠ” ì…ë ¥ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²°ì •ë˜ëŠ” ê²Œì´íŠ¸ì…ë‹ˆë‹¤."""))

cells.append(code("""\
# â”€â”€ GLA ìˆœí™˜(Recurrence) êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GLARecurrence(tf.keras.layers.Layer):
    # Gated Linear Attention - ìˆœí™˜ í˜•íƒœ êµ¬í˜„

    def __init__(self, d_model, d_key, d_value):
        super().__init__()
        self.d_key = d_key
        self.d_value = d_value

        self.W_q = tf.keras.layers.Dense(d_key)
        self.W_k = tf.keras.layers.Dense(d_key)
        self.W_v = tf.keras.layers.Dense(d_value)
        self.W_g = tf.keras.layers.Dense(d_key * d_value)  # ê²Œì´íŠ¸

    def call(self, x):
        batch, seq_len, _ = x.shape

        q = self.W_q(x)  # [B, N, d_k]
        k = self.W_k(x)  # [B, N, d_k]
        v = self.W_v(x)  # [B, N, d_v]
        g = tf.sigmoid(self.W_g(x))  # [B, N, d_k*d_v]
        g = tf.reshape(g, [batch, seq_len, self.d_key, self.d_value])

        outputs = []
        # ìƒíƒœ í–‰ë ¬ ì´ˆê¸°í™”
        s = tf.zeros([batch, self.d_key, self.d_value])

        for t in range(seq_len):
            k_t = k[:, t, :]  # [B, d_k]
            v_t = v[:, t, :]  # [B, d_v]
            q_t = q[:, t, :]  # [B, d_k]
            g_t = g[:, t, :, :]  # [B, d_k, d_v]

            # ìƒíƒœ ì—…ë°ì´íŠ¸: s_t = G_t * s_{t-1} + k_t^T v_t
            kv_outer = tf.einsum('bi,bj->bij', k_t, v_t)  # [B, d_k, d_v]
            s = g_t * s + kv_outer

            # ì¶œë ¥: o_t = q_t^T s_t
            o_t = tf.einsum('bi,bij->bj', q_t, s)  # [B, d_v]
            outputs.append(o_t)

        output = tf.stack(outputs, axis=1)  # [B, N, d_v]
        return output, s


# í…ŒìŠ¤íŠ¸
d_model = 64
d_key = 32
d_value = 32
batch_size = 2
seq_len = 20

gla = GLARecurrence(d_model, d_key, d_value)
x_test = tf.random.normal([batch_size, seq_len, d_model])
output, final_state = gla(x_test)

print(f"GLA Recurrence ê²°ê³¼:")
print(f"  ì…ë ¥: {x_test.shape}")
print(f"  ì¶œë ¥: {output.shape}")
print(f"  ìµœì¢… ìƒíƒœ: {final_state.shape}")
print(f"  ìƒíƒœ í–‰ë ¬ í¬ê¸°: {d_key} x {d_value} = {d_key * d_value} (ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€!)")

# ê²Œì´íŠ¸ ë¶„ì„
g_sample = tf.sigmoid(gla.W_g(x_test))
g_mean = tf.reduce_mean(g_sample).numpy()
g_std = tf.math.reduce_std(g_sample).numpy()
print(f"\\nê²Œì´íŠ¸ í†µê³„:")
print(f"  í‰ê· : {g_mean:.4f} (0.5ì— ê°€ê¹Œìš°ë©´ ì¤‘ë¦½ì )")
print(f"  í‘œì¤€í¸ì°¨: {g_std:.4f}")
print(f"  â†’ ê²Œì´íŠ¸ê°€ 0ì— ê°€ê¹Œìš°ë©´ ê³¼ê±° ì •ë³´ ìŠê¸°, 1ì— ê°€ê¹Œìš°ë©´ ìœ ì§€")

# ë©”ëª¨ë¦¬ ë¹„êµ
std_kv_cache = seq_len * 2 * d_key  # Standard: ëª¨ë“  K,V ì €ì¥
gla_state = d_key * d_value  # GLA: ìƒíƒœ í–‰ë ¬ë§Œ ì €ì¥
print(f"\\në©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ = {seq_len}):")
print(f"  Standard Attention KV Cache: {std_kv_cache} ì›ì†Œ")
print(f"  GLA ìƒíƒœ í–‰ë ¬: {gla_state} ì›ì†Œ")
print(f"  ì ˆê°ë¥ : {(1 - gla_state/std_kv_cache)*100:.1f}%")"""))

# â”€â”€ Cell 10: Section 4 - Qwen Hybrid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. Qwen Hybrid ì•„í‚¤í…ì²˜ <a name='4.-Qwen-Hybrid-ì•„í‚¤í…ì²˜'></a>

Qwenì€ ì„¸ ê°€ì§€ Attention ë°©ì‹ì„ ë ˆì´ì–´ë³„ë¡œ í˜¼í•©í•˜ëŠ” Hybrid êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

| ë ˆì´ì–´ ìœ í˜• | Attention ë°©ì‹ | ìš©ë„ |
|-------------|---------------|------|
| Full Attention | í‘œì¤€ Softmax | ì „ì—­ ì˜ì¡´ì„± í¬ì°© |
| Sliding Window (SWA) | ìœˆë„ìš° ë‚´ Softmax | ì§€ì—­ íŒ¨í„´ í¬ì°© |
| Linear Attention | ì»¤ë„ ê¸°ë°˜ ì„ í˜• | íš¨ìœ¨ì  ì¥ê±°ë¦¬ ìš”ì•½ |

ì´ëŸ¬í•œ êµ¬ì¡°ì˜ ì¥ì :
1. Full Attentionìœ¼ë¡œ ì¤‘ìš”í•œ ì „ì—­ ê´€ê³„ ìœ ì§€
2. SWAë¡œ ê³„ì‚°ëŸ‰ ì ˆê° (ëŒ€ë¶€ë¶„ì˜ ë ˆì´ì–´)
3. Linearë¡œ ì´ˆì¥ê±°ë¦¬ ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬"""))

cells.append(code("""\
# â”€â”€ Qwen Hybrid ì•„í‚¤í…ì²˜ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sliding_window_attention(Q, K, V, window_size=256):
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° Attention (ê°„ì†Œí™” ë²„ì „)
    batch, seq_len, d = Q.shape
    d_float = tf.cast(d, tf.float32)

    # ì „ì²´ attention ê³„ì‚° í›„ ìœˆë„ìš° ë§ˆìŠ¤í¬ ì ìš©
    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d_float)

    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§ˆìŠ¤í¬ ìƒì„±
    positions = tf.range(seq_len)
    row_pos = tf.expand_dims(positions, 1)  # [N, 1]
    col_pos = tf.expand_dims(positions, 0)  # [1, N]
    mask = tf.cast(tf.abs(row_pos - col_pos) <= window_size // 2, tf.float32)

    # causal ë§ˆìŠ¤í¬ë„ ì ìš©
    causal_mask = tf.cast(col_pos <= row_pos, tf.float32)
    combined_mask = mask * causal_mask

    scores = scores + (1.0 - combined_mask) * (-1e9)
    weights = tf.nn.softmax(scores, axis=-1)
    return tf.matmul(weights, V)

# Hybrid êµ¬ì¡°: ë ˆì´ì–´ë³„ ë°©ì‹ ë°°ì •
n_layers = 24
layer_types = []
for i in range(n_layers):
    if i % 6 == 0:
        layer_types.append('Full')
    elif i % 6 == 3:
        layer_types.append('Linear')
    else:
        layer_types.append('SWA')

print(f"Qwen Hybrid ì•„í‚¤í…ì²˜ ({n_layers} ë ˆì´ì–´):")
print(f"{'ë ˆì´ì–´':>6} | {'ìœ í˜•':<8} | ì—­í• ")
print("-" * 40)
type_counts = {'Full': 0, 'SWA': 0, 'Linear': 0}
for i, lt in enumerate(layer_types):
    type_counts[lt] += 1
    role = {'Full': 'ì „ì—­ ì˜ì¡´ì„±', 'SWA': 'ì§€ì—­ íŒ¨í„´', 'Linear': 'ì¥ê±°ë¦¬ ìš”ì•½'}[lt]
    if i < 12 or i >= n_layers - 2:
        print(f"  L{i:>3} | {lt:<8} | {role}")
    elif i == 12:
        print(f"   ... | ...      | ...")

print(f"\\në ˆì´ì–´ ìœ í˜• ë¶„í¬:")
for lt, count in type_counts.items():
    pct = count / n_layers * 100
    bar = 'â–ˆ' * int(pct / 2)
    print(f"  {lt:<8}: {count:>2}ê°œ ({pct:.0f}%) {bar}")

# ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì—°ì‚° ë¹„êµ
seq_len_test = 512
d_test = 64
batch_t = 2
window = 128

Q_t = tf.random.normal([batch_t, seq_len_test, d_test])
K_t = tf.random.normal([batch_t, seq_len_test, d_test])
V_t = tf.random.normal([batch_t, seq_len_test, d_test])

out_full = standard_attention(Q_t, K_t, V_t)
out_swa = sliding_window_attention(Q_t, K_t, V_t, window_size=window)
out_lin = linear_attention(Q_t, K_t, V_t)

print(f"\\nì¶œë ¥ shape í™•ì¸:")
print(f"  Full Attention: {out_full.shape}")
print(f"  SWA (w={window}): {out_swa.shape}")
print(f"  Linear Attention: {out_lin.shape}")

# Hybrid ì´ FLOPs ì¶”ì •
N = 4096
d = 128
full_flops = N * N * d * type_counts['Full']
swa_flops = N * window * d * type_counts['SWA']
linear_flops = N * d * d * type_counts['Linear']
total_hybrid = full_flops + swa_flops + linear_flops
total_all_full = N * N * d * n_layers

print(f"\\nFLOPs ë¹„êµ (N={N}, d={d}):")
print(f"  ì „ì²´ Full Attention: {total_all_full/1e9:.2f} GFLOPs")
print(f"  Hybrid: {total_hybrid/1e9:.2f} GFLOPs")
print(f"  ì ˆê°ë¥ : {(1-total_hybrid/total_all_full)*100:.1f}%")"""))

# â”€â”€ Cell 12: Section 5 - Memory/Speed comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ë¹„êµ <a name='5.-ë©”ëª¨ë¦¬-ì†ë„-ë¹„êµ'></a>

ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¦ê°€í•  ë•Œ ê° Attention ë°©ì‹ì˜ ì´ë¡ ì  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì—°ì‚°ëŸ‰ì„ ë¹„êµí•©ë‹ˆë‹¤.

**ì´ë¡ ì  ë³µì¡ë„:**
- Standard: ë©”ëª¨ë¦¬ $O(N^2)$, ì—°ì‚° $O(N^2 d)$
- SWA ($w$): ë©”ëª¨ë¦¬ $O(Nw)$, ì—°ì‚° $O(Nwd)$
- Linear: ë©”ëª¨ë¦¬ $O(d^2)$, ì—°ì‚° $O(Nd^2)$"""))

cells.append(code("""\
# â”€â”€ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ìŠ¤ì¼€ì¼ë§ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
seq_lengths = np.array([256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536])
d = 128
w = 512  # SWA ìœˆë„ìš° í¬ê¸°

# ì´ë¡ ì  ë©”ëª¨ë¦¬ (Attention í–‰ë ¬ í¬ê¸°)
mem_standard = seq_lengths ** 2  # O(N^2)
mem_swa = seq_lengths * w  # O(Nw)
mem_linear = np.full_like(seq_lengths, d * d, dtype=float)  # O(d^2)

# ì´ë¡ ì  FLOPs
flops_standard = seq_lengths ** 2 * d
flops_swa = seq_lengths * w * d
flops_linear = seq_lengths * d ** 2

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§
ax1 = axes[0]
ax1.loglog(seq_lengths / 1000, mem_standard / 1e6, 'r-o', lw=2.5, ms=7, label='Standard ($O(N^2)$)')
ax1.loglog(seq_lengths / 1000, mem_swa / 1e6, 'b-s', lw=2, ms=7, label=f'SWA ($O(Nw), w={w}$)')
ax1.loglog(seq_lengths / 1000, mem_linear / 1e6, 'g-^', lw=2, ms=7, label='Linear ($O(d^2)$)')
ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)
ax1.set_ylabel('Attention ë©”ëª¨ë¦¬ (M ì›ì†Œ)', fontsize=11)
ax1.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) FLOPs ìŠ¤ì¼€ì¼ë§
ax2 = axes[1]
ax2.loglog(seq_lengths / 1000, flops_standard / 1e9, 'r-o', lw=2.5, ms=7, label='Standard ($O(N^2d)$)')
ax2.loglog(seq_lengths / 1000, flops_swa / 1e9, 'b-s', lw=2, ms=7, label=f'SWA ($O(Nwd)$)')
ax2.loglog(seq_lengths / 1000, flops_linear / 1e9, 'g-^', lw=2, ms=7, label='Linear ($O(Nd^2)$)')
ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)
ax2.set_ylabel('FLOPs (G)', fontsize=11)
ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì—°ì‚°ëŸ‰ ìŠ¤ì¼€ì¼ë§', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/attention_scaling_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/attention_scaling_comparison.png")

# ìˆ˜ì¹˜ ë¹„êµ í‘œ
print(f"\\nì‹œí€€ìŠ¤ ê¸¸ì´ë³„ FLOPs ë¹„êµ (d={d}):")
print(f"{'ê¸¸ì´':>8} | {'Standard':>12} | {'SWA':>12} | {'Linear':>12} | {'Std/Lin ë¹„ìœ¨':>12}")
print("-" * 65)
for i, N in enumerate(seq_lengths):
    ratio = flops_standard[i] / flops_linear[i]
    print(f"{N:>8} | {flops_standard[i]/1e9:>10.2f}G | {flops_swa[i]/1e9:>10.2f}G | "
          f"{flops_linear[i]/1e9:>10.2f}G | {ratio:>11.1f}x")

print(f"\\nâ†’ N=65Kì—ì„œ LinearëŠ” Standardë³´ë‹¤ {flops_standard[-1]/flops_linear[-1]:.0f}ë°° íš¨ìœ¨ì !")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Linear Attention | $\phi(Q)(\phi(K)^TV)$ â€” $O(Nd^2)$ ë³µì¡ë„ | â­â­â­ |
| GLA ìˆœí™˜ | $s_t = G_t \odot s_{t-1} + k_t^T v_t$ â€” ì„ íƒì  ë§ê° | â­â­â­ |
| RetNet | $\gamma$ ê³ ì • ê°ì‡  ê¸°ë°˜ ìˆœí™˜ | â­â­ |
| Mamba | SSM ê¸°ë°˜ ì…ë ¥ ì˜ì¡´ì  ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ | â­â­ |
| Qwen Hybrid | Full + SWA + Linear ë ˆì´ì–´ í˜¼í•© | â­â­â­ |
| ì»¤ë„ í•¨ìˆ˜ | $\phi(x) = \text{elu}(x) + 1$ â€” ì–‘ìˆ˜ ë³´ì¥ | â­â­ |
| ìƒíƒœ í–‰ë ¬ | $s_t \in \mathbb{R}^{d \times d}$ â€” ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€ | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$O_i = \frac{\phi(Q_i) \sum_{j} \phi(K_j)^T V_j}{\phi(Q_i) \sum_{j} \phi(K_j)^T}$$

$$s_t = G_t \odot s_{t-1} + k_t^T v_t, \quad o_t = q_t^T s_t$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**04_long_context_and_sparse_attn.ipynb** â€” YaRN ì»¨í…ìŠ¤íŠ¸ í™•ì¥, DeepSeek Sparse Attention, Sliding Window ë°©ë²•ë¡ ì„ í†µí•©í•˜ì—¬ 50% ì´ìƒì˜ ë¹„ìš© ì ˆê° ê¸°ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter16_sparse_attention/03_linear_attention_and_hybrids.ipynb')
