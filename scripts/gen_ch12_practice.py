"""Generate Chapter 12 practice files."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

# =================== ex01: Implement Llama Scratch ===================
ex01_cells = [
md("""# ì‹¤ìŠµ í€´ì¦ˆ: Llama ì•„í‚¤í…ì²˜ ë°‘ë°”ë‹¥ êµ¬í˜„

## ì‚¬ìš© ë°©ë²•
- ê° ë¬¸ì œ ì…€ì„ ì½ê³ , **ì§ì ‘ ë‹µì„ ì˜ˆì¸¡í•œ í›„** í’€ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”
- ì½”ë“œ ì‹¤í–‰ ì „ì— ì¢…ì´ì— ê³„ì‚°í•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤

## ëª©ì°¨
- [Q1: RMSNorm ì§ì ‘ êµ¬í˜„](#q1)
- [Q2: RoPE ì ìš© í›„ Attention Score ê³„ì‚°](#q2)
- [Q3: SwiGLU FFN Forward Pass](#q3)
- [Q4: GQA Attention êµ¬í˜„](#q4)
- [ì¢…í•© ë„ì „: ì†Œí˜• Llama Block ëª¨ë“ˆ ì¡°ë¦½](#bonus)

---"""),

code("""# í€´ì¦ˆ í™˜ê²½ ì„¤ì •
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print("í€´ì¦ˆ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")
print("ê° ë¬¸ì œë¥¼ í’€ê¸° ì „ì— ë¨¼ì € ë‹µì„ ì˜ˆì¸¡í•´ë³´ì„¸ìš”.")"""),

md(r"""---
## Q1: RMSNorm ì§ì ‘ êµ¬í˜„ <a name='q1'></a>

### ë¬¸ì œ

ë‹¤ìŒ ì¡°ê±´ìœ¼ë¡œ RMSNormì„ êµ¬í˜„í•˜ì„¸ìš”:
- ì…ë ¥: $x = [1.0, 2.0, 3.0, 4.0]$
- ê²Œì¸: $g = [1.0, 1.0, 1.0, 1.0]$
- $\epsilon = 10^{-6}$

$$\text{RMSNorm}(x_i) = \frac{x_i}{\sqrt{\frac{1}{n}\sum_j x_j^2 + \epsilon}} \cdot g_i$$

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** RMSNorm ì¶œë ¥ì˜ L2 normì€ `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q1 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q1 í’€ì´: RMSNorm ì§ì ‘ êµ¬í˜„")
print("=" * 45)

x = tf.constant([1.0, 2.0, 3.0, 4.0])
g = tf.constant([1.0, 1.0, 1.0, 1.0])
eps = 1e-6

# RMS ê³„ì‚°
rms = tf.sqrt(tf.reduce_mean(tf.square(x)) + eps)
print(f"ì…ë ¥ x: {x.numpy()}")
print(f"x^2: {tf.square(x).numpy()}")
print(f"mean(x^2): {tf.reduce_mean(tf.square(x)).numpy():.4f}")
print(f"RMS = sqrt(mean(x^2) + eps) = {rms.numpy():.6f}")

# ì •ê·œí™”
output = (x / rms) * g
print(f"\nRMSNorm ì¶œë ¥: {output.numpy()}")
print(f"ì¶œë ¥ L2 norm: {tf.norm(output).numpy():.4f}")
print()

print("[í•´ì„¤]")
print(f"  RMS = sqrt((1+4+9+16)/4) = sqrt(30/4) = sqrt(7.5) = {np.sqrt(7.5):.4f}")
print("  RMSNormì€ ë²¡í„°ë¥¼ sqrt(n) í¬ê¸°ë¡œ ì •ê·œí™”")
print(f"  ì¶œë ¥ L2 norm â‰ˆ sqrt(n) = sqrt(4) = 2.0")"""),

md(r"""---
## Q2: RoPE ì ìš© í›„ Attention Score <a name='q2'></a>

### ë¬¸ì œ

2D ë²¡í„° $q = [1, 0]$, $k = [1, 0]$ì— RoPE($\theta = \pi/4$)ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
- Q ìœ„ì¹˜: $m = 0$
- K ìœ„ì¹˜: $n = 2$

RoPE ì ìš© í›„ $q \cdot k$ (ë‚´ì )ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** ë‚´ì  ê°’ì€ `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q2 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q2 í’€ì´: RoPE ì ìš© í›„ Attention Score")
print("=" * 45)

theta = np.pi / 4  # 45ë„
q = np.array([1.0, 0.0])
k = np.array([1.0, 0.0])
m, n = 0, 2

# RoPE íšŒì „ ì ìš©
def rotate_2d(vec, angle):
    cos_a, sin_a = np.cos(angle), np.sin(angle)
    return np.array([vec[0]*cos_a - vec[1]*sin_a,
                     vec[0]*sin_a + vec[1]*cos_a])

q_rot = rotate_2d(q, m * theta)  # m=0ì´ë¯€ë¡œ íšŒì „ ì—†ìŒ
k_rot = rotate_2d(k, n * theta)  # n=2, ê°ë„=pi/2

dot = np.dot(q_rot, k_rot)

print(f"q = {q}, m = {m}")
print(f"k = {k}, n = {n}")
print(f"theta = pi/4 = {theta:.4f}")
print()
print(f"q íšŒì „ (m*theta = {m*theta:.2f}): {q_rot}")
print(f"k íšŒì „ (n*theta = {n*theta:.2f}): {k_rot}")
print(f"ë‚´ì  q_rot Â· k_rot = {dot:.4f}")
print()
print("[í•´ì„¤]")
print(f"  qëŠ” 0ë„ íšŒì „ (ê·¸ëŒ€ë¡œ): [1, 0]")
print(f"  këŠ” pi/2 íšŒì „: [cos(pi/2), sin(pi/2)] = [0, 1]")
print(f"  ë‚´ì  = 1*0 + 0*1 = 0")
print(f"  ìƒëŒ€ ê±°ë¦¬ |m-n|=2 â†’ ê°ë„ ì°¨ì´ = 2*pi/4 = pi/2 â†’ ì§êµ!")"""),

md(r"""---
## Q3: SwiGLU FFN Forward Pass <a name='q3'></a>

### ë¬¸ì œ

SwiGLU FFNì˜ ì¤‘ê°„ ì¶œë ¥ì„ ê³„ì‚°í•˜ì„¸ìš”:
- $x = [1.0, -1.0]$, $d_{model} = 2$, $d_{ff} = 3$
- $W_1 = [[1, 0, -1], [0, 1, 0]]$ (gate)
- $W_2 = [[0, 1, 1], [1, 0, -1]]$ (up)
- Swish = SiLU: $x \cdot \sigma(x)$

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** $\text{Swish}(xW_1) \otimes (xW_2)$ì˜ ê²°ê³¼ëŠ” `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q3 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q3 í’€ì´: SwiGLU FFN Forward Pass")
print("=" * 45)

x = np.array([[1.0, -1.0]])
W1 = np.array([[1, 0, -1], [0, 1, 0]])  # gate
W2 = np.array([[0, 1, 1], [1, 0, -1]])  # up

# Step 1: xW1 (gate projection)
gate_linear = x @ W1
print(f"x = {x[0]}")
print(f"xW1 (gate) = {gate_linear[0]}")

# Step 2: Swish(xW1)
def swish(z):
    return z * (1 / (1 + np.exp(-z)))

gate = swish(gate_linear)
print(f"Swish(xW1)  = {gate[0]}")

# Step 3: xW2 (up projection)
up = x @ W2
print(f"xW2 (up)    = {up[0]}")

# Step 4: element-wise product
swiglu_out = gate * up
print(f"Gate âŠ— Up   = {swiglu_out[0]}")
print()

print("[í•´ì„¤]")
print("  1. xW1 = [1*1+(-1)*0, 1*0+(-1)*1, 1*(-1)+(-1)*0] = [1, -1, -1]")
print(f"  2. Swish([1,-1,-1]) = [1*Ïƒ(1), -1*Ïƒ(-1), -1*Ïƒ(-1)]")
print(f"     = [{1*0.7311:.4f}, {-1*0.2689:.4f}, {-1*0.2689:.4f}]")
print("  3. xW2 = [0+1, 1+0, 1-(-1)] = [-1, 1, 2]")
print(f"  4. Gate âŠ— Up = element-wise product")"""),

md(r"""---
## Q4: GQA Attention êµ¬í˜„ <a name='q4'></a>

### ë¬¸ì œ

$H_Q = 4, H_{KV} = 2, d_{head} = 4$ì¼ ë•Œ:
1. Q ê·¸ë£¹ ìˆ˜ëŠ”?
2. Q í”„ë¡œì ì…˜ íŒŒë¼ë¯¸í„° ìˆ˜ vs KV í”„ë¡œì ì…˜ íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„ìœ¨ì€?

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** ê·¸ë£¹ ìˆ˜ëŠ” `?`, íŒŒë¼ë¯¸í„° ë¹„ìœ¨ì€ `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q4 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q4 í’€ì´: GQA Attention êµ¬í˜„")
print("=" * 45)

H_Q, H_KV, d_head = 4, 2, 4
d_model = H_Q * d_head  # 16

n_groups = H_Q // H_KV
print(f"H_Q = {H_Q}, H_KV = {H_KV}, d_head = {d_head}")
print(f"d_model = H_Q * d_head = {d_model}")
print(f"Q ê·¸ë£¹ ìˆ˜ = H_Q / H_KV = {n_groups}")
print()

# íŒŒë¼ë¯¸í„° ìˆ˜
wq_params = d_model * (H_Q * d_head)    # Q projection
wk_params = d_model * (H_KV * d_head)   # K projection
wv_params = d_model * (H_KV * d_head)   # V projection
wo_params = d_model * d_model            # O projection

print(f"Wq íŒŒë¼ë¯¸í„°: d_model Ã— (H_Q Ã— d_head) = {d_model} Ã— {H_Q * d_head} = {wq_params}")
print(f"Wk íŒŒë¼ë¯¸í„°: d_model Ã— (H_KV Ã— d_head) = {d_model} Ã— {H_KV * d_head} = {wk_params}")
print(f"Wv íŒŒë¼ë¯¸í„°: d_model Ã— (H_KV Ã— d_head) = {d_model} Ã— {H_KV * d_head} = {wv_params}")
print(f"Wo íŒŒë¼ë¯¸í„°: d_model Ã— d_model = {d_model} Ã— {d_model} = {wo_params}")
print()
print(f"Q / KV íŒŒë¼ë¯¸í„° ë¹„ìœ¨: {wq_params} / {wk_params} = {wq_params/wk_params:.1f}x")
print(f"MHA ëŒ€ë¹„ KV ì ˆê°ë¥ : {(1 - H_KV/H_Q)*100:.0f}%")
print()

# ì‹¤ì œ GQA ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
x = tf.random.normal((1, 8, d_model))
Wq = tf.random.normal((d_model, H_Q * d_head))
Wk = tf.random.normal((d_model, H_KV * d_head))

Q = tf.reshape(x @ Wq, (1, 8, H_Q, d_head))
K = tf.reshape(x @ Wk, (1, 8, H_KV, d_head))

# KV repeat
K_repeated = tf.repeat(K, repeats=n_groups, axis=2)

print(f"Q shape: {Q.shape} â†’ [B, S, H_Q, d_head]")
print(f"K shape (ì›ë³¸): {K.shape} â†’ [B, S, H_KV, d_head]")
print(f"K shape (ë°˜ë³µ): {K_repeated.shape} â†’ [B, S, H_Q, d_head]")
print()
print("[í•´ì„¤]")
print(f"  GQAì—ì„œ {H_KV}ê°œì˜ KV í—¤ë“œë¥¼ {n_groups}ë²ˆ ë°˜ë³µí•˜ì—¬ {H_Q}ê°œì˜ Q í—¤ë“œì— ë§¤ì¹­")"""),

md(r"""---
## ì¢…í•© ë„ì „: ì†Œí˜• Llama Block ëª¨ë“ˆ ì¡°ë¦½ <a name='bonus'></a>

### ë¬¸ì œ

ë‹¤ìŒ êµ¬ì„± ìš”ì†Œë¥¼ ëª¨ë‘ ì¡°í•©í•˜ì—¬ **ì™„ì „í•œ Llama Decoder Block**ì„ êµ¬í˜„í•˜ì„¸ìš”:
1. Pre-Norm: RMSNorm
2. Attention: GQA (H_Q=8, H_KV=2)
3. FFN: SwiGLU (d_ff = 8/3 * d_model)
4. Residual Connection

ì„¤ì •: `d_model=64, seq_len=16, batch=2`"""),

code(r"""# â”€â”€ ì¢…í•© ë„ì „ í’€ì´: ì†Œí˜• Llama Block ì¡°ë¦½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 55)
print("ì¢…í•© ë„ì „: ì†Œí˜• Llama Decoder Block")
print("=" * 55)

# êµ¬ì„± ìš”ì†Œ ì •ì˜
class RMSNorm(tf.keras.layers.Layer):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.g = self.add_weight('gain', shape=(dim,), initializer='ones')
    def call(self, x):
        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + self.eps)
        return (x / rms) * self.g

class GQAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, n_q_heads, n_kv_heads):
        super().__init__()
        self.n_q = n_q_heads
        self.n_kv = n_kv_heads
        self.d_h = d_model // n_q_heads
        self.groups = n_q_heads // n_kv_heads
        self.wq = tf.keras.layers.Dense(n_q_heads * self.d_h, use_bias=False)
        self.wk = tf.keras.layers.Dense(n_kv_heads * self.d_h, use_bias=False)
        self.wv = tf.keras.layers.Dense(n_kv_heads * self.d_h, use_bias=False)
        self.wo = tf.keras.layers.Dense(d_model, use_bias=False)

    def call(self, x):
        B, S, _ = x.shape
        q = tf.reshape(self.wq(x), (B, S, self.n_q, self.d_h))
        k = tf.reshape(self.wk(x), (B, S, self.n_kv, self.d_h))
        v = tf.reshape(self.wv(x), (B, S, self.n_kv, self.d_h))
        k = tf.repeat(k, self.groups, axis=2)
        v = tf.repeat(v, self.groups, axis=2)
        q, k, v = [tf.transpose(t, [0, 2, 1, 3]) for t in [q, k, v]]
        attn = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.sqrt(float(self.d_h)))
        return self.wo(tf.reshape(tf.transpose(tf.matmul(attn, v), [0, 2, 1, 3]), (B, S, -1)))

class SwiGLUFFN(tf.keras.layers.Layer):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = tf.keras.layers.Dense(d_ff, use_bias=False)
        self.w2 = tf.keras.layers.Dense(d_ff, use_bias=False)
        self.w3 = tf.keras.layers.Dense(d_model, use_bias=False)
    def call(self, x):
        return self.w3(tf.nn.silu(self.w1(x)) * self.w2(x))

class LlamaBlock(tf.keras.layers.Layer):
    def __init__(self, d_model, n_q, n_kv, d_ff):
        super().__init__()
        self.norm1 = RMSNorm(d_model)
        self.attn = GQAttention(d_model, n_q, n_kv)
        self.norm2 = RMSNorm(d_model)
        self.ffn = SwiGLUFFN(d_model, d_ff)
    def call(self, x):
        x = x + self.attn(self.norm1(x))   # Pre-Norm + Residual
        x = x + self.ffn(self.norm2(x))     # Pre-Norm + Residual
        return x

# ì¡°ë¦½ ë° í…ŒìŠ¤íŠ¸
d_model = 64
block = LlamaBlock(d_model=d_model, n_q=8, n_kv=2, d_ff=int(8/3*d_model))
x = tf.random.normal((2, 16, d_model))
out = block(x)

total_params = sum(tf.size(v).numpy() for v in block.trainable_variables)

print(f"ì„¤ì •: d_model={d_model}, H_Q=8, H_KV=2, d_ff={int(8/3*d_model)}")
print(f"ì…ë ¥: {x.shape}")
print(f"ì¶œë ¥: {out.shape}")
print(f"Shape ë³´ì¡´: {x.shape == out.shape}")
print(f"ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
print()
print("ì•„í‚¤í…ì²˜:")
print("  x â†’ RMSNorm â†’ GQA (H_Q=8, H_KV=2) â†’ + residual")
print("    â†’ RMSNorm â†’ SwiGLU FFN â†’ + residual â†’ output")
print()
print("ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! Llama 3 ìŠ¤íƒ€ì¼ Decoder Blockì„ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤!")"""),
]

create_notebook(ex01_cells, 'chapter12_modern_llms/practice/ex01_implement_llama_scratch.ipynb')

# =================== ex02: Custom MoE Layer ===================
ex02_cells = [
md("""# ì‹¤ìŠµ í€´ì¦ˆ: MoE ë ˆì´ì–´ ì»¤ìŠ¤í…€ êµ¬í˜„

## ì‚¬ìš© ë°©ë²•
- ê° ë¬¸ì œ ì…€ì„ ì½ê³ , **ì§ì ‘ ë‹µì„ ì˜ˆì¸¡í•œ í›„** í’€ì´ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”
- ì½”ë“œ ì‹¤í–‰ ì „ì— ì¢…ì´ì— ê³„ì‚°í•´ë³´ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤

## ëª©ì°¨
- [Q1: Top-2 Router Softmax ê²Œì´íŒ…](#q1)
- [Q2: Expert Dispatchì™€ Combine](#q2)
- [Q3: Auxiliary Loss ê³„ì‚°](#q3)
- [Q4: Shared Expert ì¶”ê°€](#q4)
- [ì¢…í•© ë„ì „: DeepSeekMoE ìŠ¤íƒ€ì¼ ì™„ì „í•œ MoE ë ˆì´ì–´](#bonus)

---"""),

code("""# í€´ì¦ˆ í™˜ê²½ ì„¤ì •
import numpy as np
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print("í€´ì¦ˆ í™˜ê²½ ì¤€ë¹„ ì™„ë£Œ!")"""),

md(r"""---
## Q1: Top-2 Router Softmax ê²Œì´íŒ… <a name='q1'></a>

### ë¬¸ì œ

4ëª…ì˜ Expertì— ëŒ€í•œ ë¼ìš°í„° ë¡œì§“ì´ $h = [1.0, 3.0, 0.5, 2.0]$ì…ë‹ˆë‹¤.
Top-2ë¥¼ ì„ íƒí•˜ê³  ì¬ì •ê·œí™”ëœ ê²Œì´íŒ… ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** ì„ íƒëœ Expert ì¸ë±ìŠ¤ëŠ” `?`, ê²Œì´íŒ… ê°€ì¤‘ì¹˜ëŠ” `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q1 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q1 í’€ì´: Top-2 Router Softmax ê²Œì´íŒ…")
print("=" * 45)

h = tf.constant([1.0, 3.0, 0.5, 2.0])
top_k = 2

# Top-2 ì„ íƒ
values, indices = tf.math.top_k(h, k=top_k)
print(f"ë¡œì§“ h: {h.numpy()}")
print(f"Top-{top_k} ì¸ë±ìŠ¤: {indices.numpy()}")
print(f"Top-{top_k} ê°’: {values.numpy()}")

# ì¬ì •ê·œí™” (Top-kì— ëŒ€í•´ì„œë§Œ softmax)
gates = tf.nn.softmax(values)
print(f"ê²Œì´íŒ… ê°€ì¤‘ì¹˜: {gates.numpy()}")
print()
print("[í•´ì„¤]")
print(f"  Top-2: Expert {indices[0].numpy()} (h={values[0].numpy():.1f}), "
      f"Expert {indices[1].numpy()} (h={values[1].numpy():.1f})")
print(f"  g_1 = exp(3) / (exp(3) + exp(2)) = {np.exp(3)/(np.exp(3)+np.exp(2)):.4f}")
print(f"  g_3 = exp(2) / (exp(3) + exp(2)) = {np.exp(2)/(np.exp(3)+np.exp(2)):.4f}")"""),

md(r"""---
## Q2: Expert Dispatchì™€ Combine <a name='q2'></a>

### ë¬¸ì œ

3ê°œì˜ í† í°, 4ëª…ì˜ Expert, Top-2 ê¸°ì¤€ìœ¼ë¡œ:
- í† í° 0 â†’ Expert [1, 3], ê°€ì¤‘ì¹˜ [0.7, 0.3]
- í† í° 1 â†’ Expert [0, 2], ê°€ì¤‘ì¹˜ [0.6, 0.4]
- í† í° 2 â†’ Expert [1, 0], ê°€ì¤‘ì¹˜ [0.5, 0.5]

Expertë³„ í• ë‹¹ëœ í† í° ìˆ˜ì™€ ê°€ì¤‘í•© ì¶œë ¥ì„ ê³„ì‚°í•˜ì„¸ìš”.

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** Expert 1ì— í• ë‹¹ëœ í† í° ìˆ˜ëŠ” `?`ê°œ ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q2 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q2 í’€ì´: Expert Dispatchì™€ Combine")
print("=" * 45)

dispatch = {
    0: {'experts': [1, 3], 'gates': [0.7, 0.3]},
    1: {'experts': [0, 2], 'gates': [0.6, 0.4]},
    2: {'experts': [1, 0], 'gates': [0.5, 0.5]},
}

n_experts = 4
expert_loads = {i: [] for i in range(n_experts)}

for tok, info in dispatch.items():
    for exp, gate in zip(info['experts'], info['gates']):
        expert_loads[exp].append((tok, gate))

print("Expertë³„ í• ë‹¹:")
for exp, assignments in expert_loads.items():
    tokens = [f"í† í°{t}(g={g})" for t, g in assignments]
    print(f"  Expert {exp}: {', '.join(tokens) if tokens else 'ì—†ìŒ'} â†’ {len(assignments)}ê°œ í† í°")
print()
print("ì¶œë ¥ ê³„ì‚° (ê°€ìƒ Expert ì¶œë ¥ ì‚¬ìš©):")
d = 4
expert_outputs = {i: np.random.randn(d) for i in range(n_experts)}
for tok, info in dispatch.items():
    combined = np.zeros(d)
    for exp, gate in zip(info['experts'], info['gates']):
        combined += gate * expert_outputs[exp]
    print(f"  í† í° {tok}: {combined[:2]}... (ì²˜ìŒ 2ì°¨ì›)")
print()
print("[í•´ì„¤]")
print("  Expert 1: í† í° 0, 2 â†’ 2ê°œ (ê°€ì¥ ë§ìŒ)")
print("  Expert 3: í† í° 0ë§Œ â†’ 1ê°œ")"""),

md(r"""---
## Q3: Auxiliary Loss ê³„ì‚° <a name='q3'></a>

### ë¬¸ì œ

$N=4$, $\alpha=0.01$, ì „ì²´ í† í° 16ê°œ:
- Expertë³„ í† í° ìˆ˜: [8, 4, 2, 2]
- í‰ê·  ë¼ìš°íŒ… í™•ë¥  $P = [0.4, 0.3, 0.15, 0.15]$

$L_{aux} = \alpha \cdot N \sum_{i=1}^{N} f_i \cdot P_i$ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** $L_{aux}$ëŠ” `?` ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q3 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q3 í’€ì´: Auxiliary Loss ê³„ì‚°")
print("=" * 45)

N = 4
alpha = 0.01
counts = np.array([8, 4, 2, 2], dtype=float)
T = counts.sum()
f = counts / T  # ì‹¤ì œ ë¶„ë°° ë¹„ìœ¨
P = np.array([0.4, 0.3, 0.15, 0.15])  # í‰ê·  ë¼ìš°íŒ… í™•ë¥ 

L_aux = alpha * N * np.sum(f * P)

print(f"N = {N}, alpha = {alpha}")
print(f"í† í° ìˆ˜: {counts.astype(int)}, ì´ T = {int(T)}")
print(f"f (ì‹¤ì œ ë¹„ìœ¨): {f}")
print(f"P (ë¼ìš°íŒ… í™•ë¥ ): {P}")
print()
print(f"f Â· P = {f * P}")
print(f"sum(f Â· P) = {np.sum(f * P):.4f}")
print(f"L_aux = {alpha} Ã— {N} Ã— {np.sum(f * P):.4f} = {L_aux:.6f}")
print()

# ê· ë“± ë¶„ë°° ì‹œ ë¹„êµ
L_uniform = alpha * N * N * (1/N)**2
print(f"ê· ë“± ë¶„ë°° ì‹œ L_aux = {L_uniform:.6f}")
print(f"í˜„ì¬ ë¶ˆê· í˜•ë„: L_aux / L_uniform = {L_aux/L_uniform:.2f}x")
print()
print("[í•´ì„¤]")
print(f"  í˜„ì¬ L_aux ({L_aux:.6f}) > ê· ë“± L_aux ({L_uniform:.6f})")
print(f"  â†’ ë¶ˆê· í˜• ìƒíƒœ. Lossë¥¼ ìµœì†Œí™”í•˜ë©´ ìë™ìœ¼ë¡œ ê· í˜• ìœ ë„!")"""),

md(r"""---
## Q4: Shared Expert ì¶”ê°€ <a name='q4'></a>

### ë¬¸ì œ

ê¸°ì¡´ MoEì— Shared Expert 1ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. ì…ë ¥ $x$ì— ëŒ€í•´:
- Shared Expert ì¶œë ¥: $E_s(x)$
- Routed Expert ì¶œë ¥: $\sum g_i E_i^r(x)$ (Top-2)

ìµœì¢… ì¶œë ¥ ê³µì‹ê³¼, Shared Expert ì¶”ê°€ê°€ íŒŒë¼ë¯¸í„° ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•˜ì„¸ìš”.

**ì—¬ëŸ¬ë¶„ì˜ ì˜ˆì¸¡:** Shared Expert ì¶”ê°€ ì‹œ í™œì„± íŒŒë¼ë¯¸í„° ì¦ê°€ëŸ‰ì€ `?`% ì…ë‹ˆë‹¤."""),

code(r"""# â”€â”€ Q4 í’€ì´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 45)
print("Q4 í’€ì´: Shared Expert ì¶”ê°€ ë¶„ì„")
print("=" * 45)

d_model = 256
d_ff = 512
n_routed = 8
top_k = 2

# Expertë‹¹ íŒŒë¼ë¯¸í„° (FFN: up + down)
expert_params = d_model * d_ff + d_ff * d_model
shared_params = expert_params  # Sharedë„ ê°™ì€ êµ¬ì¡°

# í™œì„± íŒŒë¼ë¯¸í„° (ì²˜ë¦¬ ì‹œ)
active_without_shared = top_k * expert_params
active_with_shared = (1 + top_k) * expert_params

print(f"ì„¤ì •: d_model={d_model}, d_ff={d_ff}, N_routed={n_routed}, Top-{top_k}")
print(f"Expertë‹¹ íŒŒë¼ë¯¸í„°: {expert_params:,}")
print()
print(f"{'êµ¬ì„±':<25} | {'ì´ íŒŒë¼ë¯¸í„°':>12} | {'í™œì„± íŒŒë¼ë¯¸í„°':>12}")
print("-" * 55)
print(f"{'MoE (Shared ì—†ìŒ)':<25} | {n_routed*expert_params:>12,} | {active_without_shared:>12,}")
print(f"{'DeepSeekMoE (Shared 1)':<25} | {(n_routed+1)*expert_params:>12,} | {active_with_shared:>12,}")
print()
increase = (active_with_shared / active_without_shared - 1) * 100
print(f"Shared Expert ì¶”ê°€ ì‹œ í™œì„± íŒŒë¼ë¯¸í„° ì¦ê°€: {increase:.0f}%")
print(f"  â†’ Shared Expertê°€ 'ê³µí†µ ì§€ì‹'ì„ ë‹´ë‹¹í•˜ë¯€ë¡œ Top-k ê°ì†Œ ê°€ëŠ¥")
print()
print("ìµœì¢… ì¶œë ¥:")
print("  y = E_s(x) + sum(g_i * E_i^r(x)) for i in Top-k")
print("  SharedëŠ” í•­ìƒ í™œì„± â†’ ê³µí†µ íŒ¨í„´ í•™ìŠµ")
print("  RoutedëŠ” ì„ íƒì  â†’ ì „ë¬¸ íŒ¨í„´ í•™ìŠµ")"""),

md(r"""---
## ì¢…í•© ë„ì „: DeepSeekMoE ìŠ¤íƒ€ì¼ ì™„ì „í•œ MoE ë ˆì´ì–´ <a name='bonus'></a>

### ë¬¸ì œ

ë‹¤ìŒ ì¡°ê±´ìœ¼ë¡œ **ì™„ì „í•œ DeepSeekMoE ë ˆì´ì–´**ë¥¼ êµ¬í˜„í•˜ì„¸ìš”:
1. Shared Expert 1ê°œ + Routed Expert 4ê°œ
2. Top-2 ë¼ìš°íŒ… (Softmax ê²Œì´íŒ…)
3. Auxiliary-Loss-Free í¸í–¥ ë³´ì •
4. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›"""),

code(r"""# â”€â”€ ì¢…í•© ë„ì „ í’€ì´: DeepSeekMoE ë ˆì´ì–´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 55)
print("ì¢…í•© ë„ì „: DeepSeekMoE ìŠ¤íƒ€ì¼ MoE ë ˆì´ì–´")
print("=" * 55)

class MiniDeepSeekMoE(tf.keras.layers.Layer):
    def __init__(self, d_model, d_ff, n_routed=4, top_k=2):
        super().__init__()
        self.n_routed = n_routed
        self.top_k = top_k
        
        # Shared Expert
        self.shared = tf.keras.Sequential([
            tf.keras.layers.Dense(d_ff, activation='relu'),
            tf.keras.layers.Dense(d_model)
        ])
        
        # Routed Experts
        self.experts = [
            tf.keras.Sequential([
                tf.keras.layers.Dense(d_ff, activation='relu'),
                tf.keras.layers.Dense(d_model)
            ]) for _ in range(n_routed)
        ]
        
        # Router
        self.router = tf.keras.layers.Dense(n_routed, use_bias=False)
        
        # Aux-Free bias
        self.bias = tf.Variable(tf.zeros(n_routed), trainable=False)
    
    def call(self, x):
        # Shared output
        out = self.shared(x)
        
        # Routing
        logits = self.router(x) + self.bias
        top_vals, top_idx = tf.math.top_k(logits, k=self.top_k)
        gates = tf.nn.softmax(top_vals, axis=-1)
        
        # Routed output
        for k in range(self.top_k):
            idx = top_idx[:, :, k]
            g = gates[:, :, k:k+1]
            for e in range(self.n_routed):
                mask = tf.cast(tf.equal(idx, e), tf.float32)[:, :, tf.newaxis]
                if tf.reduce_sum(mask) > 0:
                    out += self.experts[e](x) * mask * g
        
        return out

# í…ŒìŠ¤íŠ¸
d_model, d_ff = 64, 128
moe = MiniDeepSeekMoE(d_model, d_ff, n_routed=4, top_k=2)
x = tf.random.normal((2, 8, d_model))
y = moe(x)

total = sum(tf.size(v).numpy() for v in moe.trainable_variables)
print(f"ì…ë ¥: {x.shape}")
print(f"ì¶œë ¥: {y.shape}")
print(f"ì´ íŒŒë¼ë¯¸í„°: {total:,}")
print(f"í™œì„± Expert: 1(shared) + 2(routed) = 3")
print()
print("ğŸ‰ DeepSeekMoE ìŠ¤íƒ€ì¼ ë ˆì´ì–´ êµ¬í˜„ ì™„ë£Œ!")
print("   Shared Expertê°€ ê³µí†µ ì§€ì‹ì„, Routed Expertê°€ ì „ë¬¸ ì§€ì‹ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.")"""),
]

create_notebook(ex02_cells, 'chapter12_modern_llms/practice/ex02_custom_moe_layer.ipynb')
