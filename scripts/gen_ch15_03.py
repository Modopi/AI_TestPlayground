"""Generate chapter15_alignment_rlhf/03_rlhf_pipeline_overview.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” RLHF íŒŒì´í”„ë¼ì¸ ê°œìš”

## í•™ìŠµ ëª©í‘œ
- InstructGPTì˜ SFT â†’ Reward Model â†’ PPO 3ë‹¨ê³„ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤
- Bradley-Terry ì„ í˜¸ ëª¨ë¸ì˜ ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  êµ¬í˜„í•œë‹¤
- Reward Modelì˜ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìœ ë„í•˜ê³  í›ˆë ¨ ê³¼ì •ì„ êµ¬í˜„í•œë‹¤
- RLHFì˜ PPO ë‹¨ê³„ì—ì„œ KL í˜ë„í‹°ê°€ í•˜ëŠ” ì—­í• ì„ ìˆ˜ì‹ìœ¼ë¡œ ì„¤ëª…í•œë‹¤
- ì „ì²´ RLHF íŒŒì´í”„ë¼ì¸ì„ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ êµ¬í˜„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Bradley-Terry ëª¨ë¸ê³¼ RLHF ìˆ˜ì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [SFT â†’ RM â†’ PPO íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜](#2.-RLHF-íŒŒì´í”„ë¼ì¸)
3. [Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„](#3.-Bradley-Terry-ëª¨ë¸)
4. [Reward Model í•™ìŠµ](#4.-Reward-Model-í•™ìŠµ)
5. [KL Divergence í˜ë„í‹° ì‹œê°í™”](#5.-KL-Divergence-í˜ë„í‹°)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### Bradley-Terry ì„ í˜¸ ëª¨ë¸

ë‘ ì‘ë‹µ $y_w$(ì„ í˜¸), $y_l$(ë¹„ì„ í˜¸)ì— ëŒ€í•œ ì„ í˜¸ í™•ë¥ :

$$P(y_w \succ y_l \mid x) = \sigma\left(r(x, y_w) - r(x, y_l)\right)$$

- $\sigma(z) = \frac{1}{1 + e^{-z}}$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
- $r(x, y)$: ë³´ìƒ ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ìŠ¤ì¹¼ë¼ ì ìˆ˜
- $y_w \succ y_l$: $y_w$ê°€ $y_l$ë³´ë‹¤ ì„ í˜¸ë¨

### Reward Model í•™ìŠµ ì†ì‹¤

$$\mathcal{L}_{RM}(\theta) = -\mathbb{E}_{(x, y_w, y_l)}\left[\log \sigma\left(r_\theta(x, y_w) - r_\theta(x, y_l)\right)\right]$$

- ì„ í˜¸ ì‘ë‹µì˜ ë³´ìƒì„ ë¹„ì„ í˜¸ ì‘ë‹µë³´ë‹¤ **ë†’ì´ëŠ”** ë°©í–¥ìœ¼ë¡œ í•™ìŠµ

### RLHF ëª©ì í•¨ìˆ˜ (PPO ë‹¨ê³„)

$$\max_\theta \; \mathbb{E}_{x \sim D,\; y \sim \pi_\theta(\cdot|x)} \left[r_\phi(x, y)\right] - \beta \, D_{KL}\left[\pi_\theta(\cdot|x) \;\|\; \pi_{ref}(\cdot|x)\right]$$

- $r_\phi(x, y)$: í•™ìŠµëœ Reward Modelì˜ ì ìˆ˜
- $\pi_{ref}$: SFT ëª¨ë¸ (ê¸°ì¤€ ì •ì±…)
- $\beta$: KL í˜ë„í‹° ê³„ìˆ˜

### InstructGPT 3ë‹¨ê³„

| ë‹¨ê³„ | ì…ë ¥ | ì¶œë ¥ | ì†ì‹¤í•¨ìˆ˜ |
|------|------|------|---------|
| **Step 1: SFT** | $(x, y^*)$ ì‹œë²” ë°ì´í„° | $\pi_{SFT}$ | $-\log P(y^* \mid x)$ (êµì°¨ ì—”íŠ¸ë¡œí”¼) |
| **Step 2: RM** | $(x, y_w, y_l)$ ë¹„êµ ìŒ | $r_\theta$ | $-\log\sigma(r_\theta(y_w) - r_\theta(y_l))$ |
| **Step 3: PPO** | $x$ í”„ë¡¬í”„íŠ¸ | $\pi_\theta$ | $\mathbb{E}[r_\phi(y)] - \beta D_{KL}$ |

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| Bradley-Terry | $P(y_w \succ y_l) = \sigma(r(y_w) - r(y_l))$ | ìŒë³„ ë¹„êµ ì„ í˜¸ ëª¨ë¸ |
| RM Loss | $-\mathbb{E}[\log\sigma(\Delta r)]$ | ì„ í˜¸ ì ìˆ˜ ì°¨ì´ ìµœëŒ€í™” |
| RLHF Objective | $\mathbb{E}[r(y)] - \beta D_{KL}$ | ë³´ìƒ ìµœëŒ€í™” + ì•ˆì •ì„± |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ RLHF ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ RLHFê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: AIë¥¼ ê°•ì•„ì§€ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•´ìš”!

**1ë‹¨ê³„ - SFT (ë”°ë¼í•˜ê¸° í›ˆë ¨):** ë¨¼ì € ê°•ì•„ì§€ì—ê²Œ "ì•‰ì•„"ë¥¼ ì—¬ëŸ¬ ë²ˆ ë³´ì—¬ì£¼ë©´ì„œ ë”°ë¼í•˜ê²Œ í•´ìš”.
ì´ê²ƒì€ ì‚¬ëŒì´ ì“´ ì¢‹ì€ ë‹µë³€ì„ AIì—ê²Œ ë³´ì—¬ì£¼ê³  ë”°ë¼ì“°ê²Œ í•˜ëŠ” ê±°ì˜ˆìš”.

**2ë‹¨ê³„ - Reward Model (ì ìˆ˜íŒ ë§Œë“¤ê¸°):** ê°•ì•„ì§€ê°€ í•œ í–‰ë™ ì¤‘ì—ì„œ "ì´ê²Œ ë” ì¢‹ì•„!"ë¼ê³  
ì‚¬ëŒì´ ê³¨ë¼ì£¼ë©´, AIê°€ "ë¬´ì—‡ì´ ì¢‹ì€ í–‰ë™ì¸ì§€" ë°°ì›Œìš”. ì´ê²ƒì´ ë³´ìƒ ëª¨ë¸ì´ì—ìš”.

**3ë‹¨ê³„ - PPO (ììœ  ì—°ìŠµ):** ê°•ì•„ì§€ê°€ ììœ ë¡­ê²Œ í–‰ë™í•˜ë©´ì„œ, ì ìˆ˜íŒ(ë³´ìƒ ëª¨ë¸)ì„ ë³´ê³ 
ìŠ¤ìŠ¤ë¡œ ë” ì¢‹ì€ í–‰ë™ì„ ì°¾ì•„ê°€ìš”. ë‹¨, ë„ˆë¬´ ì—‰ëš±í•œ í–‰ë™ì€ ì•ˆ ë˜ë„ë¡ ì œí•œí•´ìš”(KL í˜ë„í‹°).

#### ğŸ† Bradley-Terryê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ì¶•êµ¬ íŒ€ ë­í‚¹ì„ ìƒê°í•´ ë³´ì„¸ìš”!

ë‘ íŒ€ì´ ì‹œí•©í•˜ë©´ ì ìˆ˜ê°€ ë†’ì€ íŒ€ì´ ì´ê¸¸ í™•ë¥ ì´ ë†’ì£ ? Bradley-Terry ëª¨ë¸ì€:
- "AíŒ€ ì ìˆ˜ê°€ 100ì´ê³  BíŒ€ ì ìˆ˜ê°€ 80ì´ë©´, Aê°€ ì´ê¸¸ í™•ë¥ ì€ ì–¼ë§ˆ?"ë¥¼ ê³„ì‚°í•´ìš”
- ë§ˆì°¬ê°€ì§€ë¡œ "ë‹µë³€ Aì˜ ì ìˆ˜ê°€ ë†’ê³  ë‹µë³€ Bì˜ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´, Aê°€ ì„ í˜¸ë  í™•ë¥ ì€?"ì„ ê³„ì‚°í•´ìš”

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Bradley-Terry í™•ë¥  ê³„ì‚°

ë³´ìƒ ëª¨ë¸ì´ ë‘ ì‘ë‹µì— ëŒ€í•´ $r(x, y_w) = 2.0$, $r(x, y_l) = 0.5$ë¥¼ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.
$y_w$ê°€ ì„ í˜¸ë  í™•ë¥  $P(y_w \succ y_l)$ì„ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$P(y_w \succ y_l) = \sigma(r(y_w) - r(y_l)) = \sigma(2.0 - 0.5) = \sigma(1.5)$$

$$= \frac{1}{1 + e^{-1.5}} = \frac{1}{1 + 0.2231} = \frac{1}{1.2231} \approx 0.8176$$

â†’ ì•½ 81.8%ì˜ í™•ë¥ ë¡œ $y_w$ê°€ ì„ í˜¸ë©ë‹ˆë‹¤. ë³´ìƒ ì°¨ì´ê°€ í´ìˆ˜ë¡ í™•ë¥ ì´ 1ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: Reward Model Loss ê³„ì‚°

$r_\theta(y_w) = 1.0$, $r_\theta(y_l) = 0.8$ì¼ ë•Œ ì†ì‹¤ê°’ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\mathcal{L} = -\log\sigma(r_\theta(y_w) - r_\theta(y_l)) = -\log\sigma(1.0 - 0.8) = -\log\sigma(0.2)$$

$$\sigma(0.2) = \frac{1}{1+e^{-0.2}} = \frac{1}{1.8187} \approx 0.5498$$

$$\mathcal{L} = -\log(0.5498) \approx 0.5981$$

â†’ ë³´ìƒ ì°¨ì´ê°€ ì‘ì•„ì„œ ëª¨ë¸ì˜ í™•ì‹ ë„ê°€ ë‚®ê³  ì†ì‹¤ì´ ë¹„êµì  í½ë‹ˆë‹¤. í•™ìŠµì„ í†µí•´ ì°¨ì´ë¥¼ ë” ë²Œë ¤ì•¼ í•©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 2. SFT â†’ RM â†’ PPO íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ <a name='2.-RLHF-íŒŒì´í”„ë¼ì¸'></a>

InstructGPTì˜ 3ë‹¨ê³„ RLHF íŒŒì´í”„ë¼ì¸ì„ ê°„ì†Œí™”ëœ í˜•íƒœë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.
ì‹¤ì œ LLM ëŒ€ì‹  ì‘ì€ MLP ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ì›ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: SFT â”‚ â”€â”€â†’ â”‚  Step 2: RM  â”‚ â”€â”€â†’ â”‚  Step 3: PPOâ”‚
â”‚  ì§€ë„í•™ìŠµ     â”‚     â”‚  ì„ í˜¸ í•™ìŠµ    â”‚     â”‚  ê°•í™”í•™ìŠµ     â”‚
â”‚  (x, y*) ìŒ  â”‚     â”‚  (x, y_w>y_l)â”‚     â”‚  r(x,y) ìµœëŒ€í™”â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```"""))

# â”€â”€ Cell 7: RLHF pipeline simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ RLHF 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ì†Œí™”: ì…ë ¥ xëŠ” 5ì°¨ì› ë²¡í„°, ì¶œë ¥ yëŠ” 3ì°¨ì› ë²¡í„°

input_dim = 5
output_dim = 3
hidden_dim = 16
n_samples = 200

# ì‹œë²” ë°ì´í„° ìƒì„± (SFTìš©)
np.random.seed(42)
X_demo = np.random.randn(n_samples, input_dim).astype(np.float32)
# "ì¢‹ì€" ì‘ë‹µ: ì…ë ¥ì˜ ë¹„ì„ í˜• ë³€í™˜
true_transform = lambda x: np.tanh(x @ np.random.randn(input_dim, output_dim) * 0.5)
np.random.seed(42)
Y_demo = true_transform(X_demo).astype(np.float32)

# ============================================================
# Step 1: SFT (Supervised Fine-Tuning)
# ============================================================
print("=" * 55)
print("  Step 1: SFT (ì§€ë„í•™ìŠµ ë¯¸ì„¸ì¡°ì •)")
print("=" * 55)

sft_model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_dim, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(hidden_dim, activation='relu'),
    tf.keras.layers.Dense(output_dim)
])
sft_model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='mse')

sft_history = sft_model.fit(X_demo, Y_demo, epochs=50, batch_size=32,
                             verbose=0, validation_split=0.2)

sft_loss = sft_history.history['loss'][-1]
sft_val_loss = sft_history.history['val_loss'][-1]
print(f"  SFT ìµœì¢… ì†ì‹¤: {sft_loss:.4f}")
print(f"  SFT ê²€ì¦ ì†ì‹¤: {sft_val_loss:.4f}")

# SFT ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ê¸°ì¤€ ì •ì±…)
ref_weights = [w.numpy().copy() for w in sft_model.trainable_variables]

# ì˜ˆì¸¡ ì˜ˆì‹œ
sample_x = X_demo[:3]
sample_pred = sft_model.predict(sample_x, verbose=0)
print(f"\\n  ì˜ˆì¸¡ ì˜ˆì‹œ (ì…ë ¥ 3ê°œ):")
for i in range(3):
    print(f"    y_pred[{i}] = [{', '.join(f'{v:.3f}' for v in sample_pred[i])}]")
    print(f"    y_true[{i}] = [{', '.join(f'{v:.3f}' for v in Y_demo[i])}]")"""))

# â”€â”€ Cell 8: Bradley-Terry section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„ <a name='3.-Bradley-Terry-ëª¨ë¸'></a>

ì„ í˜¸ ìŒ ë°ì´í„° $(x, y_w, y_l)$ì„ ìƒì„±í•˜ê³ , Bradley-Terry í™•ë¥  ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤.

$$P(y_w \succ y_l \mid x) = \sigma(r_\theta(x, y_w) - r_\theta(x, y_l))$$

ë³´ìƒ ì°¨ì´ì— ë”°ë¥¸ ì„ í˜¸ í™•ë¥  ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 9: Bradley-Terry implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„ í˜¸ ìŒ ë°ì´í„° ìƒì„±
n_pairs = 500
np.random.seed(42)

X_pairs = np.random.randn(n_pairs, input_dim).astype(np.float32)

# ì„ í˜¸ ì‘ë‹µ: SFT ëª¨ë¸ì˜ ì¶œë ¥ + ì•½ê°„ì˜ ê°œì„ 
Y_preferred = sft_model.predict(X_pairs, verbose=0) + np.random.randn(n_pairs, output_dim).astype(np.float32) * 0.1
# ë¹„ì„ í˜¸ ì‘ë‹µ: ëœë¤ ë…¸ì´ì¦ˆê°€ í° ì¶œë ¥
Y_rejected = sft_model.predict(X_pairs, verbose=0) + np.random.randn(n_pairs, output_dim).astype(np.float32) * 0.8

# Bradley-Terry í™•ë¥  ì‹œê°í™”
reward_diffs = np.linspace(-5, 5, 200)
bt_probs = 1.0 / (1.0 + np.exp(-reward_diffs))

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) Bradley-Terry í™•ë¥  ê³¡ì„ 
ax1 = axes[0]
ax1.plot(reward_diffs, bt_probs, 'b-', lw=2.5, label=r'$\sigma(\Delta r)$')
ax1.axhline(y=0.5, color='gray', ls='--', lw=1.5, alpha=0.5)
ax1.axvline(x=0, color='gray', ls='--', lw=1.5, alpha=0.5)
ax1.fill_between(reward_diffs, bt_probs, 0.5,
                 where=(reward_diffs > 0), alpha=0.15, color='green',
                 label=r'$r(y_w) > r(y_l)$')
ax1.fill_between(reward_diffs, bt_probs, 0.5,
                 where=(reward_diffs < 0), alpha=0.15, color='red',
                 label=r'$r(y_w) < r(y_l)$')
ax1.set_xlabel(r'ë³´ìƒ ì°¨ì´ $r(y_w) - r(y_l)$', fontsize=11)
ax1.set_ylabel(r'$P(y_w \succ y_l)$', fontsize=11)
ax1.set_title('Bradley-Terry ì„ í˜¸ í™•ë¥ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) Reward Model Loss ê³¡ì„ 
ax2 = axes[1]
rm_loss_vals = -np.log(bt_probs + 1e-8)
ax2.plot(reward_diffs, rm_loss_vals, 'r-', lw=2.5, label=r'$-\log\sigma(\Delta r)$')
ax2.set_xlabel(r'ë³´ìƒ ì°¨ì´ $\Delta r$', fontsize=11)
ax2.set_ylabel('Loss', fontsize=11)
ax2.set_title('Reward Model ì†ì‹¤ í•¨ìˆ˜', fontweight='bold')
ax2.set_ylim(0, 5)
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/bradley_terry_model.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/bradley_terry_model.png")

# ìˆ˜ì¹˜ ì˜ˆì‹œ
print(f"\\nBradley-Terry í™•ë¥  ì˜ˆì‹œ:")
print(f"{'Î”r':>6} | {'P(y_w â‰» y_l)':>14} | {'RM Loss':>10}")
print(f"{'-'*36}")
for dr in [-2.0, -1.0, 0.0, 0.5, 1.0, 2.0, 3.0]:
    p = 1 / (1 + np.exp(-dr))
    loss = -np.log(p)
    print(f"{dr:>+6.1f} | {p:>14.4f} | {loss:>10.4f}")"""))

# â”€â”€ Cell 10: Reward Model training section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. Reward Model í•™ìŠµ <a name='4.-Reward-Model-í•™ìŠµ'></a>

Reward Modelì€ ì…ë ¥ $(x, y)$ë¥¼ ë°›ì•„ ìŠ¤ì¹¼ë¼ ì ìˆ˜ $r_\theta(x, y)$ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

ì†ì‹¤ í•¨ìˆ˜:

$$\mathcal{L}_{RM}(\theta) = -\frac{1}{N}\sum_{i=1}^{N} \log \sigma\left(r_\theta(x_i, y_w^i) - r_\theta(x_i, y_l^i)\right)$$"""))

# â”€â”€ Cell 11: Reward Model training â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Reward Model í•™ìŠµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reward Model: (x, y)ë¥¼ concatí•˜ì—¬ ìŠ¤ì¹¼ë¼ ë³´ìƒ ì¶œë ¥
reward_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu',
                          input_shape=(input_dim + output_dim,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1)  # ìŠ¤ì¹¼ë¼ ë³´ìƒ
])

optimizer_rm = tf.keras.optimizers.Adam(learning_rate=0.001)

# í•™ìŠµ ë°ì´í„° ì¤€ë¹„
X_w = np.concatenate([X_pairs, Y_preferred], axis=1)
X_l = np.concatenate([X_pairs, Y_rejected], axis=1)

n_epochs = 50
batch_size = 64
rm_losses = []
rm_accuracies = []

for epoch in range(n_epochs):
    epoch_losses = []
    epoch_correct = 0

    indices = np.random.permutation(n_pairs)
    for start in range(0, n_pairs, batch_size):
        batch_idx = indices[start:start + batch_size]

        with tf.GradientTape() as tape:
            r_w = reward_model(X_w[batch_idx], training=True)
            r_l = reward_model(X_l[batch_idx], training=True)

            # Bradley-Terry loss
            diff = r_w - r_l
            loss = -tf.reduce_mean(tf.math.log_sigmoid(diff))

        grads = tape.gradient(loss, reward_model.trainable_variables)
        optimizer_rm.apply_gradients(zip(grads, reward_model.trainable_variables))

        epoch_losses.append(loss.numpy())
        epoch_correct += tf.reduce_sum(
            tf.cast(diff > 0, tf.float32)).numpy()

    avg_loss = np.mean(epoch_losses)
    accuracy = epoch_correct / n_pairs
    rm_losses.append(avg_loss)
    rm_accuracies.append(accuracy)

    if (epoch + 1) % 10 == 0:
        print(f"  Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, "
              f"ì„ í˜¸ ì •í™•ë„={accuracy:.4f}")

print(f"\\nReward Model í•™ìŠµ ì™„ë£Œ")
print(f"  ìµœì¢… ì†ì‹¤: {rm_losses[-1]:.4f}")
print(f"  ìµœì¢… ì„ í˜¸ ì •í™•ë„: {rm_accuracies[-1]:.4f}")

# ë³´ìƒ ë¶„í¬ í™•ì¸
r_w_all = reward_model(X_w).numpy().flatten()
r_l_all = reward_model(X_l).numpy().flatten()
print(f"\\në³´ìƒ ë¶„í¬:")
print(f"  ì„ í˜¸ ì‘ë‹µ r(y_w): í‰ê· ={r_w_all.mean():.3f}, í‘œì¤€í¸ì°¨={r_w_all.std():.3f}")
print(f"  ë¹„ì„ í˜¸ ì‘ë‹µ r(y_l): í‰ê· ={r_l_all.mean():.3f}, í‘œì¤€í¸ì°¨={r_l_all.std():.3f}")
print(f"  í‰ê·  ë³´ìƒ ì°¨ì´: {(r_w_all - r_l_all).mean():.3f}")"""))

# â”€â”€ Cell 12: RM training visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Reward Model í•™ìŠµ ê³¼ì • ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# (1) í•™ìŠµ ì†ì‹¤
ax1 = axes[0]
ax1.plot(rm_losses, 'b-', lw=2, label='RM Loss')
ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('Reward Model í•™ìŠµ ì†ì‹¤', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ì„ í˜¸ ì •í™•ë„
ax2 = axes[1]
ax2.plot(rm_accuracies, 'g-', lw=2, label='ì„ í˜¸ ì •í™•ë„')
ax2.axhline(y=0.5, color='red', ls='--', lw=1.5, label='ëœë¤ ê¸°ì¤€ì„ ')
ax2.set_xlabel('Epoch', fontsize=11)
ax2.set_ylabel('Accuracy', fontsize=11)
ax2.set_title('ì„ í˜¸ ìŒ ë¶„ë¥˜ ì •í™•ë„', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0.4, 1.05)

# (3) ë³´ìƒ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
ax3 = axes[2]
ax3.hist(r_w_all, bins=30, alpha=0.6, color='green', label=r'$r(y_w)$ ì„ í˜¸', density=True)
ax3.hist(r_l_all, bins=30, alpha=0.6, color='red', label=r'$r(y_l)$ ë¹„ì„ í˜¸', density=True)
ax3.axvline(x=r_w_all.mean(), color='darkgreen', ls='--', lw=2)
ax3.axvline(x=r_l_all.mean(), color='darkred', ls='--', lw=2)
ax3.set_xlabel('ë³´ìƒ ì ìˆ˜', fontsize=11)
ax3.set_ylabel('ë°€ë„', fontsize=11)
ax3.set_title('í•™ìŠµëœ ë³´ìƒ ë¶„í¬', fontweight='bold')
ax3.legend(fontsize=9)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/reward_model_training.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/reward_model_training.png")
print(f"ì„ í˜¸ ì‘ë‹µ(ë…¹ìƒ‰)ê³¼ ë¹„ì„ í˜¸ ì‘ë‹µ(ë¹¨ê°„ìƒ‰)ì˜ ë³´ìƒ ë¶„í¬ê°€ ì˜ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")"""))

# â”€â”€ Cell 13: KL divergence section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. KL Divergence í˜ë„í‹° ì‹œê°í™” <a name='5.-KL-Divergence-í˜ë„í‹°'></a>

RLHFì˜ PPO ë‹¨ê³„ì—ì„œ KL í˜ë„í‹°ëŠ” í•™ìŠµ ì •ì±…ì´ ê¸°ì¤€ ì •ì±…(SFT)ì—ì„œ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤:

$$\max_\theta \; \mathbb{E}_{y \sim \pi_\theta}\left[r_\phi(x, y)\right] - \beta \, D_{KL}\left[\pi_\theta \| \pi_{ref}\right]$$

$$D_{KL}\left[\pi_\theta \| \pi_{ref}\right] = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$$

$\beta$ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ â†’ Reward Hacking (ë³´ìƒ ëª¨ë¸ì˜ í—ˆì ì„ ì•…ìš©)  
$\beta$ê°€ ë„ˆë¬´ í¬ë©´ â†’ SFT ëª¨ë¸ì—ì„œ ê±°ì˜ ë²—ì–´ë‚˜ì§€ ëª»í•¨"""))

# â”€â”€ Cell 14: KL divergence visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ KL Divergence í˜ë„í‹° ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¤ì–‘í•œ Î² ê°’ì— ë”°ë¥¸ RLHF ëª©ì í•¨ìˆ˜ ë¶„ì„

# ì‹œë®¬ë ˆì´ì…˜: ì •ì±…ì´ ê¸°ì¤€ì—ì„œ ì ì§„ì ìœ¼ë¡œ ë²—ì–´ë‚˜ëŠ” ê²½ìš°
n_vocab = 20
ref_logits = np.random.randn(n_vocab) * 0.5
ref_probs = np.exp(ref_logits) / np.exp(ref_logits).sum()

# ë³´ìƒì´ ë†’ì€ í† í°ìœ¼ë¡œ ì •ì±…ì´ ì´ë™í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤
high_reward_tokens = [3, 7, 12]
shift_amounts = np.linspace(0, 3, 50)

betas = [0.01, 0.05, 0.1, 0.3, 0.5]
results = {b: {'kl': [], 'reward': [], 'objective': []} for b in betas}

for shift in shift_amounts:
    # ì •ì±… ì´ë™: ì„ í˜¸ í† í°ì— ë³´ë„ˆìŠ¤
    shifted_logits = ref_logits.copy()
    for t in high_reward_tokens:
        shifted_logits[t] += shift
    shifted_probs = np.exp(shifted_logits) / np.exp(shifted_logits).sum()

    # KL divergence ê³„ì‚°
    kl = np.sum(shifted_probs * np.log(shifted_probs / (ref_probs + 1e-10) + 1e-10))

    # ë³´ìƒ ê³„ì‚° (ì„ í˜¸ í† í° í™•ë¥  í•©)
    reward = sum(shifted_probs[t] for t in high_reward_tokens)

    for beta in betas:
        results[beta]['kl'].append(kl)
        results[beta]['reward'].append(reward)
        results[beta]['objective'].append(reward - beta * kl)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# (1) ë³´ìƒ vs KL íŠ¸ë ˆì´ë“œì˜¤í”„
ax1 = axes[0]
kls = results[0.01]['kl']
rewards = results[0.01]['reward']
ax1.plot(kls, rewards, 'b-o', lw=2, ms=3, label='ë³´ìƒ vs KL')
ax1.set_xlabel(r'$D_{KL}[\pi_\theta \| \pi_{ref}]$', fontsize=11)
ax1.set_ylabel('Expected Reward', fontsize=11)
ax1.set_title('ë³´ìƒ-KL íŠ¸ë ˆì´ë“œì˜¤í”„', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ë‹¤ì–‘í•œ Î²ì— ë”°ë¥¸ ëª©ì í•¨ìˆ˜
ax2 = axes[1]
colors_beta = ['#2196F3', '#FF9800', '#E91E63', '#9C27B0', '#4CAF50']
for beta, color in zip(betas, colors_beta):
    ax2.plot(shift_amounts, results[beta]['objective'],
             lw=2, color=color, label=f'Î²={beta}')
    # ìµœì ì  í‘œì‹œ
    opt_idx = np.argmax(results[beta]['objective'])
    ax2.plot(shift_amounts[opt_idx], results[beta]['objective'][opt_idx],
             '*', ms=15, color=color)
ax2.set_xlabel('ì •ì±… ì´ë™ëŸ‰', fontsize=11)
ax2.set_ylabel(r'$\mathbb{E}[r] - \beta D_{KL}$', fontsize=11)
ax2.set_title(r'RLHF ëª©ì í•¨ìˆ˜ ($\beta$ ë³€í™”)', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

# (3) KL ë°œì‚° ê³¡ì„ 
ax3 = axes[2]
ax3.plot(shift_amounts, results[0.01]['kl'], 'r-', lw=2.5,
         label=r'$D_{KL}[\pi_\theta \| \pi_{ref}]$')
ax3.fill_between(shift_amounts, 0, results[0.01]['kl'],
                 alpha=0.1, color='red')
ax3.set_xlabel('ì •ì±… ì´ë™ëŸ‰', fontsize=11)
ax3.set_ylabel(r'$D_{KL}$', fontsize=11)
ax3.set_title('KL Divergence ì¦ê°€', fontweight='bold')
ax3.legend(fontsize=9)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/kl_divergence_penalty.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/kl_divergence_penalty.png")

# ìµœì ì  ë¶„ì„
print(f"\\nÎ²ë³„ ìµœì  ì •ì±… ì´ë™ ì§€ì :")
print(f"{'Î²':>6} | {'ìµœì  ì´ë™ëŸ‰':>10} | {'ë³´ìƒ':>8} | {'KL':>8} | {'ëª©ì í•¨ìˆ˜':>10}")
print(f"{'-'*52}")
for beta in betas:
    opt_idx = np.argmax(results[beta]['objective'])
    print(f"{beta:>6.2f} | {shift_amounts[opt_idx]:>10.2f} | "
          f"{results[beta]['reward'][opt_idx]:>8.4f} | "
          f"{results[beta]['kl'][opt_idx]:>8.4f} | "
          f"{results[beta]['objective'][opt_idx]:>+10.4f}")
print(f"\\n  â†’ Î²ê°€ í´ìˆ˜ë¡ ì •ì±…ì´ ê¸°ì¤€ì— ê°€ê¹Œì´ ë¨¸ë¬´ë¦„ (ë³´ìˆ˜ì )")
print(f"  â†’ Î²ê°€ ì‘ì„ìˆ˜ë¡ ë³´ìƒ ê·¹ëŒ€í™”ì— ì§‘ì¤‘ (ê³µê²©ì  â†’ Reward Hacking ìœ„í—˜)")"""))

# â”€â”€ Cell 15: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| InstructGPT 3ë‹¨ê³„ | SFT â†’ Reward Model â†’ PPO | â­â­â­ |
| Bradley-Terry | $P(y_w \succ y_l) = \sigma(\Delta r)$ â€” ìŒë³„ ì„ í˜¸ í™•ë¥  | â­â­â­ |
| RM Loss | $-\mathbb{E}[\log\sigma(r(y_w) - r(y_l))]$ | â­â­â­ |
| RLHF Objective | $\mathbb{E}[r(y)] - \beta D_{KL}[\pi_\theta \| \pi_{ref}]$ | â­â­â­ |
| KL í˜ë„í‹° | $\beta D_{KL}$ â€” ê¸°ì¤€ ì •ì±…ì—ì„œ ì´íƒˆ ì œí•œ | â­â­â­ |
| Reward Hacking | Î²ê°€ ì‘ì„ ë•Œ ë³´ìƒ ëª¨ë¸ì˜ í—ˆì ì„ ì•…ìš©í•˜ëŠ” í˜„ìƒ | â­â­ |
| SFT | ì‹œë²” ë°ì´í„°ë¡œ ì§€ë„í•™ìŠµ â€” RLHFì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$P(y_w \succ y_l \mid x) = \sigma\left(r(x, y_w) - r(x, y_l)\right)$$

$$\mathcal{L}_{RM}(\theta) = -\mathbb{E}\left[\log \sigma\left(r_\theta(x, y_w) - r_\theta(x, y_l)\right)\right]$$

$$\max_\theta \; \mathbb{E}_{y \sim \pi_\theta}\left[r_\phi(x, y)\right] - \beta \, D_{KL}\left[\pi_\theta \| \pi_{ref}\right]$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**04_dpo_and_preference_learning.ipynb** â€” Reward Model ì—†ì´ë„ ì„ í˜¸ í•™ìŠµì´ ê°€ëŠ¥í•œ DPO(Direct Preference Optimization)ì˜ ë² ì´ì¦ˆ ë„ì¶œê³¼ RLHF ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter15_alignment_rlhf/03_rlhf_pipeline_overview.ipynb')
