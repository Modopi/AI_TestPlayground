"""Generate chapter14_extreme_inference/01_inference_bottlenecks.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 14: ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” â€” ì¶”ë¡  ë³‘ëª© ë¶„ì„

## í•™ìŠµ ëª©í‘œ
- LLM ì¶”ë¡ ì˜ ë‘ ë‹¨ê³„(Prefill / Decode)ê°€ **ë¬¼ë¦¬ì ìœ¼ë¡œ ë‹¤ë¥¸ ë³‘ëª©**ì„ ê°–ëŠ” ì›ì¸ì„ ì´í•´í•œë‹¤
- Arithmetic Intensity(ì—°ì‚° ê°•ë„)ë¥¼ ì´ìš©í•´ ê° ë‹¨ê³„ê°€ **Compute-bound / Memory-bound**ì¸ì§€ íŒë³„í•œë‹¤
- Roofline ëª¨ë¸ì„ LLM ì¶”ë¡ ì— ì ìš©í•˜ì—¬ **í•˜ë“œì›¨ì–´ í™œìš©ë¥ **ì„ ì‹œê°í™”í•œë‹¤
- TTFT(Time To First Token)ì™€ TPOT(Time Per Output Token)ì˜ **ìˆ˜ì‹ì„ ë„ì¶œ**í•˜ê³  ì¸¡ì •í•œë‹¤
- ë°°ì¹˜ í¬ê¸°(Batch Size)ê°€ **ì²˜ë¦¬ëŸ‰(Throughput)ê³¼ ì§€ì—°(Latency)**ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ì—°ì‚° ê°•ë„ì™€ Roofline ëª¨ë¸](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Prefill vs Decode ë‹¨ê³„ ë¶„ì„](#2.-Prefill-vs-Decode)
3. [Roofline ì‹œê°í™”](#3.-Roofline-ì‹œê°í™”)
4. [TTFT / TPOT ì¸¡ì • ì‹œë®¬ë ˆì´ì…˜](#4.-TTFT-/-TPOT)
5. [ë°°ì¹˜ í¬ê¸°ì™€ ì²˜ë¦¬ëŸ‰ ê´€ê³„](#5.-ë°°ì¹˜-í¬ê¸°ì™€-ì²˜ë¦¬ëŸ‰)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### Arithmetic Intensity (ì—°ì‚° ê°•ë„)

í•˜ë“œì›¨ì–´ê°€ **ì—°ì‚° ë³‘ëª©(Compute-bound)**ì¸ì§€ **ë©”ëª¨ë¦¬ ë³‘ëª©(Memory-bound)**ì¸ì§€ íŒë‹¨í•˜ëŠ” í•µì‹¬ ì§€í‘œì…ë‹ˆë‹¤:

$$\text{AI} = \frac{\text{FLOPs (ì—°ì‚°ëŸ‰)}}{\text{Bytes Transferred (ë©”ëª¨ë¦¬ ì´ë™ëŸ‰)}}$$

- $\text{AI}$: Arithmetic Intensity (ë‹¨ìœ„: FLOPs/Byte)
- $\text{FLOPs}$: ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚° íšŸìˆ˜
- $\text{Bytes}$: HBM â†” ì—°ì‚° ìœ ë‹› ê°„ ë°ì´í„° ì´ë™ëŸ‰

### Roofline ëª¨ë¸

GPUì˜ ì´ë¡ ì  ìµœëŒ€ ì„±ëŠ¥ì„ ë‘ ê°€ì§€ í•œê³„ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤:

$$\text{Performance} = \min\left(\text{Peak FLOPs},\; \text{AI} \times \text{Memory Bandwidth}\right)$$

ê²½ê³„ì (Ridge Point):

$$\text{AI}_{ridge} = \frac{\text{Peak FLOPs (FLOPS)}}{\text{Memory Bandwidth (Bytes/s)}}$$

- $\text{AI} < \text{AI}_{ridge}$: **Memory-bound** (ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì´ ë³‘ëª©)
- $\text{AI} > \text{AI}_{ridge}$: **Compute-bound** (ì—°ì‚° ëŠ¥ë ¥ì´ ë³‘ëª©)

### Prefill vs Decodeì˜ ì—°ì‚° ê°•ë„

| ë‹¨ê³„ | ì—°ì‚° | AI | ë³‘ëª© |
|------|------|-----|------|
| Prefill | $QK^T$ í–‰ë ¬ê³± ($S \times S$) | $\text{AI}_{prefill} \approx \frac{2 \cdot S \cdot d}{2d + 2S} \approx S$ | **Compute-bound** |
| Decode | ë²¡í„°-í–‰ë ¬ ê³± ($1 \times S$) | $\text{AI}_{decode} \approx \frac{2d}{2d + 2S} \approx 1$ | **Memory-bound** |

- $S$: ì‹œí€€ìŠ¤ ê¸¸ì´
- $d$: íˆë“  ì°¨ì›

### TTFTì™€ TPOT

$$\text{TTFT} = \frac{2 \cdot P \cdot S_{input}}{\text{GPU FLOPs}} \quad \text{(Prefill ì‹œê°„, Compute-bound)}$$

$$\text{TPOT} = \frac{2P \cdot \text{bytes\_per\_param}}{\text{Memory Bandwidth}} \quad \text{(Decode í•œ í† í°, Memory-bound)}$$

- $P$: ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜
- $S_{input}$: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
- $\text{bytes\_per\_param}$: íŒŒë¼ë¯¸í„°ë‹¹ ë°”ì´íŠ¸ (FP16=2, INT8=1)

**ìš”ì•½ í‘œ:**

| ì§€í‘œ | ìˆ˜ì‹ | ì˜ë¯¸ |
|------|------|------|
| Arithmetic Intensity | $\text{FLOPs} / \text{Bytes}$ | ì—°ì‚° ëŒ€ë¹„ ë©”ëª¨ë¦¬ ë¹„ìœ¨ |
| Ridge Point | $\text{Peak FLOPS} / \text{BW}$ | Computeâ†”Memory ê²½ê³„ |
| TTFT | $2PS_{in} / \text{FLOPS}$ | ì²« í† í° ìƒì„± ì‹œê°„ |
| TPOT | $2P \cdot b / \text{BW}$ | í† í° ë‹¹ ìƒì„± ì‹œê°„ |"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì¶”ë¡  ë³‘ëª© ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ Prefillê³¼ Decodeê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: LLMì´ ë‹µë³€í•˜ëŠ” ê³¼ì •ì„ **ì‹œí—˜**ì— ë¹„ìœ í•´ ë´…ì‹œë‹¤!

**Prefill(ë¬¸ì œ ì½ê¸°)**: ì‹œí—˜ ë¬¸ì œë¥¼ ì­‰ ì½ëŠ” ë‹¨ê³„ì˜ˆìš”. ë¬¸ì œê°€ ê¸¸ë©´ ì½ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ì§€ë§Œ, 
ëˆˆ(=GPU)ì€ ê³„ì† ë°”ì˜ê²Œ ì½ê³  ìˆì–´ìš”. â†’ **ê³„ì‚°ì´ ë°”ìœ(Compute-bound)** ìƒíƒœ!

**Decode(ë‹µ ì“°ê¸°)**: í•œ ê¸€ìì”© ë‹µì„ ì“°ëŠ” ë‹¨ê³„ì˜ˆìš”. ë¨¸ë¦¿ì†(=GPU)ì€ ë¹ ë¥¸ë° 
ì†(=ë©”ëª¨ë¦¬)ì´ ëŠë ¤ì„œ í•œ ê¸€ìì”©ë§Œ ì“¸ ìˆ˜ ìˆì–´ìš”. â†’ **ë©”ëª¨ë¦¬ê°€ ë°”ìœ(Memory-bound)** ìƒíƒœ!

#### ğŸ”ï¸ Rooflineì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìˆ˜ë„ê¼­ì§€ì™€ ë¬¼í†µì„ ìƒê°í•´ ë³´ì„¸ìš”!

- **ìˆ˜ë„ê¼­ì§€** = ë©”ëª¨ë¦¬ ëŒ€ì—­í­ (ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì†ë„)
- **ë¬¼í†µ** = GPU ì—°ì‚° ìœ ë‹› (ê³„ì‚°í•˜ëŠ” ì†ë„)
- ë¬¼í†µì´ ì•„ë¬´ë¦¬ ì»¤ë„, ìˆ˜ë„ê¼­ì§€ê°€ ì¢ìœ¼ë©´ ë¬¼(=ë°ì´í„°)ì´ ì²œì²œíˆ ì°¨ìš” â†’ Memory-bound
- ìˆ˜ë„ê¼­ì§€ê°€ ë„“ì–´ë„, ë¬¼í†µì´ ì‘ìœ¼ë©´ ë„˜ì³ìš” â†’ Compute-bound

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Arithmetic Intensity ê³„ì‚°

í–‰ë ¬ê³± $C = AB$ì—ì„œ $A \in \mathbb{R}^{M \times K}$, $B \in \mathbb{R}^{K \times N}$ì¼ ë•Œ:
- FLOPs = $2MKN$
- Bytes = $2(MK + KN + MN)$ (FP16 ê¸°ì¤€)

$M=1, K=4096, N=4096$ (Decode ë‹¨ê³„)ì¼ ë•Œ AIë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{FLOPs} = 2 \times 1 \times 4096 \times 4096 = 33,554,432$$

$$\text{Bytes} = 2(1 \times 4096 + 4096 \times 4096 + 1 \times 4096) = 2(4096 + 16,777,216 + 4096) = 33,570,816$$

$$\text{AI} = \frac{33,554,432}{33,570,816} \approx 1.0 \;\text{FLOPs/Byte}$$

â†’ AI â‰ˆ 1ë¡œ **Memory-bound**! Decode ë‹¨ê³„ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì½ëŠ” ë¹„ìš©ì´ ì—°ì‚°ì„ ì••ë„í•©ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: TTFT ì˜ˆì¸¡

Llama 3 8B ($P = 8 \times 10^9$), ì…ë ¥ 512 í† í°, A100 GPU (312 TFLOPS FP16) ì¼ ë•Œ TTFTëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{TTFT} = \frac{2 \times 8 \times 10^9 \times 512}{312 \times 10^{12}} = \frac{8.192 \times 10^{12}}{312 \times 10^{12}} \approx 26.3\text{ ms}$$

â†’ ì•½ 26msë¡œ, ì…ë ¥ ê¸¸ì´ì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€í•©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 2. Prefill vs Decode ë‹¨ê³„ ë¶„ì„ <a name='2.-Prefill-vs-Decode'></a>

Llama 3 8Bì˜ ì‹¤ì œ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë‹¨ê³„ì˜ FLOPs, ë©”ëª¨ë¦¬ ì´ë™ëŸ‰, Arithmetic Intensityë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

| íŒŒë¼ë¯¸í„° | ê°’ |
|---------|-----|
| ë ˆì´ì–´ ìˆ˜ ($L$) | 32 |
| íˆë“  ì°¨ì› ($d_{model}$) | 4096 |
| Q í—¤ë“œ ìˆ˜ ($n_q$) | 32 |
| KV í—¤ë“œ ìˆ˜ ($n_{kv}$) | 8 |
| í—¤ë“œ ì°¨ì› ($d_{head}$) | 128 |
| FFN ì°¨ì› ($d_{ff}$) | 14336 |
| ì´ íŒŒë¼ë¯¸í„° | ~8B |"""))

# â”€â”€ Cell 7: Prefill vs Decode FLOPs/memory analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Prefill vs Decode FLOPs/ë©”ëª¨ë¦¬ ë¶„ì„ (Llama 3 8B) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Llama 3 8B íŒŒë¼ë¯¸í„°
L = 32          # ë ˆì´ì–´ ìˆ˜
d_model = 4096  # íˆë“  ì°¨ì›
n_q = 32        # Q í—¤ë“œ ìˆ˜
n_kv = 8        # KV í—¤ë“œ ìˆ˜
d_head = 128    # í—¤ë“œ ì°¨ì›
d_ff = 14336    # FFN ì°¨ì›
P = 8e9         # ì´ íŒŒë¼ë¯¸í„°

seq_lengths = [128, 256, 512, 1024, 2048, 4096]
batch_size = 1

print(f"{'':=<70}")
print(f"  Llama 3 8B: Prefill vs Decode ì—°ì‚° ë¶„ì„")
print(f"{'':=<70}")
print(f"{'Seq Len':>8} | {'Prefill FLOPs':>15} | {'Decode FLOPs':>15} | {'AI(Prefill)':>12} | {'AI(Decode)':>11}")
print(f"{'-'*70}")

prefill_ais = []
decode_ais = []

for S in seq_lengths:
    # Prefill: ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬ (í–‰ë ¬-í–‰ë ¬ ê³±)
    # Attention: Q*K^T + Attn*V â†’ 2 * n_q * S * S * d_head per layer
    attn_flops_prefill = L * 2 * n_q * S * S * d_head * 2
    # Linear projections: QKV + Output â†’ ê° d_model * d_model * S * 2
    qkv_flops = L * 2 * S * d_model * (d_model + 2 * n_kv * d_head) * 2
    # FFN: gate + up + down (SwiGLU)
    ffn_flops = L * 2 * S * d_model * d_ff * 3 * 2
    prefill_flops = attn_flops_prefill + qkv_flops + ffn_flops

    # ë©”ëª¨ë¦¬: ëª¨ë“  ê°€ì¤‘ì¹˜ ì½ê¸° + í™œì„±í™”
    weight_bytes = 2 * P * 2  # FP16, ì½ê¸° 1íšŒ
    activation_bytes = 2 * L * S * d_model * 2
    prefill_bytes = weight_bytes + activation_bytes

    # Decode: í† í° 1ê°œì”© (ë²¡í„°-í–‰ë ¬ ê³±)
    attn_flops_decode = L * 2 * n_q * 1 * S * d_head * 2
    qkv_flops_d = L * 2 * 1 * d_model * (d_model + 2 * n_kv * d_head) * 2
    ffn_flops_d = L * 2 * 1 * d_model * d_ff * 3 * 2
    decode_flops = attn_flops_decode + qkv_flops_d + ffn_flops_d

    # Decode ë©”ëª¨ë¦¬: ê°€ì¤‘ì¹˜ ì „ì²´ + KV cache ì½ê¸°
    kv_cache_bytes = 2 * L * n_kv * d_head * S * 2 * 2  # K,V * layers * FP16
    decode_bytes = weight_bytes + kv_cache_bytes

    ai_prefill = prefill_flops / prefill_bytes
    ai_decode = decode_flops / decode_bytes
    prefill_ais.append(ai_prefill)
    decode_ais.append(ai_decode)

    print(f"{S:>8} | {prefill_flops:>15.2e} | {decode_flops:>15.2e} | {ai_prefill:>12.1f} | {ai_decode:>11.1f}")

print(f"\\nê²°ë¡ :")
print(f"  Prefill AI: {min(prefill_ais):.0f} ~ {max(prefill_ais):.0f} (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë¹„ë¡€ ì¦ê°€)")
print(f"  Decode AI:  {min(decode_ais):.1f} ~ {max(decode_ais):.1f} (í•­ìƒ ë‚®ìŒ â†’ Memory-bound)")"""))

# â”€â”€ Cell 8: Section 3 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 3. Roofline ì‹œê°í™” <a name='3.-Roofline-ì‹œê°í™”'></a>

A100 GPU ìŠ¤í™ì„ ê¸°ì¤€ìœ¼ë¡œ Roofline ëª¨ë¸ì„ ê·¸ë¦¬ê³ , Prefill/Decode ì—°ì‚° ì§€ì ì„ í‘œì‹œí•©ë‹ˆë‹¤.

| A100 ìŠ¤í™ | ê°’ |
|-----------|-----|
| FP16 Peak | 312 TFLOPS |
| HBM ëŒ€ì—­í­ | 2.0 TB/s |
| Ridge Point | 156 FLOPs/Byte |"""))

# â”€â”€ Cell 9: Roofline visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Roofline ëª¨ë¸ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
peak_flops = 312e12   # A100 FP16: 312 TFLOPS
mem_bw = 2.0e12       # A100 HBM: 2.0 TB/s
ridge_point = peak_flops / mem_bw  # 156 FLOPs/Byte

ai_range = np.logspace(-1, 4, 500)
roofline = np.minimum(peak_flops, ai_range * mem_bw)

fig, ax = plt.subplots(1, 1, figsize=(10, 6))

ax.loglog(ai_range, roofline / 1e12, 'b-', lw=3, label='Roofline (A100)')
ax.axvline(x=ridge_point, color='gray', ls='--', lw=1.5, alpha=0.7, label=f'Ridge Point = {ridge_point:.0f}')

seq_labels = [128, 512, 2048, 4096]
colors_p = ['#2196F3', '#1976D2', '#0D47A1', '#0A3069']
colors_d = ['#FF9800', '#F57C00', '#E65100', '#BF360C']

for i, S in enumerate(seq_labels):
    idx = seq_lengths.index(S)
    ai_p = prefill_ais[idx]
    perf_p = min(peak_flops, ai_p * mem_bw)
    ax.plot(ai_p, perf_p / 1e12, 'o', ms=12, color=colors_p[i],
            label=f'Prefill S={S}', zorder=5)

    ai_d = decode_ais[idx]
    perf_d = min(peak_flops, ai_d * mem_bw)
    ax.plot(ai_d, perf_d / 1e12, 's', ms=10, color=colors_d[i],
            label=f'Decode S={S}', zorder=5)

ax.fill_between([0.1, ridge_point], [0.0001, 0.0001], [1000, 1000],
                alpha=0.05, color='orange')
ax.fill_between([ridge_point, 10000], [0.0001, 0.0001], [1000, 1000],
                alpha=0.05, color='blue')
ax.text(2, 200, 'Memory\\nBound', fontsize=14, color='orange', fontweight='bold', alpha=0.6)
ax.text(1500, 200, 'Compute\\nBound', fontsize=14, color='blue', fontweight='bold', alpha=0.6)

ax.set_xlabel('Arithmetic Intensity (FLOPs/Byte)', fontsize=11)
ax.set_ylabel('Performance (TFLOPS)', fontsize=11)
ax.set_title('Roofline Model: LLM Prefill vs Decode (A100 GPU)', fontweight='bold')
ax.legend(fontsize=8, loc='lower right', ncol=2)
ax.grid(True, alpha=0.3, which='both')
ax.set_xlim(0.1, 10000)
ax.set_ylim(0.1, 500)

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/roofline_prefill_decode.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/roofline_prefill_decode.png")
print(f"\\nRidge Point: {ridge_point:.0f} FLOPs/Byte")
print(f"Prefill â†’ Compute-bound ì˜ì—­ (AI >> Ridge)")
print(f"Decode  â†’ Memory-bound ì˜ì—­ (AI << Ridge)")"""))

# â”€â”€ Cell 10: Section 4 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. TTFT / TPOT ì¸¡ì • ì‹œë®¬ë ˆì´ì…˜ <a name='4.-TTFT-/-TPOT'></a>

ì‹¤ì œ GPU ì—†ì´ë„ **ì´ë¡ ì  TTFTì™€ TPOTë¥¼ ê³„ì‚°**í•˜ì—¬ ì¶”ë¡  ì‹œê°„ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

$$\text{TTFT} \approx \frac{2 \cdot P \cdot S_{input}}{\text{GPU FLOPS}}, \quad \text{TPOT} \approx \frac{2P \cdot \text{bytes\_per\_param}}{\text{Memory BW}}$$"""))

# â”€â”€ Cell 11: TTFT/TPOT measurement simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ TTFT / TPOT ì´ë¡  ê³„ì‚° ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPU ìŠ¤í™
gpus = {
    'A100 (FP16)': {'flops': 312e12, 'bw': 2.0e12},
    'H100 (FP16)': {'flops': 989e12, 'bw': 3.35e12},
    'H200 (FP16)': {'flops': 989e12, 'bw': 4.8e12},
}

P = 8e9
bytes_per_param = 2  # FP16
input_lengths = [128, 256, 512, 1024, 2048]
output_length = 128

print(f"{'':=<80}")
print(f"  Llama 3 8B ì¶”ë¡  ì‹œê°„ ì˜ˆì¸¡ (TTFT + TPOT)")
print(f"{'':=<80}")

for gpu_name, specs in gpus.items():
    flops = specs['flops']
    bw = specs['bw']

    tpot = (2 * P * bytes_per_param) / bw * 1000  # ms

    print(f"\\n  GPU: {gpu_name}")
    print(f"  {'Input Len':>10} | {'TTFT (ms)':>10} | {'TPOT (ms)':>10} | {'Total 128tok (ms)':>18} | {'tok/s':>8}")
    print(f"  {'-'*65}")

    for S_in in input_lengths:
        ttft = (2 * P * S_in) / flops * 1000  # ms
        total = ttft + tpot * output_length
        tps = output_length / (total / 1000)

        print(f"  {S_in:>10} | {ttft:>10.1f} | {tpot:>10.2f} | {total:>18.1f} | {tps:>8.1f}")

print(f"\\ní•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
print(f"  1. TTFTëŠ” ì…ë ¥ ê¸¸ì´ì— ë¹„ë¡€ (Compute-bound)")
print(f"  2. TPOTëŠ” ì…ë ¥ ê¸¸ì´ì™€ ë¬´ê´€ (Memory-bound, ê°€ì¤‘ì¹˜ ì½ê¸° ì‹œê°„)")
print(f"  3. H200ì˜ ë†’ì€ ëŒ€ì—­í­(4.8TB/s)ì´ TPOTë¥¼ í¬ê²Œ ê°œì„ ")"""))

# â”€â”€ Cell 12: Section 5 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 5. ë°°ì¹˜ í¬ê¸°ì™€ ì²˜ë¦¬ëŸ‰ ê´€ê³„ <a name='5.-ë°°ì¹˜-í¬ê¸°ì™€-ì²˜ë¦¬ëŸ‰'></a>

ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´ **ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì½ê¸°**ë¡œ ì—¬ëŸ¬ ìš”ì²­ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì²˜ë¦¬ëŸ‰ì´ í–¥ìƒë©ë‹ˆë‹¤.
ê·¸ëŸ¬ë‚˜ KV Cache ë©”ëª¨ë¦¬ê°€ ë°°ì¹˜ì— ë¹„ë¡€í•˜ì—¬ ì¦ê°€í•˜ë¯€ë¡œ GPU ë©”ëª¨ë¦¬ê°€ í•œê³„ì ì´ ë©ë‹ˆë‹¤."""))

# â”€â”€ Cell 13: Batch size effect on throughput â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë°°ì¹˜ í¬ê¸° vs ì²˜ë¦¬ëŸ‰/ì§€ì—° ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# A100 80GB ê¸°ì¤€
gpu_mem = 80e9   # 80 GB
peak_flops = 312e12
mem_bw = 2.0e12
P = 8e9

L, n_kv, d_head = 32, 8, 128
S_max = 2048
bytes_per_param = 2

model_mem = P * bytes_per_param
print(f"ëª¨ë¸ ê°€ì¤‘ì¹˜ ë©”ëª¨ë¦¬: {model_mem / 1e9:.1f} GB")

batch_sizes = list(range(1, 65))
throughputs = []
latencies = []
kv_mems = []

for B in batch_sizes:
    kv_cache_per_token = 2 * L * n_kv * d_head * bytes_per_param
    kv_cache = B * S_max * kv_cache_per_token
    total_mem = model_mem + kv_cache

    if total_mem > gpu_mem:
        throughputs.append(None)
        latencies.append(None)
        kv_mems.append(kv_cache / 1e9)
        continue

    kv_mems.append(kv_cache / 1e9)

    # Decode: ê°€ì¤‘ì¹˜ 1íšŒ ì½ê¸° + KV cache ì½ê¸°ë¡œ Bê°œ í† í° ë™ì‹œ ìƒì„±
    weight_read_time = (P * bytes_per_param) / mem_bw
    kv_read_time = (kv_cache) / mem_bw
    compute_time = (2 * P * B) / peak_flops

    step_time = max(weight_read_time + kv_read_time, compute_time)
    latency = step_time * 1000  # ms per step
    throughput = B / step_time  # tokens/s

    throughputs.append(throughput)
    latencies.append(latency)

max_batch = max(i+1 for i, t in enumerate(throughputs) if t is not None)
print(f"ìµœëŒ€ ë°°ì¹˜ í¬ê¸° (A100 80GB, S={S_max}): {max_batch}")
print(f"\\n{'Batch':>6} | {'Throughput':>12} | {'Latency':>10} | {'KV Cache':>10} | {'Total Mem':>10}")
print(f"{'-'*58}")

for i, B in enumerate(batch_sizes):
    if B in [1, 2, 4, 8, 16, 32, max_batch]:
        if throughputs[i] is not None:
            total = model_mem / 1e9 + kv_mems[i]
            print(f"{B:>6} | {throughputs[i]:>10.0f} t/s | {latencies[i]:>8.2f} ms | {kv_mems[i]:>8.1f} GB | {total:>8.1f} GB")

print(f"\\nê²°ë¡ :")
print(f"  ë°°ì¹˜ ì¦ê°€ â†’ ì²˜ë¦¬ëŸ‰ ì¦ê°€ (ê°€ì¤‘ì¹˜ ì½ê¸°ë¥¼ ê³µìœ )")
print(f"  ë°°ì¹˜ ì¦ê°€ â†’ KV Cache ë©”ëª¨ë¦¬ ì¦ê°€ â†’ GPU ë©”ëª¨ë¦¬ í•œê³„")"""))

# â”€â”€ Cell 14: Batch throughput visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë°°ì¹˜ í¬ê¸° vs ì²˜ë¦¬ëŸ‰ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
valid_b = [b for b, t in zip(batch_sizes, throughputs) if t is not None]
valid_t = [t for t in throughputs if t is not None]
valid_l = [l for l in latencies if l is not None]
valid_kv = [kv_mems[i] for i, t in enumerate(throughputs) if t is not None]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

ax1 = axes[0]
ax1.plot(valid_b, valid_t, 'b-o', lw=2, ms=4, label='Throughput')
ax1.set_xlabel('Batch Size', fontsize=11)
ax1.set_ylabel('Throughput (tokens/s)', fontsize=11)
ax1.set_title('Batch Size vs Throughput', fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=9)

ax2 = axes[1]
ax2.plot(valid_b, valid_l, 'r-s', lw=2, ms=4, label='Latency per step')
ax2.set_xlabel('Batch Size', fontsize=11)
ax2.set_ylabel('Latency (ms)', fontsize=11)
ax2.set_title('Batch Size vs Latency', fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=9)

ax3 = axes[2]
total_mems = [model_mem / 1e9 + kv for kv in valid_kv]
ax3.bar(valid_b, [model_mem / 1e9] * len(valid_b), color='steelblue', label='Model Weights')
ax3.bar(valid_b, valid_kv, bottom=[model_mem / 1e9] * len(valid_b), color='coral', label='KV Cache')
ax3.axhline(y=80, color='red', ls='--', lw=2, label='A100 80GB Limit')
ax3.set_xlabel('Batch Size', fontsize=11)
ax3.set_ylabel('Memory (GB)', fontsize=11)
ax3.set_title('Memory Breakdown', fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.legend(fontsize=9)

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/batch_throughput_analysis.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/batch_throughput_analysis.png")
print(f"\\nìµœëŒ€ ì²˜ë¦¬ëŸ‰: {max(valid_t):.0f} tokens/s (Batch={valid_b[valid_t.index(max(valid_t))]})") """))

# â”€â”€ Cell 15: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Arithmetic Intensity | FLOPs / Bytes â€” ì—°ì‚° vs ë©”ëª¨ë¦¬ ë³‘ëª© íŒë³„ | â­â­â­ |
| Prefill (Compute-bound) | ì „ì²´ ì‹œí€€ìŠ¤ í–‰ë ¬ê³±, AI âˆ S | â­â­â­ |
| Decode (Memory-bound) | í•œ í† í°ì”© ë²¡í„°-í–‰ë ¬ê³±, AI â‰ˆ 1 | â­â­â­ |
| Roofline Model | Peak FLOPSì™€ Bandwidthë¡œ ì„±ëŠ¥ ìƒí•œ ëª¨ë¸ë§ | â­â­â­ |
| TTFT | ì²« í† í° ì‹œê°„, ì…ë ¥ ê¸¸ì´ì— ë¹„ë¡€ | â­â­ |
| TPOT | í† í° ë‹¹ ì‹œê°„, ë©”ëª¨ë¦¬ ëŒ€ì—­í­ì— ë°˜ë¹„ë¡€ | â­â­ |
| Batch Size íš¨ê³¼ | ì²˜ë¦¬ëŸ‰â†‘, KV Cache ë©”ëª¨ë¦¬â†‘ | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\text{AI} = \frac{\text{FLOPs}}{\text{Bytes Transferred}}, \quad \text{Performance} = \min(\text{Peak FLOPS}, \text{AI} \times \text{BW})$$

$$\text{TTFT} = \frac{2PS_{in}}{\text{GPU FLOPS}}, \quad \text{TPOT} = \frac{2P \cdot b}{\text{Memory BW}}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**02_flash_attention_deepdive.ipynb** â€” FlashAttentionì˜ IO ë³µì¡ë„ ìˆ˜ì‹, Tiling + Recomputation ì›ë¦¬, v1â†’v2â†’v3 ì„±ëŠ¥ ë°œì „ì‚¬ë¥¼ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter14_extreme_inference/01_inference_bottlenecks.ipynb')
