"""Generate chapter15_alignment_rlhf/02_actor_critic_and_ppo.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” Actor-Criticê³¼ PPO

## í•™ìŠµ ëª©í‘œ
- Advantage Functionì˜ ìˆ˜í•™ì  ì˜ë¯¸ì™€ ë¶„ì‚° ê°ì†Œ íš¨ê³¼ë¥¼ ì´í•´í•œë‹¤
- Actor-Critic êµ¬ì¡°ì—ì„œ Criticì´ Baseline ì—­í• ì„ í•˜ëŠ” ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- PPO-Clip ëª©ì í•¨ìˆ˜ì˜ 3êµ¬ê°„ ë™ì‘ì„ ìˆ˜ì‹ìœ¼ë¡œ ì™„ì „ ì „ê°œí•˜ê³  ì‹œê°í™”í•œë‹¤
- KL í˜ë„í‹° ë°©ì‹ê³¼ Clip ë°©ì‹ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•˜ê³  êµ¬í˜„í•œë‹¤
- A2Cì—ì„œ PPOë¡œì˜ ë°œì „ ê³¼ì •ê³¼ LLM ì •ë ¬ì—ì„œì˜ ì—­í• ì„ ì„¤ëª…í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Advantage Functionê³¼ PPO](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Advantage Function ê³„ì‚° ë°ëª¨](#2.-Advantage-Function-ê³„ì‚°)
3. [PPO-Clip ëª©ì í•¨ìˆ˜ 3êµ¬ê°„ ì‹œê°í™”](#3.-PPO-Clip-3êµ¬ê°„-ì‹œê°í™”)
4. [A2C vs PPO ë¹„êµ êµ¬í˜„](#4.-A2C-vs-PPO-ë¹„êµ)
5. [KL í˜ë„í‹° vs Clip ë¹„êµ](#5.-KL-í˜ë„í‹°-vs-Clip)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### Advantage Function

REINFORCEì˜ ë†’ì€ ë¶„ì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Baseline** $b(s)$ë¥¼ ë¹¼ì¤ë‹ˆë‹¤:

$$\nabla_\theta J(\theta) = \mathbb{E}\left[(G_t - b(s_t)) \nabla_\theta \log \pi_\theta(a_t|s_t)\right]$$

ìµœì ì˜ Baselineì€ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ $V^\pi(s)$ì´ë©°, ì´ë•Œ $(G_t - V^\pi(s_t))$ê°€ **Advantage**ì…ë‹ˆë‹¤:

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

- $A^\pi(s, a) > 0$: í•´ë‹¹ í–‰ë™ì´ **í‰ê· ë³´ë‹¤ ë‚˜ìŒ** â†’ í™•ë¥  ì¦ê°€
- $A^\pi(s, a) < 0$: í•´ë‹¹ í–‰ë™ì´ **í‰ê· ë³´ë‹¤ ëª»í•¨** â†’ í™•ë¥  ê°ì†Œ
- $A^\pi(s, a) = 0$: í‰ê·  ìˆ˜ì¤€

### GAE (Generalized Advantage Estimation)

TD ì˜¤ì°¨ë¥¼ ê°€ì¤‘ í•©ì‚°í•˜ì—¬ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì œì–´í•©ë‹ˆë‹¤:

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

$$\hat{A}_t^{GAE} = \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}$$

- $\lambda = 0$: 1-step TD (ë‚®ì€ ë¶„ì‚°, ë†’ì€ í¸í–¥)
- $\lambda = 1$: Monte Carlo (ë†’ì€ ë¶„ì‚°, ë‚®ì€ í¸í–¥)

### PPO-Clip ëª©ì í•¨ìˆ˜

í™•ë¥  ë¹„ìœ¨:

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

**PPO-Clip ëª©ì í•¨ìˆ˜:**

$$L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t,\; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

- $\epsilon$: í´ë¦¬í•‘ ë²”ìœ„ (ë³´í†µ 0.1 ~ 0.2)

**3êµ¬ê°„ ë¶„ì„:**

| êµ¬ê°„ | ì¡°ê±´ | $\hat{A}_t > 0$ (ì¢‹ì€ í–‰ë™) | $\hat{A}_t < 0$ (ë‚˜ìœ í–‰ë™) |
|------|------|-----|------|
| ì¢Œì¸¡ | $r_t < 1 - \epsilon$ | í´ë¦¬í•‘ë¨ (í™•ë¥  ê°ì†Œ ì œí•œ) | ì›ë˜ ê°’ ì‚¬ìš© |
| ì¤‘ì•™ | $1-\epsilon \leq r_t \leq 1+\epsilon$ | ì›ë˜ ê°’ ì‚¬ìš© | ì›ë˜ ê°’ ì‚¬ìš© |
| ìš°ì¸¡ | $r_t > 1 + \epsilon$ | ì›ë˜ ê°’ ì‚¬ìš© | í´ë¦¬í•‘ë¨ (í™•ë¥  ì¦ê°€ ì œí•œ) |

### KL í˜ë„í‹° ë°©ì‹ (PPO-Penalty)

$$L^{KL}(\theta) = \mathbb{E}\left[r_t(\theta)\hat{A}_t - \beta D_{KL}[\pi_{\theta_{old}} \| \pi_\theta]\right]$$

- $\beta$: KL í˜ë„í‹° ê³„ìˆ˜ (ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥)

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| Advantage | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ | í‰ê·  ëŒ€ë¹„ í–‰ë™ì˜ ìƒëŒ€ì  ê°€ì¹˜ |
| PPO-Clip | $\min(r_t \hat{A}_t, \text{clip}(r_t) \hat{A}_t)$ | ì •ì±… ì—…ë°ì´íŠ¸ ë²”ìœ„ ì œí•œ |
| í™•ë¥  ë¹„ìœ¨ | $r_t = \pi_\theta / \pi_{\theta_{old}}$ | ìƒˆ ì •ì±… / ì´ì „ ì •ì±… í™•ë¥  ë¹„ |
| GAE | $\hat{A}_t = \sum_l (\gamma\lambda)^l \delta_{t+l}$ | í¸í–¥-ë¶„ì‚° ê· í˜• Advantage |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ PPO ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ Advantageê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì ìˆ˜ë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!

- ë°˜ í‰ê· ì´ 80ì ($V(s)$)ì¸ë° ë‚´ê°€ 95ì ($Q(s,a)$)ì„ ë°›ìœ¼ë©´ â†’ **Advantage = +15** (ì˜í–ˆë‹¤!)
- ë°˜ í‰ê· ì´ 80ì ì¸ë° ë‚´ê°€ 60ì ì„ ë°›ìœ¼ë©´ â†’ **Advantage = -20** (ì¢€ ë” ë…¸ë ¥í•´ì•¼!)
- ì¦‰, AdvantageëŠ” "í‰ê·  ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ì˜í–ˆë‚˜?"ë¥¼ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”.

#### ğŸ”’ PPO-Clipì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìì „ê±° ë³´ì¡° ë°”í€´ë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!

PPO-Clipì€ AIê°€ í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³€í•˜ì§€ ì•Šë„ë¡ **ë³´ì¡° ë°”í€´**ë¥¼ ë‹¬ì•„ì£¼ëŠ” ê±°ì˜ˆìš”:
- ì •ì±…ì´ í¬ê²Œ ë°”ë€Œë©´($r_t$ê°€ 1ì—ì„œ ë©€ì–´ì§€ë©´) â†’ **"ì ê¹, ë„ˆë¬´ ê¸‰í•˜ê²Œ ë°”ê¾¸ì§€ ë§ˆ!"** ë¼ê³  ì œí•œ
- ì •ì±…ì´ ì¡°ê¸ˆë§Œ ë°”ë€Œë©´($r_t \\approx 1$) â†’ **"ì¢‹ì•„, ê·¸ ì •ë„ë©´ ê´œì°®ì•„!"** ë¼ê³  í—ˆìš©
- ì´ë ‡ê²Œ í•˜ë©´ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë¼ìš”!

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Advantage ë¶€í˜¸ í•´ì„

ì–´ë–¤ ìƒíƒœì—ì„œ $Q^\pi(s, \text{left}) = 8$, $Q^\pi(s, \text{right}) = 3$, $V^\pi(s) = 5$ì¼ ë•Œ:
- $A^\pi(s, \text{left})$ê³¼ $A^\pi(s, \text{right})$ë¥¼ ê°ê° êµ¬í•˜ì„¸ìš”.
- ê° í–‰ë™ì˜ í™•ë¥ ì€ ì–´ë–»ê²Œ ë³€í•´ì•¼ í•˜ë‚˜ìš”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$A^\pi(s, \text{left}) = Q^\pi(s, \text{left}) - V^\pi(s) = 8 - 5 = +3$$

$$A^\pi(s, \text{right}) = Q^\pi(s, \text{right}) - V^\pi(s) = 3 - 5 = -2$$

- left í–‰ë™: $A > 0$ â†’ í™•ë¥ ì„ **ì¦ê°€**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤ (í‰ê· ë³´ë‹¤ ì¢‹ì€ í–‰ë™)
- right í–‰ë™: $A < 0$ â†’ í™•ë¥ ì„ **ê°ì†Œ**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤ (í‰ê· ë³´ë‹¤ ë‚˜ìœ í–‰ë™)
</details>

#### ë¬¸ì œ 2: PPO-Clip ëª©ì í•¨ìˆ˜ ê°’ ê³„ì‚°

$r_t = 1.3$, $\hat{A}_t = 2.0$, $\epsilon = 0.2$ì¼ ë•Œ $L^{CLIP}$ì˜ ê°’ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{ì›ë˜}: r_t \hat{A}_t = 1.3 \times 2.0 = 2.6$$

$$\text{í´ë¦¬í•‘}: \text{clip}(1.3, 0.8, 1.2) \times 2.0 = 1.2 \times 2.0 = 2.4$$

$$L^{CLIP} = \min(2.6, 2.4) = 2.4$$

â†’ $r_t > 1+\epsilon$ì´ê³  $\hat{A}_t > 0$ì´ë¯€ë¡œ í´ë¦¬í•‘ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì¢‹ì€ í–‰ë™ì„ ë” í•˜ë ¤ëŠ” ê²ƒì€ ë§‰ì§€ ì•ŠìŒ). 
ì‹¤ì œë¡œ $\min$ì„ ì·¨í•˜ë©´ 2.4ê°€ ë˜ì–´, í™•ë¥  ì¦ê°€í­ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì œí•œë©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. Advantage Function ê³„ì‚° ë°ëª¨ <a name='2.-Advantage-Function-ê³„ì‚°'></a>

ê°„ë‹¨í•œ ì—í”¼ì†Œë“œë¥¼ ìƒì„±í•˜ê³  **TD ì”ì°¨, GAE Advantage**ë¥¼ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.

$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t), \quad \hat{A}_t^{GAE} = \sum_{l=0}^{T-t}(\gamma\lambda)^l \delta_{t+l}$$"""))

# â”€â”€ Cell 7: Advantage calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Advantage Function ê³„ì‚° ë°ëª¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨í•œ ì—í”¼ì†Œë“œ (5 timestep)
rewards = np.array([1.0, 0.5, 2.0, -1.0, 3.0])
values  = np.array([2.0, 1.5, 3.0, 0.5, 2.5])  # V(s_t) ì¶”ì •ê°’
gamma = 0.99
lam = 0.95
T = len(rewards)
next_value = 0.0  # ì—í”¼ì†Œë“œ ì¢…ë£Œ

print(f"ì—í”¼ì†Œë“œ ë°ì´í„°:")
print(f"{'t':>4} | {'r_t':>6} | {'V(s_t)':>8} | {'V(s_t+1)':>10}")
print(f"{'-'*38}")
for t in range(T):
    v_next = values[t+1] if t < T-1 else next_value
    print(f"{t:>4} | {rewards[t]:>6.1f} | {values[t]:>8.2f} | {v_next:>10.2f}")

# TD ì”ì°¨ ê³„ì‚°
deltas = np.zeros(T)
for t in range(T):
    v_next = values[t+1] if t < T-1 else next_value
    deltas[t] = rewards[t] + gamma * v_next - values[t]

print(f"\\nTD ì”ì°¨ Î´_t:")
for t in range(T):
    print(f"  Î´_{t} = r_{t} + Î³Â·V(s_{t+1}) - V(s_{t}) = "
          f"{rewards[t]:.1f} + {gamma}Ã—{values[t+1] if t<T-1 else next_value:.2f} - {values[t]:.2f} = {deltas[t]:+.4f}")

# GAE ê³„ì‚° (ì—­ìˆœ ëˆ„ì )
gae_advantages = np.zeros(T)
gae = 0
for t in reversed(range(T)):
    gae = deltas[t] + gamma * lam * gae
    gae_advantages[t] = gae

# Monte Carlo Advantage (ë¹„êµìš©)
mc_returns = np.zeros(T)
G = next_value
for t in reversed(range(T)):
    G = rewards[t] + gamma * G
    mc_returns[t] = G
mc_advantages = mc_returns - values

print(f"\\n{'t':>4} | {'Î´_t (TD)':>10} | {'GAE(Î»={lam})':>12} | {'MC Advantage':>14}")
print(f"{'-'*50}")
for t in range(T):
    print(f"{t:>4} | {deltas[t]:>+10.4f} | {gae_advantages[t]:>+12.4f} | {mc_advantages[t]:>+14.4f}")

print(f"\\në¶„ì‚° ë¹„êµ:")
print(f"  TD(Î»=0) ë¶„ì‚°:  {np.var(deltas):.4f}")
print(f"  GAE(Î»={lam}) ë¶„ì‚°: {np.var(gae_advantages):.4f}")
print(f"  MC(Î»=1) ë¶„ì‚°:  {np.var(mc_advantages):.4f}")"""))

# â”€â”€ Cell 8: Section 3 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. PPO-Clip ëª©ì í•¨ìˆ˜ 3êµ¬ê°„ ì‹œê°í™” <a name='3.-PPO-Clip-3êµ¬ê°„-ì‹œê°í™”'></a>

PPO-Clipì˜ í•µì‹¬ì€ í™•ë¥  ë¹„ìœ¨ $r_t(\theta)$ì— ë”°ë¼ ëª©ì í•¨ìˆ˜ê°€ **3êµ¬ê°„ìœ¼ë¡œ ë¶„ë¦¬**ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

$$L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t,\; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

- **êµ¬ê°„ 1** ($r_t < 1-\epsilon$): $\hat{A}_t > 0$ì¼ ë•Œ í´ë¦¬í•‘ â†’ ì¢‹ì€ í–‰ë™ì˜ í™•ë¥  **ê°ì†Œ ì œí•œ**
- **êµ¬ê°„ 2** ($1-\epsilon \leq r_t \leq 1+\epsilon$): í´ë¦¬í•‘ ì—†ìŒ â†’ ììœ ë¡œìš´ ì—…ë°ì´íŠ¸
- **êµ¬ê°„ 3** ($r_t > 1+\epsilon$): $\hat{A}_t < 0$ì¼ ë•Œ í´ë¦¬í•‘ â†’ ë‚˜ìœ í–‰ë™ì˜ í™•ë¥  **ì¦ê°€ ì œí•œ**"""))

# â”€â”€ Cell 9: PPO-Clip 3-region visualization (CRITICAL) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ PPO-Clip 3êµ¬ê°„ ì‹œê°í™” (í•µì‹¬!) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
epsilon = 0.2
r = np.linspace(0.3, 2.0, 500)

def ppo_clip_objective(r, A, eps):
    # ì›ë˜ ëª©ì 
    surr1 = r * A
    # í´ë¦¬í•‘ëœ ëª©ì 
    r_clipped = np.clip(r, 1 - eps, 1 + eps)
    surr2 = r_clipped * A
    return np.minimum(surr1, surr2)

fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# (1) A > 0 (ì¢‹ì€ í–‰ë™)
ax1 = axes[0]
A_pos = 1.0
surr1_pos = r * A_pos
r_clip = np.clip(r, 1 - epsilon, 1 + epsilon)
surr2_pos = r_clip * A_pos
L_pos = np.minimum(surr1_pos, surr2_pos)

ax1.plot(r, surr1_pos, 'b--', lw=1.5, alpha=0.5, label=r'$r_t \hat{A}_t$ (ì›ë˜)')
ax1.plot(r, surr2_pos, 'r--', lw=1.5, alpha=0.5, label=r'$\mathrm{clip}(r_t) \hat{A}_t$')
ax1.plot(r, L_pos, 'g-', lw=3, label=r'$L^{CLIP}$ (ìµœì¢…)')

# 3êµ¬ê°„ ë°°ê²½ í‘œì‹œ
ax1.axvspan(0.3, 1-epsilon, alpha=0.1, color='red', label=f'êµ¬ê°„1: r < {1-epsilon}')
ax1.axvspan(1-epsilon, 1+epsilon, alpha=0.1, color='green', label=f'êµ¬ê°„2: í´ë¦¬í•‘ ì—†ìŒ')
ax1.axvspan(1+epsilon, 2.0, alpha=0.1, color='blue', label=f'êµ¬ê°„3: r > {1+epsilon}')

ax1.axvline(x=1.0, color='gray', ls=':', lw=1)
ax1.axvline(x=1-epsilon, color='red', ls='--', lw=1.5, alpha=0.7)
ax1.axvline(x=1+epsilon, color='blue', ls='--', lw=1.5, alpha=0.7)
ax1.set_xlabel(r'$r_t(\theta)$', fontsize=11)
ax1.set_ylabel(r'Objective', fontsize=11)
ax1.set_title(r'$\hat{A}_t > 0$ (ì¢‹ì€ í–‰ë™)', fontweight='bold', fontsize=13)
ax1.legend(fontsize=8, loc='upper left')
ax1.grid(True, alpha=0.3)
ax1.set_xlim(0.3, 2.0)

# (2) A < 0 (ë‚˜ìœ í–‰ë™)
ax2 = axes[1]
A_neg = -1.0
surr1_neg = r * A_neg
surr2_neg = r_clip * A_neg
L_neg = np.minimum(surr1_neg, surr2_neg)

ax2.plot(r, surr1_neg, 'b--', lw=1.5, alpha=0.5, label=r'$r_t \hat{A}_t$ (ì›ë˜)')
ax2.plot(r, surr2_neg, 'r--', lw=1.5, alpha=0.5, label=r'$\mathrm{clip}(r_t) \hat{A}_t$')
ax2.plot(r, L_neg, 'g-', lw=3, label=r'$L^{CLIP}$ (ìµœì¢…)')

ax2.axvspan(0.3, 1-epsilon, alpha=0.1, color='red', label=f'êµ¬ê°„1: r < {1-epsilon}')
ax2.axvspan(1-epsilon, 1+epsilon, alpha=0.1, color='green', label=f'êµ¬ê°„2: í´ë¦¬í•‘ ì—†ìŒ')
ax2.axvspan(1+epsilon, 2.0, alpha=0.1, color='blue', label=f'êµ¬ê°„3: r > {1+epsilon}')

ax2.axvline(x=1.0, color='gray', ls=':', lw=1)
ax2.axvline(x=1-epsilon, color='red', ls='--', lw=1.5, alpha=0.7)
ax2.axvline(x=1+epsilon, color='blue', ls='--', lw=1.5, alpha=0.7)
ax2.set_xlabel(r'$r_t(\theta)$', fontsize=11)
ax2.set_ylabel(r'Objective', fontsize=11)
ax2.set_title(r'$\hat{A}_t < 0$ (ë‚˜ìœ í–‰ë™)', fontweight='bold', fontsize=13)
ax2.legend(fontsize=8, loc='lower left')
ax2.grid(True, alpha=0.3)
ax2.set_xlim(0.3, 2.0)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/ppo_clip_3regions.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/ppo_clip_3regions.png")

# ìˆ˜ì¹˜ ë¶„ì„
print(f"\\nPPO-Clip 3êµ¬ê°„ ìˆ˜ì¹˜ ë¶„ì„ (Îµ={epsilon}):")
print(f"{'':=<60}")
test_ratios = [0.5, 0.8, 1.0, 1.2, 1.5]
for rt in test_ratios:
    for A_val, A_label in [(1.0, "A>0"), (-1.0, "A<0")]:
        L_val = min(rt * A_val, np.clip(rt, 1-epsilon, 1+epsilon) * A_val)
        clipped = "âœ‚ï¸ í´ë¦¬í•‘" if abs(L_val - rt * A_val) > 1e-6 else "âœ… ì›ë˜ê°’"
        print(f"  r_t={rt:.1f}, {A_label}: L_clip={L_val:+.2f} ({clipped})")"""))

# â”€â”€ Cell 10: Section 4 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 4. A2C vs PPO ë¹„êµ êµ¬í˜„ <a name='4.-A2C-vs-PPO-ë¹„êµ'></a>

ê°„ë‹¨í•œ ì—°ì† Bandit í™˜ê²½ì—ì„œ **A2C(Advantage Actor-Critic)**ì™€ **PPO-Clip**ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.

| ì•Œê³ ë¦¬ì¦˜ | ëª©ì í•¨ìˆ˜ | íŠ¹ì§• |
|---------|---------|------|
| A2C | $\\hat{A}_t \\nabla_\\theta \\log \\pi_\\theta$ | ë‹¨ìˆœí•˜ì§€ë§Œ í° ì—…ë°ì´íŠ¸ ê°€ëŠ¥ |
| PPO-Clip | $\\min(r_t \\hat{A}_t, \\text{clip}(r_t) \\hat{A}_t)$ | ì—…ë°ì´íŠ¸ í¬ê¸° ì œí•œ |"""))

# â”€â”€ Cell 11: A2C vs PPO comparison code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ A2C vs PPO ë¹„êµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
n_actions = 5
true_values = np.array([0.3, -0.2, 1.8, 0.5, -0.5])
n_episodes = 300
epsilon = 0.2

def run_a2c(lr=0.15):
    logits = tf.Variable(tf.zeros(n_actions))
    value_est = tf.Variable(0.0)
    opt = tf.keras.optimizers.Adam(learning_rate=lr)
    rewards_hist = []

    for ep in range(n_episodes):
        with tf.GradientTape() as tape:
            probs = tf.nn.softmax(logits)
            action = tf.random.categorical(tf.math.log(probs[tf.newaxis, :]), 1)[0, 0]
            reward = true_values[action.numpy()] + np.random.randn() * 0.3

            # Advantage = r - V(s)
            advantage = reward - value_est
            # Actor loss
            log_prob = tf.math.log(probs[action] + 1e-8)
            actor_loss = -advantage * log_prob
            # Critic loss
            critic_loss = tf.square(advantage)
            loss = actor_loss + 0.5 * critic_loss

        grads = tape.gradient(loss, [logits, value_est])
        opt.apply_gradients(zip(grads, [logits, value_est]))
        rewards_hist.append(reward)

    return rewards_hist

def run_ppo(lr=0.15, eps=0.2, n_updates=3):
    logits = tf.Variable(tf.zeros(n_actions))
    value_est = tf.Variable(0.0)
    opt = tf.keras.optimizers.Adam(learning_rate=lr)
    rewards_hist = []

    for ep in range(n_episodes):
        # ì´ì „ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
        old_probs = tf.nn.softmax(logits).numpy().copy()
        action_idx = np.random.choice(n_actions, p=old_probs)
        reward = true_values[action_idx] + np.random.randn() * 0.3
        advantage = reward - value_est.numpy()

        # PPO: ì—¬ëŸ¬ ë²ˆ ì—…ë°ì´íŠ¸
        for _ in range(n_updates):
            with tf.GradientTape() as tape:
                probs = tf.nn.softmax(logits)
                # í™•ë¥  ë¹„ìœ¨
                ratio = probs[action_idx] / (old_probs[action_idx] + 1e-8)
                # PPO-Clip ëª©ì 
                surr1 = ratio * advantage
                surr2 = tf.clip_by_value(ratio, 1.0 - eps, 1.0 + eps) * advantage
                actor_loss = -tf.minimum(surr1, surr2)
                # Critic loss
                critic_loss = tf.square(reward - value_est)
                loss = actor_loss + 0.5 * critic_loss

            grads = tape.gradient(loss, [logits, value_est])
            opt.apply_gradients(zip(grads, [logits, value_est]))

        rewards_hist.append(reward)

    return rewards_hist

# ì—¬ëŸ¬ ë²ˆ ì‹¤í—˜í•˜ì—¬ í‰ê· 
n_runs = 5
a2c_all = []
ppo_all = []

for run in range(n_runs):
    np.random.seed(run * 100)
    tf.random.set_seed(run * 100)
    a2c_all.append(run_a2c())

    np.random.seed(run * 100)
    tf.random.set_seed(run * 100)
    ppo_all.append(run_ppo())

a2c_mean = np.mean(a2c_all, axis=0)
ppo_mean = np.mean(ppo_all, axis=0)
a2c_std = np.std(a2c_all, axis=0)
ppo_std = np.std(ppo_all, axis=0)

window = 20
a2c_smooth = np.convolve(a2c_mean, np.ones(window)/window, mode='valid')
ppo_smooth = np.convolve(ppo_mean, np.ones(window)/window, mode='valid')

print(f"A2C vs PPO ë¹„êµ ({n_runs}íšŒ í‰ê· )")
print(f"{'':=<50}")
print(f"  ìµœì  ë³´ìƒ: {true_values.max():.1f} (Arm {np.argmax(true_values)})")
print(f"  A2C ìµœì¢… í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(a2c_mean[-50:]):.3f}")
print(f"  PPO ìµœì¢… í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(ppo_mean[-50:]):.3f}")
print(f"  A2C ë³´ìƒ ë¶„ì‚° (ë§ˆì§€ë§‰ 50):     {np.mean(a2c_std[-50:]):.3f}")
print(f"  PPO ë³´ìƒ ë¶„ì‚° (ë§ˆì§€ë§‰ 50):     {np.mean(ppo_std[-50:]):.3f}")"""))

# â”€â”€ Cell 12: A2C vs PPO visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ A2C vs PPO í•™ìŠµ ê³¡ì„  ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) í•™ìŠµ ê³¡ì„  ë¹„êµ
ax1 = axes[0]
x_range = range(len(a2c_smooth))
ax1.plot(x_range, a2c_smooth, 'b-', lw=2, label='A2C', alpha=0.9)
ax1.plot(x_range, ppo_smooth, 'r-', lw=2, label='PPO-Clip', alpha=0.9)
ax1.axhline(y=true_values.max(), color='green', ls='--', lw=1.5,
            label=f'ìµœì  ë³´ìƒ ({true_values.max():.1f})')
ax1.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)
ax1.set_ylabel('í‰ê·  ë³´ìƒ (ì´ë™ í‰ê· )', fontsize=11)
ax1.set_title('A2C vs PPO í•™ìŠµ ê³¡ì„ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ë¶„ì‚° ë¹„êµ (ë¡¤ë§)
ax2 = axes[1]
a2c_rolling_var = np.array([np.var(a2c_mean[max(0,i-window):i+1])
                            for i in range(len(a2c_mean))])
ppo_rolling_var = np.array([np.var(ppo_mean[max(0,i-window):i+1])
                            for i in range(len(ppo_mean))])
ax2.plot(a2c_rolling_var, 'b-', lw=2, label='A2C ë¶„ì‚°', alpha=0.9)
ax2.plot(ppo_rolling_var, 'r-', lw=2, label='PPO ë¶„ì‚°', alpha=0.9)
ax2.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)
ax2.set_ylabel('ë³´ìƒ ë¶„ì‚° (ë¡¤ë§)', fontsize=11)
ax2.set_title('í•™ìŠµ ì•ˆì •ì„± ë¹„êµ', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/a2c_vs_ppo.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/a2c_vs_ppo.png")
print(f"PPOëŠ” A2C ëŒ€ë¹„ ë” ì•ˆì •ì ì´ê³  ì¼ê´€ëœ í•™ìŠµ ê³¡ì„ ì„ ë³´ì…ë‹ˆë‹¤.")"""))

# â”€â”€ Cell 13: Section 5 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. KL í˜ë„í‹° vs Clip ë¹„êµ <a name='5.-KL-í˜ë„í‹°-vs-Clip'></a>

PPOì—ëŠ” ë‘ ê°€ì§€ ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤:

**PPO-Clip** (ì‹¤ë¬´ì—ì„œ ì£¼ë¡œ ì‚¬ìš©):

$$L^{CLIP}(\theta) = \mathbb{E}\left[\min(r_t \hat{A}_t, \text{clip}(r_t, 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]$$

**PPO-Penalty** (KL í˜ë„í‹°):

$$L^{KL}(\theta) = \mathbb{E}\left[r_t \hat{A}_t\right] - \beta D_{KL}[\pi_{\theta_{old}} \| \pi_\theta]$$

| íŠ¹ì„± | PPO-Clip | PPO-Penalty |
|------|----------|-------------|
| í•˜ì´í¼íŒŒë¼ë¯¸í„° | $\epsilon$ (ê³ ì •) | $\beta$ (ì ì‘ ì¡°ì ˆ) |
| êµ¬í˜„ ë‚œì´ë„ | ì‰¬ì›€ | ì¤‘ê°„ |
| ì•ˆì •ì„± | ë†’ìŒ | $\beta$ ì¡°ì ˆì— ë¯¼ê° |
| ì‚¬ìš©ì²˜ | ëŒ€ë¶€ë¶„ì˜ RL | RLHF (InstructGPT) |"""))

# â”€â”€ Cell 14: KL penalty vs Clip comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ KL í˜ë„í‹° vs Clip ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
r = np.linspace(0.3, 2.0, 500)
epsilon = 0.2
A = 1.0

# PPO-Clip ëª©ì 
surr1 = r * A
r_clip = np.clip(r, 1 - epsilon, 1 + epsilon)
surr2 = r_clip * A
L_clip = np.minimum(surr1, surr2)

# PPO-Penalty: L = r*A - Î² * KL
# KL â‰ˆ (r - 1) - log(r) (ë‹¨ì¼ í–‰ë™ KL ê·¼ì‚¬)
kl_approx = (r - 1) - np.log(r + 1e-8)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# (1) PPO-Clip ëª©ì 
ax1 = axes[0]
ax1.plot(r, L_clip, 'g-', lw=3, label=r'$L^{CLIP}$')
ax1.plot(r, surr1, 'b--', lw=1.5, alpha=0.4, label=r'$r_t \hat{A}_t$')
ax1.axvline(x=1-epsilon, color='red', ls='--', lw=1, alpha=0.5)
ax1.axvline(x=1+epsilon, color='red', ls='--', lw=1, alpha=0.5)
ax1.axvline(x=1.0, color='gray', ls=':', lw=1)
ax1.set_xlabel(r'$r_t(\theta)$', fontsize=11)
ax1.set_ylabel('Objective', fontsize=11)
ax1.set_title('PPO-Clip', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) PPO-Penalty (ë‹¤ì–‘í•œ Î²)
ax2 = axes[1]
betas = [0.01, 0.05, 0.1, 0.3]
colors = ['#2196F3', '#FF9800', '#E91E63', '#9C27B0']
for beta, color in zip(betas, colors):
    L_kl = r * A - beta * kl_approx
    ax2.plot(r, L_kl, lw=2, color=color, label=f'Î²={beta}')
ax2.plot(r, surr1, 'k--', lw=1, alpha=0.3, label=r'$r_t \hat{A}_t$ (ì›ë˜)')
ax2.axvline(x=1.0, color='gray', ls=':', lw=1)
ax2.set_xlabel(r'$r_t(\theta)$', fontsize=11)
ax2.set_ylabel('Objective', fontsize=11)
ax2.set_title('PPO-Penalty (ë‹¤ì–‘í•œ Î²)', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

# (3) KL divergence ê·¼ì‚¬
ax3 = axes[2]
ax3.plot(r, kl_approx, 'r-', lw=2.5, label=r'$D_{KL} \approx (r-1) - \log r$')
ax3.fill_between(r, 0, kl_approx, alpha=0.1, color='red')
ax3.axvline(x=1.0, color='gray', ls=':', lw=1, label='r=1 (ì •ì±… ë™ì¼)')
ax3.set_xlabel(r'$r_t(\theta)$', fontsize=11)
ax3.set_ylabel(r'$D_{KL}$', fontsize=11)
ax3.set_title('KL Divergence ê·¼ì‚¬', fontweight='bold')
ax3.legend(fontsize=9)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/kl_penalty_vs_clip.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/kl_penalty_vs_clip.png")

# ì •ëŸ‰ì  ë¹„êµ
print(f"\\nPPO-Clip vs PPO-Penalty ë¹„êµ:")
print(f"{'':=<55}")
print(f"{'r_t':>6} | {'L_clip':>8} | {'L_penalty(Î²=0.1)':>18} | {'KL':>8}")
print(f"{'-'*45}")
for rt in [0.5, 0.8, 1.0, 1.2, 1.5, 1.8]:
    lc = min(rt * A, np.clip(rt, 1-epsilon, 1+epsilon) * A)
    kl = (rt - 1) - np.log(rt)
    lp = rt * A - 0.1 * kl
    print(f"{rt:>6.1f} | {lc:>+8.3f} | {lp:>+18.3f} | {kl:>8.3f}")"""))

# â”€â”€ Cell 15: PPO in LLM context discussion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ PPOì˜ LLM ì •ë ¬ ì ìš© ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RLHFì—ì„œ PPOê°€ ì‚¬ìš©ë˜ëŠ” ë§¥ë½ì„ ê°„ë‹¨íˆ ì‹œë®¬ë ˆì´ì…˜

vocab_size = 50
seq_len = 8
n_steps = 200
epsilon = 0.2
beta_kl = 0.05

# ê¸°ì¤€ ì •ì±… (uniformì— ê°€ê¹Œìš´ ì†Œí”„íŠ¸ë§¥ìŠ¤)
ref_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32) * 0.1)
ref_probs = tf.nn.softmax(ref_logits).numpy()

# í•™ìŠµ ì •ì±…
policy_logits = tf.Variable(tf.identity(ref_logits))
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

# ê°„ë‹¨í•œ ë³´ìƒ ëª¨ë¸: íŠ¹ì • í† í°(ID 10~15)ì„ ì„ í˜¸
preferred_tokens = set(range(10, 16))

kl_history = []
reward_history = []

for step in range(n_steps):
    old_probs = tf.nn.softmax(policy_logits).numpy().copy()

    # í† í° ìƒì„±
    action = np.random.choice(vocab_size, p=old_probs)

    # ë³´ìƒ ëª¨ë¸ ì ìˆ˜
    reward = 1.0 if action in preferred_tokens else -0.1

    with tf.GradientTape() as tape:
        probs = tf.nn.softmax(policy_logits)

        # PPO-Clip + KL í˜ë„í‹° (RLHF í‘œì¤€ ë°©ì‹)
        ratio = probs[action] / (old_probs[action] + 1e-8)
        advantage = reward - tf.reduce_sum(probs * tf.constant(
            [1.0 if i in preferred_tokens else -0.1 for i in range(vocab_size)]))

        surr1 = ratio * advantage
        surr2 = tf.clip_by_value(ratio, 1.0 - epsilon, 1.0 + epsilon) * advantage
        clip_loss = -tf.minimum(surr1, surr2)

        # KL í˜ë„í‹°
        kl = tf.reduce_sum(probs * tf.math.log(probs / (ref_probs + 1e-8) + 1e-8))
        total_loss = clip_loss + beta_kl * kl

    grads = tape.gradient(total_loss, [policy_logits])
    optimizer.apply_gradients(zip(grads, [policy_logits]))

    kl_history.append(kl.numpy())
    reward_history.append(reward)

final_probs = tf.nn.softmax(policy_logits).numpy()
preferred_mass = sum(final_probs[i] for i in preferred_tokens)
print(f"RLHF ìŠ¤íƒ€ì¼ PPO ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
print(f"{'':=<50}")
print(f"  ì„ í˜¸ í† í°(ID 10~15) ì´ í™•ë¥ :")
print(f"    ì´ˆê¸°: {sum(ref_probs[i] for i in preferred_tokens):.4f}")
print(f"    ìµœì¢…: {preferred_mass:.4f}")
print(f"  ìµœì¢… KL divergence: {kl_history[-1]:.4f}")
print(f"  í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(reward_history[-50:]):.3f}")
print(f"\\n  â†’ KL í˜ë„í‹° ë•ë¶„ì— ê¸°ì¤€ ì •ì±…ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ì„œ")
print(f"    ì„ í˜¸ í† í°ì˜ í™•ë¥ ì´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.")"""))

# â”€â”€ Cell 16: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Advantage Function | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ â€” í‰ê·  ëŒ€ë¹„ í–‰ë™ ê°€ì¹˜ | â­â­â­ |
| GAE | $\hat{A}_t = \sum_l (\gamma\lambda)^l \delta_{t+l}$ â€” í¸í–¥/ë¶„ì‚° ì¡°ì ˆ | â­â­ |
| PPO-Clip | $\min(r_t\hat{A}_t, \text{clip}(r_t)\hat{A}_t)$ â€” 3êµ¬ê°„ ì œí•œ | â­â­â­ |
| PPO-Penalty | $r_t\hat{A}_t - \beta D_{KL}$ â€” KL ê¸°ë°˜ ì œí•œ | â­â­ |
| í™•ë¥  ë¹„ìœ¨ $r_t$ | $\pi_\theta / \pi_{\theta_{old}}$ â€” ì •ì±… ë³€í™” ì¸¡ì • | â­â­â­ |
| Actor-Critic | Actor(ì •ì±…) + Critic(ê°€ì¹˜) ë™ì‹œ í•™ìŠµ | â­â­ |
| Trust Region | ì •ì±… ì—…ë°ì´íŠ¸ ë²”ìœ„ë¥¼ ì‹ ë¢° ì˜ì—­ ë‚´ë¡œ ì œí•œ | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s)$$

$$L^{CLIP}(\theta) = \mathbb{E}\left[\min\left(r_t(\theta)\hat{A}_t,\; \text{clip}(r_t, 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$$

$$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**03_rlhf_pipeline_overview.ipynb** â€” InstructGPTì˜ SFTâ†’Reward Modelâ†’PPO 3ë‹¨ê³„ RLHF íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³ , Bradley-Terry ì„ í˜¸ ëª¨ë¸ì˜ ìˆ˜ì‹ì„ ë„ì¶œí•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter15_alignment_rlhf/02_actor_critic_and_ppo.ipynb')
