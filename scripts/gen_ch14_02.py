"""Generate chapter14_extreme_inference/02_flash_attention_deepdive.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 14: ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” â€” FlashAttention ì‹¬ì¸µ ë¶„ì„

## í•™ìŠµ ëª©í‘œ
- í‘œì¤€ Attentionì˜ **IO ë³µì¡ë„**ë¥¼ ë¶„ì„í•˜ê³  HBM ì ‘ê·¼ì´ ë³‘ëª©ì„ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•œë‹¤
- FlashAttentionì˜ **Tiling + Online Softmax + Recomputation** ì „ëµì˜ ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤
- SRAM í¬ê¸° $M$ì— ë”°ë¥¸ **IO ë³µì¡ë„ $O(N^2d^2/M)$**ì„ ìœ ë„í•œë‹¤
- FlashAttention v1 â†’ v2 â†’ v3ì˜ **ì„±ëŠ¥ ê°œì„  í¬ì¸íŠ¸**ë¥¼ ë¹„êµ ë¶„ì„í•œë‹¤
- ë¸”ë¡ ë‹¨ìœ„ Attention ê³„ì‚°ì„ **TensorFlowë¡œ ì§ì ‘ ì‹œë®¬ë ˆì´ì…˜**í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Attention IO ë³µì¡ë„](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [í‘œì¤€ Attentionì˜ ë©”ëª¨ë¦¬ ë³‘ëª©](#2.-í‘œì¤€-Attention)
3. [FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜](#3.-Tiling-ì‹œë®¬ë ˆì´ì…˜)
4. [IO ë³µì¡ë„ ë¹„êµ ì‹œê°í™”](#4.-IO-ë³µì¡ë„-ì‹œê°í™”)
5. [v1 â†’ v2 â†’ v3 ë°œì „ì‚¬](#5.-FlashAttention-ë°œì „ì‚¬)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### í‘œì¤€ Attentionì˜ IO ë³µì¡ë„

Attention ì—°ì‚°: $\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V$

ì—¬ê¸°ì„œ $Q, K, V \in \mathbb{R}^{N \times d}$ì´ê³ , $N$ì€ ì‹œí€€ìŠ¤ ê¸¸ì´, $d$ëŠ” í—¤ë“œ ì°¨ì›ì…ë‹ˆë‹¤.

**í‘œì¤€ êµ¬í˜„ì˜ HBM ì ‘ê·¼:**

| ë‹¨ê³„ | ì½ê¸° | ì“°ê¸° | HBM ì ‘ê·¼ |
|------|------|------|----------|
| $S = QK^T/\sqrt{d}$ | $Q, K$ ($2Nd$) | $S$ ($N^2$) | $O(Nd + N^2)$ |
| $P = \text{softmax}(S)$ | $S$ ($N^2$) | $P$ ($N^2$) | $O(N^2)$ |
| $O = PV$ | $P$ ($N^2$), $V$ ($Nd$) | $O$ ($Nd$) | $O(N^2 + Nd)$ |

$$\text{Total HBM I/O} = O(Nd + N^2) \quad \text{bytes (ì›ì†Œ ë‹¨ìœ„)}$$

í•µì‹¬ ë¬¸ì œ: **$N^2$ í¬ê¸°ì˜ ì¤‘ê°„ í–‰ë ¬ $S, P$ê°€ HBMì— ì €ì¥**ë¨!

### FlashAttentionì˜ IO ë³µì¡ë„

Tilingì„ ì ìš©í•˜ë©´ $S, P$ë¥¼ **SRAMì—ì„œë§Œ ì²˜ë¦¬**í•˜ê³  HBMì— ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤:

$$\text{FlashAttention HBM I/O} = O\left(\frac{N^2 d^2}{M}\right)$$

- $M$: SRAM í¬ê¸° (A100: 192KB per SM, ì „ì²´ ~20MB)
- ì¡°ê±´: $d^2 \leq M$ (í—¤ë“œ ì°¨ì›ì˜ ì œê³±ì´ SRAMì— ë“¤ì–´ê°€ì•¼ í•¨)

### Tiling ì „ëµ

ë¸”ë¡ í¬ê¸°: $B_r = \lceil M / (4d) \rceil$, $B_c = \min\left(\lceil M / (4d) \rceil, d\right)$

ë‚´ë¶€ ë£¨í”„ì—ì„œ ë¸”ë¡ $(i, j)$ì˜ ì—°ì‚°:

$$S_{ij} = Q_i K_j^T \in \mathbb{R}^{B_r \times B_c}$$

Online Softmax (Milakov & Gimelshein 2018):

$$m_i^{(j)} = \max(m_i^{(j-1)}, \text{rowmax}(S_{ij}))$$

$$\ell_i^{(j)} = e^{m_i^{(j-1)} - m_i^{(j)}} \ell_i^{(j-1)} + \text{rowsum}(e^{S_{ij} - m_i^{(j)}})$$

$$O_i^{(j)} = \text{diag}(e^{m_i^{(j-1)} - m_i^{(j)}}) O_i^{(j-1)} + e^{S_{ij} - m_i^{(j)}} V_j$$

**ìš”ì•½ í‘œ:**

| ë°©ë²• | HBM I/O | ë©”ëª¨ë¦¬ ì‚¬ìš© | ì •í™•ë„ |
|------|---------|------------|--------|
| í‘œì¤€ Attention | $O(Nd + N^2)$ | $O(N^2)$ | Exact |
| FlashAttention | $O(N^2d^2/M)$ | $O(N)$ | Exact |
| Sparse Attention | $O(N\sqrt{N})$ | $O(N\sqrt{N})$ | Approximate |"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ FlashAttention ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ Tiling(íƒ€ì¼ë§)ì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: í¼ì¦ì„ í’€ ë•Œ **ì „ì²´ ê·¸ë¦¼ì„ ì±…ìƒì— í¼ì¹˜ëŠ” ê²ƒ** vs **ì‘ì€ ì¡°ê°ì”© ë§ì¶”ëŠ” ê²ƒ**ì„ ë¹„êµí•´ ë´…ì‹œë‹¤!

**í‘œì¤€ Attention**: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 4096ì´ë©´, 4096Ã—4096 = 1,677ë§Œ ê°œì˜ ìˆ«ìë¥¼ í•œêº¼ë²ˆì— 
ë©”ëª¨ì¥(HBM)ì— ì¨ì•¼ í•´ìš”. ë©”ëª¨ì¥ê¹Œì§€ ì™•ë³µí•˜ëŠë¼ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ìš”! ğŸ“

**FlashAttention**: ì‘ì€ ë¸”ë¡(ì˜ˆ: 64Ã—64)ì”© ì˜ë¼ì„œ **ë¨¸ë¦¿ì† ì¹ íŒ(SRAM)**ì—ì„œ ë°”ë¡œ ê³„ì‚°í•´ìš”.
ì¹ íŒì€ ì‘ì§€ë§Œ ì•„ì£¼ ë¹¨ë¼ì„œ, ë©”ëª¨ì¥ì— ì™•ë³µí•˜ì§€ ì•Šì•„ë„ ë¼ìš”! ğŸ§ 

#### ğŸ”„ Online SoftmaxëŠ” ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì ìˆ˜ì˜ í‰ê· ì„ êµ¬í•  ë•Œ, ëª¨ë“  ì ìˆ˜ë¥¼ ë‹¤ ëª¨ì•„ì•¼ í• ê¹Œìš”?

ì•„ë‹ˆìš”! í•œ ëª…ì”© ì ìˆ˜ë¥¼ ë°›ìœ¼ë©´ì„œ "ì§€ê¸ˆê¹Œì§€ì˜ ìµœëŒ€ê°’"ê³¼ "ëˆ„ì  í•©"ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ë¼ìš”.
FlashAttentionë„ ì „ì²´ í–‰ì„ ë³´ì§€ ì•Šê³ , **ë¸”ë¡ë³„ë¡œ softmaxë¥¼ ì ì§„ì ìœ¼ë¡œ ê³„ì‚°**í•©ë‹ˆë‹¤!

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: HBM ì ‘ê·¼ëŸ‰ ë¹„êµ

$N=2048$, $d=128$, SRAM $M = 100\text{KB} = 100 \times 1024 / 2 = 51200$ ì›ì†Œ (FP16)ì¼ ë•Œ:

1. í‘œì¤€ Attentionì˜ HBM I/O (ì›ì†Œ ìˆ˜)
2. FlashAttentionì˜ HBM I/O (ì›ì†Œ ìˆ˜)
3. ì ˆê° ë¹„ìœ¨

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

**í‘œì¤€ Attention:**
$$\text{I/O} = Nd + N^2 + N^2 + Nd + N^2 = 3N^2 + 2Nd$$
$$= 3 \times 2048^2 + 2 \times 2048 \times 128 = 12,582,912 + 524,288 = 13,107,200$$

**FlashAttention:**
$$\text{I/O} = O\left(\frac{N^2 d^2}{M}\right) = \frac{2048^2 \times 128^2}{51200} = \frac{68,719,476,736}{51200} \approx 1,342,177$$

**ì ˆê° ë¹„ìœ¨:** $13,107,200 / 1,342,177 \approx 9.8\times$ â†’ **ì•½ 10ë°° ì ˆê°!**
</details>

#### ë¬¸ì œ 2: ë¸”ë¡ í¬ê¸° ê³„ì‚°

SRAM $M = 192\text{KB}$, $d = 128$, FP16 (2 bytes)ì¼ ë•Œ ì ì ˆí•œ ë¸”ë¡ í¬ê¸° $B_r$ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

SRAMì— $Q_i$ ($B_r \times d$), $K_j$ ($B_c \times d$), $S_{ij}$ ($B_r \times B_c$), $O_i$ ($B_r \times d$)ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.

$$M \geq (B_r \cdot d + B_c \cdot d + B_r \cdot B_c + B_r \cdot d) \times 2\;\text{bytes}$$

$B_r = B_c$ë¡œ ë†“ìœ¼ë©´: $M \geq (3Bd + B^2) \times 2$

$$192 \times 1024 \geq (3B \times 128 + B^2) \times 2$$
$$98304 \geq 768B + 2B^2$$

$B = 64$: $768 \times 64 + 2 \times 4096 = 49152 + 8192 = 57344 < 98304$ âœ“

$B = 128$: $768 \times 128 + 2 \times 16384 = 98304 + 32768 = 131072 > 98304$ âœ—

â†’ $B_r = B_c = 64$ê°€ ì ì ˆí•©ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 2. í‘œì¤€ Attentionì˜ ë©”ëª¨ë¦¬ ë³‘ëª© <a name='2.-í‘œì¤€-Attention'></a>

í‘œì¤€ Attentionê³¼ FlashAttentionì˜ **HBM ì ‘ê·¼ íšŸìˆ˜**ë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ë¡œ ë¹„êµí•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 7: Standard vs tiled memory access comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ í‘œì¤€ vs FlashAttention HBM ì ‘ê·¼ëŸ‰ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
d = 128
M_sram_bytes = 192 * 1024  # 192KB SRAM (A100 per SM)
M_sram_elements = M_sram_bytes // 2  # FP16

seq_lengths = [256, 512, 1024, 2048, 4096, 8192, 16384]

print(f"SRAM í¬ê¸°: {M_sram_bytes / 1024:.0f} KB ({M_sram_elements} FP16 ì›ì†Œ)")
print(f"í—¤ë“œ ì°¨ì› d: {d}")
print(f"\\n{'N':>7} | {'Standard I/O':>14} | {'Flash I/O':>14} | {'ì ˆê° ë°°ìœ¨':>10} | {'Standard Mem':>14} | {'Flash Mem':>12}")
print(f"{'-'*82}")

standard_ios = []
flash_ios = []
standard_mems = []
flash_mems = []

for N in seq_lengths:
    # í‘œì¤€ Attention
    std_io = 3 * N * N + 2 * N * d  # S=QK^T, P=softmax, O=PV
    std_mem = N * N  # S/P í–‰ë ¬ ì €ì¥

    # FlashAttention
    flash_io = (N * N * d * d) / M_sram_elements
    flash_mem = N  # O(N) ì¶”ê°€ ë©”ëª¨ë¦¬

    ratio = std_io / flash_io if flash_io > 0 else float('inf')

    standard_ios.append(std_io)
    flash_ios.append(flash_io)
    standard_mems.append(std_mem * 2 / 1e6)  # MB
    flash_mems.append(flash_mem * 2 / 1e6)

    print(f"{N:>7} | {std_io:>14.2e} | {flash_io:>14.2e} | {ratio:>9.1f}x | {std_mem * 2 / 1e6:>12.1f} MB | {flash_mem * 2 / 1e6:>10.4f} MB")

print(f"\\ní•µì‹¬:")
print(f"  í‘œì¤€ Attention: O(N^2) ë©”ëª¨ë¦¬ â†’ N=16384ì—ì„œ {standard_mems[-1]:.0f} MB!")
print(f"  FlashAttention: O(N) ë©”ëª¨ë¦¬  â†’ N=16384ì—ì„œ {flash_mems[-1]:.4f} MB")"""))

# â”€â”€ Cell 8: Section 3 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜ <a name='3.-Tiling-ì‹œë®¬ë ˆì´ì…˜'></a>

ë¸”ë¡ ë‹¨ìœ„ë¡œ Attentionì„ ê³„ì‚°í•˜ëŠ” FlashAttention ì•Œê³ ë¦¬ì¦˜ì„ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
í•µì‹¬ì€ **Online Softmax**: ì „ì²´ í–‰ì„ ë³´ì§€ ì•Šê³ ë„ ì •í™•í•œ softmaxë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

$$O_i = \frac{\sum_j e^{S_{ij} - m_i} V_j}{\sum_j e^{S_{ij} - m_i}} \quad \text{where } m_i = \max_j S_{ij}$$"""))

# â”€â”€ Cell 9: FlashAttention tiling simulation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ FlashAttention Tiling ì‹œë®¬ë ˆì´ì…˜ (ë¸”ë¡ ë‹¨ìœ„ ê³„ì‚°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def standard_attention(Q, K, V):
    # í‘œì¤€ Attention: ì „ì²´ S í–‰ë ¬ì„ ë©”ëª¨ë¦¬ì— ìƒì„±
    d_k = tf.cast(tf.shape(Q)[-1], tf.float32)
    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d_k)
    weights = tf.nn.softmax(scores, axis=-1)
    output = tf.matmul(weights, V)
    return output

def flash_attention_sim(Q, K, V, block_size=64):
    # FlashAttention ì‹œë®¬ë ˆì´ì…˜: ë¸”ë¡ ë‹¨ìœ„ + Online Softmax
    N = Q.shape[0]
    d = Q.shape[1]

    O = tf.zeros_like(Q)
    l = tf.zeros([N, 1])  # softmax ë¶„ëª¨
    m = tf.fill([N, 1], -1e9)  # í–‰ë³„ ìµœëŒ€ê°’

    n_blocks = (N + block_size - 1) // block_size
    hbm_reads = 0

    for j in range(n_blocks):
        kj_start = j * block_size
        kj_end = min((j + 1) * block_size, N)

        Kj = K[kj_start:kj_end]  # [Bc, d]
        Vj = V[kj_start:kj_end]  # [Bc, d]
        hbm_reads += (kj_end - kj_start) * d * 2  # K, V ì½ê¸°

        for i in range(n_blocks):
            qi_start = i * block_size
            qi_end = min((i + 1) * block_size, N)

            Qi = Q[qi_start:qi_end]  # [Br, d]
            hbm_reads += (qi_end - qi_start) * d  # Q ì½ê¸°

            Sij = tf.matmul(Qi, Kj, transpose_b=True) / tf.sqrt(tf.cast(d, tf.float32))

            m_old = m[qi_start:qi_end]
            m_new_block = tf.reduce_max(Sij, axis=-1, keepdims=True)
            m_new = tf.maximum(m_old, m_new_block)

            exp_old = tf.exp(m_old - m_new)
            exp_new = tf.exp(Sij - m_new)

            l_old = l[qi_start:qi_end]
            l_new = exp_old * l_old + tf.reduce_sum(exp_new, axis=-1, keepdims=True)

            O_old = O[qi_start:qi_end]
            O_new = exp_old * O_old + tf.matmul(exp_new, Vj)

            # í…ì„œ ê°±ì‹  (scatter ëŒ€ì‹  ìŠ¬ë¼ì´ìŠ¤)
            O = tf.concat([O[:qi_start], O_new, O[qi_end:]], axis=0)
            l = tf.concat([l[:qi_start], l_new, l[qi_end:]], axis=0)
            m = tf.concat([m[:qi_start], m_new, m[qi_end:]], axis=0)

    O = O / l
    return O, hbm_reads

# í…ŒìŠ¤íŠ¸
N, d = 256, 64
Q = tf.random.normal([N, d])
K = tf.random.normal([N, d])
V = tf.random.normal([N, d])

std_out = standard_attention(Q, K, V)
flash_out, hbm_reads = flash_attention_sim(Q, K, V, block_size=64)

diff = tf.reduce_max(tf.abs(std_out - flash_out)).numpy()
print(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {N}, í—¤ë“œ ì°¨ì›: {d}, ë¸”ë¡ í¬ê¸°: 64")
print(f"í‘œì¤€ Attention vs FlashAttention ìµœëŒ€ ì˜¤ì°¨: {diff:.2e}")
print(f"ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼ ì—¬ë¶€: {diff < 1e-4}")
print(f"FlashAttention ì‹œë®¬ë ˆì´ì…˜ HBM ì½ê¸°: {hbm_reads:,} ì›ì†Œ")
print(f"í‘œì¤€ Attention HBM ì½ê¸° (ì´ë¡ ): {3*N*N + 2*N*d:,} ì›ì†Œ")"""))

# â”€â”€ Cell 10: Section 4 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 4. IO ë³µì¡ë„ ë¹„êµ ì‹œê°í™” <a name='4.-IO-ë³µì¡ë„-ì‹œê°í™”'></a>

ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ HBM I/OëŸ‰ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 11: IO complexity visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ IO ë³µì¡ë„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# HBM I/O ë¹„êµ
ax1 = axes[0]
ax1.semilogy(seq_lengths, standard_ios, 'r-o', lw=2.5, ms=8, label='Standard: $O(Nd + N^2)$')
ax1.semilogy(seq_lengths, flash_ios, 'b-s', lw=2.5, ms=8, label='FlashAttn: $O(N^2d^2/M)$')
ax1.fill_between(seq_lengths, flash_ios, standard_ios, alpha=0.1, color='green')
ax1.set_xlabel('Sequence Length (N)', fontsize=11)
ax1.set_ylabel('HBM I/O (elements, log scale)', fontsize=11)
ax1.set_title('HBM I/O: Standard vs FlashAttention', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, which='both')

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
ax2 = axes[1]
ax2.semilogy(seq_lengths, standard_mems, 'r-o', lw=2.5, ms=8, label='Standard: $O(N^2)$')
ax2.semilogy(seq_lengths, flash_mems, 'b-s', lw=2.5, ms=8, label='FlashAttn: $O(N)$')
ax2.axhline(y=80*1024, color='gray', ls='--', lw=1.5, label='A100 80GB')
ax2.set_xlabel('Sequence Length (N)', fontsize=11)
ax2.set_ylabel('Peak Memory (MB, log scale)', fontsize=11)
ax2.set_title('Memory Usage: Standard vs FlashAttention', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, which='both')

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/flash_attention_io_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/flash_attention_io_comparison.png")

# ì ˆê° ë¹„ìœ¨ í‘œ
print(f"\\nì‹œí€€ìŠ¤ ê¸¸ì´ë³„ I/O ì ˆê° ë¹„ìœ¨:")
for i, N in enumerate(seq_lengths):
    ratio = standard_ios[i] / flash_ios[i]
    print(f"  N={N:>6}: {ratio:>6.1f}x ì ˆê°")"""))

# â”€â”€ Cell 12: Section 5 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 5. FlashAttention v1 â†’ v2 â†’ v3 ë°œì „ì‚¬ <a name='5.-FlashAttention-ë°œì „ì‚¬'></a>

FlashAttentionì€ ì„¸ ë²„ì „ì— ê±¸ì³ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ë²„ì „ì˜ í•µì‹¬ ê¸°ì—¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 13: v1â†’v2â†’v3 comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ FlashAttention v1 â†’ v2 â†’ v3 ë¹„êµí‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 90)
print("  FlashAttention Version Comparison")
print("=" * 90)

headers = ['í•­ëª©', 'v1 (2022.05)', 'v2 (2023.07)', 'v3 (2024.07)']
rows = [
    ['ë…¼ë¬¸', 'Dao et al.', 'Dao (ë‹¨ë…)', 'Shah, Dao et al.'],
    ['íƒ€ê²Ÿ GPU', 'A100', 'A100/H100', 'H100 (Hopper)'],
    ['IO ë³µì¡ë„', 'O(NÂ²dÂ²/M)', 'O(NÂ²dÂ²/M) (ë™ì¼)', 'O(NÂ²dÂ²/M) (ë™ì¼)'],
    ['í•µì‹¬ ê°œì„ ', 'Tiling+Online Softmax', 'ë£¨í”„ ìˆœì„œ ìµœì í™”', 'Warp íŠ¹í™”+ë¹„ë™ê¸°'],
    ['Forward ì†ë„', '~2-4x vs PyTorch', '~2x vs v1', '~1.5-2x vs v2'],
    ['Backward ì§€ì›', 'O (Recomputation)', 'O (ê°œì„ ëœ ë¶„í• )', 'O (ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸)'],
    ['Causal Mask', 'ì§€ì›', 'ìµœì í™”ë¨ (~50% ì ˆê°)', 'ì¶”ê°€ ìµœì í™”'],
    ['MQA/GQA', 'ë¯¸ì§€ì›', 'ì§€ì›', 'ì™„ì „ ì§€ì›'],
    ['FP8 ì§€ì›', 'ë¯¸ì§€ì›', 'ë¯¸ì§€ì›', 'ì§€ì› (H100)'],
    ['ì›Œí”„ ë³‘ë ¬í™”', 'ë°°ì¹˜+í—¤ë“œ', '+ì‹œí€€ìŠ¤ ë¶„í• ', '+warpgroup íŠ¹í™”'],
    ['ë¹„ë™ê¸° ì—°ì‚°', 'ë¯¸ì‚¬ìš©', 'ë¯¸ì‚¬ìš©', 'TMA+WGMMA ì˜¤ë²„ë©'],
    ['í”¼í¬ TFLOPS', '~125 (A100)', '~230 (H100)', '~740 (H100, FP8)'],
]

col_widths = [18, 22, 22, 22]
header_line = ' | '.join(h.ljust(w) for h, w in zip(headers, col_widths))
print(header_line)
print('-' * len(header_line))

for row in rows:
    line = ' | '.join(val.ljust(w) for val, w in zip(row, col_widths))
    print(line)

print(f"\\ní•µì‹¬ ë°œì „ ìš”ì•½:")
print(f"  v1: Tiling + Online Softmax â†’ O(N^2) ë©”ëª¨ë¦¬ë¥¼ O(N)ìœ¼ë¡œ ì ˆê°")
print(f"  v2: ë£¨í”„ ìˆœì„œ ì¬ë°°ì¹˜ + ì‹œí€€ìŠ¤ ë³‘ë ¬í™” â†’ 2x ì†ë„ í–¥ìƒ")
print(f"  v3: Hopper ì „ìš© ìµœì í™” (TMA, WGMMA) â†’ FP8ê¹Œì§€ í™•ì¥, 740 TFLOPS")

# ì„±ëŠ¥ ì‹œê°í™”
versions = ['v1\\n(A100)', 'v2\\n(A100)', 'v2\\n(H100)', 'v3\\n(H100 FP16)', 'v3\\n(H100 FP8)']
tflops = [125, 190, 230, 420, 740]
peak_util = [40, 61, 23, 43, 75]  # GPU utilization %

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

colors = ['#90CAF9', '#42A5F5', '#1E88E5', '#1565C0', '#0D47A1']

ax1 = axes[0]
bars = ax1.bar(versions, tflops, color=colors, edgecolor='black', lw=0.5)
for bar, val in zip(bars, tflops):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 15,
             f'{val}', ha='center', fontsize=10, fontweight='bold')
ax1.set_ylabel('TFLOPS', fontsize=11)
ax1.set_title('FlashAttention ë²„ì „ë³„ ì„±ëŠ¥', fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

ax2 = axes[1]
bars2 = ax2.bar(versions, peak_util, color=colors, edgecolor='black', lw=0.5)
for bar, val in zip(bars2, peak_util):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,
             f'{val}%', ha='center', fontsize=10, fontweight='bold')
ax2.set_ylabel('GPU Utilization (%)', fontsize=11)
ax2.set_title('FlashAttention ë²„ì „ë³„ GPU í™œìš©ë¥ ', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/flash_attention_versions.png', dpi=100, bbox_inches='tight')
plt.close()
print("\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/flash_attention_versions.png")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| í‘œì¤€ Attention I/O | $O(Nd + N^2)$ HBM ì ‘ê·¼, $O(N^2)$ ë©”ëª¨ë¦¬ | â­â­â­ |
| FlashAttention I/O | $O(N^2d^2/M)$ HBM ì ‘ê·¼, $O(N)$ ë©”ëª¨ë¦¬ | â­â­â­ |
| Tiling | Q, K, Vë¥¼ ë¸”ë¡ìœ¼ë¡œ ë¶„í• í•˜ì—¬ SRAMì—ì„œ ì²˜ë¦¬ | â­â­â­ |
| Online Softmax | ë¸”ë¡ë³„ ì ì§„ì  softmax â€” ì „ì²´ í–‰ ì—†ì´ë„ ì •í™• | â­â­â­ |
| Recomputation | Backwardì—ì„œ S, Pë¥¼ ì¬ê³„ì‚° (ì €ì¥ ì•ˆ í•¨) | â­â­ |
| v1 â†’ v2 ê°œì„  | ë£¨í”„ ìˆœì„œ + ì‹œí€€ìŠ¤ ë³‘ë ¬í™” | â­â­ |
| v3 (Hopper) | TMA + WGMMA ë¹„ë™ê¸°, FP8 ì§€ì› | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\text{Standard I/O} = O(Nd + N^2), \quad \text{Flash I/O} = O\left(\frac{N^2 d^2}{M}\right)$$

$$\text{Online Softmax: } m_i^{new} = \max(m_i^{old}, \max(S_{ij})), \quad \ell_i^{new} = e^{m_i^{old}-m_i^{new}}\ell_i^{old} + \sum e^{S_{ij}-m_i^{new}}$$

$$\text{Block size: } B_r = \left\lceil \frac{M}{4d} \right\rceil$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**03_speculative_decoding.ipynb** â€” Draft-Verify íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ Decode ë‹¨ê³„ë¥¼ ê°€ì†í•˜ëŠ” Speculative Decodingì˜ ìˆ˜í•™ì  ì›ë¦¬ì™€ Medusa/EAGLE ë“± ìµœì‹  ê¸°ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter14_extreme_inference/02_flash_attention_deepdive.ipynb')
