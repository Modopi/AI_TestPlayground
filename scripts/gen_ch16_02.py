"""Generate chapter16_sparse_attention/02_multi_head_latent_attention.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Multi-head Latent Attention (MLA)

## í•™ìŠµ ëª©í‘œ
- MLAì˜ KV ì••ì¶•(down-projection)ê³¼ ë³µì›(up-projection) ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•œë‹¤
- MHA, MQA, GQA, MLAì˜ KV Cache í¬ê¸°ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•œë‹¤
- MLAê°€ GQA ëŒ€ë¹„ ë‹¬ì„±í•˜ëŠ” ë©”ëª¨ë¦¬ ì ˆê°ë¥ ì„ ìˆ˜ì¹˜ë¡œ ë¶„ì„í•œë‹¤
- ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ KV Cache ë©”ëª¨ë¦¬ ë³€í™”ë¥¼ ì‹œê°í™”í•œë‹¤
- MLAì˜ attention quality ìœ ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Attentionê³¼ KV ì••ì¶•](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [MLA Down/Up Projection êµ¬í˜„](#2.-MLA-Projection-êµ¬í˜„)
3. [GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ](#3.-ë©”ëª¨ë¦¬-ë¹„êµ)
4. [KV Cache í¬ê¸° ê³„ì‚°](#4.-KV-Cache-í¬ê¸°-ê³„ì‚°)
5. [Attention Quality ë¹„êµ](#5.-Attention-Quality-ë¹„êµ)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### í‘œì¤€ Multi-Head Attention (MHA) ë³µìŠµ

$$\text{Attn}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_h}}\right)V$$

KV Cache í¬ê¸° (í† í°ë‹¹):
$$M_{KV}^{MHA} = 2 \times H \times d_h = 2 \times d_{model}$$

- $H$: í—¤ë“œ ìˆ˜, $d_h$: í—¤ë“œ ì°¨ì›, $d_{model} = H \times d_h$

### Grouped-Query Attention (GQA)

$$M_{KV}^{GQA} = 2 \times H_{kv} \times d_h$$

- $H_{kv} \ll H$: KV í—¤ë“œ ìˆ˜ (Q í—¤ë“œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ê³µìœ )
- ì ˆê°ë¥ : $H_{kv} / H$ (ì˜ˆ: Llama 3 8Bì—ì„œ $H=32, H_{kv}=8$ â†’ 75% ì ˆê°)

### Multi-head Latent Attention (MLA) â€” DeepSeek-V2/V3

**Step 1: Down-projection (KV ì••ì¶•)**

$$c_t^{KV} = W_d^{KV} h_t \in \mathbb{R}^{d_c}$$

- $W_d^{KV} \in \mathbb{R}^{d_c \times d_{model}}$: ì••ì¶• í–‰ë ¬
- $d_c \ll d_{model}$: ì••ì¶• ì°¨ì› (ì˜ˆ: DeepSeek-V2ì—ì„œ $d_c = 512$, $d_{model} = 5120$)

**Step 2: Up-projection (KV ë³µì›)**

$$[k_t^C;\; v_t^C] = W_u^{KV} c_t^{KV} \in \mathbb{R}^{2 \times H \times d_h}$$

- $W_u^{KV} \in \mathbb{R}^{(2Hd_h) \times d_c}$: ë³µì› í–‰ë ¬
- ë³µì›ëœ $k_t^C, v_t^C$ë¡œ í‘œì¤€ attention ìˆ˜í–‰

**KV Cache í¬ê¸° (MLA, í† í°ë‹¹):**

$$M_{KV}^{MLA} = d_c \quad \text{(ì˜¤ì§ ì••ì¶• ë²¡í„°ë§Œ ì €ì¥)}$$

**ì ˆê°ë¥ :**

$$\text{ì ˆê°ë¥ } = \frac{d_c}{2 \times H_{kv} \times d_h} \quad \text{(GQA ëŒ€ë¹„)}$$

**ìš”ì•½ í‘œ:**

| ë°©ì‹ | KV Cache (í† í°ë‹¹) | ì˜ˆì‹œ ($d=5120, H=40, d_h=128$) |
|------|-------------------|--------------------------------|
| MHA | $2Hd_h = 2d_{model}$ | $10240$ |
| GQA ($H_{kv}=8$) | $2 H_{kv} d_h$ | $2048$ |
| MQA | $2d_h$ | $256$ |
| MLA ($d_c=512$) | $d_c$ | $512$ |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ MLA ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ MLAê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ë„ì„œê´€ì— ì±…(KV Cache)ì„ ë³´ê´€í•  ë•Œë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!

- **MHA**: ëª¨ë“  ì±…ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³´ê´€ â†’ ê³µê°„ì´ ì—„ì²­ë‚˜ê²Œ í•„ìš”í•´ìš”
- **GQA**: ë¹„ìŠ·í•œ ì±…ë¼ë¦¬ ë¬¶ì–´ì„œ ëŒ€í‘œ í•œ ê¶Œë§Œ ë³´ê´€ â†’ ê³µê°„ ì ˆì•½!
- **MLA**: ëª¨ë“  ì±…ì˜ **ìš”ì•½ë³¸**(ì••ì¶• ë²¡í„°)ë§Œ ë³´ê´€í•˜ê³ , í•„ìš”í•  ë•Œ ì›ë³¸ì„ ë³µì›í•´ìš” â†’ ìµœê³ ì˜ ì ˆì•½!

#### ğŸ“¦ ì–´ë–»ê²Œ ì••ì¶•í•˜ë‚˜ìš”?

> ğŸ’¡ **ë¹„ìœ **: 5120ê°œì˜ ìˆ«ìë¥¼ 512ê°œë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì€, 
> ê¸´ ë¬¸ì¥ì„ í•µì‹¬ í‚¤ì›Œë“œë¡œ ìš”ì•½í•˜ëŠ” ê²ƒê³¼ ê°™ì•„ìš”!

1. **ì••ì¶•(Down)**: $5120 \\rightarrow 512$ (10ë°° ì¤„ì´ê¸°)
2. **ì €ì¥**: 512ê°œì˜ ìˆ«ìë§Œ KV Cacheì— ì €ì¥
3. **ë³µì›(Up)**: $512 \\rightarrow 5120$ (í•„ìš”í•  ë•Œ ë³µì›)

í•µì‹¬ì€ **ì €ì¥í•  ë•Œë§Œ ì‘ê²Œ, ì‚¬ìš©í•  ë•ŒëŠ” ì›ë˜ í¬ê¸°ë¡œ** ëŒë¦¬ëŠ” ê±°ì˜ˆìš”!

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: MLA ì••ì¶•ë¥  ê³„ì‚°

DeepSeek-V2 ê¸°ì¤€: $d_{model}=5120$, $H=40$, $d_h=128$, $d_c=512$.
MLAì˜ MHA ëŒ€ë¹„ KV Cache ì ˆê°ë¥ (%)ì„ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$M_{KV}^{MHA} = 2 \times H \times d_h = 2 \times 40 \times 128 = 10240$$

$$M_{KV}^{MLA} = d_c = 512$$

$$\text{ì ˆê°ë¥ } = 1 - \frac{512}{10240} = 1 - 0.05 = 0.95 = 95\%$$

â†’ MLAëŠ” MHA ëŒ€ë¹„ **95%** KV Cache ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤!
</details>

#### ë¬¸ì œ 2: GQA vs MLA ë¹„êµ

$H_{kv}=8$ GQAì™€ $d_c=512$ MLAì˜ KV Cache í¬ê¸°ë¥¼ ë¹„êµí•˜ì„¸ìš” ($d_h=128$).

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$M_{KV}^{GQA} = 2 \times 8 \times 128 = 2048$$

$$M_{KV}^{MLA} = 512$$

$$\frac{M_{KV}^{MLA}}{M_{KV}^{GQA}} = \frac{512}{2048} = 0.25$$

â†’ MLAëŠ” GQA ëŒ€ë¹„ **75% ì¶”ê°€ ì ˆê°**ì„ ë‹¬ì„±í•©ë‹ˆë‹¤ (4ë°° ë” ì‘ìŒ).
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 - MLA Projection Implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. MLA Down/Up Projection êµ¬í˜„ <a name='2.-MLA-Projection-êµ¬í˜„'></a>

MLAì˜ í•µì‹¬ì€ KVë¥¼ ì €ì°¨ì› ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:

1. **Down-projection**: $c_t^{KV} = W_d^{KV} h_t$ ($d_{model} \rightarrow d_c$)
2. **Up-projection**: $[k_t^C; v_t^C] = W_u^{KV} c_t^{KV}$ ($d_c \rightarrow 2 H d_h$)

ì´ë¥¼ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤."""))

cells.append(code("""\
# â”€â”€ MLA Down/Up Projection êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class MLAProjection(tf.keras.layers.Layer):
    # Multi-head Latent Attentionì˜ KV ì••ì¶•/ë³µì› ë ˆì´ì–´

    def __init__(self, d_model, n_heads, d_head, d_compress):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_head = d_head
        self.d_compress = d_compress

        # Down-projection: d_model -> d_compress
        self.W_down = tf.keras.layers.Dense(d_compress, use_bias=False, name='kv_down')

        # Up-projection: d_compress -> 2 * n_heads * d_head (K + V)
        self.W_up = tf.keras.layers.Dense(2 * n_heads * d_head, use_bias=False, name='kv_up')

        # Q projection (ë³„ë„)
        self.W_q = tf.keras.layers.Dense(n_heads * d_head, use_bias=False, name='q_proj')

    def call(self, h):
        batch, seq_len, _ = h.shape

        # Q projection
        q = self.W_q(h)
        q = tf.reshape(q, [batch, seq_len, self.n_heads, self.d_head])

        # KV Down-projection (ì••ì¶•)
        c_kv = self.W_down(h)  # [B, S, d_compress]

        # KV Up-projection (ë³µì›)
        kv = self.W_up(c_kv)  # [B, S, 2*H*d_h]
        kv = tf.reshape(kv, [batch, seq_len, 2, self.n_heads, self.d_head])
        k, v = kv[:, :, 0], kv[:, :, 1]

        return q, k, v, c_kv


# íŒŒë¼ë¯¸í„° ì„¤ì • (DeepSeek-V2 ìŠ¤ì¼€ì¼ ë‹¤ìš´)
d_model = 512
n_heads = 8
d_head = 64
d_compress = 64  # 8ë°° ì••ì¶•

mla = MLAProjection(d_model, n_heads, d_head, d_compress)

# í…ŒìŠ¤íŠ¸ ì…ë ¥
batch_size = 2
seq_len = 16
h = tf.random.normal([batch_size, seq_len, d_model])

q, k, v, c_kv = mla(h)

print(f"MLA Projection ê²°ê³¼:")
print(f"  ì…ë ¥ h: {h.shape}")
print(f"  Q: {q.shape}")
print(f"  K (ë³µì›): {k.shape}")
print(f"  V (ë³µì›): {v.shape}")
print(f"  ì••ì¶• ë²¡í„° c_kv: {c_kv.shape}")
print(f"\\nì••ì¶•ë¥ :")
print(f"  ì›ë³¸ KV í¬ê¸°: {2 * n_heads * d_head} (= 2 x {n_heads} x {d_head})")
print(f"  ì••ì¶• KV í¬ê¸°: {d_compress}")
print(f"  ì••ì¶•ë¥ : {d_compress / (2 * n_heads * d_head):.2%} ({(2 * n_heads * d_head) / d_compress:.0f}x ì••ì¶•)")

# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ
mla_params = d_model * d_compress + d_compress * (2 * n_heads * d_head)
mha_params = d_model * (2 * n_heads * d_head)  # K, V projection
print(f"\\níŒŒë¼ë¯¸í„° ìˆ˜:")
print(f"  MLA (Down + Up): {mla_params:,}")
print(f"  MHA (K + V proj): {mha_params:,}")
print(f"  MLA ì˜¤ë²„í—¤ë“œ: {mla_params/mha_params:.2f}x (ì¶”ë¡  ì‹œ KV CacheëŠ” {d_compress}ë§Œ ì €ì¥)")"""))

# â”€â”€ Cell 8: Section 3 - Memory Comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ <a name='3.-ë©”ëª¨ë¦¬-ë¹„êµ'></a>

ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¦ê°€í•  ë•Œ ê° ë°©ì‹ì˜ KV Cache ë©”ëª¨ë¦¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

| ë°©ì‹ | KV Cache í¬ê¸° (ë°”ì´íŠ¸/í† í°/ë ˆì´ì–´) |
|------|----------------------------------|
| MHA | $2 \times H \times d_h \times \text{bytes}$ |
| GQA | $2 \times H_{kv} \times d_h \times \text{bytes}$ |
| MQA | $2 \times d_h \times \text{bytes}$ |
| MLA | $d_c \times \text{bytes}$ |"""))

cells.append(code("""\
# â”€â”€ GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ ì¶•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DeepSeek-V2 ê¸°ì¤€ íŒŒë¼ë¯¸í„°
d_model_real = 5120
n_heads_real = 40
d_head_real = 128
n_layers = 60
bytes_per_elem = 2  # FP16

configs = {
    'MHA': {'kv_per_token': 2 * n_heads_real * d_head_real},
    'GQA (H_kv=8)': {'kv_per_token': 2 * 8 * d_head_real},
    'GQA (H_kv=4)': {'kv_per_token': 2 * 4 * d_head_real},
    'MQA': {'kv_per_token': 2 * d_head_real},
    'MLA (d_c=512)': {'kv_per_token': 512},
}

seq_lengths = np.array([512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072])

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ì´ KV Cache ë©”ëª¨ë¦¬ (GB) â€” ì‹œí€€ìŠ¤ ê¸¸ì´ë³„
ax1 = axes[0]
colors = ['red', 'orange', 'blue', 'purple', 'green']
markers = ['o', 's', '^', 'D', 'v']

for (name, cfg), color, marker in zip(configs.items(), colors, markers):
    memory_gb = seq_lengths * n_layers * cfg['kv_per_token'] * bytes_per_elem / (1024**3)
    ax1.semilogy(seq_lengths / 1000, memory_gb, f'-{marker}',
                 color=color, lw=2, ms=7, label=name)

ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)
ax1.set_ylabel('KV Cache ë©”ëª¨ë¦¬ (GB)', fontsize=11)
ax1.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ KV Cache ë©”ëª¨ë¦¬', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)
ax1.axhline(y=80, color='gray', ls=':', lw=1.5, alpha=0.7)
ax1.text(seq_lengths[-1]/1000*0.5, 85, 'H100 80GB', fontsize=9, color='gray')

# (2) MHA ëŒ€ë¹„ ì ˆê°ë¥ 
ax2 = axes[1]
mha_mem = configs['MHA']['kv_per_token']
for (name, cfg), color, marker in zip(list(configs.items())[1:], colors[1:], markers[1:]):
    savings = (1 - cfg['kv_per_token'] / mha_mem) * 100
    ax2.barh(name, savings, color=color, alpha=0.7, edgecolor='black')
    ax2.text(savings + 1, name, f'{savings:.1f}%', va='center', fontsize=10, fontweight='bold')

ax2.set_xlabel('MHA ëŒ€ë¹„ KV Cache ì ˆê°ë¥  (%)', fontsize=11)
ax2.set_title('KV Cache ì ˆê°ë¥  ë¹„êµ', fontweight='bold')
ax2.set_xlim(0, 105)
ax2.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/mha_gqa_mla_memory.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/mha_gqa_mla_memory.png")

# ìˆ˜ì¹˜ í‘œ
print(f"\\nKV Cache ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ = 32K, {n_layers} ë ˆì´ì–´, FP16):")
print(f"{'ë°©ì‹':<16} | {'í† í°ë‹¹ KV':>12} | {'ë©”ëª¨ë¦¬ (GB)':>12} | {'MHA ëŒ€ë¹„':>10}")
print("-" * 58)
for name, cfg in configs.items():
    mem_gb = 32768 * n_layers * cfg['kv_per_token'] * bytes_per_elem / (1024**3)
    ratio = cfg['kv_per_token'] / mha_mem
    print(f"{name:<16} | {cfg['kv_per_token']:>12} | {mem_gb:>12.2f} | {ratio:>9.2%}")"""))

# â”€â”€ Cell 10: Section 4 - KV Cache calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. KV Cache í¬ê¸° ê³„ì‚° <a name='4.-KV-Cache-í¬ê¸°-ê³„ì‚°'></a>

ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ KV Cache í¬ê¸° ê³„ì‚°:

$$M_{KV} = B \times S \times L \times M_{per\_token\_per\_layer} \times \text{bytes}$$

- $B$: ë°°ì¹˜ í¬ê¸°
- $S$: ì‹œí€€ìŠ¤ ê¸¸ì´
- $L$: ë ˆì´ì–´ ìˆ˜"""))

cells.append(code("""\
# â”€â”€ ì‹¤ì œ ëª¨ë¸ë³„ KV Cache í¬ê¸° ê³„ì‚°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
models = {
    'Llama-3-8B': {
        'n_layers': 32, 'n_heads': 32, 'd_head': 128,
        'n_kv_heads': 8, 'method': 'GQA',
        'kv_per_token': lambda: 2 * 8 * 128
    },
    'Llama-3-70B': {
        'n_layers': 80, 'n_heads': 64, 'd_head': 128,
        'n_kv_heads': 8, 'method': 'GQA',
        'kv_per_token': lambda: 2 * 8 * 128
    },
    'DeepSeek-V2': {
        'n_layers': 60, 'n_heads': 128, 'd_head': 128,
        'd_compress': 512, 'method': 'MLA',
        'kv_per_token': lambda: 512
    },
    'DeepSeek-V3': {
        'n_layers': 61, 'n_heads': 128, 'd_head': 128,
        'd_compress': 512, 'method': 'MLA',
        'kv_per_token': lambda: 512
    },
}

batch_size = 1
seq_length = 4096
fp16_bytes = 2

print(f"ëª¨ë¸ë³„ KV Cache í¬ê¸° (B={batch_size}, S={seq_length}, FP16)")
print("=" * 80)
print(f"{'ëª¨ë¸':<18} | {'ë°©ì‹':<6} | {'ë ˆì´ì–´':>6} | {'í† í°ë‹¹ KV':>10} | {'ì´ ë©”ëª¨ë¦¬':>12} | {'ë°°ì¹˜8':>12}")
print("-" * 80)

for name, cfg in models.items():
    kv = cfg['kv_per_token']()
    mem = batch_size * seq_length * cfg['n_layers'] * kv * fp16_bytes
    mem_gb = mem / (1024**3)
    mem_batch8 = mem_gb * 8
    print(f"{name:<18} | {cfg['method']:<6} | {cfg['n_layers']:>6} | {kv:>10} | {mem_gb:>10.3f} GB | {mem_batch8:>10.3f} GB")

# DeepSeek-V3 MLA vs ê°€ìƒì˜ GQA ë¹„êµ
print(f"\\nDeepSeek-V3: MLA vs ê°€ìƒì˜ GQA ë¹„êµ (S={seq_length}):")
mla_kv = 512 * 61 * seq_length * fp16_bytes
gqa8_kv = (2 * 8 * 128) * 61 * seq_length * fp16_bytes
gqa4_kv = (2 * 4 * 128) * 61 * seq_length * fp16_bytes
print(f"  MLA (d_c=512):     {mla_kv / (1024**3):.3f} GB")
print(f"  GQA (H_kv=8):      {gqa8_kv / (1024**3):.3f} GB")
print(f"  GQA (H_kv=4):      {gqa4_kv / (1024**3):.3f} GB")
print(f"  MLA/GQA(8) ë¹„ìœ¨:   {mla_kv/gqa8_kv:.2%}")
print(f"  MLA/GQA(4) ë¹„ìœ¨:   {mla_kv/gqa4_kv:.2%}")"""))

# â”€â”€ Cell 12: Section 5 - Attention quality â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. Attention Quality ë¹„êµ <a name='5.-Attention-Quality-ë¹„êµ'></a>

MLAì˜ KV ì••ì¶•ì´ attention í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
ì••ì¶• ì°¨ì› $d_c$ê°€ í´ìˆ˜ë¡ ë³µì› í’ˆì§ˆì´ ë†’ì§€ë§Œ, ë©”ëª¨ë¦¬ ì ˆì•½ì€ ì¤„ì–´ë“­ë‹ˆë‹¤.

$$\text{Reconstruction Error} = \frac{\|KV_{original} - W_u \cdot W_d \cdot h\|_F}{\|KV_{original}\|_F}$$"""))

cells.append(code("""\
# â”€â”€ MLA ì••ì¶• ì°¨ì›ë³„ Attention Quality ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
d_model_test = 256
n_heads_test = 4
d_head_test = 64
seq_len_test = 64
batch_test = 4

# ì›ë³¸ KV ìƒì„± (MHA)
h_test = tf.random.normal([batch_test, seq_len_test, d_model_test])
W_kv_original = tf.keras.layers.Dense(2 * n_heads_test * d_head_test, use_bias=False)
kv_original = W_kv_original(h_test)

# ë‹¤ì–‘í•œ ì••ì¶• ì°¨ì›ì—ì„œ ë³µì› í’ˆì§ˆ ì¸¡ì •
compress_dims = [16, 32, 64, 128, 256, 512]
errors = []
savings = []

print(f"MLA ì••ì¶• ì°¨ì›ë³„ ë³µì› í’ˆì§ˆ:")
print(f"{'d_c':>6} | {'ë³µì› ì˜¤ì°¨':>12} | {'KV Cache ì ˆê°ë¥ ':>16} | {'Attn ìœ ì‚¬ë„':>12}")
print("-" * 55)

for d_c in compress_dims:
    # Down-projection + Up-projection
    W_down = tf.keras.layers.Dense(d_c, use_bias=False)
    W_up = tf.keras.layers.Dense(2 * n_heads_test * d_head_test, use_bias=False)

    c_kv = W_down(h_test)
    kv_reconstructed = W_up(c_kv)

    # ë³µì› ì˜¤ì°¨ (Frobenius norm)
    error = tf.norm(kv_original - kv_reconstructed) / tf.norm(kv_original)
    error_val = error.numpy()
    errors.append(error_val)

    # KV Cache ì ˆê°ë¥ 
    original_size = 2 * n_heads_test * d_head_test
    saving = (1 - d_c / original_size) * 100
    savings.append(saving)

    # Attention ìœ ì‚¬ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
    kv_orig_flat = tf.reshape(kv_original, [-1, kv_original.shape[-1]])
    kv_recon_flat = tf.reshape(kv_reconstructed, [-1, kv_reconstructed.shape[-1]])
    cos_sim = tf.reduce_mean(
        tf.reduce_sum(kv_orig_flat * kv_recon_flat, axis=-1) /
        (tf.norm(kv_orig_flat, axis=-1) * tf.norm(kv_recon_flat, axis=-1) + 1e-8)
    ).numpy()

    print(f"{d_c:>6} | {error_val:>12.4f} | {saving:>14.1f}% | {cos_sim:>12.4f}")

# ì‹œê°í™”: ì••ì¶• ì°¨ì› vs ë³µì› ì˜¤ì°¨/ì ˆê°ë¥  íŠ¸ë ˆì´ë“œì˜¤í”„
fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))

color1 = 'tab:blue'
ax1.set_xlabel('ì••ì¶• ì°¨ì› ($d_c$)', fontsize=11)
ax1.set_ylabel('ë³µì› ì˜¤ì°¨ (ìƒëŒ€)', fontsize=11, color=color1)
ax1.plot(compress_dims, errors, 'b-o', lw=2.5, ms=8, label='ë³µì› ì˜¤ì°¨')
ax1.tick_params(axis='y', labelcolor=color1)
ax1.grid(True, alpha=0.3)

ax2_twin = ax1.twinx()
color2 = 'tab:red'
ax2_twin.set_ylabel('KV Cache ì ˆê°ë¥  (%)', fontsize=11, color=color2)
ax2_twin.plot(compress_dims, savings, 'r--s', lw=2, ms=7, label='ì ˆê°ë¥ ')
ax2_twin.tick_params(axis='y', labelcolor=color2)

ax1.set_title('MLA ì••ì¶• ì°¨ì›ë³„ ë³µì› í’ˆì§ˆ vs ë©”ëª¨ë¦¬ ì ˆê°', fontweight='bold')

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2_twin.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=9, loc='center right')

plt.tight_layout()
plt.savefig('chapter16_sparse_attention/mla_quality_tradeoff.png', dpi=100, bbox_inches='tight')
plt.close()
print("\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/mla_quality_tradeoff.png")
print(f"\\nâ†’ d_cê°€ ì»¤ì§ˆìˆ˜ë¡ ë³µì› ì˜¤ì°¨ ê°ì†Œ, í•˜ì§€ë§Œ ì ˆê°ë¥ ë„ ê°ì†Œ")
print(f"â†’ DeepSeek-V2/V3ì€ d_c=512ë¡œ 93.75% ì ˆê°ê³¼ ë†’ì€ í’ˆì§ˆì„ ë™ì‹œì— ë‹¬ì„±")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| MLA Down-projection | $c_t^{KV} = W_d^{KV} h_t$ â€” KVë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶• | â­â­â­ |
| MLA Up-projection | $[k_t^C; v_t^C] = W_u^{KV} c_t^{KV}$ â€” KV ë³µì› | â­â­â­ |
| KV Cache ì ˆê° | MLA: $d_c$ vs GQA: $2H_{kv}d_h$ | â­â­â­ |
| ì••ì¶• ì°¨ì› ì„ íƒ | $d_c$ê°€ í´ìˆ˜ë¡ í’ˆì§ˆâ†‘, ì ˆê°ë¥ â†“ â€” íŠ¸ë ˆì´ë“œì˜¤í”„ | â­â­ |
| MHAâ†’GQAâ†’MLA ì§„í™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì˜ ë‹¨ê³„ì  ë°œì „ | â­â­â­ |
| DeepSeek-V2/V3 ì„¤ì • | $d_c=512$, 95% MHA ëŒ€ë¹„ ì ˆê° | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$c_t^{KV} = W_d^{KV} h_t \in \mathbb{R}^{d_c}, \quad d_c \ll d_{model}$$

$$[k_t^C;\; v_t^C] = W_u^{KV} c_t^{KV} \in \mathbb{R}^{2Hd_h}$$

$$\text{KV Cache ì ˆê°ë¥ } = 1 - \frac{d_c}{2 \times H \times d_h}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**03_linear_attention_and_hybrids.ipynb** â€” GLA, RetNet, Mamba ë“± Linear Attention ê³„ì—´ ê¸°ë²•ê³¼ Qwenì˜ SWA+Full+Linear Hybrid êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter16_sparse_attention/02_multi_head_latent_attention.ipynb')
