#!/usr/bin/env python3
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""# Chapter 14: ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” â€” ì–‘ìí™” ì‹¬í™” (GPTQ & AWQ)

## í•™ìŠµ ëª©í‘œ
- **Post-Training Quantization(PTQ)**ì˜ ê¸°ë³¸ ì›ë¦¬ì™€ ê· ì¼ ì–‘ìí™”(Uniform Quantization)ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤
- **GPTQ**ì˜ Hessian ê¸°ë°˜ 2ì°¨ ìµœì í™”ê°€ ê°€ì¤‘ì¹˜ ì–‘ìí™” ì˜¤ì°¨ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê³¼ì •ì„ ë„ì¶œí•œë‹¤
- **AWQ(Activation-aware Weight Quantization)**ì˜ ì±„ë„ë³„ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¤‘ìš” ê°€ì¤‘ì¹˜ë¥¼ ë³´í˜¸í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ êµ¬í˜„í•œë‹¤
- W4A16, INT8, FP8 ë“± ë‹¤ì–‘í•œ ì–‘ìí™” í¬ë§·ì˜ **ì •ë°€ë„-ì†ë„-ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„**ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•œë‹¤
- SNR(Signal-to-Noise Ratio)ê³¼ ë¹„íŠ¸í­ì˜ ê´€ê³„ë¥¼ ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ì–‘ìí™” ì´ë¡ ](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [ê· ì¼ ì–‘ìí™” ë°ëª¨ (INT8, INT4)](#2.-ê· ì¼-ì–‘ìí™”-ë°ëª¨)
3. [ì–‘ìí™” ì˜¤ì°¨ vs ë¹„íŠ¸í­ ì‹œê°í™”](#3.-ì–‘ìí™”-ì˜¤ì°¨-ì‹œê°í™”)
4. [AWQ í•µì‹¬ ì±„ë„ íƒì§€ ì‹œë®¬ë ˆì´ì…˜](#4.-AWQ-í•µì‹¬-ì±„ë„)
5. [W4A16 / INT8 / FP8 ë¹„êµ ë²¤ì¹˜ë§ˆí¬](#5.-í¬ë§·-ë¹„êµ)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math Foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### ê· ì¼ ì–‘ìí™” (Uniform Quantization)

ì‹¤ìˆ˜ê°’ì„ $b$-bit ì •ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë³¸ ê³µì‹:

$$\Delta = \frac{x_{max} - x_{min}}{2^b - 1}$$

$$x_q = \text{round}\left(\frac{x - x_{min}}{\Delta}\right), \quad x_q \in \{0, 1, \ldots, 2^b - 1\}$$

$$\hat{x} = x_q \cdot \Delta + x_{min} \quad \text{(ì—­ì–‘ìí™”)}$$

- $\Delta$: ì–‘ìí™” ìŠ¤í… í¬ê¸° (step size)
- $b$: ë¹„íŠ¸í­ (bit-width)
- $x_q$: ì–‘ìí™”ëœ ì •ìˆ˜ê°’
- $\hat{x}$: ë³µì›ëœ ê·¼ì‚¬ê°’

### ì–‘ìí™” ì˜¤ì°¨ì™€ SNR

ì–‘ìí™” ì˜¤ì°¨ëŠ” ê· ì¼ ë¶„í¬ë¥¼ ë”°ë¥´ë©°:

$$\text{MSE}_{quant} = \frac{\Delta^2}{12}$$

$$\text{SNR} = \frac{\text{Signal Power}}{\text{Noise Power}} = \frac{\sigma_x^2}{\Delta^2/12} \propto 2^{2b}$$

$$\text{SNR}_{dB} = 6.02b + C \quad \text{(ë¹„íŠ¸ë‹¹ ì•½ 6dB í–¥ìƒ)}$$

### GPTQ: Hessian ê¸°ë°˜ ìµœì í™”

GPTQëŠ” ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜ $\hat{W}$ê°€ **ì¶œë ¥ ì˜¤ì°¨**ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ 2ì°¨ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

$$\min_{\hat{W}} \|WX - \hat{W}X\|_F^2$$

Hessian $H = 2XX^T$ë¥¼ ì´ìš©í•œ ìµœì  ë³´ì •:

$$\delta_i = \frac{w_i - \text{quant}(w_i)}{[H^{-1}]_{ii}}, \quad w_j \leftarrow w_j - \frac{[H^{-1}]_{ij}}{[H^{-1}]_{ii}} \cdot (w_i - \text{quant}(w_i))$$

- $w_i$: $i$ë²ˆì§¸ ì—´ì˜ ê°€ì¤‘ì¹˜
- $H^{-1}$: Hessian ì—­í–‰ë ¬ (Cholesky ë¶„í•´ë¡œ íš¨ìœ¨ì  ê³„ì‚°)
- í•œ ì—´ì„ ì–‘ìí™”í•  ë•Œ ë‚˜ë¨¸ì§€ ì—´ì— ì˜¤ì°¨ë¥¼ **ë¶„ì‚°**

### AWQ: Activation-aware ìŠ¤ì¼€ì¼ë§

AWQëŠ” í™œì„±í™”ê°€ í° **ì¤‘ìš” ì±„ë„**ì„ ë³´í˜¸í•˜ê¸° ìœ„í•´ ì±„ë„ë³„ ìŠ¤ì¼€ì¼ $S$ë¥¼ ì ìš©í•©ë‹ˆë‹¤:

$$\min_Q \|WX - Q(W \cdot \text{diag}(S)) \cdot \text{diag}(S)^{-1} X\|^2$$

$$S_j = \left(\frac{\max(|X_j|)}{\max(|W_j|)}\right)^\alpha, \quad \alpha \in [0, 1]$$

- $S_j$: $j$ë²ˆì§¸ ì±„ë„ì˜ ìŠ¤ì¼€ì¼ íŒ©í„°
- $\alpha$: í™œì„±í™”-ê°€ì¤‘ì¹˜ ê· í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë³´í†µ $\alpha \approx 0.5$)
- í° í™œì„±í™”ë¥¼ ê°€ì§„ ì±„ë„ â†’ í° $S_j$ â†’ ê°€ì¤‘ì¹˜ë¥¼ í‚¤ì›Œì„œ ì–‘ìí™” í•´ìƒë„ í™•ë³´

**ìš”ì•½ í‘œ:**

| ê¸°ë²• | ìˆ˜ì‹ | í•µì‹¬ ì•„ì´ë””ì–´ |
|------|------|--------------|
| ê· ì¼ ì–‘ìí™” | $\Delta = (x_{max}-x_{min})/(2^b-1)$ | ë“±ê°„ê²© ë§¤í•‘ |
| SNR | $\propto 2^{2b}$ (ë¹„íŠ¸ë‹¹ 6dB) | ë¹„íŠ¸í­ â†‘ â†’ ì •ë°€ë„ â†‘ |
| GPTQ | $\min \|WX - \hat{W}X\|_F^2$ | Hessianìœ¼ë¡œ ì˜¤ì°¨ ë¶„ì‚° |
| AWQ | $S_j = (\max|X_j| / \max|W_j|)^\alpha$ | ì¤‘ìš” ì±„ë„ ìŠ¤ì¼€ì¼ ë³´í˜¸ |"""))

# â”€â”€ Cell 3: ğŸ£ Friendly Explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì–‘ìí™” ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ ì–‘ìí™”(Quantization)ê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ìƒ‰ì—°í•„ ì„¸íŠ¸ë¥¼ ì¤„ì´ëŠ” ê²ƒê³¼ ê°™ì•„ìš”!

128ìƒ‰ ìƒ‰ì—°í•„(FP16)ì´ ìˆë‹¤ê³  í•´ë´ìš”. ë§¤ìš° ì •ë°€í•˜ê²Œ ê·¸ë¦´ ìˆ˜ ìˆì§€ë§Œ, ê°€ë°©ì´ ë¬´ê²ì£ .
ì–‘ìí™”ëŠ” **16ìƒ‰(INT4)ì´ë‚˜ 8ìƒ‰(INT8)** ìƒ‰ì—°í•„ ì„¸íŠ¸ë¡œ ë°”ê¾¸ëŠ” ê±°ì˜ˆìš”. ê°€ë°©ì´ í›¨ì”¬ ê°€ë²¼ì›Œì ¸ìš”! ğŸ’

#### ğŸ¨ GPTQ vs AWQì˜ ì°¨ì´

| ë°©ë²• | ë¹„ìœ  | ì „ëµ |
|------|------|------|
| GPTQ | ìƒ‰ì„ ì¤„ì¼ ë•Œ **ë¹„ìŠ·í•œ ìƒ‰ë¼ë¦¬ ë¬¶ì–´ì„œ** ì˜¤ì°¨ë¥¼ ê³ ë¥´ê²Œ ë‚˜ëˆ” | Hessianìœ¼ë¡œ ì˜¤ì°¨ ìµœì†Œí™” |
| AWQ | **ìì£¼ ì“°ëŠ” ìƒ‰(ì¤‘ìš” ì±„ë„)**ì€ ê¼­ ë‚¨ê¸°ê³ , ì•ˆ ì“°ëŠ” ìƒ‰ë§Œ í•©ì¹¨ | í™œì„±í™” ê¸°ë°˜ ì±„ë„ ë³´í˜¸ |

GPTQëŠ” "ìˆ˜í•™ì ìœ¼ë¡œ ê°€ì¥ ì ì€ ì˜¤ì°¨"ë¥¼ ì¶”êµ¬í•˜ê³ ,
AWQëŠ” "ì‹¤ì œë¡œ ë§ì´ ì“°ëŠ” ê²ƒì„ ë³´í˜¸"í•˜ëŠ” ì‹¤ìš©ì  ì ‘ê·¼ì´ì—ìš”!

#### ğŸ’¾ ëª¨ë¸ í¬ê¸° ë¹„êµ

| ì •ë°€ë„ | Llama 3 8B í¬ê¸° | ë¹„ìœ  |
|--------|-----------------|------|
| FP16 (16ë¹„íŠ¸) | ~16 GB | ğŸ“š ë°±ê³¼ì‚¬ì „ ì „ì§‘ |
| INT8 (8ë¹„íŠ¸) | ~8 GB | ğŸ“– ìš”ì•½ë³¸ |
| INT4 (4ë¹„íŠ¸) | ~4 GB | ğŸ“ í•µì‹¬ ë©”ëª¨ |"""))

# â”€â”€ Cell 4: ğŸ“ Practice Problems â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: ì–‘ìí™” ìŠ¤í… í¬ê¸°

ê°€ì¤‘ì¹˜ ë²”ìœ„ê°€ $[-2.0, 2.0]$ì¼ ë•Œ INT8($b=8$)ê³¼ INT4($b=4$)ì˜ ì–‘ìí™” ìŠ¤í… í¬ê¸° $\Delta$ë¥¼ ê°ê° êµ¬í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\Delta_{INT8} = \frac{2.0 - (-2.0)}{2^8 - 1} = \frac{4.0}{255} \approx 0.0157$$

$$\Delta_{INT4} = \frac{2.0 - (-2.0)}{2^4 - 1} = \frac{4.0}{15} \approx 0.267$$

INT4ì˜ ìŠ¤í… í¬ê¸°ëŠ” INT8ì˜ ì•½ 17ë°° â†’ ê·¸ë§Œí¼ ì–‘ìí™” ì˜¤ì°¨ê°€ í½ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: SNR ê³„ì‚°

8ë¹„íŠ¸ â†’ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”í•  ë•Œ SNRì€ ëª‡ dB ê°ì†Œí•˜ë‚˜ìš”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\Delta \text{SNR} = 6.02 \times (8 - 4) = 24.08 \text{ dB ê°ì†Œ}$$

ë¹„íŠ¸í­ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ë©´ SNRì´ ì•½ 24dB(â‰ˆ250ë°°) ì•…í™”ë©ë‹ˆë‹¤.
</details>"""))

# â”€â”€ Cell 5: Import Cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ í™˜ê²½ ì„¤ì • ë° ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 - Uniform Quantization Demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ ê· ì¼ ì–‘ìí™” ë°ëª¨ (INT8, INT4) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì •ê·œë¶„í¬ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ ë‹¤ì–‘í•œ ë¹„íŠ¸í­ìœ¼ë¡œ ì–‘ìí™”í•©ë‹ˆë‹¤

def uniform_quantize(x, num_bits):
    x_min = np.min(x)
    x_max = np.max(x)
    n_levels = 2**num_bits - 1
    delta = (x_max - x_min) / n_levels
    x_q = np.round((x - x_min) / delta).astype(int)
    x_q = np.clip(x_q, 0, n_levels)
    x_hat = x_q * delta + x_min
    return x_hat, delta

np.random.seed(42)
weights = np.random.randn(1000) * 0.5

print("=" * 65)
print(f"{'ë¹„íŠ¸í­':>8} | {'ìŠ¤í…(Î”)':>10} | {'MSE':>12} | {'SNR(dB)':>10} | {'ìµœëŒ€ì˜¤ì°¨':>10}")
print("-" * 65)

bit_widths = [2, 3, 4, 6, 8, 16]
results = {}

for bits in bit_widths:
    w_hat, delta = uniform_quantize(weights, bits)
    mse = np.mean((weights - w_hat)**2)
    signal_power = np.var(weights)
    snr_db = 10 * np.log10(signal_power / mse) if mse > 0 else float('inf')
    max_err = np.max(np.abs(weights - w_hat))
    results[bits] = {"mse": mse, "snr": snr_db, "delta": delta, "w_hat": w_hat}
    print(f"{bits:>8} | {delta:>10.6f} | {mse:>12.8f} | {snr_db:>10.2f} | {max_err:>10.6f}")

print("=" * 65)
print(f"\nì´ë¡ ì  SNR ì¦ê°€: ë¹„íŠ¸ë‹¹ ì•½ 6.02 dB")
for i in range(1, len(bit_widths)):
    b1, b2 = bit_widths[i-1], bit_widths[i]
    snr_diff = results[b2]["snr"] - results[b1]["snr"]
    per_bit = snr_diff / (b2 - b1)
    print(f"  {b1}â†’{b2}ë¹„íŠ¸: {snr_diff:.2f} dB ì¦ê°€ ({per_bit:.2f} dB/bit)")"""))

# â”€â”€ Cell 7: Section 3 - Quantization Error Visualization â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ ì–‘ìí™” ì˜¤ì°¨ vs ë¹„íŠ¸í­ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)
weights = np.random.randn(5000) * 0.5

test_bits = list(range(1, 17))
mse_list = []
snr_list = []
theoretical_snr = []
signal_power = np.var(weights)

for b in test_bits:
    w_hat, delta = uniform_quantize(weights, b)
    mse = np.mean((weights - w_hat)**2)
    snr_db = 10 * np.log10(signal_power / mse) if mse > 0 else 100
    mse_list.append(mse)
    snr_list.append(snr_db)
    theoretical_snr.append(6.02 * b + 10 * np.log10(12 * signal_power / (np.max(weights) - np.min(weights))**2))

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# ì™¼ìª½: MSE vs ë¹„íŠ¸í­
ax1 = axes[0]
ax1.semilogy(test_bits, mse_list, 'b-o', lw=2, ms=6, label='ì‹¤ì¸¡ MSE')
ax1.set_xlabel('ë¹„íŠ¸í­ (bits)', fontsize=11)
ax1.set_ylabel('MSE (log scale)', fontsize=11)
ax1.set_title('ì–‘ìí™” MSE vs ë¹„íŠ¸í­', fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=9)
for b_mark in [4, 8]:
    idx = test_bits.index(b_mark)
    ax1.axvline(x=b_mark, color='red', ls='--', lw=1, alpha=0.5)
    ax1.annotate(f'INT{b_mark}\nMSE={mse_list[idx]:.2e}',
                 xy=(b_mark, mse_list[idx]), xytext=(b_mark+1, mse_list[idx]*5),
                 fontsize=8, arrowprops=dict(arrowstyle='->', color='red'))

# ì˜¤ë¥¸ìª½: SNR vs ë¹„íŠ¸í­
ax2 = axes[1]
ax2.plot(test_bits, snr_list, 'r-o', lw=2, ms=6, label='ì‹¤ì¸¡ SNR')
ax2.plot(test_bits, theoretical_snr, 'g--', lw=2, label='ì´ë¡  SNR (6.02b + C)')
ax2.set_xlabel('ë¹„íŠ¸í­ (bits)', fontsize=11)
ax2.set_ylabel('SNR (dB)', fontsize=11)
ax2.set_title('SNR vs ë¹„íŠ¸í­ (ì´ë¡  vs ì‹¤ì¸¡)', fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=9)
ax2.axhline(y=snr_list[test_bits.index(4)], color='gray', ls=':', lw=1, alpha=0.5)
ax2.axhline(y=snr_list[test_bits.index(8)], color='gray', ls=':', lw=1, alpha=0.5)

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/quantization_error_vs_bits.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/quantization_error_vs_bits.png")
print(f"\nINT4 SNR: {snr_list[test_bits.index(4)]:.2f} dB")
print(f"INT8 SNR: {snr_list[test_bits.index(8)]:.2f} dB")
print(f"ì°¨ì´: {snr_list[test_bits.index(8)] - snr_list[test_bits.index(4)]:.2f} dB "
      f"(ì´ë¡ : {6.02*4:.2f} dB)")"""))

# â”€â”€ Cell 8: Section 4 Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""## 4. AWQ í•µì‹¬ ì±„ë„ íƒì§€ ì‹œë®¬ë ˆì´ì…˜ <a name='4.-AWQ-í•µì‹¬-ì±„ë„'></a>

### AWQì˜ í•µì‹¬ ê´€ì°°

ëª¨ë“  ê°€ì¤‘ì¹˜ ì±„ë„ì´ ë™ë“±í•˜ê²Œ ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤:
- ì†Œìˆ˜(1~5%)ì˜ ì±„ë„ì´ **í™œì„±í™” í¬ê¸°ê°€ ê·¹ë‹¨ì ìœ¼ë¡œ í°** "salient channel"
- ì´ ì±„ë„ì„ ì–‘ìí™”í•˜ë©´ ì¶œë ¥ ì˜¤ì°¨ê°€ ê¸‰ê²©íˆ ì¦ê°€

### ìŠ¤ì¼€ì¼ íŒ©í„°ì˜ ì—­í• 

$$W'_j = W_j \cdot S_j \quad \rightarrow \quad \text{ì–‘ìí™”} \quad \rightarrow \quad \hat{W}'_j$$
$$\text{ì¶œë ¥} = \hat{W}'_j \cdot (X_j / S_j)$$

$S_j > 1$ì´ë©´ ê°€ì¤‘ì¹˜ê°€ ì»¤ì ¸ì„œ ì–‘ìí™” í•´ìƒë„(ë ˆë²¨ ìˆ˜ ëŒ€ë¹„ ìœ íš¨ ë²”ìœ„)ê°€ í–¥ìƒë©ë‹ˆë‹¤."""))

# â”€â”€ Cell 9: AWQ Salient Channel Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ AWQ í•µì‹¬ ì±„ë„(Salient Channel) íƒì§€ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Llama-like ê°€ì¤‘ì¹˜ì—ì„œ í™œì„±í™” ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš” ì±„ë„ì„ ì‹ë³„í•©ë‹ˆë‹¤

np.random.seed(42)

hidden_dim = 256
seq_len = 128
out_dim = 256

W = np.random.randn(out_dim, hidden_dim) * 0.02
X = np.random.randn(seq_len, hidden_dim) * 0.5

# ì¼ë¶€ ì±„ë„ì— í° í™œì„±í™”(salient channel) ìƒì„±
salient_channels = [10, 42, 100, 180, 220]
for ch in salient_channels:
    X[:, ch] *= 15.0

activation_magnitude = np.max(np.abs(X), axis=0)
weight_magnitude = np.max(np.abs(W), axis=1)[:hidden_dim]

# ìƒìœ„ 5% ì±„ë„ì„ salientë¡œ ì‹ë³„
threshold = np.percentile(activation_magnitude, 95)
detected_salient = np.where(activation_magnitude > threshold)[0]

print("=== AWQ Salient Channel íƒì§€ ===\n")
print(f"íˆë“  ì°¨ì›: {hidden_dim}")
print(f"í™œì„±í™” í¬ê¸° ê¸°ì¤€ ìƒìœ„ 5% ì„ê³„ê°’: {threshold:.4f}")
print(f"íƒì§€ëœ salient ì±„ë„ ìˆ˜: {len(detected_salient)}")
print(f"íƒì§€ëœ ì±„ë„: {detected_salient.tolist()}")
print(f"ì‹¤ì œ salient ì±„ë„: {salient_channels}")
print(f"ì •í™•ë„: {len(set(detected_salient) & set(salient_channels))}/{len(salient_channels)}")

# AWQ ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚°
alpha = 0.5
S = np.ones(hidden_dim)
for j in range(hidden_dim):
    act_mag = activation_magnitude[j]
    w_mag = np.max(np.abs(W[:, j]))
    if w_mag > 0:
        S[j] = (act_mag / w_mag) ** alpha

# ìŠ¤ì¼€ì¼ ì ìš© ì „í›„ ì–‘ìí™” ì˜¤ì°¨ ë¹„êµ
Y_original = X @ W.T

# ì–‘ìí™” ì—†ì´ (ê¸°ì¤€)
Y_fp = Y_original

# ì¼ë°˜ INT4 ì–‘ìí™”
W_q_naive, _ = uniform_quantize(W, 4)
Y_naive = X @ W_q_naive.T
mse_naive = np.mean((Y_fp - Y_naive)**2)

# AWQ INT4 ì–‘ìí™”
W_scaled = W * S[np.newaxis, :]
W_scaled_q, _ = uniform_quantize(W_scaled, 4)
X_descaled = X / S[np.newaxis, :]
Y_awq = X_descaled @ W_scaled_q.T
mse_awq = np.mean((Y_fp - Y_awq)**2)

print(f"\n{'ë°©ë²•':<25} | {'MSE':>15} | {'ìƒëŒ€ ì˜¤ì°¨':>12}")
print("-" * 58)
print(f"{'Naive INT4'::<25} | {mse_naive:>15.8f} | {1.0:>12.4f}x")
print(f"{'AWQ INT4 (Î±=0.5)'::<25} | {mse_awq:>15.8f} | {mse_awq/mse_naive:>12.4f}x")
print(f"\nAWQ ì˜¤ì°¨ ê°ì†Œ: {(1 - mse_awq/mse_naive)*100:.1f}%")"""))

# â”€â”€ Cell 10: AWQ Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ AWQ ì±„ë„ë³„ í™œì„±í™”/ìŠ¤ì¼€ì¼ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))

# 1. ì±„ë„ë³„ í™œì„±í™” í¬ê¸°
ax1 = axes[0]
ax1.bar(range(hidden_dim), activation_magnitude, color='steelblue', alpha=0.6, width=1.0)
for ch in salient_channels:
    ax1.bar(ch, activation_magnitude[ch], color='red', alpha=0.9, width=2.0)
ax1.axhline(y=threshold, color='orange', ls='--', lw=2, label=f'95th percentile = {threshold:.1f}')
ax1.set_xlabel('ì±„ë„ ì¸ë±ìŠ¤', fontsize=11)
ax1.set_ylabel('ìµœëŒ€ í™œì„±í™” í¬ê¸°', fontsize=11)
ax1.set_title('ì±„ë„ë³„ í™œì„±í™” í¬ê¸° (ë¹¨ê°„ìƒ‰=salient)', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3, axis='y')

# 2. AWQ ìŠ¤ì¼€ì¼ íŒ©í„°
ax2 = axes[1]
ax2.bar(range(hidden_dim), S, color='forestgreen', alpha=0.6, width=1.0)
for ch in salient_channels:
    ax2.bar(ch, S[ch], color='red', alpha=0.9, width=2.0)
ax2.set_xlabel('ì±„ë„ ì¸ë±ìŠ¤', fontsize=11)
ax2.set_ylabel('ìŠ¤ì¼€ì¼ íŒ©í„° S', fontsize=11)
ax2.set_title('AWQ ìŠ¤ì¼€ì¼ íŒ©í„° ë¶„í¬', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# 3. ì–‘ìí™” ì˜¤ì°¨ ë¹„êµ
ax3 = axes[2]
methods = ['Naive\nINT4', 'AWQ\nINT4']
mses = [mse_naive, mse_awq]
colors = ['salmon', 'lightgreen']
bars = ax3.bar(methods, mses, color=colors, edgecolor='black', width=0.5)
ax3.set_ylabel('MSE', fontsize=11)
ax3.set_title('ì–‘ìí™” ë°©ë²•ë³„ ì¶œë ¥ MSE', fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')
for bar, mse_val in zip(bars, mses):
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.02,
             f'{mse_val:.6f}', ha='center', fontsize=9, fontweight='bold')

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/awq_channel_analysis.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/awq_channel_analysis.png")
print(f"\nSalient ì±„ë„ì˜ í‰ê·  ìŠ¤ì¼€ì¼: {np.mean([S[ch] for ch in salient_channels]):.2f}")
print(f"ì¼ë°˜ ì±„ë„ì˜ í‰ê·  ìŠ¤ì¼€ì¼: {np.mean(np.delete(S, salient_channels)):.2f}")"""))

# â”€â”€ Cell 11: Section 5 Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""## 5. W4A16 / INT8 / FP8 ë¹„êµ ë²¤ì¹˜ë§ˆí¬ <a name='5.-í¬ë§·-ë¹„êµ'></a>

### ì–‘ìí™” í¬ë§· ë¹„êµ

| í¬ë§· | ê°€ì¤‘ì¹˜ | í™œì„±í™” | ë©”ëª¨ë¦¬(7B ê¸°ì¤€) | íŠ¹ì§• |
|------|--------|--------|----------------|------|
| FP16 | 16-bit | 16-bit | ~14 GB | ê¸°ì¤€ì„  |
| W8A8 (INT8) | 8-bit | 8-bit | ~7 GB | GPTQ/SmoothQuant |
| W4A16 | 4-bit | 16-bit | ~3.5 GB | GPTQ/AWQ, í™œì„±í™” ìœ ì§€ |
| FP8 (E4M3) | 8-bit | 8-bit | ~7 GB | H100 ë„¤ì´í‹°ë¸Œ, ë†’ì€ ì •ë°€ë„ |
| W4A8 | 4-bit | 8-bit | ~3.5 GB | QoQ ë“± ìµœì‹  ê¸°ë²• |"""))

# â”€â”€ Cell 12: Format Comparison Benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ W4A16 / INT8 / FP8 ë¹„êµ ë²¤ì¹˜ë§ˆí¬ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¤ì–‘í•œ ì–‘ìí™” í¬ë§·ì˜ ë©”ëª¨ë¦¬, ì†ë„, ì •ë°€ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤

np.random.seed(42)

# ì‹œë®¬ë ˆì´ì…˜ ê¸°ë°˜ ë²¤ì¹˜ë§ˆí¬ (Llama 3 8B ê¸°ì¤€)
model_params_B = 8.03
formats = {
    "FP16": {"w_bits": 16, "a_bits": 16, "memory_gb": model_params_B * 2,
             "relative_speed": 1.0, "ppl_degradation": 0.0},
    "FP8 (E4M3)": {"w_bits": 8, "a_bits": 8, "memory_gb": model_params_B * 1,
                    "relative_speed": 1.8, "ppl_degradation": 0.05},
    "INT8 (W8A8)": {"w_bits": 8, "a_bits": 8, "memory_gb": model_params_B * 1,
                     "relative_speed": 1.7, "ppl_degradation": 0.1},
    "W4A16 (GPTQ)": {"w_bits": 4, "a_bits": 16, "memory_gb": model_params_B * 0.5,
                      "relative_speed": 2.2, "ppl_degradation": 0.3},
    "W4A16 (AWQ)": {"w_bits": 4, "a_bits": 16, "memory_gb": model_params_B * 0.5,
                     "relative_speed": 2.3, "ppl_degradation": 0.15},
}

print("=" * 85)
print(f"{'í¬ë§·':<18} | {'Wë¹„íŠ¸':>5} | {'Aë¹„íŠ¸':>5} | {'ë©”ëª¨ë¦¬(GB)':>10} | "
      f"{'ìƒëŒ€ì†ë„':>8} | {'PPL ì—´í™”':>8}")
print("-" * 85)
for name, f in formats.items():
    print(f"{name:<18} | {f['w_bits']:>5} | {f['a_bits']:>5} | "
          f"{f['memory_gb']:>10.2f} | {f['relative_speed']:>7.1f}x | "
          f"+{f['ppl_degradation']:>6.2f}")
print("=" * 85)

# ì‹¤ì œ ì–‘ìí™” ì •ë°€ë„ ì‹œë®¬ë ˆì´ì…˜
weight_matrix = np.random.randn(512, 512) * 0.02
calibration_data = np.random.randn(64, 512) * 0.5

bit_configs = [
    ("FP16", 16),
    ("FP8", 8),
    ("INT8", 8),
    ("INT4 (GPTQ)", 4),
    ("INT4 (AWQ)", 4),
]

print(f"\n{'í¬ë§·':<18} | {'ì¶œë ¥ MSE':>15} | {'Cosine Sim':>12} | {'Max Error':>10}")
print("-" * 65)

Y_ref = calibration_data @ weight_matrix.T

for name, bits in bit_configs:
    if "AWQ" in name:
        act_mag = np.max(np.abs(calibration_data), axis=0)
        w_mag = np.max(np.abs(weight_matrix), axis=0)
        S = np.where(w_mag > 0, (act_mag / np.maximum(w_mag, 1e-8))**0.5, 1.0)
        W_s = weight_matrix * S[np.newaxis, :]
        W_q, _ = uniform_quantize(W_s, bits)
        X_d = calibration_data / S[np.newaxis, :]
        Y_q = X_d @ W_q.T
    else:
        W_q, _ = uniform_quantize(weight_matrix, bits)
        Y_q = calibration_data @ W_q.T

    mse = np.mean((Y_ref - Y_q)**2)
    cos_sim = np.mean([
        np.dot(Y_ref[i], Y_q[i]) / (np.linalg.norm(Y_ref[i]) * np.linalg.norm(Y_q[i]) + 1e-10)
        for i in range(len(Y_ref))
    ])
    max_err = np.max(np.abs(Y_ref - Y_q))
    print(f"{name:<18} | {mse:>15.10f} | {cos_sim:>12.8f} | {max_err:>10.6f}")

print("-" * 65)"""))

# â”€â”€ Cell 13: Benchmark Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code(r"""# â”€â”€ ì–‘ìí™” í¬ë§· ì¢…í•© ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

names = list(formats.keys())
memories = [formats[n]["memory_gb"] for n in names]
speeds = [formats[n]["relative_speed"] for n in names]
ppls = [formats[n]["ppl_degradation"] for n in names]
colors_bar = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974']

# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
ax1 = axes[0]
bars1 = ax1.barh(range(len(names)), memories, color=colors_bar, edgecolor='black', height=0.6)
ax1.set_xlabel('ë©”ëª¨ë¦¬ (GB)', fontsize=11)
ax1.set_title('ëª¨ë¸ ë©”ëª¨ë¦¬ (Llama 3 8B)', fontweight='bold')
ax1.set_yticks(range(len(names)))
ax1.set_yticklabels(names, fontsize=9)
ax1.grid(True, alpha=0.3, axis='x')
for i, (bar, mem) in enumerate(zip(bars1, memories)):
    ax1.text(mem + 0.2, i, f'{mem:.1f} GB', va='center', fontsize=9)

# 2. ìƒëŒ€ ì¶”ë¡  ì†ë„
ax2 = axes[1]
bars2 = ax2.barh(range(len(names)), speeds, color=colors_bar, edgecolor='black', height=0.6)
ax2.set_xlabel('ìƒëŒ€ ì†ë„ (FP16 = 1.0x)', fontsize=11)
ax2.set_title('ì¶”ë¡  ì†ë„ ë¹„êµ', fontweight='bold')
ax2.set_yticks(range(len(names)))
ax2.set_yticklabels(names, fontsize=9)
ax2.grid(True, alpha=0.3, axis='x')
for i, (bar, spd) in enumerate(zip(bars2, speeds)):
    ax2.text(spd + 0.02, i, f'{spd:.1f}x', va='center', fontsize=9)

# 3. ì •ë°€ë„ ì—´í™”
ax3 = axes[2]
bars3 = ax3.barh(range(len(names)), ppls, color=colors_bar, edgecolor='black', height=0.6)
ax3.set_xlabel('Perplexity ì—´í™” (+)', fontsize=11)
ax3.set_title('ì •ë°€ë„ ì—´í™” (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontweight='bold')
ax3.set_yticks(range(len(names)))
ax3.set_yticklabels(names, fontsize=9)
ax3.grid(True, alpha=0.3, axis='x')
for i, (bar, ppl) in enumerate(zip(bars3, ppls)):
    ax3.text(ppl + 0.005, i, f'+{ppl:.2f}', va='center', fontsize=9)

plt.tight_layout()
plt.savefig('chapter14_extreme_inference/quantization_format_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter14_extreme_inference/quantization_format_comparison.png")
print(f"\nìµœì  ê· í˜•: AWQ W4A16 â€” ë©”ëª¨ë¦¬ {formats['W4A16 (AWQ)']['memory_gb']:.1f}GB, "
      f"ì†ë„ {formats['W4A16 (AWQ)']['relative_speed']:.1f}x, "
      f"PPL ì—´í™” +{formats['W4A16 (AWQ)']['ppl_degradation']:.2f}")
print("AWQëŠ” GPTQ ëŒ€ë¹„ PPL ì—´í™”ê°€ ì ˆë°˜ ìˆ˜ì¤€ìœ¼ë¡œ ì‹¤ìš©ì  ê´€ì ì—ì„œ ìš°ìˆ˜")"""))

# â”€â”€ Cell 14: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| ê· ì¼ ì–‘ìí™” | $\Delta = (x_{max}-x_{min})/(2^b-1)$ | â­â­ |
| SNRê³¼ ë¹„íŠ¸í­ | $\text{SNR} \propto 2^{2b}$, ë¹„íŠ¸ë‹¹ 6dB | â­â­â­ |
| GPTQ | Hessian 2ì°¨ ìµœì í™”ë¡œ ì—´ë³„ ì˜¤ì°¨ ìµœì†Œí™” | â­â­â­ |
| AWQ | í™œì„±í™” ê¸°ë°˜ ì±„ë„ ìŠ¤ì¼€ì¼ë§ìœ¼ë¡œ ì¤‘ìš” ê°€ì¤‘ì¹˜ ë³´í˜¸ | â­â­â­ |
| W4A16 | ê°€ì¤‘ì¹˜ 4ë¹„íŠ¸, í™œì„±í™” 16ë¹„íŠ¸ â€” ì‹¤ìš©ì  ìµœì ì  | â­â­â­ |
| FP8 (E4M3) | H100 ë„¤ì´í‹°ë¸Œ, í›ˆë ¨/ì¶”ë¡  ëª¨ë‘ ì ìš© ê°€ëŠ¥ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\Delta = \frac{x_{max} - x_{min}}{2^b - 1}, \quad \text{SNR}_{dB} = 6.02b + C$$

$$\text{GPTQ}: \min_{\hat{W}} \|WX - \hat{W}X\|_F^2 \quad \text{(Hessian ê¸°ë°˜ ì—´ë³„ ìµœì í™”)}$$

$$\text{AWQ}: S_j = \left(\frac{\max|X_j|}{\max|W_j|}\right)^\alpha \quad \text{(í™œì„±í™” ê¸°ë°˜ ìŠ¤ì¼€ì¼)}$$

### ì°¸ê³  ë…¼ë¬¸
- Frantar et al., "GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers" (arxiv 2210.17323)
- Lin et al., "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration" (arxiv 2306.00978)

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ (RLHF/DPO)** â€” Policy Gradientì—ì„œ PPO-Clipê¹Œì§€ ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³ , RLHF íŒŒì´í”„ë¼ì¸ê³¼ DPOì˜ ìˆ˜í•™ì  ë“±ê°€ì„±ì„ ì¦ëª…í•©ë‹ˆë‹¤."""))

path = '/workspace/chapter14_extreme_inference/05_quantization_gptq_awq.ipynb'
create_notebook(cells, path)
