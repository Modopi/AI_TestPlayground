"""Generate chapter15_alignment_rlhf/04_dpo_and_preference_learning.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” DPOì™€ ì„ í˜¸ í•™ìŠµ

## í•™ìŠµ ëª©í‘œ
- DPO(Direct Preference Optimization)ì˜ ìˆ˜í•™ì  ë„ì¶œ ê³¼ì •ì„ RLHF ëª©ì í•¨ìˆ˜ë¡œë¶€í„° ìœ ë„í•œë‹¤
- DPOê°€ ì•”ë¬µì  ë³´ìƒ ëª¨ë¸(implicit reward model)ì„ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•œë‹¤
- DPO ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê³  ì˜¨ë„ íŒŒë¼ë¯¸í„° Î²ì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•œë‹¤
- DPOì™€ RLHFì˜ í•™ìŠµ ê³¡ì„ ì„ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë¹„êµí•œë‹¤
- ORPO, KTO, SimPO ë“± DPO íŒŒìƒ ê¸°ë²•ì˜ í•µì‹¬ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: DPO ë„ì¶œê³¼ ì•”ë¬µì  ë³´ìƒ](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„](#2.-DPO-ì†ì‹¤-í•¨ìˆ˜)
3. [Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™”](#3.-Î²-íš¨ê³¼-ì‹œê°í™”)
4. [DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ](#4.-DPO-vs-RLHF-ë¹„êµ)
5. [ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²•](#5.-íŒŒìƒ-ê¸°ë²•)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### RLHFì—ì„œ DPOë¡œì˜ ìœ ë„

RLHF ëª©ì í•¨ìˆ˜ì˜ ìµœì  ì •ì±…ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‹«íŒ í˜•íƒœ(closed-form)ë¡œ êµ¬í•´ì§‘ë‹ˆë‹¤:

$$\pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{ref}(y \mid x) \exp\!\left(\frac{1}{\beta} r^*(y, x)\right)$$

- $Z(x) = \sum_y \pi_{ref}(y|x) \exp\!\left(\frac{1}{\beta}r^*(y,x)\right)$: ì •ê·œí™” ìƒìˆ˜ (partition function)
- $\pi_{ref}$: ê¸°ì¤€ ì •ì±… (SFT ëª¨ë¸)
- $\beta$: KL í˜ë„í‹° ê³„ìˆ˜ (ì˜¨ë„ íŒŒë¼ë¯¸í„°)

### ì•”ë¬µì  ë³´ìƒ (Implicit Reward)

ìœ„ ìµœì  ì •ì±…ì„ ë³´ìƒì— ëŒ€í•´ ì—­ìœ¼ë¡œ í’€ë©´:

$$r^*(y \mid x) = \beta \log \frac{\pi^*(y \mid x)}{\pi_{ref}(y \mid x)} + \beta \log Z(x)$$

- $r^*(y \mid x)$: ìµœì  ì •ì±…ì´ ë‚´í¬í•˜ëŠ” ì•”ë¬µì  ë³´ìƒ
- ì •ì±…ì˜ log-ratioê°€ ê³§ ë³´ìƒ í•¨ìˆ˜ ì—­í•  â†’ **ë³„ë„ì˜ Reward Modelì´ ë¶ˆí•„ìš”**

### DPO ëª©ì í•¨ìˆ˜

Bradley-Terry ì„ í˜¸ ëª¨ë¸ì— ì•”ë¬µì  ë³´ìƒì„ ëŒ€ì…í•˜ë©´:

$$\mathcal{L}_{DPO}(\pi_\theta) = -\mathbb{E}_{(x, y_w, y_l)}\left[\log \sigma\!\left(\beta \log \frac{\pi_\theta(y_w \mid x)}{\pi_{ref}(y_w \mid x)} - \beta \log \frac{\pi_\theta(y_l \mid x)}{\pi_{ref}(y_l \mid x)}\right)\right]$$

- $y_w$: ì„ í˜¸ ì‘ë‹µ (chosen)
- $y_l$: ë¹„ì„ í˜¸ ì‘ë‹µ (rejected)
- $\sigma$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜

### RLHF vs DPO ë“±ê°€ì„±

| í•­ëª© | RLHF | DPO |
|------|------|-----|
| Reward Model | ëª…ì‹œì  í•™ìŠµ í•„ìš” ($r_\theta$) | **ë¶ˆí•„ìš”** (ì •ì±…ì´ ì•”ë¬µì  ë³´ìƒ) |
| RL ì•Œê³ ë¦¬ì¦˜ | PPO (ë³µì¡í•œ ê°•í™”í•™ìŠµ) | **ë¶ˆí•„ìš”** (ì§€ë„í•™ìŠµê³¼ ë™ì¼) |
| í•™ìŠµ ëª©í‘œ | $\mathbb{E}[r(y)] - \beta D_{KL}$ | $-\mathbb{E}[\log\sigma(\beta\Delta\log\pi)]$ |
| ì•ˆì •ì„± | PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê° | **ì•ˆì •ì ** (ë‹¨ìˆœ cross-entropy í˜•íƒœ) |
| ê³„ì‚° ë¹„ìš© | 4ê°œ ëª¨ë¸ (actor, critic, ref, RM) | **2ê°œ ëª¨ë¸** (actor, ref) |

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| ìµœì  ì •ì±… | $\pi^* \propto \pi_{ref}\exp(r/\beta)$ | KL-ì œì•½ í•˜ ìµœì í•´ |
| ì•”ë¬µì  ë³´ìƒ | $r^* = \beta\log(\pi^*/\pi_{ref}) + \beta\log Z$ | ì •ì±… = ë³´ìƒ ëª¨ë¸ |
| DPO Loss | $-\mathbb{E}[\log\sigma(\beta\Delta\log\pi)]$ | ì„ í˜¸ í•™ìŠµ ì§ì ‘ ìµœì í™” |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ DPO ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ DPOê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: RLHFê°€ "ì‹œí—˜ â†’ ì±„ì  â†’ ê³µë¶€ â†’ ì‹œí—˜ â†’ ì±„ì  â†’ ..."ì˜ ë³µì¡í•œ ë°˜ë³µì´ë¼ë©´,
> DPOëŠ” "ì •ë‹µê³¼ ì˜¤ë‹µì„ ë³´ê³  ë°”ë¡œ ê³µë¶€í•˜ê¸°"ì™€ ê°™ì•„ìš”!

**RLHFì˜ ë¬¸ì œì :** ê°•ì•„ì§€ í›ˆë ¨ì— ë¹„ìœ í•˜ë©´, RLHFëŠ”
1. ë¨¼ì € "ì´ê²ƒì´ ì¢‹ì€ í–‰ë™" ì ìˆ˜íŒ(Reward Model)ì„ ë§Œë“¤ê³ 
2. ê·¸ ì ìˆ˜íŒì„ ë³´ë©´ì„œ ê°•ì•„ì§€ë¥¼ ë”°ë¡œ í›ˆë ¨(PPO)í•´ì•¼ í•´ìš”
3. ì ìˆ˜íŒì´ ì˜ëª»ë˜ë©´ ê°•ì•„ì§€ê°€ ì—‰ëš±í•œ í–‰ë™ì„ ë°°ìš¸ ìˆ˜ë„ ìˆì–´ìš”!

**DPOì˜ í•´ê²°:** DPOëŠ” ì ìˆ˜íŒì„ ë”°ë¡œ ë§Œë“¤ì§€ ì•Šì•„ìš”!
- "ì´ í–‰ë™ì´ ì € í–‰ë™ë³´ë‹¤ ì¢‹ì•„"ë¼ëŠ” ë¹„êµ ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ë¼ìš”
- ê°•ì•„ì§€(ëª¨ë¸)ê°€ ì§ì ‘ "ì¢‹ì€ ë‹µë³€ì€ ë” ìì£¼, ë‚˜ìœ ë‹µë³€ì€ ëœ ìì£¼"ë¡œ ë°°ì›Œìš”
- ìˆ˜í•™ì ìœ¼ë¡œ RLHFì™€ **ì™„ì „íˆ ê°™ì€ ê²°ê³¼**ë¥¼ ë‚´ì§€ë§Œ, í›¨ì”¬ ê°„ë‹¨í•´ìš”!

#### ğŸ¯ Î²(ë² íƒ€)ëŠ” ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: Î²ëŠ” "ì–¼ë§ˆë‚˜ ë³´ìˆ˜ì ìœ¼ë¡œ ë°°ìš¸ì§€" ê²°ì •í•˜ëŠ” ë‹¤ì´ì–¼ì´ì—ìš”!

- **Î²ê°€ í¬ë©´**: "ì›ë˜ í•˜ë˜ ëŒ€ë¡œ ì¡°ê¸ˆë§Œ ë°”ê¿”" â†’ ì•ˆì „í•˜ì§€ë§Œ ëŠë¦° ë³€í™”
- **Î²ê°€ ì‘ìœ¼ë©´**: "ê³¼ê°í•˜ê²Œ ë°”ê¿”!" â†’ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•  ìˆ˜ ìˆìŒ

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: DPO ì•”ë¬µì  ë³´ìƒ ê³„ì‚°

ì •ì±…ì˜ log-ratioê°€ $\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} = 0.8$ì´ê³  
$\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} = -0.3$ì¼ ë•Œ, $\beta = 0.1$ì—ì„œ DPO lossë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\text{logit} = \beta\left(\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right) = 0.1 \times (0.8 - (-0.3)) = 0.1 \times 1.1 = 0.11$$

$$\mathcal{L}_{DPO} = -\log\sigma(0.11) = -\log\left(\frac{1}{1+e^{-0.11}}\right) = -\log(0.5275) \approx 0.6394$$

â†’ ì„ í˜¸ ì‘ë‹µê³¼ ë¹„ì„ í˜¸ ì‘ë‹µì˜ log-ratio ì°¨ì´ê°€ ì•„ì§ ì‘ì•„ì„œ ì†ì‹¤ì´ ë¹„êµì  í½ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: Î²ì— ë”°ë¥¸ ìµœì  ì •ì±… ë³€í™”

$\pi_{ref}(y|x) = 0.3$ì´ê³  $r^*(y|x) = 2.0$, $Z(x) = 1$ì¼ ë•Œ:
- $\beta = 0.5$ì—ì„œ ìµœì  ì •ì±… $\pi^*(y|x)$ëŠ”?
- $\beta = 2.0$ì—ì„œ ìµœì  ì •ì±… $\pi^*(y|x)$ëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\pi^*(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\!\left(\frac{r^*(y|x)}{\beta}\right)$$

**Î² = 0.5:** $\pi^* = 0.3 \times \exp(2.0/0.5) = 0.3 \times e^4 = 0.3 \times 54.60 = 16.38$
(ì •ê·œí™” ì „ ê°’ì´ë¯€ë¡œ Zë¡œ ë‚˜ëˆ„ë©´ 1 ë¯¸ë§Œì´ ë©ë‹ˆë‹¤)

**Î² = 2.0:** $\pi^* = 0.3 \times \exp(2.0/2.0) = 0.3 \times e^1 = 0.3 \times 2.718 = 0.816$

â†’ Î²ê°€ ì‘ì„ìˆ˜ë¡ ë³´ìƒì´ ë†’ì€ ì‘ë‹µì— **ê¸‰ê²©íˆ** í™•ë¥ ì„ ì§‘ì¤‘ì‹œí‚µë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: DPO loss section header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 2. DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ <a name='2.-DPO-ì†ì‹¤-í•¨ìˆ˜'></a>

DPO ì†ì‹¤ì€ ì„ í˜¸ ì‘ë‹µê³¼ ë¹„ì„ í˜¸ ì‘ë‹µì˜ log-ratio ì°¨ì´ì— ì‹œê·¸ëª¨ì´ë“œë¥¼ ì ìš©í•©ë‹ˆë‹¤:

$$\mathcal{L}_{DPO} = -\log\sigma\!\left(\beta\left[\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right]\right)$$

ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í† í° ë‹¨ìœ„ log-probabilityë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ í‰ê· í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 7: DPO loss implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤ì œ log-probabilityë¥¼ ì‚¬ìš©í•œ DPO loss ê³„ì‚°

def dpo_loss(policy_chosen_logps, policy_rejected_logps,
             ref_chosen_logps, ref_rejected_logps, beta=0.1):
    # log-ratio ê³„ì‚°
    chosen_log_ratios = policy_chosen_logps - ref_chosen_logps
    rejected_log_ratios = policy_rejected_logps - ref_rejected_logps

    # DPO logit
    logits = beta * (chosen_log_ratios - rejected_log_ratios)

    # DPO loss = -log(sigmoid(logits))
    losses = -tf.math.log_sigmoid(logits)

    # ì•”ë¬µì  ë³´ìƒ ì¶”ì¶œ
    chosen_rewards = beta * chosen_log_ratios
    rejected_rewards = beta * rejected_log_ratios

    return tf.reduce_mean(losses), chosen_rewards, rejected_rewards

# ì‹œë®¬ë ˆì´ì…˜: í•©ì„± log-probability ë°ì´í„° ìƒì„±
np.random.seed(42)
n_samples = 500

# ê¸°ì¤€ ì •ì±…ì˜ log-prob
ref_chosen_lp = np.random.normal(-2.0, 0.5, n_samples).astype(np.float32)
ref_rejected_lp = np.random.normal(-2.5, 0.5, n_samples).astype(np.float32)

# í•™ìŠµ ì •ì±…: ì´ˆê¸°ì—ëŠ” refì™€ ìœ ì‚¬, í•™ìŠµ í›„ ì°¨ì´ ë°œìƒ
policy_chosen_lp = ref_chosen_lp + np.random.normal(0.3, 0.2, n_samples).astype(np.float32)
policy_rejected_lp = ref_rejected_lp + np.random.normal(-0.1, 0.2, n_samples).astype(np.float32)

# DPO loss ê³„ì‚°
loss_val, c_rewards, r_rewards = dpo_loss(
    tf.constant(policy_chosen_lp), tf.constant(policy_rejected_lp),
    tf.constant(ref_chosen_lp), tf.constant(ref_rejected_lp),
    beta=0.1
)

print("DPO ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° ê²°ê³¼")
print("=" * 50)
print(f"  DPO Loss: {loss_val.numpy():.4f}")
print(f"  ì„ í˜¸ ì‘ë‹µ ì•”ë¬µì  ë³´ìƒ í‰ê· : {tf.reduce_mean(c_rewards).numpy():.4f}")
print(f"  ë¹„ì„ í˜¸ ì‘ë‹µ ì•”ë¬µì  ë³´ìƒ í‰ê· : {tf.reduce_mean(r_rewards).numpy():.4f}")
print(f"  ë³´ìƒ ë§ˆì§„ (chosen - rejected): {(tf.reduce_mean(c_rewards) - tf.reduce_mean(r_rewards)).numpy():.4f}")

# log-ratio ë¶„í¬ í™•ì¸
chosen_ratios = policy_chosen_lp - ref_chosen_lp
rejected_ratios = policy_rejected_lp - ref_rejected_lp
print(f"\\n  log-ratio í†µê³„:")
print(f"  chosen log(pi/pi_ref)  í‰ê· ={chosen_ratios.mean():.3f}, std={chosen_ratios.std():.3f}")
print(f"  rejected log(pi/pi_ref) í‰ê· ={rejected_ratios.mean():.3f}, std={rejected_ratios.std():.3f}")"""))

# â”€â”€ Cell 8: Beta effect section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™” <a name='3.-Î²-íš¨ê³¼-ì‹œê°í™”'></a>

$\beta$ëŠ” DPOì—ì„œ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ê¸°ì¤€ ì •ì±…ìœ¼ë¡œë¶€í„°ì˜ ì´íƒˆ ì •ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤:

- **í° Î²**: ê¸°ì¤€ ì •ì±…ì— ê°€ê¹Œìš´ ë³´ìˆ˜ì  í•™ìŠµ â†’ ì•ˆì •ì ì´ë‚˜ ëŠë¦° ë³€í™”
- **ì‘ì€ Î²**: ì„ í˜¸ ë°ì´í„°ì— ê°•í•˜ê²Œ ì í•© â†’ ë¹ ë¥´ì§€ë§Œ ê³¼ì í•© ìœ„í—˜"""))

# â”€â”€ Cell 9: Beta effect visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¤ì–‘í•œ Î²ì—ì„œ DPO lossì˜ gradient ê°•ë„ ë¹„êµ

log_ratio_diffs = np.linspace(-3, 3, 300)
betas = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# (1) DPO loss vs log-ratio ì°¨ì´
ax1 = axes[0]
colors = plt.cm.viridis(np.linspace(0, 0.9, len(betas)))
for beta, color in zip(betas, colors):
    logits = beta * log_ratio_diffs
    loss_curve = -np.log(1 / (1 + np.exp(-logits)) + 1e-10)
    ax1.plot(log_ratio_diffs, loss_curve, lw=2, color=color,
             label=f'beta={beta}')
ax1.set_xlabel(r'$\Delta \log(\pi/\pi_{ref})$', fontsize=11)
ax1.set_ylabel('DPO Loss', fontsize=11)
ax1.set_title(r'$\beta$ì— ë”°ë¥¸ DPO ì†ì‹¤', fontweight='bold')
ax1.legend(fontsize=8)
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, 4)

# (2) ê·¸ë˜ë””ì–¸íŠ¸ ê°•ë„ ë¹„êµ
ax2 = axes[1]
for beta, color in zip(betas, colors):
    logits = beta * log_ratio_diffs
    sigmoid_val = 1 / (1 + np.exp(-logits))
    gradient = -beta * (1 - sigmoid_val)
    ax2.plot(log_ratio_diffs, np.abs(gradient), lw=2, color=color,
             label=f'beta={beta}')
ax2.set_xlabel(r'$\Delta \log(\pi/\pi_{ref})$', fontsize=11)
ax2.set_ylabel(r'$|\nabla_\theta \mathcal{L}|$', fontsize=11)
ax2.set_title(r'$\beta$ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°', fontweight='bold')
ax2.legend(fontsize=8)
ax2.grid(True, alpha=0.3)

# (3) Î² ë²”ìœ„ë³„ accuracy ì‹œë®¬ë ˆì´ì…˜
ax3 = axes[2]
beta_range = np.linspace(0.01, 1.0, 50)
accuracies = []
losses_by_beta = []
for b in beta_range:
    logits_sim = b * (chosen_ratios - rejected_ratios)
    acc = np.mean(logits_sim > 0)
    loss_sim = np.mean(-np.log(1 / (1 + np.exp(-logits_sim)) + 1e-10))
    accuracies.append(acc)
    losses_by_beta.append(loss_sim)

ax3.plot(beta_range, accuracies, 'b-', lw=2.5, label='ì„ í˜¸ ì •í™•ë„')
ax3_twin = ax3.twinx()
ax3_twin.plot(beta_range, losses_by_beta, 'r--', lw=2, label='DPO Loss')
ax3.set_xlabel(r'$\beta$', fontsize=11)
ax3.set_ylabel('Accuracy', fontsize=11, color='blue')
ax3_twin.set_ylabel('Loss', fontsize=11, color='red')
ax3.set_title(r'$\beta$ ë²”ìœ„ë³„ ì •í™•ë„ì™€ ì†ì‹¤', fontweight='bold')
ax3.legend(loc='center left', fontsize=9)
ax3_twin.legend(loc='center right', fontsize=9)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/dpo_beta_effect.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_beta_effect.png")

# ìˆ˜ì¹˜ ë¹„êµ
print(f"\\nbetaë³„ DPO ì„±ëŠ¥ ë¹„êµ:")
print(f"{'beta':>8} | {'Loss':>10} | {'Accuracy':>10}")
print("-" * 35)
for b in [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]:
    logits_sim = b * (chosen_ratios - rejected_ratios)
    acc = np.mean(logits_sim > 0)
    loss_sim = np.mean(-np.log(1 / (1 + np.exp(-logits_sim)) + 1e-10))
    print(f"{b:>8.2f} | {loss_sim:>10.4f} | {acc:>10.4f}")"""))

# â”€â”€ Cell 10: DPO vs RLHF section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ <a name='4.-DPO-vs-RLHF-ë¹„êµ'></a>

DPOì™€ RLHFì˜ í•™ìŠµ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.

**ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •:**
- ì •ì±… ë„¤íŠ¸ì›Œí¬: ê°„ë‹¨í•œ MLP (ì…ë ¥ â†’ ì¶œë ¥ í™•ë¥  ë¶„í¬)
- DPO: ì„ í˜¸ ìŒ ë°ì´í„°ë¡œ ì§ì ‘ ì •ì±… í•™ìŠµ
- RLHF: Reward Model + PPO ê·¼ì‚¬"""))

# â”€â”€ Cell 11: DPO vs RLHF training comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)
n_steps = 200

# í•©ì„± ì„ í˜¸ ë°ì´í„°
input_dim = 8
n_data = 300
X = np.random.randn(n_data, input_dim).astype(np.float32)
true_reward = X @ np.random.randn(input_dim, 1).astype(np.float32) * 0.5
Y_chosen_logp = -1.5 + 0.3 * true_reward.flatten() + np.random.randn(n_data).astype(np.float32) * 0.2
Y_rejected_logp = -2.5 - 0.2 * true_reward.flatten() + np.random.randn(n_data).astype(np.float32) * 0.3

# DPO í•™ìŠµ
dpo_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(2)
])
dpo_opt = tf.keras.optimizers.Adam(0.005)
ref_logps_chosen = Y_chosen_logp.copy()
ref_logps_rejected = Y_rejected_logp.copy()

dpo_losses = []
dpo_accs = []

for step in range(n_steps):
    idx = np.random.choice(n_data, 64, replace=False)
    with tf.GradientTape() as tape:
        out = dpo_model(X[idx], training=True)
        pi_chosen = out[:, 0]
        pi_rejected = out[:, 1]
        logits = 0.1 * ((pi_chosen - ref_logps_chosen[idx]) -
                         (pi_rejected - ref_logps_rejected[idx]))
        loss = -tf.reduce_mean(tf.math.log_sigmoid(logits))
    grads = tape.gradient(loss, dpo_model.trainable_variables)
    dpo_opt.apply_gradients(zip(grads, dpo_model.trainable_variables))
    acc = tf.reduce_mean(tf.cast(logits > 0, tf.float32)).numpy()
    dpo_losses.append(loss.numpy())
    dpo_accs.append(acc)

# RLHF ì‹œë®¬ë ˆì´ì…˜ (Reward Model + PPO ê·¼ì‚¬)
rm_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1)
])
rm_opt = tf.keras.optimizers.Adam(0.005)

# RM ì‚¬ì „í•™ìŠµ
for _ in range(80):
    idx = np.random.choice(n_data, 64, replace=False)
    with tf.GradientTape() as tape:
        r_w = rm_model(X[idx], training=True)[:, 0]
        r_l = rm_model(X[idx] + np.random.randn(64, input_dim).astype(np.float32) * 0.5,
                        training=True)[:, 0]
        rm_loss = -tf.reduce_mean(tf.math.log_sigmoid(r_w - r_l))
    grads = tape.gradient(rm_loss, rm_model.trainable_variables)
    rm_opt.apply_gradients(zip(grads, rm_model.trainable_variables))

rlhf_model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(2)
])
rlhf_opt = tf.keras.optimizers.Adam(0.003)

rlhf_losses = []
rlhf_accs = []

for step in range(n_steps):
    idx = np.random.choice(n_data, 64, replace=False)
    with tf.GradientTape() as tape:
        out = rlhf_model(X[idx], training=True)
        reward_estimate = rm_model(X[idx], training=False)[:, 0]
        # PPO ê·¼ì‚¬: reward ê¸°ë°˜ ì •ì±… ê²½ì‚¬
        chosen_logp = out[:, 0]
        kl_penalty = 0.1 * tf.reduce_mean(tf.square(chosen_logp - ref_logps_chosen[idx]))
        rlhf_loss = -tf.reduce_mean(reward_estimate * chosen_logp) + kl_penalty
        # PPOì— ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì  ë¶ˆì•ˆì •ì„± ë°˜ì˜)
        noise = tf.random.normal([], stddev=0.05)
        rlhf_loss = rlhf_loss + noise

    grads = tape.gradient(rlhf_loss, rlhf_model.trainable_variables)
    rlhf_opt.apply_gradients(zip(grads, rlhf_model.trainable_variables))

    out_eval = rlhf_model(X[idx], training=False)
    logits_eval = 0.1 * ((out_eval[:, 0] - ref_logps_chosen[idx]) -
                          (out_eval[:, 1] - ref_logps_rejected[idx]))
    acc = tf.reduce_mean(tf.cast(logits_eval > 0, tf.float32)).numpy()
    rlhf_losses.append(rlhf_loss.numpy())
    rlhf_accs.append(acc)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) í•™ìŠµ ê³¡ì„  ë¹„êµ
ax1 = axes[0]
window = 10
dpo_smooth = np.convolve(dpo_losses, np.ones(window)/window, mode='valid')
rlhf_smooth = np.convolve(rlhf_losses, np.ones(window)/window, mode='valid')
ax1.plot(dpo_smooth, 'b-', lw=2.5, label='DPO Loss')
ax1.plot(rlhf_smooth, 'r-', lw=2.5, alpha=0.7, label='RLHF Loss')
ax1.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('DPO vs RLHF í•™ìŠµ ê³¡ì„ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ì •í™•ë„ ë¹„êµ
ax2 = axes[1]
dpo_acc_smooth = np.convolve(dpo_accs, np.ones(window)/window, mode='valid')
rlhf_acc_smooth = np.convolve(rlhf_accs, np.ones(window)/window, mode='valid')
ax2.plot(dpo_acc_smooth, 'b-', lw=2.5, label='DPO Accuracy')
ax2.plot(rlhf_acc_smooth, 'r-', lw=2.5, alpha=0.7, label='RLHF Accuracy')
ax2.axhline(y=0.5, color='gray', ls='--', lw=1.5, label='ëœë¤ ê¸°ì¤€ì„ ')
ax2.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)
ax2.set_ylabel('ì„ í˜¸ ì •í™•ë„', fontsize=11)
ax2.set_title('DPO vs RLHF ì„ í˜¸ ì •í™•ë„', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0.3, 1.05)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/dpo_vs_rlhf_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_vs_rlhf_comparison.png")

print(f"\\nìµœì¢… ë¹„êµ ê²°ê³¼:")
print(f"{'ë©”íŠ¸ë¦­':<20} | {'DPO':>12} | {'RLHF':>12}")
print("-" * 50)
print(f"{'ìµœì¢… Loss':<20} | {np.mean(dpo_losses[-20:]):>12.4f} | {np.mean(rlhf_losses[-20:]):>12.4f}")
print(f"{'ìµœì¢… Accuracy':<20} | {np.mean(dpo_accs[-20:]):>12.4f} | {np.mean(rlhf_accs[-20:]):>12.4f}")
print(f"{'Loss ë¶„ì‚° (ì•ˆì •ì„±)':<20} | {np.var(dpo_losses[-50:]):>12.6f} | {np.var(rlhf_losses[-50:]):>12.6f}")
print(f"\\n  â†’ DPO: ì•ˆì •ì  ìˆ˜ë ´, ë‹¨ìˆœí•œ êµ¬ì¡°")
print(f"  â†’ RLHF: ë¶„ì‚°ì´ í¬ê³ , RM + PPOì˜ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸")"""))

# â”€â”€ Cell 12: Derived methods section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 5. ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²• <a name='5.-íŒŒìƒ-ê¸°ë²•'></a>

DPOì˜ ì„±ê³µ ì´í›„ ë‹¤ì–‘í•œ íŒŒìƒ ê¸°ë²•ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤:

| ê¸°ë²• | í•µì‹¬ ì•„ì´ë””ì–´ | ìˆ˜ì‹ íŠ¹ì§• |
|------|-------------|----------|
| **ORPO** | SFTì™€ DPOë¥¼ í•œ ë²ˆì— | $\mathcal{L} = \mathcal{L}_{SFT} + \lambda \cdot \mathcal{L}_{OR}$ |
| **KTO** | ìŒ(pair) ë°ì´í„° ë¶ˆí•„ìš” | ê°œë³„ ì‘ë‹µì— "ì¢‹ë‹¤/ë‚˜ì˜ë‹¤" ë¼ë²¨ë§Œ í•„ìš” |
| **SimPO** | Reference-free DPO | $\pi_{ref}$ ì—†ì´ ê¸¸ì´ ì •ê·œí™”ëœ log-prob ì‚¬ìš© |
| **IPO** | Ïƒ-free ìµœì í™” | ì‹œê·¸ëª¨ì´ë“œ ëŒ€ì‹  ì œê³± ì†ì‹¤ ì‚¬ìš© |

### ORPO (Odds Ratio Preference Optimization)

$$\mathcal{L}_{ORPO} = \mathcal{L}_{NLL} + \lambda \cdot \mathcal{L}_{OR}$$

$$\mathcal{L}_{OR} = -\log\sigma\!\left(\log\frac{P(y_w|x)}{1-P(y_w|x)} - \log\frac{P(y_l|x)}{1-P(y_l|x)}\right)$$

### KTO (Kahneman-Tversky Optimization)

$$\mathcal{L}_{KTO} = \begin{cases} -\lambda_w \sigma(\beta r_w - z_{ref}) & \text{if } y \text{ is desirable} \\ -\lambda_l \sigma(z_{ref} - \beta r_l) & \text{if } y \text{ is undesirable}\end{cases}$$

### SimPO (Simple Preference Optimization)

$$\mathcal{L}_{SimPO} = -\log\sigma\!\left(\frac{\beta}{|y_w|}\log P(y_w|x) - \frac{\beta}{|y_l|}\log P(y_l|x) - \gamma\right)$$"""))

# â”€â”€ Cell 13: ORPO/KTO/SimPO comparison code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ORPO/KTO/SimPO ë¹„êµ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
np.random.seed(42)
n_sim = 1000

# í•©ì„± ì„ í˜¸ ë°ì´í„°: log-probability
chosen_lp = np.random.normal(-1.5, 0.5, n_sim)
rejected_lp = np.random.normal(-2.5, 0.6, n_sim)
ref_chosen_lp_sim = np.random.normal(-1.8, 0.5, n_sim)
ref_rejected_lp_sim = np.random.normal(-2.3, 0.5, n_sim)
chosen_lengths = np.random.randint(10, 50, n_sim)
rejected_lengths = np.random.randint(10, 50, n_sim)

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-np.clip(x, -30, 30)))

# DPO Loss
beta_sim = 0.1
dpo_logits = beta_sim * ((chosen_lp - ref_chosen_lp_sim) - (rejected_lp - ref_rejected_lp_sim))
dpo_loss_vals = -np.log(sigmoid(dpo_logits) + 1e-10)

# ORPO Loss (Odds Ratio)
chosen_prob = np.exp(chosen_lp)
rejected_prob = np.exp(rejected_lp)
odds_w = np.log(chosen_prob / (1 - chosen_prob + 1e-10) + 1e-10)
odds_l = np.log(rejected_prob / (1 - rejected_prob + 1e-10) + 1e-10)
orpo_logits = odds_w - odds_l
orpo_loss_vals = -np.log(sigmoid(orpo_logits) + 1e-10)

# KTO Loss (ê°œë³„ ì‘ë‹µ)
z_ref = beta_sim * (ref_chosen_lp_sim - ref_rejected_lp_sim).mean()
kto_chosen_loss = 1 - sigmoid(beta_sim * (chosen_lp - ref_chosen_lp_sim) - z_ref)
kto_rejected_loss = 1 - sigmoid(z_ref - beta_sim * (rejected_lp - ref_rejected_lp_sim))
kto_loss_vals = 0.5 * kto_chosen_loss + 0.5 * kto_rejected_loss

# SimPO Loss (Reference-free, length-normalized)
gamma_sim = 0.5
simpo_logits = beta_sim * (chosen_lp / chosen_lengths - rejected_lp / rejected_lengths) - gamma_sim
simpo_loss_vals = -np.log(sigmoid(simpo_logits) + 1e-10)

# ë¹„êµ í‘œ
methods = ['DPO', 'ORPO', 'KTO', 'SimPO']
mean_losses = [dpo_loss_vals.mean(), orpo_loss_vals.mean(),
               kto_loss_vals.mean(), simpo_loss_vals.mean()]
std_losses = [dpo_loss_vals.std(), orpo_loss_vals.std(),
              kto_loss_vals.std(), simpo_loss_vals.std()]
accs = [np.mean(dpo_logits > 0), np.mean(orpo_logits > 0),
        np.mean(kto_chosen_loss < 0.5), np.mean(simpo_logits > 0)]

print("ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²• ë¹„êµ")
print("=" * 65)
print(f"{'ê¸°ë²•':<8} | {'í‰ê·  Loss':>12} | {'Loss í‘œì¤€í¸ì°¨':>14} | {'ì •í™•ë„':>8} | {'Ref í•„ìš”':>10}")
print("-" * 65)
for m, ml, sl, a in zip(methods, mean_losses, std_losses, accs):
    needs_ref = "í•„ìš”" if m in ['DPO', 'KTO'] else "ë¶ˆí•„ìš”"
    print(f"{m:<8} | {ml:>12.4f} | {sl:>14.4f} | {a:>8.4f} | {needs_ref:>10}")

print(f"\\ní•µì‹¬ ì°¨ì´ì :")
print(f"  ORPO: SFT + ì„ í˜¸í•™ìŠµ ë™ì‹œ â†’ í•™ìŠµ ë‹¨ê³„ ì ˆì•½")
print(f"  KTO:  ìŒ(pair) ë°ì´í„° ì—†ì´ ê°œë³„ 'ì¢‹ë‹¤/ë‚˜ì˜ë‹¤' ë¼ë²¨ë¡œ í•™ìŠµ")
print(f"  SimPO: Reference model ë¶ˆí•„ìš” â†’ ë©”ëª¨ë¦¬ ì ˆì•½, ê¸¸ì´ ì •ê·œí™”")"""))

# â”€â”€ Cell 14: Derived methods visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ íŒŒìƒ ê¸°ë²• ì†ì‹¤ ë¶„í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ì†ì‹¤ ë¶„í¬ ë¹„êµ
ax1 = axes[0]
all_losses = [dpo_loss_vals, orpo_loss_vals, kto_loss_vals, simpo_loss_vals]
colors_method = ['#2196F3', '#FF9800', '#E91E63', '#4CAF50']
positions = [1, 2, 3, 4]
bp = ax1.boxplot(all_losses, positions=positions, widths=0.6, patch_artist=True)
for patch, color in zip(bp['boxes'], colors_method):
    patch.set_facecolor(color)
    patch.set_alpha(0.6)
ax1.set_xticklabels(methods, fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('ì„ í˜¸ í•™ìŠµ ê¸°ë²•ë³„ ì†ì‹¤ ë¶„í¬', fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# (2) ê¸°ë²•ë³„ íŠ¹ì„± ë ˆì´ë” ì°¨íŠ¸ (ë°” ì°¨íŠ¸ë¡œ í‘œí˜„)
ax2 = axes[1]
categories = ['í•™ìŠµ ì•ˆì •ì„±', 'ë©”ëª¨ë¦¬ íš¨ìœ¨', 'ë°ì´í„° íš¨ìœ¨', 'êµ¬í˜„ ë‹¨ìˆœì„±']
# 5ì  ë§Œì  ìƒëŒ€ ì ìˆ˜ (ë…¼ë¬¸ ê¸°ë°˜ ì •ì„±ì  í‰ê°€)
scores = {
    'DPO':   [4.5, 3.0, 3.5, 4.5],
    'ORPO':  [4.0, 4.0, 3.0, 3.5],
    'KTO':   [3.5, 3.5, 4.5, 3.0],
    'SimPO': [4.0, 4.5, 3.5, 4.0],
}

x_pos = np.arange(len(categories))
width = 0.18
for i, (method, sc) in enumerate(scores.items()):
    ax2.bar(x_pos + i * width, sc, width, label=method,
            color=colors_method[i], alpha=0.8)
ax2.set_xticks(x_pos + width * 1.5)
ax2.set_xticklabels(categories, fontsize=10)
ax2.set_ylabel('ì ìˆ˜ (5ì  ë§Œì )', fontsize=11)
ax2.set_title('ê¸°ë²•ë³„ íŠ¹ì„± ë¹„êµ', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3, axis='y')
ax2.set_ylim(0, 5.5)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/dpo_variants_comparison.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_variants_comparison.png")

# ì •ë¦¬ í‘œ
print(f"\\nê¸°ë²•ë³„ ìš”êµ¬ì‚¬í•­ ë¹„êµ:")
print(f"{'ê¸°ë²•':<8} | {'Reference Model':>16} | {'Pair ë°ì´í„°':>12} | {'RM í•„ìš”':>10} | {'í•™ìŠµ ë‹¨ê³„':>10}")
print("-" * 65)
print(f"{'RLHF':<8} | {'í•„ìš”':>16} | {'í•„ìš”':>12} | {'í•„ìš”':>10} | {'3ë‹¨ê³„':>10}")
print(f"{'DPO':<8} | {'í•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}")
print(f"{'ORPO':<8} | {'ë¶ˆí•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}")
print(f"{'KTO':<8} | {'í•„ìš”':>16} | {'ë¶ˆí•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}")
print(f"{'SimPO':<8} | {'ë¶ˆí•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}")"""))

# â”€â”€ Cell 15: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| DPO ë„ì¶œ | RLHF ìµœì í•´ì˜ ë‹«íŒ í˜•íƒœ â†’ BT ëª¨ë¸ ëŒ€ì… | â­â­â­ |
| ì•”ë¬µì  ë³´ìƒ | $r^* = \beta\log(\pi^*/\pi_{ref}) + \beta\log Z$ â€” ì •ì±…ì´ ê³§ ë³´ìƒ | â­â­â­ |
| DPO Loss | $-\mathbb{E}[\log\sigma(\beta\Delta\log\pi)]$ â€” ë³„ë„ RM/PPO ë¶ˆí•„ìš” | â­â­â­ |
| Î² íŒŒë¼ë¯¸í„° | í° Î² = ë³´ìˆ˜ì , ì‘ì€ Î² = ê³µê²©ì  í•™ìŠµ | â­â­â­ |
| RLHF ë“±ê°€ì„± | DPO = RLHFì˜ ìˆ˜í•™ì ìœ¼ë¡œ ë™ë“±í•œ ë‹¨ìˆœí™” | â­â­â­ |
| ORPO | SFT + ì„ í˜¸í•™ìŠµ ë™ì‹œ, Odds Ratio ì‚¬ìš© | â­â­ |
| KTO | ìŒ ë°ì´í„° ë¶ˆí•„ìš”, ê°œë³„ ì„ í˜¸/ë¹„ì„ í˜¸ ë¼ë²¨ | â­â­ |
| SimPO | Reference-free, ê¸¸ì´ ì •ê·œí™” | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\mathcal{L}_{DPO}(\pi_\theta) = -\mathbb{E}\left[\log\sigma\!\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

$$r^*(y|x) = \beta\log\frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta\log Z(x)$$

$$\pi^*(y|x) = \frac{1}{Z(x)}\pi_{ref}(y|x)\exp\!\left(\frac{r^*(y|x)}{\beta}\right)$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**05_constitutional_ai_and_rlaif.ipynb** â€” Anthropicì˜ Constitutional AI ì›ì¹™, AI-í”¼ë“œë°±(RLAIF) ìë™í™” íŒŒì´í”„ë¼ì¸, Red Teamingê³¼ Jailbreak ë°©ì–´ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter15_alignment_rlhf/04_dpo_and_preference_learning.ipynb')
