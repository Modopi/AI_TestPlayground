"""Generate chapter15_alignment_rlhf/01_rl_fundamentals_mdp_policy.ipynb"""
import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” RL ê¸°ì´ˆì™€ MDP/Policy Gradient

## í•™ìŠµ ëª©í‘œ
- MDP(Markov Decision Process)ì˜ ìˆ˜í•™ì  ì •ì˜ì™€ êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´í•œë‹¤
- Bellman ë°©ì •ì‹ì˜ ìœ ë„ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ì „ê°œí•˜ê³  Value Iterationì„ êµ¬í˜„í•œë‹¤
- REINFORCE(Policy Gradient) ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³  êµ¬í˜„í•œë‹¤
- ë¦¬ì›Œë“œ ì‹ í˜¸(Reward Signal)ê°€ NLP ì •ë ¬ ë¬¸ì œì—ì„œ ì–´ë–»ê²Œ ì„¤ê³„ë˜ëŠ”ì§€ ì´í•´í•œë‹¤
- ê°•í™”í•™ìŠµê³¼ ì§€ë„í•™ìŠµì˜ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ê³ , LLM ì •ë ¬ì—ì„œì˜ RL í•„ìš”ì„±ì„ ì„¤ëª…í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: MDPì™€ Bellman ë°©ì •ì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [GridWorld MDP êµ¬í˜„](#2.-GridWorld-MDP-êµ¬í˜„)
3. [Value Iteration ì‹œê°í™”](#3.-Value-Iteration-ì‹œê°í™”)
4. [REINFORCE Policy Gradient](#4.-REINFORCE-Policy-Gradient)
5. [NLPë¥¼ ìœ„í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„](#5.-NLP-ë¦¬ì›Œë“œ-ì‹ í˜¸-ì„¤ê³„)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math foundations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### MDP (Markov Decision Process)

MDPëŠ” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤:

$$\mathcal{M} = (S, A, P, R, \gamma)$$

- $S$: ìƒíƒœ ê³µê°„ (State space)
- $A$: í–‰ë™ ê³µê°„ (Action space)
- $P(s' | s, a)$: ìƒíƒœ ì „ì´ í™•ë¥  (Transition probability)
- $R(s, a)$: ë³´ìƒ í•¨ìˆ˜ (Reward function)
- $\gamma \in [0, 1)$: í• ì¸ ì¸ì (Discount factor)

### ì •ì±…ê³¼ ê°€ì¹˜ í•¨ìˆ˜

**ì •ì±… (Policy):**

$$\pi(a | s) = P(A_t = a \mid S_t = s)$$

**ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ (State Value Function):**

$$V^\pi(s) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s\right]$$

**í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ (Action Value Function):**

$$Q^\pi(s, a) = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \mid S_t = s, A_t = a\right]$$

### Bellman ë°©ì •ì‹

$$V^\pi(s) = \sum_{a} \pi(a|s) \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')\right]$$

**Bellman ìµœì  ë°©ì •ì‹:**

$$V^*(s) = \max_{a} \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s')\right]$$

### REINFORCE (Policy Gradient)

ëª©ì  í•¨ìˆ˜:

$$J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T} R(s_t, a_t)\right]$$

**Policy Gradient Theorem:**

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[G_t \nabla_\theta \log \pi_\theta(a_t | s_t)\right]$$

- $G_t = \sum_{k=0}^{T-t} \gamma^k R_{t+k+1}$: ëˆ„ì  ë³´ìƒ (Return)
- $\nabla_\theta \log \pi_\theta(a_t|s_t)$: Score function (ë¡œê·¸ ì •ì±…ì˜ ê¸°ìš¸ê¸°)

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| MDP 5-íŠœí”Œ | $(S, A, P, R, \gamma)$ | ìˆœì°¨ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬ |
| Bellman ë°©ì •ì‹ | $V^\pi(s) = \sum_a \pi(a|s)[R + \gamma \sum_{s'} PV^\pi]$ | í˜„ì¬ ê°€ì¹˜ = ì¦‰ê° ë³´ìƒ + ë¯¸ë˜ ê°€ì¹˜ |
| Policy Gradient | $\nabla_\theta J = \mathbb{E}[G_t \nabla_\theta \log \pi_\theta]$ | ë†’ì€ ë³´ìƒ â†’ í•´ë‹¹ í–‰ë™ í™•ë¥  ì¦ê°€ |
| Return | $G_t = \sum_k \gamma^k R_{t+k+1}$ | ë¯¸ë˜ ë³´ìƒì˜ í• ì¸ í•© |

---"""))

# â”€â”€ Cell 3: ğŸ£ friendly explanation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ê°•í™”í•™ìŠµ ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ MDPê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ë³´ë“œê²Œì„ì„ ìƒê°í•´ ë³´ì„¸ìš”! ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ì„œ(í–‰ë™) ì¹¸ì„ ì´ë™í•˜ê³ (ìƒíƒœ ì „ì´), 
> ì–´ë–¤ ì¹¸ì— ë„ì°©í•˜ë©´ ìš©ëˆì„ ë°›ê³ (ë³´ìƒ), ì–´ë–¤ ì¹¸ì— ê°€ë©´ ë²Œê¸ˆì„ ë‚´ìš”(ìŒì˜ ë³´ìƒ).

MDPëŠ” ì´ëŸ° ê²Œì„ì˜ ê·œì¹™ì„ ìˆ˜í•™ìœ¼ë¡œ ì •ë¦¬í•œ ê±°ì˜ˆìš”:
- **ìƒíƒœ(S)**: ì§€ê¸ˆ ë‚´ê°€ ì–´ë””ì— ìˆëŠ”ì§€ (ê²Œì„íŒ ìœ„ì˜ ë‚´ ìœ„ì¹˜)
- **í–‰ë™(A)**: ë‚´ê°€ í•  ìˆ˜ ìˆëŠ” ì„ íƒ (ìœ„/ì•„ë˜/ì™¼ìª½/ì˜¤ë¥¸ìª½ ì´ë™)
- **ë³´ìƒ(R)**: í–‰ë™ì˜ ê²°ê³¼ë¡œ ë°›ëŠ” ì ìˆ˜ (ìš©ëˆ ë˜ëŠ” ë²Œê¸ˆ)
- **í• ì¸ì¸ì(Î³)**: ë‚˜ì¤‘ì— ë°›ì„ ë³´ìƒì€ ì¡°ê¸ˆ ëœ ì¤‘ìš”í•´ìš” (ì§€ê¸ˆ 100ì› > ë‚´ì¼ 100ì›)

#### ğŸ¯ Policy Gradientê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ëˆˆì„ ê°ê³  ë‹¤íŠ¸ë¥¼ ë˜ì§€ëŠ” ì—°ìŠµì„ í•œë‹¤ê³  í•´ ë´ìš”!

ì²˜ìŒì—ëŠ” ì•„ë¬´ ë°ë‚˜ ë˜ì§€ì§€ë§Œ, **ê³¼ë…ì— ë§ì„ ë•Œë§ˆë‹¤ ê·¸ë•Œ ë˜ì§„ ë°©ë²•ì„ ë” ë§ì´ ì¨ìš”**.
ì´ê²ƒì´ ë°”ë¡œ Policy Gradientì˜ í•µì‹¬ì´ì—ìš”:
- ì¢‹ì€ ê²°ê³¼($G_t$ê°€ í¼) â†’ ê·¸ í–‰ë™ì„ **ë” ìì£¼** í•˜ë„ë¡ ì •ì±… ì—…ë°ì´íŠ¸
- ë‚˜ìœ ê²°ê³¼($G_t$ê°€ ì‘ìŒ) â†’ ê·¸ í–‰ë™ì„ **ëœ ìì£¼** í•˜ë„ë¡ ì •ì±… ì—…ë°ì´íŠ¸

---"""))

# â”€â”€ Cell 4: ğŸ“ Exercises â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Bellman ë°©ì •ì‹ ê³„ì‚°

2ê°œì˜ ìƒíƒœ $s_1, s_2$ì™€ ê²°ì •ì  ì •ì±… $\pi(s_1) = a_R$ (ì˜¤ë¥¸ìª½)ì´ ìˆìŠµë‹ˆë‹¤.
- $R(s_1, a_R) = 5$, $P(s_2 | s_1, a_R) = 1.0$
- $R(s_2, a_R) = 10$ (ì¢…ë£Œ ìƒíƒœ), $\gamma = 0.9$

$V^\pi(s_1)$ì„ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$V^\pi(s_2) = 10 \quad \text{(ì¢…ë£Œ ìƒíƒœ, ì´í›„ ë³´ìƒ ì—†ìŒ)}$$

$$V^\pi(s_1) = R(s_1, a_R) + \gamma \cdot P(s_2|s_1, a_R) \cdot V^\pi(s_2)$$

$$= 5 + 0.9 \times 1.0 \times 10 = 5 + 9 = 14$$

â†’ $V^\pi(s_1) = 14$. í˜„ì¬ ë³´ìƒ(5)ê³¼ í• ì¸ëœ ë¯¸ë˜ ë³´ìƒ(9)ì˜ í•©ì…ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: Return ê³„ì‚°

ë³´ìƒ ì‹œí€€ìŠ¤ $R_1=1, R_2=2, R_3=3$ì´ê³  $\gamma=0.5$ì¼ ë•Œ $G_0$ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$G_0 = R_1 + \gamma R_2 + \gamma^2 R_3 = 1 + 0.5 \times 2 + 0.25 \times 3 = 1 + 1 + 0.75 = 2.75$$

â†’ í• ì¸ ì¸ìê°€ ì‘ì„ìˆ˜ë¡ ê·¼ì‹œì•ˆì (ê°€ê¹Œìš´ ë³´ìƒ ì„ í˜¸), í´ìˆ˜ë¡ ì›ì‹œì•ˆì (ë¯¸ë˜ ë³´ìƒë„ ì¤‘ì‹œ)ì…ë‹ˆë‹¤.
</details>

---"""))

# â”€â”€ Cell 5: Import cell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"NumPy ë²„ì „: {np.__version__}")"""))

# â”€â”€ Cell 6: Section 2 header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 2. GridWorld MDP êµ¬í˜„ <a name='2.-GridWorld-MDP-êµ¬í˜„'></a>

4Ã—4 ê²©ì ì„¸ê³„(GridWorld)ë¥¼ MDPë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ìƒ/í•˜/ì¢Œ/ìš°ë¡œ ì´ë™í•˜ë©°, 
ëª©í‘œ ì§€ì ì— ë„ë‹¬í•˜ë©´ +1 ë³´ìƒ, í•¨ì •ì— ë¹ ì§€ë©´ -1 ë³´ìƒì„ ë°›ìŠµë‹ˆë‹¤.

| êµ¬ì„± ìš”ì†Œ | ì„¤ì • |
|-----------|------|
| ìƒíƒœ ê³µê°„ | 4Ã—4 = 16ê°œ ìƒíƒœ |
| í–‰ë™ ê³µê°„ | {ìƒ, í•˜, ì¢Œ, ìš°} = 4ê°œ |
| ë³´ìƒ | ëª©í‘œ(+1), í•¨ì •(-1), ì´ë™(-0.04) |
| í• ì¸ ì¸ì | $\\gamma = 0.99$ |"""))

# â”€â”€ Cell 7: GridWorld MDP implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ GridWorld MDP êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class GridWorldMDP:
    # 4x4 ê²©ì ì„¸ê³„ MDP êµ¬í˜„
    # ìƒíƒœ: 0~15 (4x4 ê²©ì), í–‰ë™: 0=ìƒ, 1=í•˜, 2=ì¢Œ, 3=ìš°

    def __init__(self, size=4, gamma=0.99):
        self.size = size
        self.n_states = size * size
        self.n_actions = 4
        self.gamma = gamma

        # íŠ¹ìˆ˜ ìƒíƒœ ì„¤ì •
        self.goal = 15       # ìš°í•˜ë‹¨ = ëª©í‘œ
        self.trap = 11       # í•¨ì •
        self.terminal = {self.goal, self.trap}

        # ë³´ìƒ ì„¤ì •
        self.rewards = np.full(self.n_states, -0.04)  # ì´ë™ ë¹„ìš©
        self.rewards[self.goal] = 1.0   # ëª©í‘œ ë„ë‹¬
        self.rewards[self.trap] = -1.0  # í•¨ì •

        # í–‰ë™ ë°©í–¥: (í–‰ ë³€í™”, ì—´ ë³€í™”)
        self.action_effects = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}
        self.action_names = ['â†‘', 'â†“', 'â†', 'â†’']

    def _to_rc(self, s):
        return s // self.size, s % self.size

    def _to_s(self, r, c):
        return r * self.size + c

    def step(self, state, action):
        # ì¢…ë£Œ ìƒíƒœë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if state in self.terminal:
            return state, 0.0

        r, c = self._to_rc(state)
        dr, dc = self.action_effects[action]
        nr, nc = r + dr, c + dc

        # ê²©ì ë²½ ì²´í¬
        if 0 <= nr < self.size and 0 <= nc < self.size:
            next_state = self._to_s(nr, nc)
        else:
            next_state = state

        return next_state, self.rewards[next_state]

    def get_transition_prob(self, state, action):
        # ê²°ì •ì  ì „ì´ (í™•ë¥  1.0)
        next_state, reward = self.step(state, action)
        return [(next_state, reward, 1.0)]


env = GridWorldMDP()
print(f"GridWorld MDP ìƒì„± ì™„ë£Œ")
print(f"  ìƒíƒœ ê³µê°„: {env.n_states}ê°œ (4Ã—4 ê²©ì)")
print(f"  í–‰ë™ ê³µê°„: {env.n_actions}ê°œ ({', '.join(env.action_names)})")
print(f"  í• ì¸ ì¸ì: Î³ = {env.gamma}")
print(f"  ëª©í‘œ ìƒíƒœ: {env.goal} (ë³´ìƒ: +{env.rewards[env.goal]})")
print(f"  í•¨ì • ìƒíƒœ: {env.trap} (ë³´ìƒ: {env.rewards[env.trap]})")
print(f"  ì´ë™ ë¹„ìš©: {env.rewards[0]}")

# ê²©ì ë³´ìƒ ë§µ ì¶œë ¥
print(f"\\në³´ìƒ ë§µ (4Ã—4):")
reward_grid = env.rewards.reshape(4, 4)
for r in range(4):
    row_str = ""
    for c in range(4):
        s = r * 4 + c
        if s == env.goal:
            row_str += " [+1.0 G] "
        elif s == env.trap:
            row_str += " [-1.0 T] "
        else:
            row_str += f" [{env.rewards[s]:+.2f}] "
    print(f"  {row_str}")"""))

# â”€â”€ Cell 8: Value Iteration section â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 3. Value Iteration ì‹œê°í™” <a name='3.-Value-Iteration-ì‹œê°í™”'></a>

Bellman ìµœì  ë°©ì •ì‹ì„ ë°˜ë³µì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ìµœì  ê°€ì¹˜ í•¨ìˆ˜ $V^*$ë¥¼ êµ¬í•©ë‹ˆë‹¤:

$$V_{k+1}(s) = \max_{a} \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V_k(s')\right]$$

ìˆ˜ë ´ ì¡°ê±´: $\max_s |V_{k+1}(s) - V_k(s)| < \theta$"""))

# â”€â”€ Cell 9: Value Iteration code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Value Iteration êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def value_iteration(env, theta=1e-6, max_iter=1000):
    V = np.zeros(env.n_states)
    history = [V.copy()]

    for iteration in range(max_iter):
        V_new = np.zeros(env.n_states)

        for s in range(env.n_states):
            if s in env.terminal:
                V_new[s] = env.rewards[s]
                continue

            q_values = []
            for a in range(env.n_actions):
                transitions = env.get_transition_prob(s, a)
                q = sum(prob * (reward + env.gamma * V[ns])
                        for ns, reward, prob in transitions)
                q_values.append(q)
            V_new[s] = max(q_values)

        delta = np.max(np.abs(V_new - V))
        V = V_new
        history.append(V.copy())

        if delta < theta:
            print(f"Value Iteration ìˆ˜ë ´: {iteration + 1}íšŒ ë°˜ë³µ")
            break

    # ìµœì  ì •ì±… ì¶”ì¶œ
    policy = np.zeros(env.n_states, dtype=int)
    for s in range(env.n_states):
        if s in env.terminal:
            continue
        q_values = []
        for a in range(env.n_actions):
            transitions = env.get_transition_prob(s, a)
            q = sum(prob * (reward + env.gamma * V[ns])
                    for ns, reward, prob in transitions)
            q_values.append(q)
        policy[s] = np.argmax(q_values)

    return V, policy, history


V_star, pi_star, vi_history = value_iteration(env)

print(f"\\nìµœì  ê°€ì¹˜ í•¨ìˆ˜ V* (4Ã—4):")
V_grid = V_star.reshape(4, 4)
for r in range(4):
    row_str = "  "
    for c in range(4):
        row_str += f"{V_grid[r, c]:+7.3f} "
    print(row_str)

print(f"\\nìµœì  ì •ì±… Ï€* (4Ã—4):")
for r in range(4):
    row_str = "  "
    for c in range(4):
        s = r * 4 + c
        if s == env.goal:
            row_str += "   G    "
        elif s == env.trap:
            row_str += "   T    "
        else:
            row_str += f"   {env.action_names[pi_star[s]]}    "
    print(row_str)"""))

# â”€â”€ Cell 10: Value Iteration visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ Value Iteration ìˆ˜ë ´ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# (1) Value ìˆ˜ë ´ ê³¼ì •
ax1 = axes[0]
iterations_to_show = [0, 1, 5, 10, 20, len(vi_history)-1]
for it in iterations_to_show:
    if it < len(vi_history):
        ax1.plot(range(env.n_states), vi_history[it],
                 'o-', ms=5, lw=1.5, label=f'iter={it}')
ax1.set_xlabel('State', fontsize=11)
ax1.set_ylabel('V(s)', fontsize=11)
ax1.set_title('Value Iteration ìˆ˜ë ´ ê³¼ì •', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ìµœì  ê°€ì¹˜ í•¨ìˆ˜ íˆíŠ¸ë§µ
ax2 = axes[1]
V_grid = V_star.reshape(4, 4)
im = ax2.imshow(V_grid, cmap='RdYlGn', interpolation='nearest')
for r in range(4):
    for c in range(4):
        s = r * 4 + c
        label = f'{V_grid[r,c]:.2f}'
        if s == env.goal:
            label += '\\n(G)'
        elif s == env.trap:
            label += '\\n(T)'
        ax2.text(c, r, label, ha='center', va='center', fontsize=9, fontweight='bold')
fig.colorbar(im, ax=ax2, shrink=0.8)
ax2.set_title('ìµœì  ê°€ì¹˜ í•¨ìˆ˜ V*', fontweight='bold')

# (3) ìµœì  ì •ì±… ì‹œê°í™”
ax3 = axes[2]
arrow_map = {0: (0, 0.3), 1: (0, -0.3), 2: (-0.3, 0), 3: (0.3, 0)}
ax3.set_xlim(-0.5, 3.5)
ax3.set_ylim(3.5, -0.5)
for r in range(4):
    for c in range(4):
        s = r * 4 + c
        if s == env.goal:
            ax3.add_patch(plt.Rectangle((c-0.4, r-0.4), 0.8, 0.8,
                                        color='green', alpha=0.3))
            ax3.text(c, r, 'G', ha='center', va='center', fontsize=14, fontweight='bold')
        elif s == env.trap:
            ax3.add_patch(plt.Rectangle((c-0.4, r-0.4), 0.8, 0.8,
                                        color='red', alpha=0.3))
            ax3.text(c, r, 'T', ha='center', va='center', fontsize=14, fontweight='bold')
        else:
            dx, dy = arrow_map[pi_star[s]]
            ax3.annotate('', xy=(c+dx, r-dy), xytext=(c, r),
                        arrowprops=dict(arrowstyle='->', color='blue', lw=2.5))
ax3.set_title('ìµœì  ì •ì±… Ï€*', fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.set_xticks(range(4))
ax3.set_yticks(range(4))

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/value_iteration_gridworld.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/value_iteration_gridworld.png")
print(f"ì´ ìˆ˜ë ´ ë°˜ë³µ ìˆ˜: {len(vi_history) - 1}")"""))

# â”€â”€ Cell 11: REINFORCE section header â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 4. REINFORCE Policy Gradient <a name='4.-REINFORCE-Policy-Gradient'></a>

REINFORCE ì•Œê³ ë¦¬ì¦˜ì€ ì •ì±…ì„ ì§ì ‘ ë§¤ê°œë³€ìˆ˜í™”í•˜ì—¬ ê¸°ìš¸ê¸° ìƒìŠ¹ë²•ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.

**ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„:**
1. í˜„ì¬ ì •ì±… $\pi_\theta$ë¡œ ì—í”¼ì†Œë“œ ìƒì„±: $\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \ldots)$
2. ê° ì‹œê°„ ë‹¨ê³„ì˜ Return ê³„ì‚°: $G_t = \sum_{k=0}^{T-t} \gamma^k r_{t+k+1}$
3. ì •ì±… ê¸°ìš¸ê¸° ê³„ì‚°: $\nabla_\theta J(\theta) = \sum_t G_t \nabla_\theta \log \pi_\theta(a_t|s_t)$
4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)$

ê°„ë‹¨í•œ **K-Armed Bandit** ë¬¸ì œì— REINFORCEë¥¼ ì ìš©í•´ ë´…ë‹ˆë‹¤."""))

# â”€â”€ Cell 12: REINFORCE implementation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ REINFORCE: K-Armed Bandit ë¬¸ì œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5ê°œì˜ ìŠ¬ë¡¯ë¨¸ì‹ (arm), ê°ê° ë‹¤ë¥¸ í‰ê·  ë³´ìƒì„ ê°€ì§
n_arms = 5
true_rewards = np.array([0.2, -0.5, 1.5, 0.8, -0.2])

# TFë¡œ ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬í˜„ (Softmax ì •ì±…)
policy_logits = tf.Variable(tf.zeros(n_arms), dtype=tf.float32)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

n_episodes = 500
reward_history = []
action_probs_history = []

print(f"K-Armed Bandit REINFORCE")
print(f"  íŒ” ê°œìˆ˜: {n_arms}")
print(f"  ì‹¤ì œ ë³´ìƒ í‰ê· : {true_rewards}")
print(f"  ìµœì  íŒ”: arm {np.argmax(true_rewards)} (ë³´ìƒ={true_rewards.max()})")
print()

for ep in range(n_episodes):
    with tf.GradientTape() as tape:
        # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ
        probs = tf.nn.softmax(policy_logits)
        action = tf.random.categorical(tf.math.log(probs[tf.newaxis, :]), 1)[0, 0]

        # ë³´ìƒ ìˆ˜ì§‘ (ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í¬í•¨)
        reward = true_rewards[action.numpy()] + np.random.randn() * 0.5

        # REINFORCE loss: -G * log Ï€(a|s)
        log_prob = tf.math.log(probs[action] + 1e-8)
        loss = -reward * log_prob

    grads = tape.gradient(loss, [policy_logits])
    optimizer.apply_gradients(zip(grads, [policy_logits]))

    reward_history.append(reward)
    action_probs_history.append(tf.nn.softmax(policy_logits).numpy().copy())

    if (ep + 1) % 100 == 0:
        curr_probs = tf.nn.softmax(policy_logits).numpy()
        avg_reward = np.mean(reward_history[-50:])
        best_arm = np.argmax(curr_probs)
        print(f"  ì—í”¼ì†Œë“œ {ep+1:4d}: í‰ê·  ë³´ìƒ(ìµœê·¼ 50)={avg_reward:+.3f}, "
              f"ìµœì„  arm={best_arm} (P={curr_probs[best_arm]:.3f})")

final_probs = tf.nn.softmax(policy_logits).numpy()
print(f"\\nìµœì¢… ì •ì±… ë¶„í¬:")
for i in range(n_arms):
    bar = 'â–ˆ' * int(final_probs[i] * 40)
    print(f"  Arm {i} (ì‹¤ì œ r={true_rewards[i]:+.1f}): P={final_probs[i]:.4f} {bar}")"""))

# â”€â”€ Cell 13: REINFORCE visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ REINFORCE í•™ìŠµ ê³¼ì • ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) ë³´ìƒ ì´ë™ í‰ê· 
ax1 = axes[0]
window = 20
rewards_smooth = np.convolve(reward_history, np.ones(window)/window, mode='valid')
ax1.plot(range(len(rewards_smooth)), rewards_smooth, 'b-', lw=2, label='ì´ë™ í‰ê·  (w=20)')
ax1.axhline(y=true_rewards.max(), color='red', ls='--', lw=1.5,
            label=f'ìµœì  ë³´ìƒ ({true_rewards.max()})')
ax1.fill_between(range(len(rewards_smooth)),
                 rewards_smooth - 0.3, rewards_smooth + 0.3,
                 alpha=0.1, color='blue')
ax1.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)
ax1.set_ylabel('ë³´ìƒ', fontsize=11)
ax1.set_title('REINFORCE í•™ìŠµ ê³¡ì„ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ì •ì±… í™•ë¥  ë³€í™”
ax2 = axes[1]
probs_arr = np.array(action_probs_history)
for i in range(n_arms):
    ax2.plot(probs_arr[:, i], lw=2, label=f'Arm {i} (r={true_rewards[i]:+.1f})')
ax2.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)
ax2.set_ylabel('ì„ íƒ í™•ë¥ ', fontsize=11)
ax2.set_title('ì •ì±… í™•ë¥  ë³€í™” (Softmax)', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter15_alignment_rlhf/reinforce_bandit.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/reinforce_bandit.png")
print(f"ìµœì¢… ì„ íƒ í™•ë¥ : Arm {np.argmax(final_probs)} = {final_probs.max():.4f}")"""))

# â”€â”€ Cell 14: NLP Reward Signal Design â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md("""\
## 5. NLPë¥¼ ìœ„í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„ <a name='5.-NLP-ë¦¬ì›Œë“œ-ì‹ í˜¸-ì„¤ê³„'></a>

LLM ì •ë ¬(Alignment)ì—ì„œ RLì„ ì‚¬ìš©í•  ë•Œ, ë¦¬ì›Œë“œ ì‹ í˜¸ë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í•˜ëŠ”ì§€ê°€ í•µì‹¬ì…ë‹ˆë‹¤.

| ë¦¬ì›Œë“œ ìœ í˜• | ìˆ˜ì‹/ì„¤ëª… | ì˜ˆì‹œ |
|-------------|-----------|------|
| ì¸ê°„ ì„ í˜¸ ê¸°ë°˜ | $r(x, y) = \\text{RewardModel}(x, y)$ | RLHF |
| ê·œì¹™ ê¸°ë°˜ | $r = \\mathbb{1}[\\text{ì¡°ê±´ ì¶©ì¡±}]$ | ê¸¸ì´ ì œí•œ, ì•ˆì „ì„± |
| ìë™ ë©”íŠ¸ë¦­ | $r = \\text{BLEU}(y, y^*)$ ë˜ëŠ” $\\text{ROUGE}$ | ë²ˆì—­ í’ˆì§ˆ |
| KL í˜ë„í‹° | $r_{total} = r(x,y) - \\beta D_{KL}[\\pi_\\theta \\| \\pi_{ref}]$ | ì •ì±… ì´íƒˆ ë°©ì§€ |"""))

# â”€â”€ Cell 15: NLP Reward Design code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(code("""\
# â”€â”€ NLP ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„ ì˜ˆì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ìƒì„± ë¬¸ì œ: í† í° ìƒì„± ì‹œ ë‹¤ì–‘í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ë¥¼ ì ìš©

vocab_size = 100
seq_len = 10
n_samples = 1000

# 1. ê¸¸ì´ ê¸°ë°˜ ë¦¬ì›Œë“œ: ì§§ì€ ì‘ë‹µì— ë³´ë„ˆìŠ¤
def length_reward(seq, target_len=5):
    actual_len = len(seq)
    return -abs(actual_len - target_len) / target_len

# 2. ë‹¤ì–‘ì„± ë¦¬ì›Œë“œ: ë°˜ë³µ í† í° íŒ¨ë„í‹°
def diversity_reward(seq):
    unique_ratio = len(set(seq)) / len(seq)
    return unique_ratio

# 3. ì•ˆì „ì„± ë¦¬ì›Œë“œ: ê¸ˆì§€ í† í°(ID 0~9)ì´ ì—†ìœ¼ë©´ ë³´ìƒ
def safety_reward(seq, forbidden=set(range(10))):
    has_forbidden = any(t in forbidden for t in seq)
    return 0.0 if has_forbidden else 1.0

# 4. KL í˜ë„í‹°: ê¸°ì¤€ ì •ì±…ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡
def kl_penalty(pi_logits, ref_logits, beta=0.1):
    pi_probs = tf.nn.softmax(pi_logits)
    ref_probs = tf.nn.softmax(ref_logits)
    kl = tf.reduce_sum(pi_probs * tf.math.log(pi_probs / (ref_probs + 1e-8) + 1e-8))
    return -beta * kl.numpy()

# ëœë¤ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ë¦¬ì›Œë“œ ë¶„í¬ ì‹œê°í™”
np.random.seed(42)
length_rewards = []
diversity_rewards = []
safety_rewards = []

for _ in range(n_samples):
    seq = np.random.randint(0, vocab_size, size=seq_len).tolist()
    length_rewards.append(length_reward(seq))
    diversity_rewards.append(diversity_reward(seq))
    safety_rewards.append(safety_reward(seq))

print(f"ë¦¬ì›Œë“œ ì‹ í˜¸ í†µê³„ (ëœë¤ ì‹œí€€ìŠ¤ {n_samples}ê°œ):")
print(f"{'ë¦¬ì›Œë“œ ìœ í˜•':<20} | {'í‰ê· ':>8} | {'í‘œì¤€í¸ì°¨':>8} | {'ìµœì†Œ':>8} | {'ìµœëŒ€':>8}")
print(f"{'-'*62}")
for name, values in [("ê¸¸ì´ ê¸°ë°˜", length_rewards),
                      ("ë‹¤ì–‘ì„± ê¸°ë°˜", diversity_rewards),
                      ("ì•ˆì „ì„± ê¸°ë°˜", safety_rewards)]:
    vals = np.array(values)
    print(f"{name:<20} | {vals.mean():>8.3f} | {vals.std():>8.3f} | {vals.min():>8.3f} | {vals.max():>8.3f}")

# KL í˜ë„í‹° ì˜ˆì‹œ
pi_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32))
ref_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32))
kl_val = kl_penalty(pi_logits, ref_logits, beta=0.1)
print(f"\\nKL í˜ë„í‹° ì˜ˆì‹œ (Î²=0.1): {kl_val:.4f}")
print(f"  â†’ ì •ì±…ì´ ê¸°ì¤€ ì •ì±…ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ìŒì˜ ë³´ìƒì´ ì»¤ì§")
print(f"  â†’ ì´ëŠ” RLHFì—ì„œ ëª¨ë¸ì´ reward hackingí•˜ëŠ” ê²ƒì„ ë°©ì§€")"""))

# â”€â”€ Cell 16: Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cells.append(md(r"""\
## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| MDP | $(S, A, P, R, \gamma)$ â€” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬ | â­â­â­ |
| Bellman ë°©ì •ì‹ | $V^\pi(s) = \sum_a \pi(a|s)[R + \gamma \sum_{s'} PV^\pi]$ | â­â­â­ |
| Value Iteration | Bellman ìµœì  ë°©ì •ì‹ ë°˜ë³µ â†’ ìµœì  ì •ì±… | â­â­ |
| REINFORCE | $\nabla_\theta J = \mathbb{E}[G_t \nabla_\theta \log \pi_\theta]$ | â­â­â­ |
| Return ($G_t$) | ë¯¸ë˜ ë³´ìƒì˜ í• ì¸ í•©: $\sum_k \gamma^k R_{t+k+1}$ | â­â­ |
| KL í˜ë„í‹° | $-\beta D_{KL}[\pi_\theta \| \pi_{ref}]$ â€” ì •ì±… ì´íƒˆ ë°©ì§€ | â­â­â­ |
| ë¦¬ì›Œë“œ ì„¤ê³„ | ì¸ê°„ ì„ í˜¸ / ê·œì¹™ / ë©”íŠ¸ë¦­ ê¸°ë°˜ ë³´ìƒ ì‹ í˜¸ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$V^\pi(s) = \sum_{a} \pi(a|s) \left[R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^\pi(s')\right]$$

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\left[G_t \nabla_\theta \log \pi_\theta(a_t | s_t)\right]$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**02_actor_critic_and_ppo.ipynb** â€” Advantage Functionì„ ë„ì…í•˜ì—¬ REINFORCEì˜ ë†’ì€ ë¶„ì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , A2Cì—ì„œ PPO-Clipê¹Œì§€ ìˆ˜ì‹ì„ ì™„ì „ ì „ê°œí•©ë‹ˆë‹¤."""))

create_notebook(cells, 'chapter15_alignment_rlhf/01_rl_fundamentals_mdp_policy.ipynb')
