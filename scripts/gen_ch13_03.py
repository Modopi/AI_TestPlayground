"""Generate Chapter 13-03: UNet for Diffusion."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

cells = [
# â”€â”€â”€ Cell 0: í—¤ë” â”€â”€â”€
md(r"""# Chapter 13: ìƒì„± AI ì‹¬í™” â€” í™•ì‚° ëª¨ë¸ìš© UNet ì•„í‚¤í…ì²˜

## í•™ìŠµ ëª©í‘œ
- **Sinusoidal Time Embedding**ì˜ ìˆ˜ì‹ê³¼ êµ¬í˜„ì„ ì´í•´í•˜ê³  ì‹œê°„ ì •ë³´ê°€ ëª¨ë¸ì— ì£¼ì…ë˜ëŠ” ì›ë¦¬ë¥¼ íŒŒì•…í•œë‹¤
- **ì”ì°¨ ë¸”ë¡(Residual Block)**ì— ì‹œê°„ ì¡°ê±´ì„ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ êµ¬í˜„í•œë‹¤
- **Cross-Attention** ë©”ì»¤ë‹ˆì¦˜ì´ í…ìŠ¤íŠ¸/í´ë˜ìŠ¤ ì¡°ê±´ ì •ë³´ë¥¼ UNetì— ì „ë‹¬í•˜ëŠ” ê³¼ì •ì„ ì´í•´í•œë‹¤
- MNIST(28Ã—28)ìš© **ê°„ë‹¨í•œ UNet ì•„í‚¤í…ì²˜**ë¥¼ ë°‘ë°”ë‹¥ë¶€í„° TensorFlowë¡œ êµ¬í˜„í•œë‹¤
- UNetì˜ **ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°**ì™€ **Skip Connection**ì´ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì´ìœ ë¥¼ ë¶„ì„í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ì‹œê°„ ì„ë² ë”©ê³¼ Cross-Attention](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [Sinusoidal Time Embedding êµ¬í˜„](#2.-ì‹œê°„-ì„ë² ë”©)
3. [ì”ì°¨ ë¸”ë¡ê³¼ ì‹œê°„ ì¡°ê±´ ì£¼ì…](#3.-ì”ì°¨-ë¸”ë¡)
4. [28Ã—28 MNISTìš© UNet êµ¬í˜„](#4.-UNet-êµ¬í˜„)
5. [UNet íŠ¹ì§• ë§µ ê°œë… ì‹œê°í™”](#5.-íŠ¹ì§•-ë§µ-ì‹œê°í™”)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""),

# â”€â”€â”€ Cell 1: ìˆ˜í•™ì  ê¸°ì´ˆ â”€â”€â”€
md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### Sinusoidal Time Embedding

Transformerì˜ ìœ„ì¹˜ ì¸ì½”ë”©(Vaswani et al., 2017)ì„ ì‹œê°„ $t$ì— ì ìš©í•©ë‹ˆë‹¤:

$$PE(t, 2i) = \sin\!\left(\frac{t}{10000^{2i/d}}\right), \quad PE(t, 2i+1) = \cos\!\left(\frac{t}{10000^{2i/d}}\right)$$

- $t$: í˜„ì¬ í™•ì‚° íƒ€ì„ìŠ¤í… (ì •ìˆ˜)
- $d$: ì„ë² ë”© ì°¨ì› (ë³´í†µ 128 ë˜ëŠ” 256)
- $i = 0, 1, \ldots, d/2 - 1$: ì£¼íŒŒìˆ˜ ì¸ë±ìŠ¤
- $10000^{2i/d}$: ì£¼íŒŒìˆ˜ê°€ $i$ì— ë”°ë¼ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ â†’ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ ì‹œê°„ ì •ë³´ ì¸ì½”ë”©

**ì™œ ì‚¬ì¸/ì½”ì‚¬ì¸ì¸ê°€?**

| íŠ¹ì„± | ì„¤ëª… |
|------|------|
| ìœ ì¼ì„± | ëª¨ë“  $t$ì— ëŒ€í•´ ê³ ìœ í•œ ë²¡í„° ìƒì„± |
| ì—°ì†ì„± | ì¸ì ‘ $t$ ê°’ì˜ ì„ë² ë”©ì´ ìœ ì‚¬ |
| ì£¼ê¸°ì„± | ì €ì°¨ì›: ë¹ ë¥¸ ì§„ë™ (ë¯¸ì„¸ ì‹œê°„), ê³ ì°¨ì›: ëŠë¦° ì§„ë™ (ê±°ì‹œ ì‹œê°„) |
| ì™¸ì‚½ | í•™ìŠµ ì‹œ ë³´ì§€ ëª»í•œ $t$ ê°’ì—ë„ ì˜ë¯¸ ìˆëŠ” ì„ë² ë”© ì œê³µ |

### Cross-Attention (ì¡°ê±´ ì£¼ì…)

í…ìŠ¤íŠ¸/í´ë˜ìŠ¤ ë“± ì™¸ë¶€ ì¡°ê±´ $c$ë¥¼ UNet ì¤‘ê°„ íŠ¹ì§•ì— ì£¼ì…í•©ë‹ˆë‹¤:

$$\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Self-Attention vs Cross-Attention:**

| êµ¬ë¶„ | Q ì¶œì²˜ | K, V ì¶œì²˜ |
|------|--------|-----------|
| Self-Attention | ì´ë¯¸ì§€ íŠ¹ì§• $h$ | ì´ë¯¸ì§€ íŠ¹ì§• $h$ |
| Cross-Attention | ì´ë¯¸ì§€ íŠ¹ì§• $h$ | ì¡°ê±´ ì„ë² ë”© $c$ (í…ìŠ¤íŠ¸ ë“±) |

$$Q = hW^Q, \quad K = cW^K, \quad V = cW^V$$

- $h \in \mathbb{R}^{HW \times d}$: UNet ì¤‘ê°„ì¸µ íŠ¹ì§• ë§µ (flatten)
- $c \in \mathbb{R}^{L \times d_c}$: ì¡°ê±´ ì‹œí€€ìŠ¤ (í…ìŠ¤íŠ¸ í† í° ì„ë² ë”© ë“±)
- $W^Q \in \mathbb{R}^{d \times d_k}$, $W^K \in \mathbb{R}^{d_c \times d_k}$, $W^V \in \mathbb{R}^{d_c \times d_v}$

### DDPM UNet ì „ì²´ êµ¬ì¡°

```
ì…ë ¥ x_t (noisy image) + t (timestep)
    â”‚
    â–¼
[Sinusoidal Embedding(t)] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                        â”‚ (ì‹œê°„ ì¡°ê±´)
    â–¼                                        â”‚
[Encoder Block 1] â”€â”€â”€ skip â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚ â†“ Downsample                â”‚         â”‚
[Encoder Block 2] â”€â”€â”€ skip â”€â”€â”   â”‚         â”‚
    â”‚ â†“ Downsample            â”‚   â”‚         â”‚
[Bottleneck (Attention)]      â”‚   â”‚    t_embâ”‚
    â”‚ â†‘ Upsample              â”‚   â”‚         â”‚
[Decoder Block 2] â† concat â”€â”€â”˜   â”‚         â”‚
    â”‚ â†‘ Upsample                  â”‚         â”‚
[Decoder Block 1] â† concat â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                                        â”‚
    â–¼                                        â”‚
[ì¶œë ¥ Conv] â†’ Îµ_Î¸(x_t, t) (ì˜ˆì¸¡ ë…¸ì´ì¦ˆ)    â”‚
```

**ìš”ì•½ í‘œ:**

| êµ¬ì„± ìš”ì†Œ | ìˆ˜ì‹/ì—­í•  | MNIST UNet ì„¤ì • |
|-----------|-----------|----------------|
| Time Embedding | $\sin/\cos$ + MLP | $d=128$ |
| Residual Block | $h + F(h, t_{emb})$ | Conv3Ã—3 + GroupNorm |
| Skip Connection | Encoder â†’ Decoder concat | ì±„ë„ ì¶• ì—°ê²° |
| Cross-Attention | $\text{softmax}(QK^T/\sqrt{d})V$ | (ì˜µì…˜) ì¡°ê±´ë¶€ ìƒì„± ì‹œ |
| Down/Upsample | MaxPool / UpSampling2D | 2Ã— ìŠ¤ì¼€ì¼ |"""),

# â”€â”€â”€ Cell 2: ğŸ£ ì¹œì ˆ ì„¤ëª… â”€â”€â”€
md(r"""---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ UNet ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ UNetì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: UNetì€ **ëª¨ë˜ì‹œê³„** ëª¨ì–‘ì˜ ì‹ ê²½ë§ì´ì—ìš”!
> - **ìœ„ìª½ (ì¸ì½”ë”)**: ê·¸ë¦¼ì„ ì ì  **ì‘ê²Œ ì¤„ì´ë©´ì„œ** í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ìš” (ì••ì¶•)
> - **ê°€ìš´ë° (ë³‘ëª©)**: ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ì•„ìš”
> - **ì•„ë˜ìª½ (ë””ì½”ë”)**: ë‹¤ì‹œ **í¬ê²Œ í‚¤ìš°ë©´ì„œ** ì„¸ë¶€ ì‚¬í•­ì„ ë³µì›í•´ìš”
> - **ê±´ë„ˆë›°ê¸° ì—°ê²°**: ìœ„ìª½ì—ì„œ ì•„ë˜ìª½ìœ¼ë¡œ **ì§€ë¦„ê¸¸**ì„ ë§Œë“¤ì–´, ì„¸ë¶€ ì •ë³´ê°€ ì‚¬ë¼ì§€ì§€ ì•Šê²Œ í•´ìš”!

#### â° ì‹œê°„ ì„ë² ë”©ì€ ì™œ í•„ìš”í•œê°€ìš”?

| ì§ˆë¬¸ | ë‹µë³€ |
|------|------|
| ì™œ? | UNetì´ "ì§€ê¸ˆ $t=100$ì´ì•¼" vs "$t=900$ì´ì•¼"ë¥¼ êµ¬ë¶„í•´ì•¼ í•´ìš” |
| ì–´ë–»ê²Œ? | ì‹œê°„ $t$ë¥¼ ì‚¬ì¸/ì½”ì‚¬ì¸ íŒŒë™ íŒ¨í„´ìœ¼ë¡œ ë°”ê¿”ì„œ ëª¨ë“  ì¸µì— ì•Œë ¤ì¤˜ìš” |
| ë¹„ìœ  | ì‹œí—˜ì§€ì— **ëª‡ êµì‹œì¸ì§€** ì í˜€ ìˆì–´ì•¼ ì–´ë–¤ ê³¼ëª©ì¸ì§€ ì•„ëŠ” ê²ƒê³¼ ê°™ì•„ìš”! |"""),

# â”€â”€â”€ Cell 3: ğŸ“ ì—°ìŠµ ë¬¸ì œ â”€â”€â”€
md(r"""---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Sinusoidal Embedding ê³„ì‚°

$t=50$, $d=4$ì¼ ë•Œ, ì‹œê°„ ì„ë² ë”© ë²¡í„°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$d=4$ì´ë¯€ë¡œ $i = 0, 1$ (ì£¼íŒŒìˆ˜ ì¸ë±ìŠ¤ 2ê°œ)

$$PE(50, 0) = \sin\!\left(\frac{50}{10000^{0/4}}\right) = \sin(50) \approx -0.2624$$

$$PE(50, 1) = \cos\!\left(\frac{50}{10000^{0/4}}\right) = \cos(50) \approx 0.9649$$

$$PE(50, 2) = \sin\!\left(\frac{50}{10000^{2/4}}\right) = \sin(0.5) \approx 0.4794$$

$$PE(50, 3) = \cos\!\left(\frac{50}{10000^{2/4}}\right) = \cos(0.5) \approx 0.8776$$

ì„ë² ë”© ë²¡í„°: $[-0.2624,\, 0.9649,\, 0.4794,\, 0.8776]$
</details>

#### ë¬¸ì œ 2: Skip Connectionì˜ ì—­í• 

UNetì—ì„œ Skip Connectionì„ ì œê±°í•˜ë©´ ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí• ê¹Œìš”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

Skip Connection ì—†ì´ëŠ”:
1. ë””ì½”ë”ê°€ ì¸ì½”ë”ì—ì„œ ì†ì‹¤ëœ **ê³ í•´ìƒë„ ì„¸ë¶€ ì •ë³´**(ì—£ì§€, í…ìŠ¤ì²˜)ë¥¼ ë³µì›í•˜ê¸° ì–´ë ¤ì›€
2. ë…¸ì´ì¦ˆ ì˜ˆì¸¡ $\epsilon_\theta$ì˜ **ê³µê°„ì  ì •ë°€ë„** ì €í•˜
3. ìƒì„±ëœ ì´ë¯¸ì§€ê°€ **íë¦¿í•˜ê³  ë””í…Œì¼ ë¶€ì¡±**

Skip Connectionì€ ì¸ì½”ë”ì˜ ê³ í•´ìƒë„ íŠ¹ì§•ì„ ë””ì½”ë”ì— ì§ì ‘ ì „ë‹¬í•˜ì—¬, **ìœ„ì¹˜ ì •ë°€ë„**ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
ì´ëŠ” ì›ë˜ ì˜ë£Œ ì˜ìƒ ë¶„í• (Ronneberger et al., 2015)ì„ ìœ„í•´ ì„¤ê³„ëœ UNetì˜ í•µì‹¬ ê¸°ì—¬ì…ë‹ˆë‹¤.
</details>"""),

# â”€â”€â”€ Cell 4: ì„í¬íŠ¸ â”€â”€â”€
code("""# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")"""),

# â”€â”€â”€ Cell 5: Section 2 - ì‹œê°„ ì„ë² ë”© â”€â”€â”€
md(r"""## 2. Sinusoidal Time Embedding êµ¬í˜„ <a name='2.-ì‹œê°„-ì„ë² ë”©'></a>

ê° í™•ì‚° íƒ€ì„ìŠ¤í… $t$ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. Transformer ìœ„ì¹˜ ì¸ì½”ë”©ê³¼ ë™ì¼í•œ ì›ë¦¬ì…ë‹ˆë‹¤."""),

# â”€â”€â”€ Cell 6: ì‹œê°„ ì„ë² ë”© ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ Sinusoidal Time Embedding êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def sinusoidal_embedding(t, dim):
    # t: (batch,) ì •ìˆ˜ í…ì„œ, dim: ì„ë² ë”© ì°¨ì›
    half_dim = dim // 2
    freqs = tf.exp(
        -tf.math.log(10000.0) * tf.range(half_dim, dtype=tf.float32) / half_dim
    )
    # të¥¼ floatë¡œ ë³€í™˜í•˜ì—¬ ì™¸ì  ê³„ì‚°
    t_float = tf.cast(t, tf.float32)
    args = t_float[:, None] * freqs[None, :]  # (batch, half_dim)
    embedding = tf.concat([tf.sin(args), tf.cos(args)], axis=-1)  # (batch, dim)
    return embedding

# í…ŒìŠ¤íŠ¸
test_t = tf.constant([0, 50, 100, 500, 999])
test_emb = sinusoidal_embedding(test_t, dim=128)
print(f"ì…ë ¥ íƒ€ì„ìŠ¤í…: {test_t.numpy()}")
print(f"ì„ë² ë”© shape: {test_emb.shape}")
print(f"\nt=0   ì„ë² ë”© (ì²˜ìŒ 8ê°œ): {test_emb[0, :8].numpy().round(4)}")
print(f"t=50  ì„ë² ë”© (ì²˜ìŒ 8ê°œ): {test_emb[1, :8].numpy().round(4)}")
print(f"t=500 ì„ë² ë”© (ì²˜ìŒ 8ê°œ): {test_emb[3, :8].numpy().round(4)}")
print(f"t=999 ì„ë² ë”© (ì²˜ìŒ 8ê°œ): {test_emb[4, :8].numpy().round(4)}")

# ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
def cosine_sim(a, b):
    return float(tf.reduce_sum(a * b) / (tf.norm(a) * tf.norm(b)))

print(f"\nì½”ì‚¬ì¸ ìœ ì‚¬ë„:")
print(f"  sim(t=0,   t=50)  = {cosine_sim(test_emb[0], test_emb[1]):.4f}")
print(f"  sim(t=50,  t=100) = {cosine_sim(test_emb[1], test_emb[2]):.4f}")
print(f"  sim(t=0,   t=999) = {cosine_sim(test_emb[0], test_emb[4]):.4f}")
print(f"  sim(t=500, t=999) = {cosine_sim(test_emb[3], test_emb[4]):.4f}")"""),

# â”€â”€â”€ Cell 7: ì‹œê°„ ì„ë² ë”© ì‹œê°í™” â”€â”€â”€
code(r"""# â”€â”€ ì‹œê°„ ì„ë² ë”© íˆíŠ¸ë§µ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
all_t = tf.range(0, 1000)
all_emb = sinusoidal_embedding(all_t, dim=128).numpy()

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

# (1) íˆíŠ¸ë§µ: ì „ì²´ ì„ë² ë”©
ax1 = axes[0]
im = ax1.imshow(all_emb.T, aspect='auto', cmap='RdBu_r',
                extent=[0, 1000, 128, 0], vmin=-1, vmax=1)
ax1.set_xlabel('Timestep $t$', fontsize=11)
ax1.set_ylabel('Embedding Dimension', fontsize=11)
ax1.set_title('Sinusoidal Time Embedding (d=128)', fontweight='bold')
plt.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)

# (2) ê°œë³„ ì°¨ì›ì˜ íŒŒë™
ax2 = axes[1]
dims_to_show = [0, 1, 16, 32, 63]
t_range = np.arange(1000)
for d_idx in dims_to_show:
    ax2.plot(t_range, all_emb[:, d_idx], lw=1.5, label=f'dim {d_idx}', alpha=0.8)
ax2.set_xlabel('Timestep $t$', fontsize=11)
ax2.set_ylabel('Value', fontsize=11)
ax2.set_title('Individual Dimension Waveforms', fontweight='bold')
ax2.legend(fontsize=9, ncol=2)
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/time_embedding.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/time_embedding.png")
print(f"ì €ì°¨ì›(dim 0): ë¹ ë¥¸ ì§„ë™ â†’ ë¯¸ì„¸ ì‹œê°„ êµ¬ë¶„")
print(f"ê³ ì°¨ì›(dim 63): ëŠë¦° ì§„ë™ â†’ ê±°ì‹œ ì‹œê°„ êµ¬ë¶„")"""),

# â”€â”€â”€ Cell 8: Section 3 - ì”ì°¨ ë¸”ë¡ â”€â”€â”€
md(r"""## 3. ì”ì°¨ ë¸”ë¡ê³¼ ì‹œê°„ ì¡°ê±´ ì£¼ì… <a name='3.-ì”ì°¨-ë¸”ë¡'></a>

DDPM UNetì˜ ê¸°ë³¸ êµ¬ì„± ë‹¨ìœ„ì¸ **Residual Block**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì‹œê°„ ì„ë² ë”© $t_{emb}$ê°€ ë¸”ë¡ ë‚´ë¶€ì— ì£¼ì…ë©ë‹ˆë‹¤:

$$h' = \text{Conv}(\text{GN}(\text{SiLU}(h))) + W_t \cdot t_{emb}$$
$$\text{output} = h + \text{Conv}(\text{GN}(\text{SiLU}(h')))$$

- GN: Group Normalization (ë°°ì¹˜ í¬ê¸°ì— ë…ë¦½ì )
- SiLU: $\text{SiLU}(x) = x \cdot \sigma(x)$ (Swish í™œì„±í™”)"""),

# â”€â”€â”€ Cell 9: ì”ì°¨ ë¸”ë¡ ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ ì”ì°¨ ë¸”ë¡ + ì‹œê°„ ì¡°ê±´ ì£¼ì… êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ResidualBlock(layers.Layer):
    # Conv â†’ GroupNorm â†’ SiLU â†’ Conv, with time embedding injection
    def __init__(self, out_channels, n_groups=8, **kwargs):
        super().__init__(**kwargs)
        self.out_channels = out_channels
        self.norm1 = layers.GroupNormalization(groups=n_groups)
        self.conv1 = layers.Conv2D(out_channels, 3, padding='same')
        self.time_proj = layers.Dense(out_channels)
        self.norm2 = layers.GroupNormalization(groups=n_groups)
        self.conv2 = layers.Conv2D(out_channels, 3, padding='same')
        self.skip_conv = None

    def build(self, input_shape):
        if input_shape[-1] != self.out_channels:
            self.skip_conv = layers.Conv2D(self.out_channels, 1)
        super().build(input_shape)

    def call(self, x, t_emb):
        residual = x

        h = self.norm1(x)
        h = tf.nn.silu(h)
        h = self.conv1(h)

        # ì‹œê°„ ì„ë² ë”© ì£¼ì…: (batch, dim) â†’ (batch, 1, 1, channels)
        t_proj = tf.nn.silu(t_emb)
        t_proj = self.time_proj(t_proj)[:, None, None, :]
        h = h + t_proj

        h = self.norm2(h)
        h = tf.nn.silu(h)
        h = self.conv2(h)

        if self.skip_conv is not None:
            residual = self.skip_conv(residual)

        return h + residual

# í…ŒìŠ¤íŠ¸
test_input = tf.random.normal([2, 28, 28, 1])
test_t_emb = sinusoidal_embedding(tf.constant([100, 500]), dim=128)

res_block = ResidualBlock(64)
output = res_block(test_input, test_t_emb)
print(f"ResidualBlock í…ŒìŠ¤íŠ¸:")
print(f"  ì…ë ¥ shape:  {test_input.shape}")
print(f"  t_emb shape: {test_t_emb.shape}")
print(f"  ì¶œë ¥ shape:  {output.shape}")
print(f"  íŒŒë¼ë¯¸í„° ìˆ˜: {sum(np.prod(v.shape) for v in res_block.trainable_variables):,}")
print(f"  ì¶œë ¥ í†µê³„: í‰ê· ={float(tf.reduce_mean(output)):.4f}, í‘œì¤€í¸ì°¨={float(tf.math.reduce_std(output)):.4f}")"""),

# â”€â”€â”€ Cell 10: Section 4 - UNet êµ¬í˜„ â”€â”€â”€
md(r"""## 4. 28Ã—28 MNISTìš© UNet êµ¬í˜„ <a name='4.-UNet-êµ¬í˜„'></a>

MNIST(28Ã—28Ã—1)ì— ì í•©í•œ **ì†Œí˜• UNet**ì„ êµ¬í˜„í•©ë‹ˆë‹¤. êµ¬ì¡°:

| ë‹¨ê³„ | í•´ìƒë„ | ì±„ë„ ìˆ˜ |
|------|--------|---------|
| ì…ë ¥ | 28Ã—28 | 1 |
| Encoder 1 | 28Ã—28 â†’ 14Ã—14 | 32 |
| Encoder 2 | 14Ã—14 â†’ 7Ã—7 | 64 |
| Bottleneck | 7Ã—7 | 128 |
| Decoder 2 | 7Ã—7 â†’ 14Ã—14 | 64 |
| Decoder 1 | 14Ã—14 â†’ 28Ã—28 | 32 |
| ì¶œë ¥ | 28Ã—28 | 1 |"""),

# â”€â”€â”€ Cell 11: UNet ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ MNIST 28x28 UNet ì•„í‚¤í…ì²˜ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SimpleUNet(keras.Model):
    # Encoder-Bottleneck-Decoder with skip connections and time conditioning
    def __init__(self, time_dim=128, **kwargs):
        super().__init__(**kwargs)
        self.time_dim = time_dim

        # ì‹œê°„ ì„ë² ë”© MLP
        self.time_mlp = keras.Sequential([
            layers.Dense(time_dim, activation='swish'),
            layers.Dense(time_dim),
        ])

        # ì…ë ¥ í”„ë¡œì ì…˜
        self.input_conv = layers.Conv2D(32, 3, padding='same')

        # ì¸ì½”ë”
        self.enc1 = ResidualBlock(32)
        self.down1 = layers.MaxPooling2D(2)   # 28â†’14
        self.enc2 = ResidualBlock(64)
        self.down2 = layers.MaxPooling2D(2)   # 14â†’7

        # ë³‘ëª©
        self.bottleneck1 = ResidualBlock(128)
        self.bottleneck2 = ResidualBlock(128)

        # ë””ì½”ë”
        self.up2 = layers.UpSampling2D(2)     # 7â†’14
        self.dec2 = ResidualBlock(64)
        self.up1 = layers.UpSampling2D(2)     # 14â†’28
        self.dec1 = ResidualBlock(32)

        # ì¶œë ¥
        self.output_norm = layers.GroupNormalization(groups=8)
        self.output_conv = layers.Conv2D(1, 1)

    def call(self, x, t):
        # ì‹œê°„ ì„ë² ë”©
        t_emb = sinusoidal_embedding(t, self.time_dim)
        t_emb = self.time_mlp(t_emb)

        # ì…ë ¥
        h = self.input_conv(x)

        # ì¸ì½”ë”
        h1 = self.enc1(h, t_emb)          # 28Ã—28Ã—32
        h = self.down1(h1)                  # 14Ã—14Ã—32
        h2 = self.enc2(h, t_emb)           # 14Ã—14Ã—64
        h = self.down2(h2)                  # 7Ã—7Ã—64

        # ë³‘ëª©
        h = self.bottleneck1(h, t_emb)     # 7Ã—7Ã—128
        h = self.bottleneck2(h, t_emb)     # 7Ã—7Ã—128

        # ë””ì½”ë” + Skip Connection
        h = self.up2(h)                     # 14Ã—14Ã—128
        h = tf.concat([h, h2], axis=-1)    # 14Ã—14Ã—192
        h = self.dec2(h, t_emb)            # 14Ã—14Ã—64
        h = self.up1(h)                     # 28Ã—28Ã—64
        h = tf.concat([h, h1], axis=-1)    # 28Ã—28Ã—96
        h = self.dec1(h, t_emb)            # 28Ã—28Ã—32

        # ì¶œë ¥
        h = self.output_norm(h)
        h = tf.nn.silu(h)
        return self.output_conv(h)          # 28Ã—28Ã—1

# ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
model = SimpleUNet(time_dim=128)

# Forward pass í…ŒìŠ¤íŠ¸
dummy_x = tf.random.normal([4, 28, 28, 1])
dummy_t = tf.constant([0, 250, 500, 999])
dummy_out = model(dummy_x, dummy_t)

print(f"SimpleUNet ì•„í‚¤í…ì²˜ í…ŒìŠ¤íŠ¸:")
print(f"  ì…ë ¥ x shape: {dummy_x.shape}")
print(f"  ì…ë ¥ t:       {dummy_t.numpy()}")
print(f"  ì¶œë ¥ shape:   {dummy_out.shape}")
print(f"  ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(np.prod(v.shape) for v in model.trainable_variables):,}")
print(f"  ì¶œë ¥ í†µê³„: í‰ê· ={float(tf.reduce_mean(dummy_out)):.4f}, í‘œì¤€í¸ì°¨={float(tf.math.reduce_std(dummy_out)):.4f}")

# ë ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ìˆ˜
print(f"\në ˆì´ì–´ë³„ íŒŒë¼ë¯¸í„° ìˆ˜:")
layer_params = {}
for v in model.trainable_variables:
    layer_name = v.name.split('/')[0]
    cnt = int(np.prod(v.shape))
    layer_params[layer_name] = layer_params.get(layer_name, 0) + cnt

for name, cnt in sorted(layer_params.items()):
    print(f"  {name:<30s}: {cnt:>8,}")"""),

# â”€â”€â”€ Cell 12: Section 5 - íŠ¹ì§• ë§µ ì‹œê°í™” â”€â”€â”€
md(r"""## 5. UNet íŠ¹ì§• ë§µ ê°œë… ì‹œê°í™” <a name='5.-íŠ¹ì§•-ë§µ-ì‹œê°í™”'></a>

UNetì˜ ì¸ì½”ë”ê°€ ì„œë¡œ ë‹¤ë¥¸ í•´ìƒë„ì—ì„œ ì–´ë–¤ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì‹œê°„ ì„ë² ë”©ì— ë”°ë¼ ì¶œë ¥ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""),

# â”€â”€â”€ Cell 13: íŠ¹ì§• ë§µ ì‹œê°í™” ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ UNet ì¶œë ¥: ì‹œê°„ ì¡°ê±´ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ë³€í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ (ëŒ€ê°ì„  íŒ¨í„´)
test_img = np.zeros((1, 28, 28, 1), dtype=np.float32)
for i in range(28):
    test_img[0, i, i, 0] = 1.0
    if i + 1 < 28:
        test_img[0, i, i+1, 0] = 0.5
    if i - 1 >= 0:
        test_img[0, i, i-1, 0] = 0.5

# ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤í…ì—ì„œì˜ UNet ì¶œë ¥ ë¹„êµ
timesteps_test = [0, 100, 300, 500, 700, 999]

fig, axes = plt.subplots(2, 6, figsize=(18, 6))

# ì²« ë²ˆì§¸ í–‰: ì…ë ¥ (ì‹œê°„ì— ë”°ë¼ ë…¸ì´ì¦ˆ ì¶”ê°€ëœ ì´ë¯¸ì§€)
T_test = 1000
beta_test = np.linspace(1e-4, 0.02, T_test)
alpha_test = 1.0 - beta_test
abar_test = np.cumprod(alpha_test)

for idx, t in enumerate(timesteps_test):
    # Forward processë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€
    if t == 0:
        noisy = test_img.copy()
    else:
        abar = abar_test[t-1]
        eps = np.random.randn(*test_img.shape).astype(np.float32)
        noisy = np.sqrt(abar) * test_img + np.sqrt(1 - abar) * eps

    axes[0][idx].imshow(noisy[0, :, :, 0], cmap='gray', vmin=-2, vmax=2)
    axes[0][idx].set_title(f'ì…ë ¥: $t={t}$', fontsize=9, fontweight='bold')
    axes[0][idx].axis('off')

    # UNet ì˜ˆì¸¡
    t_tensor = tf.constant([t])
    pred = model(tf.constant(noisy), t_tensor).numpy()
    axes[1][idx].imshow(pred[0, :, :, 0], cmap='RdBu_r')
    axes[1][idx].set_title(f'UNet ì¶œë ¥: $t={t}$', fontsize=9, fontweight='bold')
    axes[1][idx].axis('off')

axes[0][0].set_ylabel('ë…¸ì´ì¦ˆ ì…ë ¥', fontsize=11)
axes[1][0].set_ylabel('ì˜ˆì¸¡ ë…¸ì´ì¦ˆ', fontsize=11)
plt.suptitle('UNet: íƒ€ì„ìŠ¤í…ì— ë”°ë¥¸ ë…¸ì´ì¦ˆ ì˜ˆì¸¡ (ì´ˆê¸°í™” ì§í›„, í•™ìŠµ ì „)', fontweight='bold', y=1.02)

plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/unet_feature_maps.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/unet_feature_maps.png")
print("ì°¸ê³ : í•™ìŠµ ì „ì´ë¯€ë¡œ UNet ì¶œë ¥ì€ ë¬´ì‘ìœ„ì— ê°€ê¹ìŠµë‹ˆë‹¤.")
print("í•™ìŠµ í›„ì—ëŠ” ì…ë ¥ ë…¸ì´ì¦ˆë¥¼ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê²Œ ë©ë‹ˆë‹¤.")

# UNet ì¶œë ¥ í†µê³„
print(f"\níƒ€ì„ìŠ¤í…ë³„ UNet ì¶œë ¥ í†µê³„ (í•™ìŠµ ì „):")
print(f"{'ì‹œì ':>6} | {'í‰ê· ':>10} | {'í‘œì¤€í¸ì°¨':>10} | {'ìµœì†Œ':>10} | {'ìµœëŒ€':>10}")
print("-" * 55)
for t in timesteps_test:
    if t == 0:
        noisy_t = test_img
    else:
        abar = abar_test[t-1]
        eps = np.random.randn(*test_img.shape).astype(np.float32)
        noisy_t = np.sqrt(abar) * test_img + np.sqrt(1 - abar) * eps
    pred = model(tf.constant(noisy_t), tf.constant([t])).numpy()
    print(f"  t={t:4d} | {pred.mean():>10.4f} | {pred.std():>10.4f} | {pred.min():>10.4f} | {pred.max():>10.4f}")"""),

# â”€â”€â”€ Cell 14: ì •ë¦¬ â”€â”€â”€
md(r"""## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Sinusoidal Embedding | $PE(t,2i) = \sin(t/10000^{2i/d})$ â€” ì‹œê°„ ì •ë³´ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜ | â­â­â­ |
| Residual Block | $h + F(h, t_{emb})$ â€” ì‹œê°„ ì¡°ê±´ì´ ì£¼ì…ëœ ì”ì°¨ í•™ìŠµ | â­â­â­ |
| Skip Connection | ì¸ì½”ë” â†’ ë””ì½”ë” ì§ì ‘ ì—°ê²° â€” ê³ í•´ìƒë„ ì •ë³´ ë³´ì¡´ | â­â­â­ |
| Cross-Attention | $\text{softmax}(QK^T/\sqrt{d})V$ â€” í…ìŠ¤íŠ¸/í´ë˜ìŠ¤ ì¡°ê±´ ì£¼ì… | â­â­â­ |
| GroupNorm | ë°°ì¹˜ í¬ê¸° ë…ë¦½ì  ì •ê·œí™” â€” ì†Œë°°ì¹˜ì—ì„œë„ ì•ˆì •ì  | â­â­ |
| UNet êµ¬ì¡° | ì¸ì½”ë”-ë³‘ëª©-ë””ì½”ë” with skip â€” ë…¸ì´ì¦ˆ ì˜ˆì¸¡ì˜ í‘œì¤€ ì•„í‚¤í…ì²˜ | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$PE(t, 2i) = \sin\!\left(\frac{t}{10000^{2i/d}}\right), \quad PE(t, 2i+1) = \cos\!\left(\frac{t}{10000^{2i/d}}\right)$$

$$\text{Cross-Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**04_conditional_diffusion_cfg** â€” Classifier-Free Guidance(CFG)ë¥¼ í†µí•´ í´ë˜ìŠ¤/í…ìŠ¤íŠ¸ ì¡°ê±´ë¶€ ìƒì„±ì„ êµ¬í˜„í•˜ê³ , ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ í’ˆì§ˆ-ë‹¤ì–‘ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""),
]

if __name__ == '__main__':
    create_notebook(cells, 'chapter13_genai_diffusion/03_unet_for_diffusion.ipynb')
