"""Generate Chapter 13-01: DDPM Theory and Math."""
import sys, os
sys.path.insert(0, os.path.dirname(__file__))
from nb_helper import md, code, create_notebook

cells = [
# â”€â”€â”€ Cell 0: í—¤ë” â”€â”€â”€
md(r"""# Chapter 13: ìƒì„± AI ì‹¬í™” â€” DDPM ì´ë¡ ê³¼ ìˆ˜í•™

## í•™ìŠµ ëª©í‘œ
- í™•ì‚° ëª¨ë¸(Diffusion Model)ì˜ **ë§ˆë¥´ì½”í”„ ì²´ì¸ ê¸°ë°˜ Forward/Reverse ê³¼ì •**ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•œë‹¤
- Reparameterization trickì„ ì‚¬ìš©í•˜ì—¬ **ì„ì˜ ì‹œì  $t$ì˜ ë…¸ì´ì¦ˆ ìƒ˜í”Œ** $x_t$ë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ìœ ë„í•œë‹¤
- ELBO(Evidence Lower Bound)ì—ì„œ **ë‹¨ìˆœí™”ëœ ì†ì‹¤ í•¨ìˆ˜** $\|\epsilon - \epsilon_\theta\|^2$ì´ ë„ì¶œë˜ëŠ” ê³¼ì •ì„ ì „ê°œí•œë‹¤
- TensorFlowë¡œ **ì„ í˜• ë² íƒ€ ìŠ¤ì¼€ì¤„**ê³¼ Forward Processë¥¼ êµ¬í˜„í•˜ê³  ì‹œê°í™”í•œë‹¤
- **1D ë°ì´í„°ì— ëŒ€í•œ ê°„ë‹¨í•œ DDPM í•™ìŠµ ë£¨í”„**ë¥¼ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ì—­ë°©í–¥ ìƒì„± ê³¼ì •ì„ ì²´í—˜í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Forward Process, Reverse Process, ELBO](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [ë² íƒ€ ìŠ¤ì¼€ì¤„ê³¼ ì•ŒíŒŒ ëˆ„ì ê³± êµ¬í˜„](#2.-ë² íƒ€-ìŠ¤ì¼€ì¤„-êµ¬í˜„)
3. [Forward Noising ê³¼ì • ì‹œê°í™”](#3.-Forward-Noising-ì‹œê°í™”)
4. [ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤í…ì—ì„œì˜ ë…¸ì´ì¦ˆ ì‹œê°í™”](#4.-íƒ€ì„ìŠ¤í…ë³„-ë…¸ì´ì¦ˆ)
5. [1D DDPM í•™ìŠµ ë£¨í”„ êµ¬í˜„](#5.-1D-DDPM-í•™ìŠµ)
6. [ì •ë¦¬](#6.-ì •ë¦¬)"""),

# â”€â”€â”€ Cell 1: ìˆ˜í•™ì  ê¸°ì´ˆ â”€â”€â”€
md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### Forward Process (í™•ì‚° ê³¼ì •)

DDPMì˜ Forward ProcessëŠ” ê¹¨ë—í•œ ë°ì´í„° $x_0$ì— ì ì§„ì ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ëŠ” **ë§ˆë¥´ì½”í”„ ì²´ì¸**ì…ë‹ˆë‹¤:

$$q(x_t \mid x_{t-1}) = \mathcal{N}\!\left(x_t;\, \sqrt{1-\beta_t}\,x_{t-1},\, \beta_t I\right)$$

- $x_t$: ì‹œì  $t$ì—ì„œì˜ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ë°ì´í„°
- $\beta_t \in (0, 1)$: ì‹œì  $t$ì˜ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (ë¶„ì‚° í¬ê¸°)
- $I$: ë‹¨ìœ„ í–‰ë ¬ (ê° ì°¨ì›ì— ë…ë¦½ì ì¸ ë…¸ì´ì¦ˆ)
- $T$: ì´ í™•ì‚° ë‹¨ê³„ ìˆ˜ (ë³´í†µ 1000)

**í•µì‹¬ ì •ì˜:**

$$\alpha_t = 1 - \beta_t, \quad \bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s = \prod_{s=1}^{t}(1 - \beta_s)$$

- $\alpha_t$: ì‹œì  $t$ì—ì„œ ì›ë³¸ ì‹ í˜¸ì˜ ë³´ì¡´ ë¹„ìœ¨
- $\bar{\alpha}_t$: ì‹œì  $0$ë¶€í„° $t$ê¹Œì§€ì˜ ëˆ„ì  ì‹ í˜¸ ë³´ì¡´ ë¹„ìœ¨ (monotonically decreasing)

### Reparameterization Trick

$T$ë²ˆì˜ ìˆœì°¨ì  ë…¸ì´ì¦ˆ ì¶”ê°€ ì—†ì´, **í•œ ë²ˆì—** $x_0$ì—ì„œ $x_t$ë¥¼ ìƒ˜í”Œë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$q(x_t \mid x_0) = \mathcal{N}\!\left(x_t;\, \sqrt{\bar{\alpha}_t}\,x_0,\, (1-\bar{\alpha}_t)I\right)$$

$$\boxed{x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)}$$

| ì‹œì  | $\bar{\alpha}_t$ | $\sqrt{\bar{\alpha}_t}$ (ì‹ í˜¸ ê³„ìˆ˜) | $\sqrt{1-\bar{\alpha}_t}$ (ë…¸ì´ì¦ˆ ê³„ìˆ˜) | ì˜ë¯¸ |
|------|------------------|-----------------------------------|-----------------------------------------|------|
| $t=0$ | $1.0$ | $1.0$ | $0.0$ | ì›ë³¸ ë°ì´í„° |
| $t=T/2$ | $\approx 0.1$ | $\approx 0.32$ | $\approx 0.95$ | ëŒ€ë¶€ë¶„ ë…¸ì´ì¦ˆ |
| $t=T$ | $\approx 0$ | $\approx 0$ | $\approx 1.0$ | ìˆœìˆ˜ ë…¸ì´ì¦ˆ |

### Reverse Process (ì—­ë°©í–¥ ìƒì„±)

ì‹ ê²½ë§ $\epsilon_\theta$ê°€ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ì—¬, ë…¸ì´ì¦ˆì—ì„œ ê¹¨ë—í•œ ë°ì´í„°ë¥¼ ë³µì›í•©ë‹ˆë‹¤:

$$p_\theta(x_{t-1} \mid x_t) = \mathcal{N}\!\left(x_{t-1};\, \mu_\theta(x_t, t),\, \sigma_t^2 I\right)$$

ì—¬ê¸°ì„œ í‰ê·  $\mu_\theta$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤:

$$\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}}\left(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_\theta(x_t, t)\right)$$

- $\epsilon_\theta(x_t, t)$: ì‹ ê²½ë§ì´ ì˜ˆì¸¡í•œ ë…¸ì´ì¦ˆ
- $\sigma_t^2 = \beta_t$ ë˜ëŠ” $\sigma_t^2 = \tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t$

### ELBOì™€ Simple Loss

DDPMì˜ ìµœì í™” ëª©í‘œëŠ” **ELBO(Evidence Lower Bound)**ì—ì„œ ìœ ë„ë©ë‹ˆë‹¤:

$$\mathcal{L}_{VLB} = \mathbb{E}_q\!\left[\underbrace{D_{KL}(q(x_T|x_0)\|p(x_T))}_{L_T} + \sum_{t=2}^{T}\underbrace{D_{KL}(q(x_{t-1}|x_t,x_0)\|p_\theta(x_{t-1}|x_t))}_{L_{t-1}} - \underbrace{\log p_\theta(x_0|x_1)}_{L_0}\right]$$

Ho et al. (2020)ì€ ì´ë¥¼ **Simple Loss**ë¡œ ë‹¨ìˆœí™”í–ˆìŠµë‹ˆë‹¤:

$$\boxed{\mathcal{L}_{simple}(\theta) = \mathbb{E}_{t \sim U[1,T],\, x_0,\, \epsilon}\!\left[\|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon,\, t)\|^2\right]}$$

- $\epsilon \sim \mathcal{N}(0, I)$: ì‹¤ì œ ì¶”ê°€ëœ ë…¸ì´ì¦ˆ
- $\epsilon_\theta(\cdot, t)$: ì‹ ê²½ë§ì´ ì˜ˆì¸¡í•œ ë…¸ì´ì¦ˆ
- ì‹œì  $t$ëŠ” $\{1, 2, \ldots, T\}$ì—ì„œ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§

**ìš”ì•½ í‘œ:**

| ê³¼ì • | ìˆ˜ì‹ | ì—­í•  |
|------|------|------|
| Forward | $x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon$ | ë°ì´í„°ì— ë…¸ì´ì¦ˆ ì¶”ê°€ |
| Reverse | $x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta) + \sigma_t z$ | ë…¸ì´ì¦ˆ ì œê±° (ìƒì„±) |
| Simple Loss | $\|\epsilon - \epsilon_\theta(x_t, t)\|^2$ | ë…¸ì´ì¦ˆ ì˜ˆì¸¡ í•™ìŠµ |"""),

# â”€â”€â”€ Cell 2: ğŸ£ ì¹œì ˆ ì„¤ëª… â”€â”€â”€
md(r"""---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í™•ì‚° ëª¨ë¸ ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ í™•ì‚° ëª¨ë¸ì´ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: ê¹¨ë—í•œ ê·¸ë¦¼ ìœ„ì— **ëª¨ë˜ë¥¼ ì¡°ê¸ˆì”© ë¿Œë ¤ì„œ** ê²°êµ­ ì•„ë¬´ê²ƒë„ ì•ˆ ë³´ì´ê²Œ ë§Œë“œëŠ” ê³¼ì •(Forward)ì„ ìƒìƒí•´ ë³´ì„¸ìš”. í™•ì‚° ëª¨ë¸ì€ ì´ ê³¼ì •ì„ **ê±°ê¾¸ë¡œ** ë°°ì›Œì„œ, ëª¨ë˜ë”ë¯¸ì—ì„œ ê·¸ë¦¼ì„ ë³µì›í•˜ëŠ” ë§ˆë²•ì‚¬ì˜ˆìš”!

1. **Forward Process (ëª¨ë˜ ë¿Œë¦¬ê¸°)**: ì˜ˆìœ ê³ ì–‘ì´ ì‚¬ì§„ì— ëª¨ë˜ë¥¼ 1000ë²ˆ ë¿Œë¦¬ë©´ ê²°êµ­ íšŒìƒ‰ ë…¸ì´ì¦ˆë§Œ ë‚¨ì•„ìš”
2. **Reverse Process (ëª¨ë˜ ì¹˜ìš°ê¸°)**: AIê°€ "ì´ ëª¨ë˜ë”ë¯¸ì—ì„œ ëª¨ë˜ë¥¼ í•œ ì›€í¼ ì¹˜ìš°ë©´ ì–´ë–¤ ëª¨ì–‘ì´ ë‚˜ì˜¬ê¹Œ?"ë¥¼ í•™ìŠµí•´ìš”
3. **ìƒì„±**: ì™„ì „íˆ ëœë¤í•œ ëª¨ë˜ë”ë¯¸ì—ì„œ ì‹œì‘í•´ì„œ, í•œ ì›€í¼ì”© ì¹˜ìš°ë‹¤ ë³´ë©´... ìƒˆë¡œìš´ ê³ ì–‘ì´ ì‚¬ì§„ì´ ë‚˜íƒ€ë‚˜ìš”! ğŸ±

#### ğŸ² ì™œ $\bar{\alpha}_t$ê°€ ì¤‘ìš”í•œê°€ìš”?

| ë‹¨ê³„ | ë¹„ìœ  | ìˆ˜í•™ |
|------|------|------|
| $t=0$ (ì²˜ìŒ) | ê¹¨ë—í•œ ê·¸ë¦¼ | $\bar{\alpha}_0 = 1$ â†’ ì›ë³¸ 100% |
| $t=500$ (ì¤‘ê°„) | ê·¸ë¦¼ì´ íë¦¿ | $\bar{\alpha}_{500} \approx 0.1$ â†’ ì›ë³¸ 10% |
| $t=1000$ (ë) | ì™„ì „ ë…¸ì´ì¦ˆ | $\bar{\alpha}_{1000} \approx 0$ â†’ ì›ë³¸ 0% |

$\bar{\alpha}_t$ëŠ” "**ì›ë³¸ ê·¸ë¦¼ì´ ì–¼ë§ˆë‚˜ ë‚¨ì•„ìˆëŠ”ì§€**"ë¥¼ ì•Œë ¤ì£¼ëŠ” ìˆ«ìì˜ˆìš”!"""),

# â”€â”€â”€ Cell 3: ğŸ“ ì—°ìŠµ ë¬¸ì œ â”€â”€â”€
md(r"""---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: Reparameterization ê³„ì‚°

$\beta_1 = 0.0001$, $\beta_2 = 0.0002$ì¼ ë•Œ, $\bar{\alpha}_2$ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\alpha_1 = 1 - 0.0001 = 0.9999$$
$$\alpha_2 = 1 - 0.0002 = 0.9998$$
$$\bar{\alpha}_2 = \alpha_1 \cdot \alpha_2 = 0.9999 \times 0.9998 = 0.9997$$

$t=2$ì—ì„œëŠ” ì›ë³¸ ì‹ í˜¸ê°€ **99.97%** ë³´ì¡´ë©ë‹ˆë‹¤. ì´ˆê¸°ì—ëŠ” ë…¸ì´ì¦ˆê°€ ë§¤ìš° ì²œì²œíˆ ì¶”ê°€ë©ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 2: ë…¸ì´ì¦ˆ ê³„ìˆ˜ ë¹„êµ

$\bar{\alpha}_t = 0.5$ì¼ ë•Œ, ì‹ í˜¸ ê³„ìˆ˜ $\sqrt{\bar{\alpha}_t}$ì™€ ë…¸ì´ì¦ˆ ê³„ìˆ˜ $\sqrt{1-\bar{\alpha}_t}$ë¥¼ ê³„ì‚°í•˜ì„¸ìš”.

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\sqrt{\bar{\alpha}_t} = \sqrt{0.5} \approx 0.707$$
$$\sqrt{1 - \bar{\alpha}_t} = \sqrt{0.5} \approx 0.707$$

ì‹ í˜¸ì™€ ë…¸ì´ì¦ˆê°€ **ì •í™•íˆ ê°™ì€ ë¹„ìœ¨**! ì´ ì‹œì ì—ì„œ ì›ë³¸ê³¼ ë…¸ì´ì¦ˆê°€ ì ˆë°˜ì”© ì„ì—¬ ìˆìŠµë‹ˆë‹¤.

**ì°¸ê³ **: $\sqrt{\bar{\alpha}_t}^2 + \sqrt{1-\bar{\alpha}_t}^2 = \bar{\alpha}_t + (1-\bar{\alpha}_t) = 1$ â†’ ë¶„ì‚°ì´ í•­ìƒ ë³´ì¡´ë©ë‹ˆë‹¤.
</details>

#### ë¬¸ì œ 3: Simple Loss í•´ì„

Simple Loss $\|\epsilon - \epsilon_\theta(x_t, t)\|^2$ì—ì„œ, ì™„ë²½í•œ ëª¨ë¸($\epsilon_\theta = \epsilon$)ì¼ ë•Œ ì†ì‹¤ê°’ì€?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$\mathcal{L} = \|\epsilon - \epsilon\|^2 = \|0\|^2 = 0$$

ì†ì‹¤ì´ 0ì´ë©´ ëª¨ë¸ì´ ë…¸ì´ì¦ˆë¥¼ **ì™„ë²½í•˜ê²Œ ì˜ˆì¸¡**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹¤ì œë¡œëŠ” 0ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì´ë©°, í•™ìŠµ ì´ˆê¸°ì—ëŠ” ê°’ì´ í¬ê³  ì ì  ì¤„ì–´ë“­ë‹ˆë‹¤.
</details>"""),

# â”€â”€â”€ Cell 4: ì„í¬íŠ¸ â”€â”€â”€
code("""# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}")"""),

# â”€â”€â”€ Cell 5: ë² íƒ€ ìŠ¤ì¼€ì¤„ êµ¬í˜„ (Section 2) â”€â”€â”€
md(r"""## 2. ë² íƒ€ ìŠ¤ì¼€ì¤„ê³¼ ì•ŒíŒŒ ëˆ„ì ê³± êµ¬í˜„ <a name='2.-ë² íƒ€-ìŠ¤ì¼€ì¤„-êµ¬í˜„'></a>

DDPM ì›ë…¼ë¬¸(Ho et al., 2020; arxiv 2006.11239)ì—ì„œ ì‚¬ìš©í•œ **ì„ í˜•(linear) ë² íƒ€ ìŠ¤ì¼€ì¤„**ì„ êµ¬í˜„í•©ë‹ˆë‹¤:

$$\beta_t = \beta_{\min} + \frac{t-1}{T-1}(\beta_{\max} - \beta_{\min}), \quad t = 1, \ldots, T$$

ì›ë…¼ë¬¸ ì„¤ì •: $\beta_{\min} = 10^{-4}$, $\beta_{\max} = 0.02$, $T = 1000$"""),

# â”€â”€â”€ Cell 6: ë² íƒ€ ìŠ¤ì¼€ì¤„ ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ ì„ í˜• ë² íƒ€ ìŠ¤ì¼€ì¤„ ë° ì•ŒíŒŒ ëˆ„ì ê³± ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T = 1000
beta_min = 1e-4
beta_max = 0.02

# ì„ í˜• ìŠ¤ì¼€ì¤„: beta_1 = beta_min, beta_T = beta_max
betas = np.linspace(beta_min, beta_max, T)
alphas = 1.0 - betas
alpha_bars = np.cumprod(alphas)

print(f"ë² íƒ€ ìŠ¤ì¼€ì¤„ (ì„ í˜•)")
print(f"  Î²_1   = {betas[0]:.6f}")
print(f"  Î²_500 = {betas[499]:.6f}")
print(f"  Î²_T   = {betas[-1]:.6f}")
print()
print(f"ì•ŒíŒŒ ëˆ„ì ê³± (Î±Ì„_t)")
print(f"  á¾±_1   = {alpha_bars[0]:.6f}  (ì›ë³¸ {alpha_bars[0]*100:.2f}% ë³´ì¡´)")
print(f"  á¾±_250 = {alpha_bars[249]:.6f}  (ì›ë³¸ {alpha_bars[249]*100:.2f}% ë³´ì¡´)")
print(f"  á¾±_500 = {alpha_bars[499]:.6f}  (ì›ë³¸ {alpha_bars[499]*100:.4f}% ë³´ì¡´)")
print(f"  á¾±_750 = {alpha_bars[749]:.6f}  (ì›ë³¸ {alpha_bars[749]*100:.6f}% ë³´ì¡´)")
print(f"  á¾±_T   = {alpha_bars[-1]:.8f}  (ì›ë³¸ ê±°ì˜ 0%)")
print()
print(f"ì‹ í˜¸ ê³„ìˆ˜ âˆšá¾±_T   = {np.sqrt(alpha_bars[-1]):.6f}")
print(f"ë…¸ì´ì¦ˆ ê³„ìˆ˜ âˆš(1-á¾±_T) = {np.sqrt(1-alpha_bars[-1]):.6f}")"""),

# â”€â”€â”€ Cell 7: ì•ŒíŒŒë°” ì‹œê°í™” â”€â”€â”€
code(r"""# â”€â”€ ë² íƒ€ ìŠ¤ì¼€ì¤„ ë° ì•ŒíŒŒ ëˆ„ì ê³± ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# (1) ë² íƒ€ ìŠ¤ì¼€ì¤„
ax1 = axes[0]
ax1.plot(range(1, T+1), betas, 'b-', lw=2)
ax1.set_xlabel('Timestep $t$', fontsize=11)
ax1.set_ylabel(r'$\beta_t$', fontsize=11)
ax1.set_title(r'Linear $\beta$ Schedule', fontweight='bold')
ax1.grid(True, alpha=0.3)

# (2) ì•ŒíŒŒ ëˆ„ì ê³±
ax2 = axes[1]
ax2.plot(range(1, T+1), alpha_bars, 'r-', lw=2)
ax2.set_xlabel('Timestep $t$', fontsize=11)
ax2.set_ylabel(r'$\bar{\alpha}_t$', fontsize=11)
ax2.set_title(r'Cumulative $\bar{\alpha}_t$', fontweight='bold')
ax2.axhline(y=0.5, color='gray', ls='--', lw=1, label=r'$\bar{\alpha}_t = 0.5$')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

# (3) ì‹ í˜¸/ë…¸ì´ì¦ˆ ê³„ìˆ˜
ax3 = axes[2]
ax3.plot(range(1, T+1), np.sqrt(alpha_bars), 'g-', lw=2, label=r'Signal: $\sqrt{\bar{\alpha}_t}$')
ax3.plot(range(1, T+1), np.sqrt(1 - alpha_bars), 'm-', lw=2, label=r'Noise: $\sqrt{1-\bar{\alpha}_t}$')
ax3.set_xlabel('Timestep $t$', fontsize=11)
ax3.set_ylabel('Coefficient', fontsize=11)
ax3.set_title('Signal vs Noise Coefficients', fontweight='bold')
ax3.legend(fontsize=9)
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/beta_schedule.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/beta_schedule.png")"""),

# â”€â”€â”€ Cell 8: Forward Noising ì‹œê°í™” (Section 3) â”€â”€â”€
md(r"""## 3. Forward Noising ê³¼ì • ì‹œê°í™” <a name='3.-Forward-Noising-ì‹œê°í™”'></a>

2D ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ì‹œì‘í•˜ì—¬ Forward Processê°€ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ìˆœìˆ˜ ë…¸ì´ì¦ˆë¡œ ë³€í™˜í•˜ëŠ”ì§€ ì‹œê°í™”í•©ë‹ˆë‹¤.

$$x_t = \sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$$"""),

# â”€â”€â”€ Cell 9: Forward Noising ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ Forward Process: 2D ê°€ìš°ì‹œì•ˆì— ë…¸ì´ì¦ˆ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2D ê°€ìš°ì‹œì•ˆ í˜¼í•© ëª¨ë¸ (ì›ë³¸ ë°ì´í„°)
n_samples = 2000

# 3ê°œì˜ ê°€ìš°ì‹œì•ˆ í´ëŸ¬ìŠ¤í„°ë¡œ "ë°ì´í„° ë¶„í¬" ìƒì„±
centers = np.array([[2.0, 2.0], [-2.0, 2.0], [0.0, -2.0]])
x0_list = []
for c in centers:
    samples = np.random.randn(n_samples // 3, 2) * 0.4 + c
    x0_list.append(samples)
x0 = np.concatenate(x0_list, axis=0).astype(np.float32)

# Forward Process ì ìš©: ì—¬ëŸ¬ ì‹œì ì—ì„œì˜ x_t
timesteps_to_show = [0, 50, 200, 500, 800, 999]
noised_samples = {}

for t in timesteps_to_show:
    if t == 0:
        noised_samples[t] = x0.copy()
    else:
        abar = alpha_bars[t - 1]
        eps = np.random.randn(*x0.shape).astype(np.float32)
        x_t = np.sqrt(abar) * x0 + np.sqrt(1 - abar) * eps
        noised_samples[t] = x_t

# ì‹œê°í™”
fig, axes = plt.subplots(1, 6, figsize=(18, 3))

for idx, t in enumerate(timesteps_to_show):
    ax = axes[idx]
    data = noised_samples[t]
    ax.scatter(data[:, 0], data[:, 1], s=1, alpha=0.3, c='steelblue')
    abar_val = 1.0 if t == 0 else alpha_bars[t - 1]
    ax.set_title(f'$t={t}$\n' + r'$\bar{\alpha}=$' + f'{abar_val:.4f}', fontweight='bold', fontsize=9)
    ax.set_xlim(-5, 5)
    ax.set_ylim(-5, 5)
    ax.set_aspect('equal')
    ax.grid(True, alpha=0.3)

plt.suptitle('Forward Diffusion Process on 2D Gaussian Mixture', fontweight='bold', fontsize=12, y=1.05)
plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/forward_noising.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/forward_noising.png")
print(f"\nì‹œì ë³„ ë°ì´í„° í†µê³„:")
for t in timesteps_to_show:
    d = noised_samples[t]
    print(f"  t={t:4d} | í‰ê· : ({d[:,0].mean():.3f}, {d[:,1].mean():.3f}) | í‘œì¤€í¸ì°¨: ({d[:,0].std():.3f}, {d[:,1].std():.3f})")"""),

# â”€â”€â”€ Cell 10: íƒ€ì„ìŠ¤í…ë³„ ë…¸ì´ì¦ˆ ì‹œê°í™” (Section 4) â”€â”€â”€
md(r"""## 4. ë‹¤ì–‘í•œ íƒ€ì„ìŠ¤í…ì—ì„œì˜ ë…¸ì´ì¦ˆ ì‹œê°í™” <a name='4.-íƒ€ì„ìŠ¤í…ë³„-ë…¸ì´ì¦ˆ'></a>

1D ì‹ í˜¸ì— ëŒ€í•´ Forward Processë¥¼ ì ìš©í•˜ì—¬, ì‹œì ë³„ **ì‹ í˜¸ ëŒ€ ë…¸ì´ì¦ˆ ë¹„ìœ¨(SNR)**ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.

$$\text{SNR}(t) = \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t}$$"""),

# â”€â”€â”€ Cell 11: 1D ë…¸ì´ì¦ˆ ì‹œê°í™” ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ 1D ì‹ í˜¸ì— ëŒ€í•œ íƒ€ì„ìŠ¤í…ë³„ ë…¸ì´ì¦ˆ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1D ì›ë³¸ ì‹ í˜¸: ì‚¬ì¸íŒŒ
x_axis = np.linspace(0, 4 * np.pi, 200)
signal = np.sin(x_axis).astype(np.float32)

timesteps_demo = [0, 100, 300, 500, 700, 999]

fig, axes = plt.subplots(2, 3, figsize=(15, 7))

for idx, t in enumerate(timesteps_demo):
    row, col = divmod(idx, 3)
    ax = axes[row][col]

    if t == 0:
        noised = signal.copy()
        abar = 1.0
    else:
        abar = alpha_bars[t - 1]
        eps = np.random.randn(*signal.shape).astype(np.float32)
        noised = np.sqrt(abar) * signal + np.sqrt(1 - abar) * eps

    ax.plot(x_axis, signal, 'b-', alpha=0.3, lw=1, label='ì›ë³¸ ì‹ í˜¸')
    ax.plot(x_axis, noised, 'r-', lw=1.5, label=f'$x_t$ (t={t})')

    snr = abar / (1 - abar + 1e-10)
    ax.set_title(f't={t} | ' + r'$\bar{\alpha}$=' + f'{abar:.4f} | SNR={snr:.2f}', fontweight='bold', fontsize=9)
    ax.legend(fontsize=8, loc='upper right')
    ax.grid(True, alpha=0.3)
    ax.set_ylim(-3.5, 3.5)

plt.suptitle('Forward Process: 1D ì‚¬ì¸íŒŒì— ëŒ€í•œ íƒ€ì„ìŠ¤í…ë³„ ë…¸ì´ì¦ˆ', fontweight='bold', fontsize=12, y=1.02)
plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/timestep_noise.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/timestep_noise.png")

# SNR ë¶„ì„
print("\níƒ€ì„ìŠ¤í…ë³„ SNR (Signal-to-Noise Ratio):")
print(f"{'ì‹œì ':>6} | {'á¾±_t':>10} | {'SNR':>10} | {'SNR(dB)':>10}")
print("-" * 48)
for t in [1, 100, 250, 500, 750, 1000]:
    abar = alpha_bars[t-1]
    snr = abar / (1 - abar + 1e-10)
    snr_db = 10 * np.log10(snr + 1e-10)
    print(f"  t={t:4d} | {abar:10.6f} | {snr:10.4f} | {snr_db:10.2f} dB")"""),

# â”€â”€â”€ Cell 12: 1D DDPM í•™ìŠµ (Section 5) â”€â”€â”€
md(r"""## 5. 1D DDPM í•™ìŠµ ë£¨í”„ êµ¬í˜„ <a name='5.-1D-DDPM-í•™ìŠµ'></a>

ê°„ë‹¨í•œ 1D ë°ì´í„° ë¶„í¬(ê°€ìš°ì‹œì•ˆ í˜¼í•©)ì— ëŒ€í•´ DDPMì˜ í•™ìŠµê³¼ ìƒ˜í”Œë§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

**í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ (Algorithm 1 from Ho et al.):**
1. $x_0 \sim q(x_0)$ì—ì„œ ë°ì´í„° ìƒ˜í”Œë§
2. $t \sim \text{Uniform}\{1, \ldots, T\}$ì—ì„œ ì‹œì  ìƒ˜í”Œë§
3. $\epsilon \sim \mathcal{N}(0, I)$ì—ì„œ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§
4. ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤í…: $\nabla_\theta \|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}\,x_0 + \sqrt{1-\bar{\alpha}_t}\,\epsilon,\, t)\|^2$"""),

# â”€â”€â”€ Cell 13: 1D DDPM í•™ìŠµ ì½”ë“œ â”€â”€â”€
code(r"""# â”€â”€ 1D DDPM í•™ìŠµ ë° ìƒ˜í”Œë§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T_steps = 200
beta_min_1d = 1e-4
beta_max_1d = 0.02
betas_1d = np.linspace(beta_min_1d, beta_max_1d, T_steps).astype(np.float32)
alphas_1d = 1.0 - betas_1d
alpha_bars_1d = np.cumprod(alphas_1d).astype(np.float32)

# 1D ë°ì´í„° ë¶„í¬: 2ê°œì˜ ê°€ìš°ì‹œì•ˆ í˜¼í•©
def sample_data(n):
    mix = np.random.choice([0, 1], size=n)
    samples = np.where(mix == 0,
                       np.random.randn(n) * 0.5 + 3.0,
                       np.random.randn(n) * 0.5 - 3.0)
    return samples.astype(np.float32)

# ë…¸ì´ì¦ˆ ì˜ˆì¸¡ ëª¨ë¸ (ê°„ë‹¨í•œ MLP)
class NoisePredictor(tf.keras.Model):
    # 1D ë…¸ì´ì¦ˆ ì˜ˆì¸¡ MLP: (x_t, t_embedding) -> epsilon
    def __init__(self):
        super().__init__()
        self.time_embed = tf.keras.layers.Dense(64, activation='swish')
        self.net = tf.keras.Sequential([
            tf.keras.layers.Dense(128, activation='swish'),
            tf.keras.layers.Dense(128, activation='swish'),
            tf.keras.layers.Dense(64, activation='swish'),
            tf.keras.layers.Dense(1)
        ])

    def call(self, x_t, t):
        t_emb = self.time_embed(tf.cast(t[:, None], tf.float32) / T_steps)
        x_input = tf.concat([x_t[:, None], t_emb], axis=-1)
        return self.net(x_input)[:, 0]

model = NoisePredictor()
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

alpha_bars_tf = tf.constant(alpha_bars_1d)

# í•™ìŠµ ë£¨í”„
losses = []
n_epochs = 300
batch_size = 512

for epoch in range(n_epochs):
    x0_batch = sample_data(batch_size)
    t_batch = np.random.randint(0, T_steps, size=batch_size)
    eps_batch = np.random.randn(batch_size).astype(np.float32)

    abar_t = alpha_bars_1d[t_batch]
    x_t = np.sqrt(abar_t) * x0_batch + np.sqrt(1 - abar_t) * eps_batch

    with tf.GradientTape() as tape:
        eps_pred = model(tf.constant(x_t), tf.constant(t_batch))
        loss = tf.reduce_mean((tf.constant(eps_batch) - eps_pred) ** 2)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    losses.append(float(loss))

    if (epoch + 1) % 100 == 0:
        print(f"Epoch {epoch+1:4d}/{n_epochs} | Loss: {float(loss):.4f}")

print(f"\nìµœì¢… í•™ìŠµ Loss: {losses[-1]:.4f}")
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(np.prod(v.shape) for v in model.trainable_variables):,}")"""),

# â”€â”€â”€ Cell 14: DDPM ìƒ˜í”Œë§ ë° ê²°ê³¼ ì‹œê°í™” â”€â”€â”€
code(r"""# â”€â”€ DDPM ìƒ˜í”Œë§ (ì—­ë°©í–¥ ê³¼ì •) ë° ê²°ê³¼ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reverse Process: x_T ~ N(0,1) â†’ x_0
n_gen = 3000
x_t = np.random.randn(n_gen).astype(np.float32)

# ì—­ë°©í–¥ ìƒ˜í”Œë§
for t in reversed(range(T_steps)):
    t_tensor = tf.constant(np.full(n_gen, t, dtype=np.int32))
    eps_pred = model(tf.constant(x_t), t_tensor).numpy()

    alpha_t = alphas_1d[t]
    abar_t = alpha_bars_1d[t]
    beta_t = betas_1d[t]

    # í‰ê·  ê³„ì‚°: mu_theta
    mu = (1.0 / np.sqrt(alpha_t)) * (x_t - (beta_t / np.sqrt(1 - abar_t)) * eps_pred)

    if t > 0:
        z = np.random.randn(n_gen).astype(np.float32)
        sigma = np.sqrt(beta_t)
        x_t = mu + sigma * z
    else:
        x_t = mu

generated = x_t

# ì‹œê°í™”: ì›ë³¸ ë¶„í¬ vs ìƒì„± ë¶„í¬
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

# (1) í•™ìŠµ Loss ê³¡ì„ 
ax1 = axes[0]
ax1.plot(losses, 'b-', lw=1, alpha=0.5)
window = 20
smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')
ax1.plot(range(window-1, len(losses)), smoothed, 'r-', lw=2, label=f'{window}-ì—í­ ì´ë™í‰ê· ')
ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('Loss', fontsize=11)
ax1.set_title('í•™ìŠµ Loss ê³¡ì„ ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.grid(True, alpha=0.3)

# (2) ì›ë³¸ vs ìƒì„± ë¶„í¬
real_data = sample_data(n_gen)
ax2 = axes[1]
ax2.hist(real_data, bins=80, density=True, alpha=0.6, color='steelblue', label='ì›ë³¸ ë¶„í¬')
ax2.hist(generated, bins=80, density=True, alpha=0.6, color='coral', label='ìƒì„± ë¶„í¬')
ax2.set_xlabel('x', fontsize=11)
ax2.set_ylabel('Density', fontsize=11)
ax2.set_title('ì›ë³¸ vs DDPM ìƒì„± ë¶„í¬', fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

# (3) í•™ìŠµ ê³¡ì„  (ë¡œê·¸ ìŠ¤ì¼€ì¼)
ax3 = axes[2]
ax3.semilogy(smoothed, 'r-', lw=2)
ax3.set_xlabel('Epoch', fontsize=11)
ax3.set_ylabel('Loss (log)', fontsize=11)
ax3.set_title('Loss ê³¡ì„  (ë¡œê·¸ ìŠ¤ì¼€ì¼)', fontweight='bold')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('chapter13_genai_diffusion/ddpm_1d_result.png', dpi=100, bbox_inches='tight')
plt.close()
print("ê·¸ë˜í”„ ì €ì¥ë¨: chapter13_genai_diffusion/ddpm_1d_result.png")

# í†µê³„ ë¹„êµ
print(f"\në¶„í¬ í†µê³„ ë¹„êµ:")
print(f"{'':>12} | {'í‰ê· ':>8} | {'í‘œì¤€í¸ì°¨':>8} | {'ìµœì†Œ':>8} | {'ìµœëŒ€':>8}")
print("-" * 55)
print(f"{'ì›ë³¸ ë¶„í¬':>12} | {real_data.mean():>8.3f} | {real_data.std():>8.3f} | {real_data.min():>8.3f} | {real_data.max():>8.3f}")
print(f"{'ìƒì„± ë¶„í¬':>12} | {generated.mean():>8.3f} | {generated.std():>8.3f} | {generated.min():>8.3f} | {generated.max():>8.3f}")"""),

# â”€â”€â”€ Cell 15: ì •ë¦¬ (Section 6) â”€â”€â”€
md(r"""## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| Forward Process | $q(x_t \mid x_0) = \mathcal{N}(\sqrt{\bar\alpha_t}x_0, (1-\bar\alpha_t)I)$ â€” ë°ì´í„°ì— ì ì§„ì  ë…¸ì´ì¦ˆ ì¶”ê°€ | â­â­â­ |
| Reparameterization | $x_t = \sqrt{\bar\alpha_t}x_0 + \sqrt{1-\bar\alpha_t}\epsilon$ â€” í•œ ë²ˆì— $x_t$ ê³„ì‚° | â­â­â­ |
| Reverse Process | $\mu_\theta = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta)$ â€” ë…¸ì´ì¦ˆ ì œê±° | â­â­â­ |
| Simple Loss | $\|\epsilon - \epsilon_\theta(x_t, t)\|^2$ â€” ELBOì—ì„œ ìœ ë„ëœ ë‹¨ìˆœí™” ëª©ì í•¨ìˆ˜ | â­â­â­ |
| ë² íƒ€ ìŠ¤ì¼€ì¤„ | $\beta_t$ì˜ ìŠ¤ì¼€ì¤„ì´ ìƒì„± í’ˆì§ˆì— ê²°ì •ì  ì˜í–¥ | â­â­ |
| $\bar\alpha_t$ ëˆ„ì ê³± | Forward Processì˜ í•µì‹¬ â€” ì›ë³¸ ì‹ í˜¸ ë³´ì¡´ ë¹„ìœ¨ | â­â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$q(x_t \mid x_0) = \mathcal{N}\!\left(x_t;\, \sqrt{\bar\alpha_t}\,x_0,\, (1-\bar\alpha_t)I\right)$$

$$\mathcal{L}_{simple} = \mathbb{E}_{t,x_0,\epsilon}\!\left[\|\epsilon - \epsilon_\theta(\sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon,\, t)\|^2\right]$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
**02_noise_schedules_and_samplers** â€” Linear/Cosine/EDM ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ ë¹„êµì™€ DDIM ê°€ì† ìƒ˜í”Œë§ì„ ë‹¤ë£¹ë‹ˆë‹¤."""),
]

if __name__ == '__main__':
    create_notebook(cells, 'chapter13_genai_diffusion/01_ddpm_theory_and_math.ipynb')
