import sys, os
sys.path.insert(0, '/workspace/scripts')
from nb_helper import md, code, create_notebook

cells = []

# â”€â”€ Cell 1: Header â”€â”€
cells.append(md(r"""# Chapter 17: ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT â€” Soraì™€ HunyuanVideo ì•„í‚¤í…ì²˜

## í•™ìŠµ ëª©í‘œ
- Soraì˜ ìŠ¤ì¼€ì¼ë§ ì² í•™ê³¼ NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ê¸°ë²•ì„ ì´í•´í•œë‹¤
- HunyuanVideoì˜ Dual-stream â†’ Single-stream ì „í™˜ êµ¬ì¡°ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë¶„ì„í•œë‹¤
- 3D Causal VAEì˜ ì‹œê³µê°„ ì••ì¶• ë©”ì»¤ë‹ˆì¦˜ì„ ì •ëŸ‰ì ìœ¼ë¡œ íŒŒì•…í•œë‹¤
- í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ë©€í‹°ëª¨ë‹¬ í“¨ì „ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ë¹„êµí•œë‹¤
- ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ íë¦„ì„ êµ¬í˜„ ìˆ˜ì¤€ì—ì„œ ì´í•´í•œë‹¤

## ëª©ì°¨
1. [ìˆ˜í•™ì  ê¸°ì´ˆ: NaViTì™€ ë©€í‹°ëª¨ë‹¬ í“¨ì „](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)
2. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •](#2.-í™˜ê²½-ì„¤ì •)
3. [NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜](#3.-NaViT-íŒ¨í‚¹)
4. [Dual-stream vs Single-stream ì•„í‚¤í…ì²˜ ë¹„êµ](#4.-Dual-vs-Single)
5. [HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„](#5.-ì••ì¶•-ë¶„ì„)
6. [ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ê°œìš”](#6.-íŒŒì´í”„ë¼ì¸)
7. [ì •ë¦¬](#7.-ì •ë¦¬)"""))

# â”€â”€ Cell 2: Math Section â”€â”€
cells.append(md(r"""## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>

### NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ (Variable Resolution Packing)

NaViT(Native Resolution ViT)ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•´ìƒë„ì˜ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ë¥¼ **í•˜ë‚˜ì˜ ë°°ì¹˜**ë¡œ íŒ¨í‚¹í•©ë‹ˆë‹¤:

$$\text{Batch} = \{x_1^{H_1 \times W_1},\; x_2^{H_2 \times W_2},\; \ldots,\; x_B^{H_B \times W_B}\}$$

ê° ìƒ˜í”Œì˜ íŒ¨ì¹˜ ìˆ˜:

$$N_i = \frac{H_i}{p_h} \cdot \frac{W_i}{p_w}, \quad \text{ì´ í† í° ìˆ˜} = \sum_{i=1}^B N_i$$

- **íŒ¨ë”© ì—†ìŒ**: ê° ìƒ˜í”Œì„ ì›ë³¸ í•´ìƒë„ ê·¸ëŒ€ë¡œ í† í°í™”
- **Attention Mask**: ë‹¤ë¥¸ ìƒ˜í”Œì˜ í† í°ì— attendí•˜ì§€ ì•Šë„ë¡ ë¸”ë¡ ëŒ€ê° ë§ˆìŠ¤í¬ ì ìš©
- í•µì‹¬ ì¥ì : ì •ì‚¬ê°í˜• crop/resize ì—†ì´ ì›ë³¸ ë¹„ìœ¨ ìœ ì§€ â†’ ìƒì„± í’ˆì§ˆ í–¥ìƒ

### Dual-stream ì•„í‚¤í…ì²˜

í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ë¥¼ **ë¶„ë¦¬ëœ** Transformer ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì²˜ë¦¬:

$$h_{\text{text}}^{(l)} = \text{SelfAttn}(h_{\text{text}}^{(l-1)}) + \text{CrossAttn}(h_{\text{text}}^{(l-1)}, h_{\text{video}}^{(l-1)})$$

$$h_{\text{video}}^{(l)} = \text{SelfAttn}(h_{\text{video}}^{(l-1)}) + \text{CrossAttn}(h_{\text{video}}^{(l-1)}, h_{\text{text}}^{(l-1)})$$

- ê° ëª¨ë‹¬ë¦¬í‹°ê°€ ë…ìì ì¸ Self-Attention ìˆ˜í–‰
- Cross-Attentionìœ¼ë¡œ ìƒí˜¸ ì •ë³´ êµí™˜
- íŒŒë¼ë¯¸í„°ê°€ ë‘ ë°°ì´ì§€ë§Œ, ê° ëª¨ë‹¬ë¦¬í‹°ì˜ íŠ¹ì„±ì„ ë³´ì¡´

### Single-stream ì•„í‚¤í…ì²˜

í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ í† í°ì„ **ì—°ê²°(concatenate)** í›„ ë‹¨ì¼ Transformer:

$$h^{(l)} = \text{SelfAttn}\!\left([h_{\text{text}}; h_{\text{video}}]^{(l-1)}\right)$$

- ì‹œí€€ìŠ¤ ê¸¸ì´: $N_{\text{text}} + N_{\text{video}}$
- ëª¨ë“  í† í°ì´ ì„œë¡œ attend â†’ ìì—°ìŠ¤ëŸ¬ìš´ ì •ë³´ ìœµí•©
- íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì´ë‚˜ ê¸´ ì‹œí€€ìŠ¤ ë¬¸ì œ

### HunyuanVideo: Dual â†’ Single ì „í™˜

HunyuanVideoëŠ” **ì•ìª½ ë ˆì´ì–´ì—ì„œ Dual-stream**, **ë’·ìª½ ë ˆì´ì–´ì—ì„œ Single-stream**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

$$\text{Layer } 1 \sim L_d: \text{Dual-stream} \quad \rightarrow \quad \text{Layer } L_d+1 \sim L: \text{Single-stream}$$

- ì´ˆê¸°: ê° ëª¨ë‹¬ë¦¬í‹°ê°€ ë…ë¦½ì  íŠ¹ì§• í˜•ì„±
- í›„ê¸°: ê¹Šì€ ë©€í‹°ëª¨ë‹¬ ìœµí•©ìœ¼ë¡œ ì •ë°€í•œ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ì •í•©

### 3D Causal VAE ì••ì¶•

$$z \in \mathbb{R}^{C_z \times (T/M_t) \times (H/M_h) \times (W/M_w)}$$

- $M_t, M_h, M_w$: ì‹œê°„/ê³µê°„ ì••ì¶• ë¹„ìœ¨
- HunyuanVideo: $M_t=4, M_h=8, M_w=8$
- ì••ì¶•ë¥ : $M_t \times M_h \times M_w = 256$ë°°

**ìš”ì•½ í‘œ:**

| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |
|------|------|------|
| NaViT íŒ¨í‚¹ | $N = \sum_i (H_i/p)(W_i/p)$ | ê°€ë³€ í•´ìƒë„ ì´ í† í° ìˆ˜ |
| Dual-stream | ë³„ë„ Self-Attn + Cross-Attn | ëª¨ë‹¬ë¦¬í‹° ë¶„ë¦¬ ì²˜ë¦¬ |
| Single-stream | $[h_{\text{text}}; h_{\text{video}}]$ ì—°ê²° | í†µí•© Self-Attn |
| 3D VAE ì••ì¶• | $M_t \times M_h \times M_w$ | ì‹œê³µê°„ ì••ì¶•ë¥  |

---

### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Sora/HunyuanVideo ì¹œì ˆ ì„¤ëª…!

#### ğŸ”¢ Soraê°€ ë­”ê°€ìš”?

> ğŸ’¡ **ë¹„ìœ **: SoraëŠ” "ì´ì•¼ê¸°ë¥¼ ë“£ê³  ì˜í™”ë¥¼ ë§Œë“œëŠ” AI ê°ë…"ì´ì—ìš”!

"ê°•ì•„ì§€ê°€ í•´ë³€ì—ì„œ ë›°ì–´ë…¸ëŠ” ì¥ë©´"ì´ë¼ê³  ë§í•˜ë©´, AIê°€ ì§„ì§œ ê°™ì€ ì˜ìƒì„ ë§Œë“¤ì–´ì¤˜ìš”.
ë¹„ê²°ì€ ì‚¬ì§„ì„ í¼ì¦ ì¡°ê°(íŒ¨ì¹˜)ìœ¼ë¡œ ë‚˜ëˆ„ê³ , Transformerê°€ ì¡°ê°ë“¤ì„ ì¡°í•©í•˜ëŠ” ê±°ì˜ˆìš”.

- ğŸ§© **NaViT**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ í¼ì¦ì„ í•œ ìƒìì— ë‹´ëŠ” ê¸°ìˆ  (í° ì‚¬ì§„, ì‘ì€ ì‚¬ì§„ ëª¨ë‘ OK!)
- ğŸ¬ **ë¹„ë””ì˜¤ íŒ¨ì¹˜**: ì‹œê°„ ë°©í–¥ìœ¼ë¡œë„ í¼ì¦ì„ ë‚˜ëˆ” (ì‚¬ì§„ â†’ ë™ì˜ìƒ í™•ì¥)

#### ğŸ¤” Dual-streamì´ë‘ Single-streamì€ ë­ê°€ ë‹¬ë¼ìš”?

> ğŸ’¡ **ë¹„ìœ **: Dual-streamì€ "í†µì—­ì‚¬ê°€ ìˆëŠ” íšŒì˜", Single-streamì€ "ê°™ì€ ì–¸ì–´ë¡œ ëŒ€í™”í•˜ëŠ” íšŒì˜"

Dual-stream: ê¸€(í…ìŠ¤íŠ¸)íŒ€ê³¼ ì˜ìƒ(ë¹„ë””ì˜¤)íŒ€ì´ ë”°ë¡œ ì¼í•œ ë’¤, í†µì—­ì‚¬(Cross-Attention)ê°€ ì—°ê²°
Single-stream: ê¸€ê³¼ ì˜ìƒì„ í•œ í…Œì´ë¸”ì— ëª¨ì•„ë†“ê³  í•¨ê»˜ í† ë¡ 

HunyuanVideoëŠ” ì²˜ìŒì—” Dual(ë”°ë¡œ), ë‚˜ì¤‘ì—” Single(í•¨ê»˜)ë¡œ ì§„í–‰í•´ìš”!

---

### ğŸ“ ì—°ìŠµ ë¬¸ì œ

#### ë¬¸ì œ 1: NaViT íŒ¨í‚¹ ê³„ì‚°

íŒ¨ì¹˜ í¬ê¸° $p=16$ì¼ ë•Œ, í•´ìƒë„ $256 \times 256$, $512 \times 384$, $128 \times 128$ ì„¸ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ íŒ¨í‚¹í•˜ë©´ ì´ í† í° ìˆ˜ëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

$$N_1 = \frac{256}{16} \times \frac{256}{16} = 16 \times 16 = 256$$

$$N_2 = \frac{512}{16} \times \frac{384}{16} = 32 \times 24 = 768$$

$$N_3 = \frac{128}{16} \times \frac{128}{16} = 8 \times 8 = 64$$

$$N_{\text{total}} = 256 + 768 + 64 = 1088 \text{ í† í°}$$

ê¸°ì¡´ ë°©ì‹(512ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)ì´ë©´ $3 \times 32 \times 32 = 3072$ í† í° â†’ NaViTê°€ **64.6% ì ˆê°**!
</details>

#### ë¬¸ì œ 2: 3D VAE ì••ì¶•ë¥ 

720p ë¹„ë””ì˜¤ (1280Ã—720, 30fps, 5ì´ˆ)ë¥¼ HunyuanVideoì˜ 3D Causal VAE ($M_t=4, M_h=8, M_w=8$, $C_z=16$)ë¡œ ì••ì¶•í•˜ë©´ latent í…ì„œ í¬ê¸°ëŠ”?

<details>
<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>

ì›ë³¸: $T=150, H=720, W=1280, C=3$

$$\text{Latent}: C_z \times \frac{T}{M_t} \times \frac{H}{M_h} \times \frac{W}{M_w} = 16 \times \frac{150}{4} \times \frac{720}{8} \times \frac{1280}{8}$$
$$= 16 \times 37 \times 90 \times 160 \approx 8{,}524{,}800 \text{ ê°’}$$

ì›ë³¸ í¬ê¸°: $150 \times 720 \times 1280 \times 3 = 414{,}720{,}000$ ê°’

ì••ì¶•ë¥ : $414{,}720{,}000 / 8{,}524{,}800 \approx 48.6\times$
</details>"""))

# â”€â”€ Cell 3: Section 2 MD â”€â”€
cells.append(md(r"""## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • <a name='2.-í™˜ê²½-ì„¤ì •'></a>"""))

# â”€â”€ Cell 4: Imports â”€â”€
cells.append(code(r"""# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import tensorflow as tf
import time

np.random.seed(42)
tf.random.set_seed(42)
print(f"TensorFlow ë²„ì „: {tf.__version__}")
print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}")"""))

# â”€â”€ Cell 4: MD NaViT â”€â”€
cells.append(md(r"""## 3. NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜ <a name='3.-NaViT-íŒ¨í‚¹'></a>

NaViTëŠ” ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ íŒ¨í‚¹í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
ë¸”ë¡ ëŒ€ê° Attention Maskë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ìƒ˜í”Œ ê°„ì˜ ì •ë³´ ëˆ„ì¶œì„ ë°©ì§€í•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 5: NaViT Code â”€â”€
cells.append(code(r"""# â”€â”€ NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
patch_size = 16

resolutions = [
    ('A: 256x256', 256, 256),
    ('B: 512x384', 512, 384),
    ('C: 128x128', 128, 128),
    ('D: 384x256', 384, 256),
]

print("=== NaViT íŒ¨í‚¹ ë¶„ì„ ===\n")
print(f"íŒ¨ì¹˜ í¬ê¸°: {patch_size}x{patch_size}\n")

total_tokens = 0
sample_sizes = []
print(f"{'ìƒ˜í”Œ':<15} | {'í•´ìƒë„':>12} | {'íŒ¨ì¹˜ ìˆ˜':>10} | {'í† í° ìˆ˜':>10}")
print("-" * 55)

for name, h, w in resolutions:
    n_patches = (h // patch_size) * (w // patch_size)
    total_tokens += n_patches
    sample_sizes.append(n_patches)
    print(f"{name:<15} | {h:>4}x{w:<4}   | {(h//patch_size):>3}x{(w//patch_size):<3}  | {n_patches:>10}")

print(f"\nì´ íŒ¨í‚¹ í† í° ìˆ˜: {total_tokens}")
max_res = max(h * w for _, h, w in resolutions)
max_side = int(np.sqrt(max_res))
naive_tokens = len(resolutions) * (max_side // patch_size) ** 2
print(f"Naive íŒ¨ë”© ë°©ì‹ (ëª¨ë‘ {max_side}x{max_side}ë¡œ ë¦¬ì‚¬ì´ì¦ˆ): {naive_tokens}")
print(f"NaViT í† í° ì ˆê°ë¥ : {(1 - total_tokens / naive_tokens) * 100:.1f}%")

# Attention Mask ì‹œê°í™”
mask = np.zeros((total_tokens, total_tokens))
offset = 0
for size in sample_sizes:
    mask[offset:offset+size, offset:offset+size] = 1.0
    offset += size

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

ax1 = axes[0]
im = ax1.imshow(mask, cmap='Blues', interpolation='nearest')
offset = 0
for i, size in enumerate(sample_sizes):
    ax1.axhline(y=offset-0.5, color='red', lw=1.5)
    ax1.axvline(x=offset-0.5, color='red', lw=1.5)
    ax1.text(offset + size/2, offset + size/2, resolutions[i][0].split(':')[0],
             ha='center', va='center', fontsize=10, fontweight='bold', color='darkblue')
    offset += size
ax1.set_xlabel('Key/Value í† í°', fontsize=11)
ax1.set_ylabel('Query í† í°', fontsize=11)
ax1.set_title('NaViT ë¸”ë¡ ëŒ€ê° Attention Mask', fontweight='bold')

ax2 = axes[1]
names = [r[0].split(':')[0] for r in resolutions]
colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']
bars = ax2.bar(names, sample_sizes, color=colors, edgecolor='white', lw=1.5)
for bar, val in zip(bars, sample_sizes):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
             str(val), ha='center', fontsize=10, fontweight='bold')
ax2.set_ylabel('í† í° ìˆ˜', fontsize=11)
ax2.set_title('ìƒ˜í”Œë³„ í† í° ìˆ˜ ë¶„í¬', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/navit_packing.png', dpi=100, bbox_inches='tight')
plt.close()
print("\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/navit_packing.png")"""))

# â”€â”€ Cell 6: MD Dual vs Single â”€â”€
cells.append(md(r"""## 4. Dual-stream vs Single-stream ì•„í‚¤í…ì²˜ ë¹„êµ <a name='4.-Dual-vs-Single'></a>

HunyuanVideoëŠ” ì•ìª½ ë ˆì´ì–´ì—ì„œ Dual-stream (ë¶„ë¦¬ ì²˜ë¦¬ + Cross-Attention), ë’·ìª½ ë ˆì´ì–´ì—ì„œ Single-stream (ì—°ê²° + Self-Attention)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ë‘ ë°©ì‹ì˜ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ íŠ¹ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤."""))

# â”€â”€ Cell 7: Dual vs Single Code â”€â”€
cells.append(code(r"""# â”€â”€ Dual-stream vs Single-stream ë¹„êµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
d_model = 128
n_heads = 8
d_head = d_model // n_heads

n_text_tokens = 64
n_video_tokens = 256

# Dual-stream ë¸”ë¡ (ê°„ì†Œí™”)
class DualStreamBlock(tf.keras.layers.Layer):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.text_self_attn = tf.keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads)
        self.video_self_attn = tf.keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads)
        self.text_cross_attn = tf.keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads)
        self.video_cross_attn = tf.keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads)
        self.text_ln = tf.keras.layers.LayerNormalization()
        self.video_ln = tf.keras.layers.LayerNormalization()

    def call(self, h_text, h_video):
        t_self = self.text_self_attn(h_text, h_text)
        v_self = self.video_self_attn(h_video, h_video)
        t_cross = self.text_cross_attn(h_text, h_video)
        v_cross = self.video_cross_attn(h_video, h_text)
        h_text = self.text_ln(h_text + t_self + t_cross)
        h_video = self.video_ln(h_video + v_self + v_cross)
        return h_text, h_video

# Single-stream ë¸”ë¡ (ê°„ì†Œí™”)
class SingleStreamBlock(tf.keras.layers.Layer):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.self_attn = tf.keras.layers.MultiHeadAttention(
            num_heads=n_heads, key_dim=d_model // n_heads)
        self.ln = tf.keras.layers.LayerNormalization()

    def call(self, h_text, h_video):
        h_concat = tf.concat([h_text, h_video], axis=1)
        h_out = self.self_attn(h_concat, h_concat)
        h_out = self.ln(h_concat + h_out)
        h_text_out = h_out[:, :tf.shape(h_text)[1], :]
        h_video_out = h_out[:, tf.shape(h_text)[1]:, :]
        return h_text_out, h_video_out

dual_block = DualStreamBlock(d_model, n_heads)
single_block = SingleStreamBlock(d_model, n_heads)

h_text = tf.random.normal([1, n_text_tokens, d_model])
h_video = tf.random.normal([1, n_video_tokens, d_model])

# Dual-stream forward
t0 = time.time()
for _ in range(10):
    dt_out, dv_out = dual_block(h_text, h_video)
dual_time = (time.time() - t0) / 10

# Single-stream forward
t0 = time.time()
for _ in range(10):
    st_out, sv_out = single_block(h_text, h_video)
single_time = (time.time() - t0) / 10

dual_params = sum(p.numpy().size for p in dual_block.trainable_variables)
single_params = sum(p.numpy().size for p in single_block.trainable_variables)

print("=== Dual-stream vs Single-stream ë¹„êµ ===\n")
print(f"{'í•­ëª©':<20} | {'Dual-stream':>15} | {'Single-stream':>15}")
print("-" * 58)
print(f"{'í…ìŠ¤íŠ¸ í† í° ìˆ˜':<20} | {n_text_tokens:>15} | {n_text_tokens:>15}")
print(f"{'ë¹„ë””ì˜¤ í† í° ìˆ˜':<20} | {n_video_tokens:>15} | {n_video_tokens:>15}")
print(f"{'íŒŒë¼ë¯¸í„° ìˆ˜':<20} | {dual_params:>15,} | {single_params:>15,}")
print(f"{'ì¶”ë¡  ì‹œê°„ (ms)':<20} | {dual_time*1000:>15.2f} | {single_time*1000:>15.2f}")
print(f"{'ì¶œë ¥ text shape':<20} | {str(dt_out.shape):>15} | {str(st_out.shape):>15}")
print(f"{'ì¶œë ¥ video shape':<20} | {str(dv_out.shape):>15} | {str(sv_out.shape):>15}")

print("\nHunyuanVideo ì„¤ê³„ ì „ëµ:")
print("  - ì „ì²´ 38ê°œ ë¸”ë¡ ì¤‘ ì•ìª½ 20ê°œ: Dual-stream (ëª¨ë‹¬ë¦¬í‹°ë³„ ë…ë¦½ íŠ¹ì§• í˜•ì„±)")
print("  - ë’¤ìª½ 18ê°œ: Single-stream (ê¹Šì€ ë©€í‹°ëª¨ë‹¬ ìœµí•©)")
print("  - Cross-Attention â†’ Self-Attention ì „í™˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ + ì„±ëŠ¥ ê· í˜•")"""))

# â”€â”€ Cell 8: MD Compression â”€â”€
cells.append(md(r"""## 5. HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„ <a name='5.-ì••ì¶•-ë¶„ì„'></a>

HunyuanVideoì˜ 3D Causal VAEëŠ” ë¹„ë””ì˜¤ë¥¼ ì‹œê°„Â·ê³µê°„ ì–‘ë°©í–¥ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ DiTê°€ ì²˜ë¦¬í•  latent ê³µê°„ì„ ìƒì„±í•©ë‹ˆë‹¤.

í•µì‹¬ ì••ì¶• íŒŒë¼ë¯¸í„°:
- ì‹œê°„ ì••ì¶•ë¥ : $M_t = 4$
- ê³µê°„ ì••ì¶•ë¥ : $M_h = M_w = 8$
- Latent ì±„ë„ ìˆ˜: $C_z = 16$"""))

# â”€â”€ Cell 9: Compression Code â”€â”€
cells.append(code(r"""# â”€â”€ HunyuanVideo 3D Causal VAE ì••ì¶• í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ í•´ìƒë„ì— ëŒ€í•œ ì••ì¶•ë¥  ë¶„ì„

Mt, Mh, Mw = 4, 8, 8
Cz = 16

videos = [
    ('480p-2s', 2, 24, 480, 854, 3),
    ('720p-5s', 5, 30, 720, 1280, 3),
    ('1080p-10s', 10, 30, 1080, 1920, 3),
    ('4K-5s', 5, 30, 2160, 3840, 3),
]

print("=== HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„ ===")
print(f"ì••ì¶• ë¹„ìœ¨: Mt={Mt}, Mh={Mh}, Mw={Mw}, Cz={Cz}\n")

headers = ['ë¹„ë””ì˜¤', 'ì›ë³¸ í¬ê¸°(MB)', 'Latent í¬ê¸°(MB)', 'ì••ì¶•ë¥ ', 'DiT í† í° ìˆ˜']
print(f"{'ë¹„ë””ì˜¤':<14} | {'ì›ë³¸(MB)':>10} | {'Latent(MB)':>11} | {'ì••ì¶•ë¥ ':>8} | {'í† í° ìˆ˜':>10}")
print("-" * 65)

patch_t, patch_h, patch_w = 1, 2, 2
token_counts = []
names = []

for name, dur, fps, H, W, C in videos:
    T = dur * fps
    original = T * H * W * C * 4
    lat_T = max(1, T // Mt)
    lat_H = H // Mh
    lat_W = W // Mw
    latent = Cz * lat_T * lat_H * lat_W * 4
    ratio = original / latent

    n_tokens = (lat_T // patch_t) * (lat_H // patch_h) * (lat_W // patch_w)
    token_counts.append(n_tokens)
    names.append(name)

    print(f"{name:<14} | {original/1e6:>10.1f} | {latent/1e6:>11.2f} | {ratio:>7.1f}x | {n_tokens:>10,}")

fig, axes = plt.subplots(1, 2, figsize=(13, 5))

ax1 = axes[0]
original_sizes = []
latent_sizes = []
for name, dur, fps, H, W, C in videos:
    T = dur * fps
    original_sizes.append(T * H * W * C * 4 / 1e6)
    lat_T = max(1, T // Mt)
    latent_sizes.append(Cz * lat_T * (H // Mh) * (W // Mw) * 4 / 1e6)

x_pos = np.arange(len(videos))
width = 0.35
bars1 = ax1.bar(x_pos - width/2, original_sizes, width, label='ì›ë³¸', color='#FF5722', alpha=0.8)
bars2 = ax1.bar(x_pos + width/2, latent_sizes, width, label='Latent', color='#2196F3', alpha=0.8)
ax1.set_xticks(x_pos)
ax1.set_xticklabels(names, fontsize=9)
ax1.set_ylabel('í¬ê¸° (MB, FP32)', fontsize=11)
ax1.set_title('ì›ë³¸ vs Latent í¬ê¸° ë¹„êµ', fontweight='bold')
ax1.legend(fontsize=9)
ax1.set_yscale('log')
ax1.grid(True, alpha=0.3, axis='y')

ax2 = axes[1]
colors = ['#4CAF50', '#2196F3', '#FF9800', '#9C27B0']
bars = ax2.bar(names, token_counts, color=colors, edgecolor='white', lw=1.5)
for bar, val in zip(bars, token_counts):
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.02,
             f'{val:,}', ha='center', fontsize=9, fontweight='bold')
ax2.set_ylabel('DiT ì…ë ¥ í† í° ìˆ˜', fontsize=11)
ax2.set_title('í•´ìƒë„ë³„ DiT ì‹œí€€ìŠ¤ ê¸¸ì´', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/hunyuan_compression.png', dpi=100, bbox_inches='tight')
plt.close()

print("\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/hunyuan_compression.png")
print(f"\ní•µì‹¬: 1080p-10s ë¹„ë””ì˜¤ë„ DiTê°€ ì²˜ë¦¬ ê°€ëŠ¥í•œ {token_counts[2]:,}ê°œ í† í°ìœ¼ë¡œ ì••ì¶•!")"""))

# â”€â”€ Cell 10: MD Pipeline â”€â”€
cells.append(md(r"""## 6. ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ê°œìš” <a name='6.-íŒŒì´í”„ë¼ì¸'></a>

HunyuanVideoì˜ ì „ì²´ ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸:

1. **í…ìŠ¤íŠ¸ ì¸ì½”ë”©**: MLLM (ë©€í‹°ëª¨ë‹¬ LLM) + CLIPìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
2. **3D Causal VAE ì¸ì½”ë”©**: (í•™ìŠµ ì‹œ) ë¹„ë””ì˜¤ â†’ latent ì••ì¶•
3. **DiT Denoising**: Flow Matchingìœ¼ë¡œ latentì—ì„œ ë…¸ì´ì¦ˆ ì œê±°
4. **3D Causal VAE ë””ì½”ë”©**: latent â†’ ë¹„ë””ì˜¤ ë³µì›"""))

# â”€â”€ Cell 11: Pipeline Code â”€â”€
cells.append(code(r"""# â”€â”€ ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HunyuanVideo íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ í…ì„œ shapeìœ¼ë¡œ ì¶”ì 

print("=== HunyuanVideo ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ===\n")

# ì„¤ì •
T_frames, H, W = 32, 256, 256
Mt, Mh, Mw, Cz = 4, 8, 8, 16
d_model = 128
n_text_tokens_pipe = 77
n_steps = 30

print("[ ì…ë ¥ ì„¤ì • ]")
print(f"  ëª©í‘œ ë¹„ë””ì˜¤: {T_frames} frames x {H}x{W}")
print(f"  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: 'ê°•ì•„ì§€ê°€ í•´ë³€ì—ì„œ ë›°ì–´ë…¸ëŠ” ì¥ë©´'")
print()

# Step 1: Text Encoding
text_embed = tf.random.normal([1, n_text_tokens_pipe, d_model])
print("[ Step 1: í…ìŠ¤íŠ¸ ì¸ì½”ë”© ]")
print(f"  MLLM + CLIP â†’ text_embed: {text_embed.shape}")
print()

# Step 2: Latent shape ê³„ì‚°
lat_T = T_frames // Mt
lat_H = H // Mh
lat_W = W // Mw
print("[ Step 2: 3D Causal VAE Latent Space ]")
print(f"  ì›ë³¸ ë¹„ë””ì˜¤: ({T_frames}, {H}, {W}, 3)")
print(f"  Latent í¬ê¸°: ({Cz}, {lat_T}, {lat_H}, {lat_W})")
print(f"  ì‹œê°„ ì••ì¶•: {T_frames} â†’ {lat_T} (/{Mt})")
print(f"  ê³µê°„ ì••ì¶•: {H}x{W} â†’ {lat_H}x{lat_W} (/{Mh}x{Mw})")
print()

# Step 3: DiT Denoising (Flow Matching)
n_video_patches = lat_T * (lat_H // 2) * (lat_W // 2)
print("[ Step 3: DiT Denoising (Flow Matching) ]")
print(f"  ë¹„ë””ì˜¤ í† í° ìˆ˜: {n_video_patches}")
print(f"  í…ìŠ¤íŠ¸ í† í° ìˆ˜: {n_text_tokens_pipe}")
print(f"  Euler ODE ìŠ¤í… ìˆ˜: {n_steps}")

# ê°„ë‹¨í•œ Euler sampling ì‹œë®¬ë ˆì´ì…˜
z = tf.random.normal([1, n_video_patches, d_model])
dt = 1.0 / n_steps
print(f"\n  ì‹œë®¬ë ˆì´ì…˜ (shape ì¶”ì ):")
for step in [0, n_steps//3, 2*n_steps//3, n_steps-1]:
    t_val = step * dt
    noise_level = 1.0 - t_val
    print(f"    t={t_val:.2f} | ë…¸ì´ì¦ˆ ìˆ˜ì¤€: {noise_level:.2f} | z shape: {z.shape}")
print()

# Step 4: VAE Decode
print("[ Step 4: 3D Causal VAE ë””ì½”ë”© ]")
print(f"  Latent ({Cz}, {lat_T}, {lat_H}, {lat_W}) â†’ ë¹„ë””ì˜¤ ({T_frames}, {H}, {W}, 3)")
print()

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½
print("=" * 60)
print("HunyuanVideo ì•„í‚¤í…ì²˜ ì‚¬ì–‘ (arxiv 2412.17601)")
print("=" * 60)
specs = [
    ('DiT íŒŒë¼ë¯¸í„°', '13B'),
    ('Dual-stream ë¸”ë¡', '20ê°œ'),
    ('Single-stream ë¸”ë¡', '18ê°œ'),
    ('3D VAE ì±„ë„', '16'),
    ('ì‹œê°„ ì••ì¶•ë¥  (Mt)', '4'),
    ('ê³µê°„ ì••ì¶•ë¥  (Mh, Mw)', '8, 8'),
    ('í…ìŠ¤íŠ¸ ì¸ì½”ë”', 'MLLM + CLIP'),
    ('í›ˆë ¨ ë°©ì‹', 'Flow Matching (Rectified Flow)'),
    ('ìµœëŒ€ í•´ìƒë„', '1280x720, 129 frames'),
]
for k, v in specs:
    print(f"  {k:<24}: {v}")"""))

# â”€â”€ Cell 12: Architecture Diagram â”€â”€
cells.append(code(r"""# â”€â”€ HunyuanVideo ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
fig, ax = plt.subplots(figsize=(14, 7))
ax.set_xlim(0, 14)
ax.set_ylim(0, 8)
ax.axis('off')

box_style = dict(boxstyle='round,pad=0.4', facecolor='#E3F2FD', edgecolor='#1976D2', lw=2)
box_style2 = dict(boxstyle='round,pad=0.4', facecolor='#E8F5E9', edgecolor='#388E3C', lw=2)
box_style3 = dict(boxstyle='round,pad=0.4', facecolor='#FFF3E0', edgecolor='#F57C00', lw=2)
box_style4 = dict(boxstyle='round,pad=0.4', facecolor='#F3E5F5', edgecolor='#7B1FA2', lw=2)

# Text Encoder
ax.text(1.5, 6.5, 'Text Encoder\n(MLLM + CLIP)', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=box_style2)

# 3D VAE Encoder
ax.text(5, 6.5, '3D Causal VAE\nEncoder', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=box_style3)

# Dual-stream
ax.text(5, 4.5, 'Dual-stream Blocks\n(20 layers)\nSelf + Cross-Attn', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=box_style)

# Single-stream
ax.text(5, 2.5, 'Single-stream Blocks\n(18 layers)\nConcat Self-Attn', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=box_style4)

# 3D VAE Decoder
ax.text(9.5, 2.5, '3D Causal VAE\nDecoder', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=box_style3)

# Output
ax.text(12.5, 2.5, 'Generated\nVideo', ha='center', va='center',
        fontsize=11, fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='#FFCDD2', edgecolor='#D32F2F', lw=2))

# Noise input
ax.text(9.5, 6.5, 'Gaussian\nNoise $z_T$', ha='center', va='center',
        fontsize=10, fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='#F5F5F5', edgecolor='#616161', lw=2))

# Arrows
arrow_props = dict(arrowstyle='->', color='#333', lw=2)
ax.annotate('', xy=(3.5, 4.5), xytext=(1.5, 5.9), arrowprops=arrow_props)
ax.annotate('', xy=(5, 3.3), xytext=(5, 3.8), arrowprops=arrow_props)
ax.annotate('', xy=(5, 5.2), xytext=(5, 5.8), arrowprops=arrow_props)
ax.annotate('', xy=(8.3, 2.5), xytext=(6.8, 2.5), arrowprops=arrow_props)
ax.annotate('', xy=(11.2, 2.5), xytext=(10.8, 2.5), arrowprops=arrow_props)
ax.annotate('', xy=(7.5, 5.5), xytext=(9.5, 5.9), arrowprops=arrow_props)

# Flow Matching label
ax.text(7, 3.5, 'Flow Matching\n(Euler ODE)', ha='center', va='center',
        fontsize=9, fontstyle='italic', color='#1976D2')

ax.set_title('HunyuanVideo ì „ì²´ ì•„í‚¤í…ì²˜ (Dualâ†’Single Stream)', fontweight='bold', fontsize=14, pad=15)

plt.tight_layout()
plt.savefig('chapter17_diffusion_transformers/hunyuan_architecture.png', dpi=100, bbox_inches='tight')
plt.close()
print("ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ë¨: chapter17_diffusion_transformers/hunyuan_architecture.png")

# Sora vs HunyuanVideo ë¹„êµ
print("\n=== Sora vs HunyuanVideo ë¹„êµ ===\n")
print(f"{'íŠ¹ì„±':<22} | {'Sora (OpenAI)':>22} | {'HunyuanVideo (Tencent)':>22}")
print("-" * 74)
comparisons = [
    ('ê³µê°œ ì—¬ë¶€', 'ë¹„ê³µê°œ (ê¸°ìˆ  ë³´ê³ ì„œë§Œ)', 'ì˜¤í”ˆì†ŒìŠ¤ (ì½”ë“œ+ê°€ì¤‘ì¹˜)'),
    ('ì•„í‚¤í…ì²˜', 'DiT (ì¶”ì •)', 'Dualâ†’Single DiT'),
    ('í›ˆë ¨ ë°©ì‹', 'Flow Matching (ì¶”ì •)', 'Flow Matching'),
    ('í•´ìƒë„ ì²˜ë¦¬', 'NaViT íŒ¨í‚¹', 'Multi-resolution í•™ìŠµ'),
    ('í…ìŠ¤íŠ¸ ì¸ì½”ë”', 'ë¹„ê³µê°œ', 'MLLM + CLIP'),
    ('VAE', 'ì‹œê³µê°„ íŒ¨ì¹˜', '3D Causal VAE'),
    ('ìµœëŒ€ ê¸¸ì´', '~60ì´ˆ (ë°ëª¨)', '~5ì´ˆ (ì˜¤í”ˆì†ŒìŠ¤)'),
    ('íŒŒë¼ë¯¸í„°', 'ë¹„ê³µê°œ', '13B'),
]
for feat, sora, hunyuan in comparisons:
    print(f"{feat:<22} | {sora:>22} | {hunyuan:>22}")"""))

# â”€â”€ Cell 13: Summary â”€â”€
cells.append(md(r"""## 7. ì •ë¦¬ <a name='7.-ì •ë¦¬'></a>

### í•µì‹¬ ê°œë… ìš”ì•½

| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |
|------|------|--------|
| NaViT íŒ¨í‚¹ | ê°€ë³€ í•´ìƒë„ë¥¼ íŒ¨ë”© ì—†ì´ í•œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ | â­â­â­ |
| Dual-stream | í…ìŠ¤íŠ¸/ë¹„ë””ì˜¤ ë¶„ë¦¬ ì²˜ë¦¬ + Cross-Attention | â­â­â­ |
| Single-stream | í† í° ì—°ê²° í›„ í†µí•© Self-Attention | â­â­â­ |
| Dualâ†’Single ì „í™˜ | HunyuanVideoì˜ í•µì‹¬ ì„¤ê³„ (20+18 ë¸”ë¡) | â­â­â­ |
| 3D Causal VAE | ì‹œê³µê°„ $4\times8\times8 = 256$ë°° ì••ì¶• | â­â­ |
| Flow Matching | Rectified Flow ê¸°ë°˜ Euler ODE ìƒ˜í”Œë§ | â­â­ |

### í•µì‹¬ ìˆ˜ì‹

$$\text{NaViT ì´ í† í°} = \sum_{i=1}^B \frac{H_i}{p_h} \cdot \frac{W_i}{p_w}$$

$$h^{(l)}_{\text{single}} = \text{SelfAttn}\!\left([h_{\text{text}}; h_{\text{video}}]^{(l-1)}\right)$$

$$z \in \mathbb{R}^{C_z \times (T/M_t) \times (H/M_h) \times (W/M_w)}$$

### ë‹¤ìŒ ì±•í„° ì˜ˆê³ 
ì´ê²ƒìœ¼ë¡œ TensorFlow ê°•ì˜ ì»¤ë¦¬í˜ëŸ¼ì˜ ì „ì²´ ì´ë¡  ì±•í„°ê°€ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤. **projects/** ë””ë ‰í† ë¦¬ì˜ ì¢…í•© ì‹¤ì „ í”„ë¡œì íŠ¸ë¡œ ë„˜ì–´ê°€ì„¸ìš”!"""))

path = '/workspace/chapter17_diffusion_transformers/05_sora_and_hunyuan_architecture.ipynb'
create_notebook(cells, path)
