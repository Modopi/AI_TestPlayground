{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 09-02: í…ì„œ ë³‘ë ¬í™” ì‹¬í™” (Tensor Parallelism)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Data Parallelismì´ í•´ê²°í•  ìˆ˜ ì—†ëŠ” **'ëª¨ë¸ í¬ê¸° > GPU ë©”ëª¨ë¦¬'** ë¬¸ì œë¥¼ ì¸ì‹í•œë‹¤\n",
    "- Megatron-LMì˜ **1D Tensor Parallelism** ì›ë¦¬ë¥¼ Column/Row ë¶„í•  ìˆ˜ì‹ìœ¼ë¡œ ì§ì ‘ ì¦ëª…í•œë‹¤\n",
    "- Forward Passì—ì„œ **All-Reduceê°€ ë”± 1íšŒë§Œ** í•„ìš”í•œ ì´ìœ ë¥¼ ë¹„ì„ í˜•í•¨ìˆ˜(GeLU)ì˜ element-wise ì„±ì§ˆë¡œ ì„¤ëª…í•œë‹¤\n",
    "- Transformer MLPì™€ Multi-Head Attention ì–‘ìª½ì— TPë¥¼ ì ìš©í•˜ê³  ìˆ˜í•™ì  ë“±ê°€ì„±ì„ ì½”ë“œë¡œ ê²€ì¦í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: í–‰ë ¬ ë¶„í•  ì›ë¦¬](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ì™œ í…ì„œ ë³‘ë ¬í™”ê°€ í•„ìš”í•œê°€?](#2.-í…ì„œ-ë³‘ë ¬í™”ì˜-í•„ìš”ì„±)\n",
    "3. [Column Parallel Linear ìˆ˜ì‹ ì¦ëª… + êµ¬í˜„](#3.-Column-Parallel)\n",
    "4. [Row Parallel Linear ìˆ˜ì‹ ì¦ëª… + êµ¬í˜„](#4.-Row-Parallel)\n",
    "5. [Transformer MLP ì „ì²´ TP ì¡°í•© (í•µì‹¬!)](#5.-MLP-TP-ì¡°í•©)\n",
    "6. [Multi-Head Attentionì˜ TP ì ìš©](#6.-Attention-TP)\n",
    "7. [í†µì‹  íšŸìˆ˜ ë¶„ì„ ë° ìµœì í™”](#7.-í†µì‹ -ìµœì í™”)\n",
    "8. [ì •ë¦¬ ë° ì—°ìŠµ ë¬¸ì œ](#8.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### í–‰ë ¬ ë¶„í•  ëŒ€ìˆ˜ë²•ì¹™\n",
    "\n",
    "**ì—´(Column) ë°©í–¥ ë¶„í• **:\n",
    "$$W = [W^{(0)}, W^{(1)}, \\dots, W^{(N-1)}], \\quad W^{(i)} \\in \\mathbb{R}^{d_{in} \\times (d_{out}/N)}$$\n",
    "\n",
    "$$XW = X[W^{(0)}, W^{(1)}, \\dots] = [XW^{(0)}, XW^{(1)}, \\dots] = [Y^{(0)}, Y^{(1)}, \\dots]$$\n",
    "\n",
    "â†’ ê²°ê³¼ë¥¼ **ì—´ ë°©í–¥ìœ¼ë¡œ Concat**í•˜ë©´ ì›ë˜ ê²°ê³¼ì™€ ë™ì¼ (All-Reduce ë¶ˆí•„ìš”!)\n",
    "\n",
    "**í–‰(Row) ë°©í–¥ ë¶„í• **:\n",
    "$$W = \\begin{bmatrix} W^{(0)} \\\\ W^{(1)} \\\\ \\vdots \\end{bmatrix}, \\quad W^{(i)} \\in \\mathbb{R}^{(d_{in}/N) \\times d_{out}}$$\n",
    "\n",
    "$$XW = [X^{(0)}, X^{(1)}, \\dots] \\begin{bmatrix} W^{(0)} \\\\ W^{(1)} \\\\ \\vdots \\end{bmatrix} = \\sum_{i=0}^{N-1} X^{(i)} W^{(i)}$$\n",
    "\n",
    "â†’ ë¶€ë¶„ê³±ì„ **All-Reduce(SUM)**í•˜ë©´ ì›ë˜ ê²°ê³¼ì™€ ë™ì¼ (All-Reduce 1íšŒ í•„ìš”)\n",
    "\n",
    "### Megatron-LMì˜ í•µì‹¬ í†µì°°: Column â†’ Row ìˆœì„œ\n",
    "\n",
    "$$Y = \\underbrace{\\text{GeLU}(X A_1)}_{\\text{Column Parallel}} \\cdot \\underbrace{A_2}_{\\text{Row Parallel}}$$\n",
    "\n",
    "GeLUëŠ” **element-wise** í•¨ìˆ˜: $\\text{GeLU}([Y^{(0)}, Y^{(1)}]) = [\\text{GeLU}(Y^{(0)}), \\text{GeLU}(Y^{(1)})]$\n",
    "\n",
    "ë”°ë¼ì„œ Column Parallel í›„ GeLUë¥¼ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆê³ , **ì¤‘ê°„ í†µì‹ ì´ í•„ìš” ì—†ìŒ!**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ“š í…ì„œ ë³‘ë ¬í™”ê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> 1000í˜ì´ì§€ì§œë¦¬ êµê³¼ì„œ(ëª¨ë¸ ê°€ì¤‘ì¹˜)ê°€ ìˆëŠ”ë°, ê·¸ êµê³¼ì„œê°€ ë„ˆë¬´ ì»¤ì„œ ì±…ìƒ(GPU) í•˜ë‚˜ì— ì•ˆ ì˜¬ë¼ê°€ìš”!\n",
    ">\n",
    "> - **DP ë°©ë²•**: í•™ìƒë§ˆë‹¤ êµê³¼ì„œë¥¼ í•œ ê¶Œì”© ê°€ì§ (1000í˜ì´ì§€ Ã— í•™ìƒ ìˆ˜) â†’ ì´ë¯¸ ì±…ìƒì— ì•ˆ ì˜¬ë¼ê°€ë‹ˆ ë¶ˆê°€!  \n",
    "> - **TP ë°©ë²•**: êµê³¼ì„œë¥¼ **ì°¢ì–´ì„œ** (333í˜ì´ì§€ì”©) í•™ìƒë“¤ì—ê²Œ ë‚˜ëˆ  ì¤Œ  \n",
    ">   â†’ ê°ì ë§¡ì€ ë¶€ë¶„ì„ ê³„ì‚°í•˜ê³ , ë‹µì´ í•„ìš”í•  ë•Œë§Œ ì„œë¡œ ì „ë‹¬!\n",
    "\n",
    "#### ğŸ”§ Column vs Row ë¶„í• ì˜ ì°¨ì´\n",
    "\n",
    "> - **Column ë¶„í• **: êµê³¼ì„œì˜ **ì„¸ë¡œ(ì—´)** ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° â†’ ê°ì ë…ë¦½ì ìœ¼ë¡œ ì½ê³  ë‚˜ì¤‘ì— ì´ì–´ ë¶™ì´ê¸°\n",
    "> - **Row ë¶„í• **: êµê³¼ì„œì˜ **ê°€ë¡œ(í–‰)** ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸° â†’ ê°ì ë¶€ë¶„ ë‹µì„ êµ¬í•˜ê³  ëª¨ì•„ì„œ ë”í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: Column ë¶„í•  í›„ Concat ê²°ê³¼\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}, \\quad W = \\begin{bmatrix} 1 & 0 & 2 \\\\ 0 & 3 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$W$ë¥¼ Column ë°©í–¥ìœ¼ë¡œ 2ë¶„í•  ($N=2$):  \n",
    "$W^{(0)} = ?$, $W^{(1)} = ?$, $XW^{(0)} = ?$, $XW^{(1)} = ?$, $[XW^{(0)}, XW^{(1)}] = XW$ì¸ì§€ í™•ì¸í•˜ë¼.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$W^{(0)} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix}, \\quad W^{(1)} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "$$XW^{(0)} = \\begin{bmatrix} 1 & 6 \\\\ 3 & 12 \\end{bmatrix}, \\quad XW^{(1)} = \\begin{bmatrix} 4 \\\\ 10 \\end{bmatrix}$$\n",
    "\n",
    "$$[XW^{(0)}, XW^{(1)}] = \\begin{bmatrix} 1 & 6 & 4 \\\\ 3 & 12 & 10 \\end{bmatrix} = XW \\quad \\checkmark$$\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: í†µì‹  ìµœì†Œí™” ì„¤ê³„\n",
    "\n",
    "4-ë ˆì´ì–´ Transformerì— TP=4ë¥¼ ì ìš©í•œë‹¤.  \n",
    "ê° ë ˆì´ì–´ì—ì„œ MLP(All-Reduce 1íšŒ)ì™€ Attention(All-Reduce 1íšŒ)ì´ ìˆë‹¤ë©´,  \n",
    "Forward Pass ì „ì²´ì—ì„œ ì´ All-Reduce íšŸìˆ˜ëŠ”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "ë ˆì´ì–´ë‹¹: MLP 1íšŒ + Attention 1íšŒ = **2íšŒ**  \n",
    "4ë ˆì´ì–´: $4 \\times 2 = \\mathbf{8}$íšŒ\n",
    "\n",
    "Backwardì—ì„œë„ ë™ì¼í•˜ê²Œ 8íšŒ â†’ **ì´ 16íšŒ** (ë‹¨, ì´ê²ƒì´ ë§¤ ìŠ¤í… ë°˜ë³µë¨)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"GeLU(x) = 0.5xÂ·[1 + tanh(âˆš(2/Ï€)Â·(x + 0.044715xÂ³))]\"\"\"\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    e = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "print(\"ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
    "print(f\"â€¢ GeLU í™•ì¸: gelu(0) = {gelu(0):.4f}, gelu(1) = {gelu(1):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í…ì„œ ë³‘ë ¬í™”ì˜ í•„ìš”ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ëª¨ë¸ í¬ê¸°ë³„ ë‹¨ì¼ GPU ìˆ˜ìš© ê°€ëŠ¥ ì—¬ë¶€ ë¶„ì„\n",
    "# ---------------------------------------------------\n",
    "\n",
    "gpu_vram_GB = 80  # H100 80GB\n",
    "usable_ratio = 0.7  # 70% í™œìš© (ë‚˜ë¨¸ì§€ëŠ” activation, temp ë“±)\n",
    "\n",
    "llms = [\n",
    "    ('BERT-Large',     0.34,  1024,  24,  16),\n",
    "    ('GPT-2 XL',       1.5,   1600,  48,  25),\n",
    "    ('Llama-2 7B',     7.0,   4096,  32,  32),\n",
    "    ('Llama-2 13B',    13.0,  5120,  40,  40),\n",
    "    ('Llama-2 70B',    70.0,  8192,  80,  64),\n",
    "    ('GPT-3 175B',     175.0, 12288, 96,  96),\n",
    "    ('Megatron-Turing',530.0, 20480, 105, 128),\n",
    "]\n",
    "\n",
    "print(f\"ğŸ’¾ GPU VRAM: {gpu_vram_GB} GB (ì‚¬ìš© ê°€ëŠ¥: {gpu_vram_GB * usable_ratio:.0f} GB)\")\n",
    "print(f\"\\n{'ëª¨ë¸':<20} {'íŒŒë¼ë¯¸í„°':>10} {'í•™ìŠµë©”ëª¨ë¦¬(GB)':>15} {'ë‹¨ì¼GPU?':>10} {'ìµœì†Œ TP':>8}\")\n",
    "print(\"-\" * 68)\n",
    "\n",
    "for name, params_B, hidden, layers, heads in llms:\n",
    "    train_mem = params_B * 16  # 16P ê³µì‹\n",
    "    can_fit = train_mem <= gpu_vram_GB * usable_ratio\n",
    "    min_tp = 1\n",
    "    while train_mem / min_tp > gpu_vram_GB * usable_ratio:\n",
    "        min_tp *= 2\n",
    "    status = 'âœ…' if can_fit else 'âŒ'\n",
    "    print(f\"{name:<20} {params_B:>9.1f}B {train_mem:>14.0f} GB {status:>10} {min_tp:>8}\")\n",
    "\n",
    "print(\"\\nâ†’ 7B ëª¨ë¸ë¶€í„° ë‹¨ì¼ GPUì—ì„œ í•™ìŠµ ë¶ˆê°€ â†’ TPê°€ ë°˜ë“œì‹œ í•„ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Column Parallel Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Column Parallel Linear: ì™„ì „ êµ¬í˜„ + ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def column_parallel_linear(X, W, b, N_gpus, non_linearity=None):\n",
    "    \"\"\"\n",
    "    Wë¥¼ ì—´(Column) ë°©í–¥ìœ¼ë¡œ N_gpus ë“±ë¶„í•˜ì—¬ ë³‘ë ¬ ì—°ì‚° ìˆ˜í–‰.\n",
    "    \n",
    "    Forward: Y^(i) = f(X Â· W^(i) + b^(i)), ê²°ê³¼= [Y^(0), ..., Y^(N-1)] (Concat)\n",
    "    í†µì‹ : XëŠ” ëª¨ë“  GPUì— broadcast (ë³´í†µ ì´ë¯¸ ë³µì‚¬ë˜ì–´ ìˆìŒ)\n",
    "    \n",
    "    Returns:\n",
    "        Y_full: ë‹¨ì¼ GPU ê²°ê³¼ì™€ ë™ì¼í•œ ì¶œë ¥\n",
    "        shards: ê° GPUì˜ ë¶€ë¶„ ì¶œë ¥ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    d_out = W.shape[1]\n",
    "    split = d_out // N_gpus\n",
    "    assert d_out % N_gpus == 0, f\"d_out({d_out})ì´ N_gpus({N_gpus})ë¡œ ë‚˜ëˆ„ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤\"\n",
    "\n",
    "    shards = []\n",
    "    for i in range(N_gpus):\n",
    "        W_i = W[:, i*split:(i+1)*split]  # ì—´ ë¶„í• \n",
    "        b_i = b[i*split:(i+1)*split]\n",
    "        Y_i = X @ W_i + b_i              # GPU iì˜ ë¶€ë¶„ ì—°ì‚°\n",
    "        if non_linearity:\n",
    "            Y_i = non_linearity(Y_i)      # Element-wise â†’ ê° GPU ë…ë¦½ ìˆ˜í–‰ ê°€ëŠ¥\n",
    "        shards.append(Y_i)\n",
    "\n",
    "    Y_full = np.concatenate(shards, axis=-1)  # All-Gather (Concat)\n",
    "    return Y_full, shards\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "B, S, d_in, d_out = 2, 3, 4, 8\n",
    "N = 2\n",
    "\n",
    "X   = np.random.randn(B, S, d_in)\n",
    "W1  = np.random.randn(d_in, d_out)\n",
    "b1  = np.random.randn(d_out)\n",
    "\n",
    "# ë‹¨ì¼ GPU ê¸°ì¤€ê°’ (GeLU í¬í•¨)\n",
    "Y_single = gelu(X @ W1 + b1)  # shape: (2, 3, 8)\n",
    "\n",
    "# Column Parallel (N=2)\n",
    "Y_parallel, shards_cp = column_parallel_linear(X, W1, b1, N, non_linearity=gelu)\n",
    "\n",
    "max_err = np.abs(Y_single - Y_parallel).max()\n",
    "print(f\"[Column Parallel Linear + GeLU]\")\n",
    "print(f\"  ì…ë ¥ shape:        X={X.shape}, W1={W1.shape}, b1={b1.shape}\")\n",
    "print(f\"  ê° GPU shard:      {shards_cp[0].shape} Ã— {N}ê°œ\")\n",
    "print(f\"  Concat ê²°ê³¼:       {Y_parallel.shape}\")\n",
    "print(f\"  ë‹¨ì¼ GPU ê²°ê³¼:     {Y_single.shape}\")\n",
    "print(f\"  ìµœëŒ€ ì˜¤ì°¨:         {max_err:.2e}\")\n",
    "print(f\"  ìˆ˜í•™ì  ë“±ê°€ ê²€ì¦:  {'âœ… í†µê³¼' if max_err < 1e-10 else 'âŒ ì‹¤íŒ¨'}\")\n",
    "print()\n",
    "print(\"í•µì‹¬: GeLUëŠ” element-wiseì´ë¯€ë¡œ Column ë¶„í•  í›„ ê° GPUê°€ ë…ë¦½ ìˆ˜í–‰ ê°€ëŠ¥!\")\n",
    "print(\"â†’ All-Reduceì—†ì´ Concatë§Œìœ¼ë¡œ ì›ë˜ ê²°ê³¼ ë³µì›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Row Parallel Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Row Parallel Linear: ì™„ì „ êµ¬í˜„\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def row_parallel_linear(X_shards, W, b, N_gpus):\n",
    "    \"\"\"\n",
    "    Wë¥¼ í–‰(Row) ë°©í–¥ìœ¼ë¡œ N_gpus ë“±ë¶„í•˜ì—¬ ë³‘ë ¬ ì—°ì‚° ìˆ˜í–‰.\n",
    "    \n",
    "    ì…ë ¥: X_shards (ì´ì „ Column Parallelì˜ ê° GPU ì¶œë ¥)\n",
    "    Forward: Z^(i) = X^(i) Â· W^(i), ê²°ê³¼= Î£Z^(i) + b (All-Reduce SUM + bias)\n",
    "    í†µì‹ : ë¶€ë¶„ê³± Z^(i)ë¥¼ All-Reduce(SUM) í•„ìš” â†’ 1íšŒ ë°œìƒ\n",
    "    \n",
    "    Returns:\n",
    "        Z_full: All-Reduce ì™„ë£Œëœ ìµœì¢… ì¶œë ¥\n",
    "        partial_results: ê° GPUì˜ ë¶€ë¶„ê³± ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    d_in2 = W.shape[0]\n",
    "    split = d_in2 // N_gpus\n",
    "\n",
    "    partial_results = []\n",
    "    for i in range(N_gpus):\n",
    "        W_i    = W[i*split:(i+1)*split, :]  # í–‰ ë¶„í• \n",
    "        X_shard = X_shards[i]               # ëŒ€ì‘ë˜ëŠ” ì…ë ¥ íŒŒí‹°ì…˜\n",
    "        Z_i    = X_shard @ W_i              # GPU i ë¶€ë¶„ê³±\n",
    "        partial_results.append(Z_i)\n",
    "\n",
    "    # All-Reduce: ëª¨ë“  ë¶€ë¶„ê³± í•©ì‚° (ì‹¤ì œë¡œëŠ” NCCL All-Reduce)\n",
    "    Z_full = sum(partial_results) + b\n",
    "    return Z_full, partial_results\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "d_int, d_hidden = 8, 4  # intermediate(8) â†’ hidden(4)\n",
    "\n",
    "W2 = np.random.randn(d_int, d_hidden)\n",
    "b2 = np.random.randn(d_hidden)\n",
    "\n",
    "# ë‹¨ì¼ GPU ê¸°ì¤€ê°’\n",
    "Y_full_single = gelu(X @ W1 + b1)  # Column Parallel ì´ì „ ë‹¨ê³„\n",
    "Z_single = Y_full_single @ W2 + b2  # shape: (2, 3, 4)\n",
    "\n",
    "# Row Parallel (shards_cpëŠ” ì´ì „ Column Parallelì˜ ê° GPU ì¶œë ¥)\n",
    "Z_parallel, partials = row_parallel_linear(shards_cp, W2, b2, N)\n",
    "\n",
    "max_err2 = np.abs(Z_single - Z_parallel).max()\n",
    "print(f\"[Row Parallel Linear]\")\n",
    "print(f\"  ì…ë ¥ (ê° GPU shard): {shards_cp[0].shape} Ã— {N}ê°œ\")\n",
    "print(f\"  W2 í–‰ ë¶„í• :          {W2.shape} â†’ {N}ê°œ Ã— {W2.shape[0]//N}Ã—{W2.shape[1]}\")\n",
    "print(f\"  ê° GPU ë¶€ë¶„ê³±:       {partials[0].shape}\")\n",
    "print(f\"  All-Reduce SUM ê²°ê³¼: {Z_parallel.shape}\")\n",
    "print(f\"  ë‹¨ì¼ GPU ê²°ê³¼:       {Z_single.shape}\")\n",
    "print(f\"  ìµœëŒ€ ì˜¤ì°¨:           {max_err2:.2e}\")\n",
    "print(f\"  ìˆ˜í•™ì  ë“±ê°€ ê²€ì¦:    {'âœ… í†µê³¼' if max_err2 < 1e-10 else 'âŒ ì‹¤íŒ¨'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MLP TP ì¡°í•© (í•µì‹¬!)\n",
    "\n",
    "Column Parallel + GeLU + Row Parallelì˜ ì¡°í•©ìœ¼ë¡œ **MLP ì „ì²´ë¥¼ TPë¡œ** êµ¬í˜„í•œë‹¤.  \n",
    "ì¤‘ìš”í•œ ì : **Forwardì—ì„œ All-ReduceëŠ” ë‹¨ 1íšŒ!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì™„ì „í•œ Transformer MLPì˜ Tensor Parallel êµ¬í˜„\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def tp_mlp_forward(X, A1, b1, A2, b2, N_gpus):\n",
    "    \"\"\"\n",
    "    Megatron-LM ë°©ì‹ Tensor Parallel MLP Forward Pass.\n",
    "    \n",
    "    í‘œì¤€ MLP: Z = GeLU(XÂ·A1 + b1)Â·A2 + b2\n",
    "    \n",
    "    TP ì ìš©:\n",
    "      - A1: Column Parallel (ì—´ ë°©í–¥ ë¶„í• , All-Reduce ì—†ìŒ)\n",
    "      - GeLU: ê° GPUê°€ ë…ë¦½ ìˆ˜í–‰\n",
    "      - A2: Row Parallel (í–‰ ë°©í–¥ ë¶„í• , All-Reduce 1íšŒ)\n",
    "    \n",
    "    í†µì‹ : Forwardì—ì„œ All-Reduce **1íšŒë§Œ** ë°œìƒ!\n",
    "    \n",
    "    Returns:\n",
    "        Z: ìµœì¢… ì¶œë ¥ (ë‹¨ì¼ GPUì™€ ë™ì¼)\n",
    "        all_reduce_count: All-Reduce ë°œìƒ íšŸìˆ˜\n",
    "    \"\"\"\n",
    "    inter_dim = A1.shape[1]  # intermediate dimension\n",
    "    split = inter_dim // N_gpus\n",
    "    all_reduce_count = 0\n",
    "\n",
    "    # â”€â”€ stage 1: Column Parallel + GeLU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Y_shards = []\n",
    "    for i in range(N_gpus):\n",
    "        A1_i = A1[:, i*split:(i+1)*split]\n",
    "        b1_i = b1[i*split:(i+1)*split]\n",
    "        Y_i  = gelu(X @ A1_i + b1_i)   # ê° GPU ë…ë¦½ ìˆ˜í–‰, í†µì‹  ì—†ìŒ\n",
    "        Y_shards.append(Y_i)\n",
    "    # â†’ All-Reduce ì—†ìŒ! Concatë„ í•„ìš” ì—†ìŒ (ë‹¤ìŒ Row Parallelì— ë°”ë¡œ ë„˜ê¹€)\n",
    "\n",
    "    # â”€â”€ stage 2: Row Parallel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    Z_partials = []\n",
    "    for i in range(N_gpus):\n",
    "        A2_i = A2[i*split:(i+1)*split, :]\n",
    "        Z_i  = Y_shards[i] @ A2_i      # ê° GPU ë…ë¦½ ë¶€ë¶„ê³±\n",
    "        Z_partials.append(Z_i)\n",
    "\n",
    "    # â† All-Reduce: ì—¬ê¸°ì„œ ë”± 1íšŒ ë°œìƒ!\n",
    "    Z = sum(Z_partials) + b2\n",
    "    all_reduce_count += 1\n",
    "\n",
    "    return Z, all_reduce_count\n",
    "\n",
    "\n",
    "# â”€â”€ í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d_model = 4\n",
    "d_ff    = 16   # FFN intermediate: ë³´í†µ 4 Ã— d_model\n",
    "N       = 4    # 4ê°œ GPUë¡œ ë¶„í• \n",
    "B, S    = 2, 5\n",
    "\n",
    "X  = np.random.randn(B, S, d_model)\n",
    "A1 = np.random.randn(d_model, d_ff)\n",
    "b1 = np.random.randn(d_ff)\n",
    "A2 = np.random.randn(d_ff, d_model)\n",
    "b2 = np.random.randn(d_model)\n",
    "\n",
    "# ë‹¨ì¼ GPU ê¸°ì¤€ê°’\n",
    "Z_ref = gelu(X @ A1 + b1) @ A2 + b2\n",
    "\n",
    "# Tensor Parallel (N=4)\n",
    "Z_tp, ar_count = tp_mlp_forward(X, A1, b1, A2, b2, N)\n",
    "\n",
    "err = np.abs(Z_ref - Z_tp).max()\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Transformer MLP Tensor Parallel ê²€ì¦\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  ì„¤ì •: d_model={d_model}, d_ff={d_ff}, N_gpus={N}\")\n",
    "print(f\"  X: {X.shape}, A1: {A1.shape}, A2: {A2.shape}\")\n",
    "print(f\"  Forward All-Reduce íšŸìˆ˜: {ar_count}íšŒ\")\n",
    "print(f\"  ìµœëŒ€ ìˆ˜ì¹˜ ì˜¤ì°¨:          {err:.2e}\")\n",
    "print(f\"  ë‹¨ì¼ GPUì™€ ë“±ê°€ ê²€ì¦:    {'âœ… ì™„ì „ ì¼ì¹˜' if err < 1e-10 else 'âŒ ë¶ˆì¼ì¹˜'}\")\n",
    "print()\n",
    "print(\"  A1 â†’ Column Parallel: ê° GPUê°€ d_ff/N ì°¨ì› ì¶œë ¥\")\n",
    "print(\"  GeLU â†’ element-wise: í†µì‹  ì—†ìŒ âœ…\")\n",
    "print(\"  A2 â†’ Row Parallel: ë¶€ë¶„ê³± í•©ì‚° â†’ All-Reduce 1íšŒ âš¡\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Head Attentionì˜ TP ì ìš©\n",
    "\n",
    "Multi-Head Attentionì—ì„œë„ ë™ì¼í•œ ì›ë¦¬ë¥¼ ì ìš©í•˜ë˜, ë¶„í•  ë‹¨ìœ„ê°€ **head** ë‹¨ìœ„ì…ë‹ˆë‹¤.\n",
    "\n",
    "$$\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O$$\n",
    "\n",
    "- $W^Q, W^K, W^V$: **Column Parallel** (head ë‹¨ìœ„ë¡œ ë¶„í• )\n",
    "- $W^O$: **Row Parallel** (All-Reduce 1íšŒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Multi-Head Attention Tensor Parallel êµ¬í˜„\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"Scaled Dot-Product Attention: Attn(Q,K,V) = softmax(QK^T/âˆšd_k)V\"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.transpose(0, 1, 3, 2) / np.sqrt(d_k)  # (B, heads, S, S)\n",
    "    weights = softmax(scores, axis=-1)\n",
    "    return weights @ V  # (B, heads, S, d_k)\n",
    "\n",
    "\n",
    "def tp_multihead_attention(X, WQ, WK, WV, WO, bQ, bK, bV, bO, n_heads, N_gpus):\n",
    "    \"\"\"\n",
    "    Tensor Parallel Multi-Head Attention.\n",
    "    \n",
    "    - WQ, WK, WV: Column Parallel (n_heads/N_gpus í—¤ë“œì”© ê° GPUì—)\n",
    "    - WO: Row Parallel (Attention ì¶œë ¥ í•©ì‚°)\n",
    "    - Forward All-Reduce: 1íšŒ\n",
    "    \"\"\"\n",
    "    B, S, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    heads_per_gpu = n_heads // N_gpus\n",
    "    d_per_gpu = heads_per_gpu * d_k\n",
    "\n",
    "    attn_shards = []\n",
    "    for i in range(N_gpus):\n",
    "        # Column ë¶„í• : ê° GPUê°€ ë‹´ë‹¹ í—¤ë“œì— í•´ë‹¹í•˜ëŠ” Q, K, V íˆ¬ì˜\n",
    "        q_start = i * d_per_gpu\n",
    "        q_end   = q_start + d_per_gpu\n",
    "\n",
    "        WQ_i = WQ[:, q_start:q_end];  bQ_i = bQ[q_start:q_end]\n",
    "        WK_i = WK[:, q_start:q_end];  bK_i = bK[q_start:q_end]\n",
    "        WV_i = WV[:, q_start:q_end];  bV_i = bV[q_start:q_end]\n",
    "\n",
    "        Q_i = (X @ WQ_i + bQ_i).reshape(B, S, heads_per_gpu, d_k).transpose(0, 2, 1, 3)\n",
    "        K_i = (X @ WK_i + bK_i).reshape(B, S, heads_per_gpu, d_k).transpose(0, 2, 1, 3)\n",
    "        V_i = (X @ WV_i + bV_i).reshape(B, S, heads_per_gpu, d_k).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # ê° GPUê°€ ë‹´ë‹¹ í—¤ë“œì˜ Attention ê³„ì‚°\n",
    "        attn_i = scaled_dot_product_attention(Q_i, K_i, V_i)  # (B, h/N, S, d_k)\n",
    "        attn_i = attn_i.transpose(0, 2, 1, 3).reshape(B, S, d_per_gpu)\n",
    "        attn_shards.append(attn_i)\n",
    "\n",
    "    # Row Parallel: ê° GPUì˜ Attention ì¶œë ¥ì„ WOì— ê³±í•˜ê³  í•©ì‚°\n",
    "    output_partials = []\n",
    "    for i in range(N_gpus):\n",
    "        wo_start = i * d_per_gpu\n",
    "        wo_end   = wo_start + d_per_gpu\n",
    "        WO_i = WO[wo_start:wo_end, :]  # Row ë¶„í• \n",
    "        output_partials.append(attn_shards[i] @ WO_i)\n",
    "\n",
    "    # All-Reduce (1íšŒ)\n",
    "    output = sum(output_partials) + bO\n",
    "    return output\n",
    "\n",
    "\n",
    "# â”€â”€ ë‹¨ì¼ GPU ê¸°ì¤€ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def single_gpu_mha(X, WQ, WK, WV, WO, bQ, bK, bV, bO, n_heads):\n",
    "    B, S, d_model = X.shape\n",
    "    d_k = d_model // n_heads\n",
    "    Q = (X @ WQ + bQ).reshape(B, S, n_heads, d_k).transpose(0, 2, 1, 3)\n",
    "    K = (X @ WK + bK).reshape(B, S, n_heads, d_k).transpose(0, 2, 1, 3)\n",
    "    V = (X @ WV + bV).reshape(B, S, n_heads, d_k).transpose(0, 2, 1, 3)\n",
    "    attn = scaled_dot_product_attention(Q, K, V)\n",
    "    attn = attn.transpose(0, 2, 1, 3).reshape(B, S, d_model)\n",
    "    return attn @ WO + bO\n",
    "\n",
    "\n",
    "# â”€â”€ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d_model = 8\n",
    "n_heads = 4\n",
    "N_gpus  = 2  # í—¤ë“œ ìˆ˜ê°€ GPU ìˆ˜ì˜ ë°°ìˆ˜ì—¬ì•¼ í•¨\n",
    "B, S    = 2, 5\n",
    "\n",
    "WQ = np.random.randn(d_model, d_model); bQ = np.random.randn(d_model)\n",
    "WK = np.random.randn(d_model, d_model); bK = np.random.randn(d_model)\n",
    "WV = np.random.randn(d_model, d_model); bV = np.random.randn(d_model)\n",
    "WO = np.random.randn(d_model, d_model); bO = np.random.randn(d_model)\n",
    "X  = np.random.randn(B, S, d_model)\n",
    "\n",
    "ref = single_gpu_mha(X, WQ, WK, WV, WO, bQ, bK, bV, bO, n_heads)\n",
    "tp  = tp_multihead_attention(X, WQ, WK, WV, WO, bQ, bK, bV, bO, n_heads, N_gpus)\n",
    "\n",
    "err_attn = np.abs(ref - tp).max()\n",
    "print(f\"[Multi-Head Attention TP ê²€ì¦]\")\n",
    "print(f\"  d_model={d_model}, n_heads={n_heads}, N_gpus={N_gpus}\")\n",
    "print(f\"  GPUë‹¹ ë‹´ë‹¹ í—¤ë“œ ìˆ˜: {n_heads // N_gpus}\")\n",
    "print(f\"  ë‹¨ì¼ GPU ì¶œë ¥: {ref.shape}\")\n",
    "print(f\"  TP ì¶œë ¥:       {tp.shape}\")\n",
    "print(f\"  ìµœëŒ€ ì˜¤ì°¨:     {err_attn:.2e}\")\n",
    "print(f\"  ê²€ì¦:          {'âœ… ì™„ì „ ì¼ì¹˜' if err_attn < 1e-10 else 'âŒ ë¶ˆì¼ì¹˜'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í†µì‹  ìµœì í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Transformer Block TP í†µì‹  ë¶„ì„ ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# ì™¼ìª½: í†µì‹  íšŸìˆ˜ ìŠ¤íƒ ë°” (Forward / Backward)\n",
    "ax1 = axes[0]\n",
    "components = ['MLP\\n(Colâ†’Row)', 'MHA\\n(Colâ†’Row)', 'Transformer\\nBlock', '12-Layer\\nGPT-2']\n",
    "fwd = [1, 1, 2, 24]\n",
    "bwd = [1, 1, 2, 24]\n",
    "\n",
    "x = np.arange(len(components))\n",
    "w = 0.35\n",
    "ax1.bar(x - w/2, fwd, w, label='Forward', color='#42A5F5', alpha=0.85)\n",
    "ax1.bar(x + w/2, bwd, w, label='Backward', color='#EF5350', alpha=0.85)\n",
    "\n",
    "for xi, f, b in zip(x, fwd, bwd):\n",
    "    ax1.text(xi - w/2, f + 0.3, str(f), ha='center', fontweight='bold', fontsize=10)\n",
    "    ax1.text(xi + w/2, b + 0.3, str(b), ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax1.set_xticks(x); ax1.set_xticklabels(components, fontsize=10)\n",
    "ax1.set_ylabel('All-Reduce íšŸìˆ˜')\n",
    "ax1.set_title('Tensor Parallelism\\nTransformer All-Reduce í†µì‹  íšŸìˆ˜', fontweight='bold')\n",
    "ax1.legend(fontsize=10); ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: TP degree vs í†µì‹  ì˜¤ë²„í—¤ë“œ (ì´ë¡ )\n",
    "ax2 = axes[1]\n",
    "tp_degrees = np.arange(1, 33)\n",
    "d_model = 4096\n",
    "d_ff = 16384\n",
    "\n",
    "# Forward í†µì‹ ëŸ‰ (All-Reduce): TP ë†’ì¼ìˆ˜ë¡ ì ˆê°ë˜ëŠ” ì—°ì‚°ëŸ‰ vs ì¦ê°€í•˜ëŠ” í†µì‹  ì˜¤ë²„í—¤ë“œ\n",
    "# ì—°ì‚°ëŸ‰ âˆ d_model Ã— d_ff / TP (ê° GPUì˜ ë¶€ë‹´)\n",
    "compute_per_gpu = (d_model * d_ff) / tp_degrees  # ì •ê·œí™”\n",
    "# í†µì‹ ëŸ‰ (All-Reduce): âˆ 2K â‰ˆ ìƒìˆ˜ (K: í™œì„±í™” í¬ê¸°)\n",
    "comm_per_step = 2 * d_model * np.ones_like(tp_degrees, dtype=float)  # MLP input/output\n",
    "comm_ratio = comm_per_step / (compute_per_gpu + comm_per_step) * 100\n",
    "\n",
    "ax2.fill_between(tp_degrees, compute_per_gpu / compute_per_gpu[0] * 100,\n",
    "                 0, alpha=0.3, color='#42A5F5', label='ì—°ì‚° ì ˆê° (â†“ ì¢‹ìŒ)')\n",
    "ax2.plot(tp_degrees, comm_ratio, 'r-o', lw=2, ms=5,\n",
    "         label='í†µì‹  ì‹œê°„ ë¹„ìœ¨ (â†‘ ë¶€ë‹´)')\n",
    "ax2.axvline(x=8, color='green', linestyle='--', lw=2, label='í†µìƒ TP=8 (ë…¸ë“œ ë‚´ GPU ìˆ˜)')\n",
    "ax2.set_xlabel('Tensor Parallel Degree')\n",
    "ax2.set_ylabel('ë¹„ìœ¨ (%)')\n",
    "ax2.set_title('TP Degree vs ì—°ì‚°/í†µì‹  Trade-off\\n(Llama d_model=4096)', fontweight='bold')\n",
    "ax2.legend(fontsize=9); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ê²°ë¡ :\")\n",
    "print(\"  1. TPë¥¼ ëŠ˜ë¦´ìˆ˜ë¡ ê° GPUì˜ ì—°ì‚° ë¶€ë‹´ì€ ì¤„ì§€ë§Œ, í†µì‹  ë¹„ìš© ë¹„ìœ¨ì´ ì¦ê°€\")\n",
    "print(\"  2. NVLink(600GB/s)ë¡œ ì—°ê²°ëœ ê²½ìš° TP=8(ë…¸ë“œ ë‚´ GPU ìˆ˜)ì´ ì¼ë°˜ì ìœ¼ë¡œ ìµœì \")\n",
    "print(\"  3. ì¸í„°ë…¸ë“œ(InfiniBand)ì— TPë¥¼ ë°°ì¹˜í•˜ë©´ í†µì‹ ì´ ë³‘ëª©ì´ ë˜ì–´ ì—­íš¨ê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ë¶„í•  ë°©ì‹ | ëŒ€ìƒ ë ˆì´ì–´ | ì…ë ¥ ì²˜ë¦¬ | ì¶œë ¥ ì²˜ë¦¬ | All-Reduce |\n",
    "|-----------|------------|----------|----------|----------|\n",
    "| **Column Parallel** | $A_1$, $W^Q$, $W^K$, $W^V$ | Broadcast (ë³´í†µ ì´ë¯¸ ê³µìœ ) | Concat | âŒ ì—†ìŒ |\n",
    "| **Row Parallel** | $A_2$, $W^O$ | ì´ì „ ë‹¨ê³„ shard ë°”ë¡œ ì‚¬ìš© | All-Reduce SUM | âœ… 1íšŒ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\text{Column Parallel: } XW = X[W^{(0)}, \\dots, W^{(N-1)}] = [Y^{(0)}, \\dots, Y^{(N-1)}]$$\n",
    "\n",
    "$$\\text{Row Parallel: } XW = \\sum_{i=0}^{N-1} X^{(i)} W^{(i)} \\quad (\\text{All-Reduce SUM})$$\n",
    "\n",
    "### GeLUê°€ í•µì‹¬ì¸ ì´ìœ \n",
    "GeLUëŠ” **element-wise** í•¨ìˆ˜ â†’ Column Parallel ì¶œë ¥ì— ë°”ë¡œ ì ìš© ê°€ëŠ¥ â†’ ì¤‘ê°„ í†µì‹  Zero!\n",
    "\n",
    "$$\\text{GeLU}([Y^{(0)}, Y^{(1)}]) = [\\text{GeLU}(Y^{(0)}), \\text{GeLU}(Y^{(1)})]$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 09-03: íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”** â€” ëª¨ë¸ì„ ë ˆì´ì–´ ë‹¨ìœ„ë¡œ ìŠ¤í…Œì´ì§€ì— ë°°ì¹˜í•˜ê³ , ë§ˆì´í¬ë¡œë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§ìœ¼ë¡œ GPU ìœ íœ´ ì‹œê°„(Bubble)ì„ ìµœì†Œí™”í•˜ëŠ” GPipeì™€ 1F1B ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜í•™ì  ë¶„ì„ê³¼ êµ¬í˜„ì„ í•™ìŠµí•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python (tf_study)", "language": "python", "name": "tf_study"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}