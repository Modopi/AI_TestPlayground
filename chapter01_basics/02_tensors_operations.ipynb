{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02: 텐서(Tensor)와 연산\n",
    "\n",
    "## 학습 목표\n",
    "- Tensor의 rank(차원), shape, dtype 개념을 정확히 이해한다\n",
    "- `tf.constant`와 `tf.Variable`의 차이와 사용 시나리오를 파악한다\n",
    "- 인덱싱, 슬라이싱, reshape 등 텐서 조작 연산을 실습한다\n",
    "- 행렬 곱, reduce 연산 등 수학 연산을 코드로 구현한다\n",
    "- NumPy와의 변환 및 상호 운용성을 이해한다\n",
    "\n",
    "## 목차\n",
    "1. [수학적 기초: Rank, 행렬 곱, 브로드캐스팅](#수학적-기초)\n",
    "2. [Tensor란 무엇인가?](#tensor-기초)\n",
    "3. [tf.constant vs tf.Variable](#constant-vs-variable)\n",
    "4. [텐서 생성 함수](#텐서-생성)\n",
    "5. [인덱싱과 슬라이싱](#인덱싱)\n",
    "6. [형태 변환: reshape, transpose, squeeze](#형태-변환)\n",
    "7. [수학 연산](#수학-연산)\n",
    "8. [NumPy 변환](#numpy-변환)\n",
    "9. [요약](#요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학적 기초 <a name='수학적-기초'></a>\n",
    "\n",
    "### 1. Tensor Rank (텐서 랭크)\n",
    "\n",
    "텐서의 **rank**는 차원의 수(축의 개수)를 의미합니다:\n",
    "\n",
    "- **Rank 0 (스칼라):** $x \\in \\mathbb{R}$ — 예: $x = 5$\n",
    "- **Rank 1 (벡터):** $\\mathbf{v} \\in \\mathbb{R}^n$ — 예: $\\mathbf{v} = [1, 2, 3]$\n",
    "- **Rank 2 (행렬):** $A \\in \\mathbb{R}^{m \\times n}$ — 예: $2 \\times 3$ 행렬\n",
    "- **Rank 3 이상:** 고차원 텐서 — 예: 이미지 배치 $(N, H, W, C)$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 행렬 곱 (Matrix Multiplication)\n",
    "\n",
    "행렬 $A \\in \\mathbb{R}^{m \\times k}$와 $B \\in \\mathbb{R}^{k \\times n}$의 곱 $C = AB$:\n",
    "\n",
    "$$C_{ij} = \\sum_{k} A_{ik} B_{kj}$$\n",
    "\n",
    "결과 행렬 $C \\in \\mathbb{R}^{m \\times n}$의 각 원소 $C_{ij}$는  \n",
    "$A$의 $i$번째 **행**과 $B$의 $j$번째 **열**의 **내적(dot product)**입니다.\n",
    "\n",
    "**조건:** $A$의 열 수 = $B$의 행 수 (내부 차원 일치)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 브로드캐스팅 (Broadcasting) 규칙\n",
    "\n",
    "두 텐서 $A$, $B$의 shape이 다를 때 NumPy/TF는 자동으로 크기를 맞춥니다:\n",
    "\n",
    "**규칙:** shape을 오른쪽부터 비교하여:\n",
    "1. 차원 크기가 같으면 그대로 사용\n",
    "2. 한쪽이 $1$이면 다른 쪽 크기로 **확장(expand)**\n",
    "3. 한쪽 rank가 낮으면 왼쪽에 $1$을 추가한 후 위 규칙 적용\n",
    "\n",
    "$$A: (1, 3) + B: (4, 3) \\Rightarrow C: (4, 3)$$\n",
    "\n",
    "$$A: (3, 1) + B: (1, 4) \\Rightarrow C: (3, 4)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor란 무엇인가? <a name='tensor-기초'></a>\n",
    "\n",
    "**텐서(Tensor)**는 TensorFlow의 핵심 데이터 구조입니다.  \n",
    "N차원 배열로, 딥러닝의 모든 데이터(입력, 가중치, 출력)를 표현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank(차원)에 따른 텐서 예시\n",
    "\n",
    "# Rank 0: 스칼라 (scalar) — 단일 숫자\n",
    "scalar = tf.constant(42)\n",
    "print(f\"[Rank 0] 스칼라\")\n",
    "print(f\"  값: {scalar.numpy()}\")\n",
    "print(f\"  rank: {tf.rank(scalar).numpy()}\")\n",
    "print(f\"  shape: {scalar.shape}\")\n",
    "print(f\"  dtype: {scalar.dtype}\\n\")\n",
    "\n",
    "# Rank 1: 벡터 (vector) — 1D 배열\n",
    "vector = tf.constant([1.0, 2.0, 3.0, 4.0])\n",
    "print(f\"[Rank 1] 벡터\")\n",
    "print(f\"  값: {vector.numpy()}\")\n",
    "print(f\"  rank: {tf.rank(vector).numpy()}\")\n",
    "print(f\"  shape: {vector.shape}\")\n",
    "print(f\"  dtype: {vector.dtype}\\n\")\n",
    "\n",
    "# Rank 2: 행렬 (matrix) — 2D 배열\n",
    "matrix = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "print(f\"[Rank 2] 행렬\")\n",
    "print(f\"  값:\\n{matrix.numpy()}\")\n",
    "print(f\"  rank: {tf.rank(matrix).numpy()}\")\n",
    "print(f\"  shape: {matrix.shape}  (2행 3열)\")\n",
    "print(f\"  dtype: {matrix.dtype}\\n\")\n",
    "\n",
    "# Rank 3: 3D 텐서 — 예: 이미지 배치에서 하나의 RGB 이미지\n",
    "tensor_3d = tf.constant([[[1,2],[3,4]], [[5,6],[7,8]], [[9,10],[11,12]]])\n",
    "print(f\"[Rank 3] 3D 텐서\")\n",
    "print(f\"  shape: {tensor_3d.shape}  (3, 2, 2)\")\n",
    "print(f\"  rank: {tf.rank(tensor_3d).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype(데이터 타입) 탐색\n",
    "# TF는 다양한 수치 타입을 지원\n",
    "\n",
    "dtypes_demo = {\n",
    "    'float16': tf.constant(3.14, dtype=tf.float16),\n",
    "    'float32': tf.constant(3.14, dtype=tf.float32),   # 기본 실수형\n",
    "    'float64': tf.constant(3.14, dtype=tf.float64),\n",
    "    'int32':   tf.constant(42,   dtype=tf.int32),\n",
    "    'int64':   tf.constant(42,   dtype=tf.int64),\n",
    "    'bool':    tf.constant(True, dtype=tf.bool),\n",
    "    'string':  tf.constant(\"안녕하세요\", dtype=tf.string),\n",
    "}\n",
    "\n",
    "print(\"TensorFlow dtype 목록:\")\n",
    "for name, t in dtypes_demo.items():\n",
    "    val = t.numpy()\n",
    "    print(f\"  tf.{name:<10} | 값: {str(val):<20} | 메모리: {t.dtype.size if hasattr(t.dtype, 'size') else 'N/A'} bytes\")\n",
    "\n",
    "# dtype 변환 (casting)\n",
    "x_float = tf.constant([1.7, 2.3, 3.9])\n",
    "x_int   = tf.cast(x_float, dtype=tf.int32)   # 소수점 버림(truncate)\n",
    "print(f\"\\nfloat → int 캐스팅:\")\n",
    "print(f\"  원본 (float32): {x_float.numpy()}\")\n",
    "print(f\"  변환 (int32):   {x_int.numpy()}  (소수점 버림)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.constant vs tf.Variable <a name='constant-vs-variable'></a>\n",
    "\n",
    "TensorFlow에는 두 가지 핵심 텐서 타입이 있습니다:\n",
    "\n",
    "| 구분 | `tf.constant` | `tf.Variable` |\n",
    "|------|---------------|---------------|\n",
    "| 변경 가능 여부 | 불변 (Immutable) | 가변 (Mutable) |\n",
    "| 사용 목적 | 고정된 데이터 (입력, 하이퍼파라미터) | 학습 가능한 파라미터 (가중치, 편향) |\n",
    "| 그래디언트 추적 | 기본적으로 추적 안 함 | 자동으로 그래디언트 추적 |\n",
    "| 메모리 | 일반 메모리 | 장치 메모리 (GPU 포함) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant: 불변(immutable) 텐서\n",
    "const_tensor = tf.constant([1.0, 2.0, 3.0])\n",
    "print(\"tf.constant:\")\n",
    "print(f\"  값: {const_tensor.numpy()}\")\n",
    "print(f\"  타입: {type(const_tensor)}\")\n",
    "\n",
    "# 재할당은 새 텐서 생성 (기존 텐서 변경 아님)\n",
    "const_tensor = const_tensor * 2   # 새 텐서가 생성됨\n",
    "print(f\"  *2 후: {const_tensor.numpy()} (새 텐서 생성됨)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# tf.Variable: 가변(mutable) 텐서 — 신경망 가중치에 사용\n",
    "var_tensor = tf.Variable([1.0, 2.0, 3.0], name='weights')\n",
    "print(\"tf.Variable:\")\n",
    "print(f\"  값: {var_tensor.numpy()}\")\n",
    "print(f\"  이름: {var_tensor.name}\")\n",
    "print(f\"  학습 가능: {var_tensor.trainable}\")\n",
    "\n",
    "# Variable은 in-place 수정 가능\n",
    "var_tensor.assign([10.0, 20.0, 30.0])      # 값 교체\n",
    "print(f\"  assign 후: {var_tensor.numpy()}\")\n",
    "\n",
    "var_tensor.assign_add([1.0, 1.0, 1.0])     # 더하기\n",
    "print(f\"  assign_add 후: {var_tensor.numpy()}\")\n",
    "\n",
    "var_tensor.assign_sub([0.5, 0.5, 0.5])     # 빼기\n",
    "print(f\"  assign_sub 후: {var_tensor.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 신경망에서의 Variable 사용 예시\n",
    "# 선형 레이어의 가중치(W)와 편향(b)은 Variable이어야 학습이 가능\n",
    "\n",
    "# 입력 차원 4, 출력 차원 3인 선형 레이어 파라미터\n",
    "W = tf.Variable(tf.random.normal([4, 3], stddev=0.1), name='weight')\n",
    "b = tf.Variable(tf.zeros([3]), name='bias')\n",
    "\n",
    "print(\"신경망 파라미터 (Variable):\")\n",
    "print(f\"  가중치 W: shape={W.shape}, dtype={W.dtype}\")\n",
    "print(f\"  편향  b: shape={b.shape}, dtype={b.dtype}\")\n",
    "\n",
    "# 순전파(Forward Pass): y = Wx + b\n",
    "x_input = tf.constant([[1.0, 2.0, 3.0, 4.0]])  # 배치 크기 1, 특성 4개\n",
    "y_output = tf.matmul(x_input, W) + b\n",
    "print(f\"\\n순전파 결과: x={x_input.numpy()} → y={y_output.numpy()}\")\n",
    "print(f\"출력 shape: {y_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 생성 함수 <a name='텐서-생성'></a>\n",
    "\n",
    "TensorFlow는 다양한 패턴의 텐서를 생성하는 함수를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 생성 함수들\n",
    "\n",
    "# 0으로 채워진 텐서\n",
    "zeros = tf.zeros([3, 4])\n",
    "print(f\"tf.zeros([3,4]):\\n{zeros.numpy()}\\n\")\n",
    "\n",
    "# 1로 채워진 텐서\n",
    "ones = tf.ones([2, 3])\n",
    "print(f\"tf.ones([2,3]):\\n{ones.numpy()}\\n\")\n",
    "\n",
    "# 단위 행렬 (항등 행렬) — I: 대각선이 1, 나머지 0\n",
    "eye = tf.eye(4)\n",
    "print(f\"tf.eye(4) — 4x4 단위행렬:\\n{eye.numpy()}\\n\")\n",
    "\n",
    "# 특정 값으로 채우기\n",
    "filled = tf.fill([2, 3], value=7.0)\n",
    "print(f\"tf.fill([2,3], 7.0):\\n{filled.numpy()}\\n\")\n",
    "\n",
    "# 등간격 숫자 (range)\n",
    "range_1d = tf.range(0, 10, delta=2)   # 0, 2, 4, 6, 8\n",
    "print(f\"tf.range(0, 10, 2): {range_1d.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 난수 생성 함수들 (딥러닝에서 가중치 초기화에 필수)\n",
    "\n",
    "tf.random.set_seed(42)  # 재현성을 위한 시드 설정\n",
    "\n",
    "# 정규분포 (평균=0, 표준편차=1)\n",
    "normal = tf.random.normal([3, 3], mean=0.0, stddev=1.0)\n",
    "print(f\"tf.random.normal([3,3]):\\n{normal.numpy().round(3)}\\n\")\n",
    "\n",
    "# 균등분포 (0~1 사이)\n",
    "uniform = tf.random.uniform([3, 3], minval=0.0, maxval=1.0)\n",
    "print(f\"tf.random.uniform([3,3], 0~1):\\n{uniform.numpy().round(3)}\\n\")\n",
    "\n",
    "# Truncated Normal — 극단값 없는 정규분포 (가중치 초기화에 자주 사용)\n",
    "trunc = tf.random.truncated_normal([3, 3], mean=0.0, stddev=1.0)\n",
    "print(f\"tf.random.truncated_normal([3,3]) (±2σ 이내 값만):\\n{trunc.numpy().round(3)}\\n\")\n",
    "\n",
    "# 정수 난수\n",
    "rand_int = tf.random.uniform([2, 4], minval=0, maxval=10, dtype=tf.int32)\n",
    "print(f\"정수 난수 tf.random.uniform(int32, 0~9):\\n{rand_int.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인덱싱과 슬라이싱 <a name='인덱싱'></a>\n",
    "\n",
    "TensorFlow의 인덱싱/슬라이싱은 NumPy와 동일한 문법을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D 텐서로 인덱싱/슬라이싱 실습\n",
    "# 형태: 4행 5열 행렬\n",
    "mat = tf.constant([\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [21, 22, 23, 24, 25],\n",
    "    [31, 32, 33, 34, 35],\n",
    "    [41, 42, 43, 44, 45]\n",
    "], dtype=tf.int32)\n",
    "\n",
    "print(f\"원본 행렬 (shape={mat.shape}):\")\n",
    "print(mat.numpy())\n",
    "\n",
    "print(f\"\\n단일 원소 [1, 2] (2행 3열): {mat[1, 2].numpy()}\")\n",
    "\n",
    "# 행 슬라이싱\n",
    "print(f\"\\n1번 행 (0-indexed): {mat[1].numpy()}\")\n",
    "print(f\"0~1번 행 (상위 2행): \\n{mat[:2].numpy()}\")\n",
    "print(f\"마지막 2행: \\n{mat[-2:].numpy()}\")\n",
    "\n",
    "# 열 슬라이싱\n",
    "print(f\"\\n2번 열 전체: {mat[:, 2].numpy()}\")\n",
    "print(f\"1~3번 열:\\n{mat[:, 1:4].numpy()}\")\n",
    "\n",
    "# 부분 행렬 추출\n",
    "print(f\"\\n중앙 2x3 부분:\\n{mat[1:3, 1:4].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 고급 인덱싱: boolean mask, gather\n",
    "\n",
    "data = tf.constant([10, 20, 30, 40, 50, 60])\n",
    "\n",
    "# Boolean 마스크 인덱싱\n",
    "mask = tf.constant([True, False, True, False, True, False])\n",
    "masked = tf.boolean_mask(data, mask)\n",
    "print(f\"Boolean 마스크 인덱싱: {masked.numpy()}\")\n",
    "\n",
    "# 조건 기반 마스크\n",
    "cond_mask = data > 30\n",
    "print(f\"data > 30 마스크: {cond_mask.numpy()}\")\n",
    "print(f\"30 초과 값들: {tf.boolean_mask(data, cond_mask).numpy()}\")\n",
    "\n",
    "# gather: 특정 인덱스 값들 수집\n",
    "indices = tf.constant([0, 2, 5])  # 0번, 2번, 5번 인덱스\n",
    "gathered = tf.gather(data, indices)\n",
    "print(f\"\\ntf.gather 인덱스 [0,2,5]: {gathered.numpy()}\")\n",
    "\n",
    "# where: 조건에 따라 두 텐서에서 선택\n",
    "a = tf.constant([1, 2, 3, 4, 5])\n",
    "b = tf.constant([10, 20, 30, 40, 50])\n",
    "condition = tf.constant([True, False, True, False, True])\n",
    "result = tf.where(condition, a, b)  # True면 a에서, False면 b에서\n",
    "print(f\"\\ntf.where (True→a, False→b): {result.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태 변환: reshape, transpose, squeeze <a name='형태-변환'></a>\n",
    "\n",
    "딥러닝에서 텐서의 형태를 변환하는 작업은 매우 빈번하게 발생합니다.  \n",
    "예를 들어, 이미지 데이터를 FC 레이어에 입력하기 위해 flatten 하거나,  \n",
    "배치 차원을 추가하는 등의 작업이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape: 원소 수를 유지하며 shape 변환\n",
    "# 총 원소 수 = shape 각 차원의 곱 (불변)\n",
    "\n",
    "original = tf.range(24)   # 0~23, shape=(24,)\n",
    "print(f\"원본 shape: {original.shape}\")\n",
    "\n",
    "# 다양한 reshape\n",
    "r1 = tf.reshape(original, [6, 4])         # 6행 4열\n",
    "r2 = tf.reshape(original, [2, 3, 4])      # 2x3x4 3D 텐서\n",
    "r3 = tf.reshape(original, [2, -1])        # -1: 자동 계산 (2x12)\n",
    "r4 = tf.reshape(original, [-1])           # 1D로 펼치기 (flatten)\n",
    "\n",
    "print(f\"reshape([6,4]):    {r1.shape}\")\n",
    "print(f\"reshape([2,3,4]):  {r2.shape}\")\n",
    "print(f\"reshape([2,-1]):   {r3.shape}  (-1 → 12 자동 계산)\")\n",
    "print(f\"reshape([-1]):     {r4.shape}  (완전 flatten)\")\n",
    "\n",
    "print(f\"\\n원소 수 유지 확인: 24 = 6×4={6*4} = 2×3×4={2*3*4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose: 축(axis)의 순서를 바꿈\n",
    "\n",
    "mat = tf.reshape(tf.range(12, dtype=tf.float32), [3, 4])\n",
    "print(f\"원본 행렬 shape: {mat.shape}\")\n",
    "print(mat.numpy())\n",
    "\n",
    "# 전치 행렬: (3,4) → (4,3)\n",
    "mat_T = tf.transpose(mat)   # perm 미지정 시 역순\n",
    "print(f\"\\n전치 행렬 shape: {mat_T.shape}\")\n",
    "print(mat_T.numpy())\n",
    "\n",
    "# 3D 텐서의 축 재배열\n",
    "# 예: 이미지 데이터 (배치, 높이, 너비, 채널) → (배치, 채널, 높이, 너비)\n",
    "batch_images = tf.zeros([8, 32, 32, 3])    # NHWC 형식\n",
    "batch_nchw   = tf.transpose(batch_images, perm=[0, 3, 1, 2])  # NCHW 형식\n",
    "print(f\"\\n이미지 형식 변환:\")\n",
    "print(f\"  NHWC: {batch_images.shape} → NCHW: {batch_nchw.shape}\")\n",
    "\n",
    "# shape (2,3,4) → transpose(perm=[2,0,1]) → shape (4,2,3)\n",
    "t3 = tf.zeros([2, 3, 4])\n",
    "t3_perm = tf.transpose(t3, perm=[2, 0, 1])\n",
    "print(f\"\\n(2,3,4) → perm=[2,0,1] → {t3_perm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze와 expand_dims: 크기 1인 차원 제거/추가\n",
    "\n",
    "# 문제 상황: 모델이 (배치, 값) 형태를 요구하는데 (값,) 형태로 데이터가 있을 때\n",
    "\n",
    "vec = tf.constant([1.0, 2.0, 3.0])   # shape: (3,)\n",
    "print(f\"원본 벡터 shape: {vec.shape}\")\n",
    "\n",
    "# expand_dims: 지정 위치에 크기 1인 차원 추가\n",
    "row_vec = tf.expand_dims(vec, axis=0)   # (3,) → (1,3) — 행 벡터\n",
    "col_vec = tf.expand_dims(vec, axis=1)   # (3,) → (3,1) — 열 벡터\n",
    "print(f\"expand_dims(axis=0): {row_vec.shape}  (행 벡터)\")\n",
    "print(f\"expand_dims(axis=1): {col_vec.shape}  (열 벡터)\")\n",
    "print(f\"  열 벡터 값:\\n{col_vec.numpy()}\")\n",
    "\n",
    "# squeeze: 크기 1인 차원 제거\n",
    "with_extra = tf.zeros([1, 5, 1, 3])\n",
    "squeezed_all = tf.squeeze(with_extra)             # 모든 크기 1 차원 제거\n",
    "squeezed_ax0 = tf.squeeze(with_extra, axis=0)     # 0번 축만 제거\n",
    "print(f\"\\n원본 shape: {with_extra.shape}\")\n",
    "print(f\"squeeze(전체): {squeezed_all.shape}\")\n",
    "print(f\"squeeze(axis=0): {squeezed_ax0.shape}\")\n",
    "\n",
    "# 실제 사용 예: 배치 차원 추가 (단일 샘플을 배치로 처리할 때)\n",
    "single_image = tf.zeros([28, 28, 1])              # 단일 MNIST 이미지\n",
    "batched_image = tf.expand_dims(single_image, 0)   # (28,28,1) → (1,28,28,1)\n",
    "print(f\"\\n단일 이미지 → 배치 형태:\")\n",
    "print(f\"  {single_image.shape} → {batched_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학 연산 <a name='수학-연산'></a>\n",
    "\n",
    "TensorFlow는 원소별(element-wise) 연산과 행렬 연산을 모두 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원소별(element-wise) 기본 연산\n",
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "\n",
    "print(\"a =\\n\", a.numpy())\n",
    "print(\"b =\\n\", b.numpy())\n",
    "\n",
    "print(\"\\n원소별 연산:\")\n",
    "print(f\"a + b =\\n{tf.add(a, b).numpy()}  (== a + b)\")\n",
    "print(f\"a - b =\\n{tf.subtract(a, b).numpy()}\")\n",
    "print(f\"a * b (원소별 곱) =\\n{tf.multiply(a, b).numpy()}\")\n",
    "print(f\"a / b =\\n{tf.divide(a, b).numpy()}\")\n",
    "print(f\"a ** 2 =\\n{tf.pow(a, 2).numpy()}\")\n",
    "print(f\"sqrt(a) =\\n{tf.sqrt(a).numpy().round(4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행렬 곱: C_ij = sum_k(A_ik * B_kj)\n",
    "# tf.matmul() 또는 @ 연산자 사용\n",
    "\n",
    "# A: (2,3), B: (3,4) → C: (2,4)\n",
    "A = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)   # shape (2,3)\n",
    "B = tf.constant([[1, 0, 0, 1],\n",
    "                 [0, 1, 0, 1],\n",
    "                 [0, 0, 1, 1]], dtype=tf.float32)             # shape (3,4)\n",
    "\n",
    "C = tf.matmul(A, B)   # 또는 A @ B\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"C = A @ B shape: {C.shape}\")\n",
    "print(f\"C = A @ B:\\n{C.numpy()}\")\n",
    "\n",
    "# 수동으로 C[0,0] 검증: A의 0번 행 · B의 0번 열\n",
    "c00_manual = A[0,0]*B[0,0] + A[0,1]*B[1,0] + A[0,2]*B[2,0]\n",
    "print(f\"\\n수동 검증 C[0,0] = 1×1 + 2×0 + 3×0 = {c00_manual.numpy()}\")\n",
    "print(f\"결과 일치: C[0,0] = {C[0,0].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브로드캐스팅 실습\n",
    "# (1,3) + (4,3) → (4,3)\n",
    "\n",
    "row = tf.constant([[10, 20, 30]], dtype=tf.float32)  # shape (1,3)\n",
    "mat = tf.constant([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10,11,12]], dtype=tf.float32)    # shape (4,3)\n",
    "\n",
    "result = row + mat  # (1,3) + (4,3) → (4,3)\n",
    "print(f\"row shape: {row.shape}\")\n",
    "print(f\"mat shape: {mat.shape}\")\n",
    "print(f\"결과 shape: {result.shape}\")\n",
    "print(f\"결과 (row가 4번 복사되어 더해짐):\\n{result.numpy()}\")\n",
    "\n",
    "# (3,1) + (1,4): 두 축 모두 브로드캐스팅\n",
    "col = tf.constant([[1], [2], [3]], dtype=tf.float32)   # shape (3,1)\n",
    "row2 = tf.constant([[10, 20, 30, 40]], dtype=tf.float32) # shape (1,4)\n",
    "outer = col + row2  # (3,1) + (1,4) → (3,4)\n",
    "print(f\"\\ncol: {col.shape}, row2: {row2.shape}\")\n",
    "print(f\"outer = col + row2 shape: {outer.shape}\")\n",
    "print(f\"결과:\\n{outer.numpy()}  (외적 덧셈 — 덧셈 외적)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce 연산: 축을 따라 집계\n",
    "# 딥러닝에서 손실 계산, 배치 평균 등에 필수\n",
    "\n",
    "data = tf.constant([[1.0, 2.0, 3.0],\n",
    "                    [4.0, 5.0, 6.0],\n",
    "                    [7.0, 8.0, 9.0]])\n",
    "\n",
    "print(f\"데이터:\\n{data.numpy()}\\n\")\n",
    "\n",
    "# 전체 합/평균\n",
    "print(f\"전체 합 (reduce_sum):  {tf.reduce_sum(data).numpy()}\")\n",
    "print(f\"전체 평균 (reduce_mean): {tf.reduce_mean(data).numpy()}\")\n",
    "\n",
    "# 행 방향 집계 (axis=0: 행을 따라 열별 집계)\n",
    "print(f\"\\n열별 합   (axis=0): {tf.reduce_sum(data, axis=0).numpy()}\")\n",
    "print(f\"열별 평균 (axis=0): {tf.reduce_mean(data, axis=0).numpy()}\")\n",
    "\n",
    "# 열 방향 집계 (axis=1: 열을 따라 행별 집계)\n",
    "print(f\"\\n행별 합   (axis=1): {tf.reduce_sum(data, axis=1).numpy()}\")\n",
    "print(f\"행별 평균 (axis=1): {tf.reduce_mean(data, axis=1).numpy()}\")\n",
    "\n",
    "# keepdims=True: 차원 유지\n",
    "row_sum = tf.reduce_sum(data, axis=1, keepdims=True)\n",
    "print(f\"\\nkeep_dims=True 행별 합: shape={row_sum.shape}\")\n",
    "print(row_sum.numpy())\n",
    "\n",
    "# 최대/최솟값, argmax\n",
    "print(f\"\\n최댓값: {tf.reduce_max(data).numpy()}\")\n",
    "print(f\"최솟값: {tf.reduce_min(data).numpy()}\")\n",
    "print(f\"각 행의 argmax: {tf.argmax(data, axis=1).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형대수 연산 (tf.linalg)\n",
    "\n",
    "A = tf.constant([[3.0, 1.0], [1.0, 4.0]])\n",
    "\n",
    "# 행렬식 (determinant)\n",
    "det = tf.linalg.det(A)\n",
    "print(f\"행렬 A:\\n{A.numpy()}\")\n",
    "print(f\"행렬식 det(A) = {det.numpy():.2f}  (= 3×4 - 1×1 = 11)\")\n",
    "\n",
    "# 역행렬\n",
    "inv_A = tf.linalg.inv(A)\n",
    "print(f\"\\n역행렬 A⁻¹:\\n{inv_A.numpy().round(4)}\")\n",
    "\n",
    "# 검증: A × A⁻¹ = I (단위행렬)\n",
    "identity_check = tf.matmul(A, inv_A)\n",
    "print(f\"A × A⁻¹ (단위행렬 확인):\\n{identity_check.numpy().round(6)}\")\n",
    "\n",
    "# 고유값/고유벡터\n",
    "eigenvalues, eigenvectors = tf.linalg.eig(A)\n",
    "print(f\"\\n고유값: {eigenvalues.numpy().real.round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy 변환 <a name='numpy-변환'></a>\n",
    "\n",
    "TensorFlow와 NumPy는 긴밀하게 통합되어 있습니다.  \n",
    "두 라이브러리 간의 변환이 매우 간편합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy ↔ TensorFlow 변환\n",
    "import numpy as np\n",
    "\n",
    "# NumPy → TensorFlow\n",
    "np_array = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "tf_tensor = tf.constant(np_array)           # 방법 1: tf.constant\n",
    "tf_tensor2 = tf.convert_to_tensor(np_array) # 방법 2: convert_to_tensor\n",
    "\n",
    "print(f\"NumPy → TF:\")\n",
    "print(f\"  NumPy shape: {np_array.shape}, dtype: {np_array.dtype}\")\n",
    "print(f\"  TF shape:    {tf_tensor.shape}, dtype: {tf_tensor.dtype}\")\n",
    "\n",
    "# TensorFlow → NumPy\n",
    "back_to_numpy = tf_tensor.numpy()    # .numpy() 메서드\n",
    "print(f\"\\nTF → NumPy: {type(back_to_numpy).__name__}\")\n",
    "print(f\"  값: {back_to_numpy}\")\n",
    "\n",
    "# 제로 카피(zero-copy): CPU 메모리에서는 데이터를 복사하지 않음\n",
    "tf_from_np = tf.constant(np_array)\n",
    "np_from_tf = np.array(tf_from_np)  # NumPy 연산에 직접 TF 텐서 전달 가능\n",
    "print(f\"\\nnp.array(tf_tensor) 결과:\\n{np_from_tf}\")\n",
    "\n",
    "# TF 텐서를 NumPy 함수에 직접 사용\n",
    "result = np.sum(tf_tensor)  # TF 텐서를 NumPy 함수에 바로 전달\n",
    "print(f\"\\nnp.sum(tf_tensor) = {result}  (TF 텐서를 NumPy 함수에 직접 사용)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유용한 텐서 조작 함수 모음\n",
    "\n",
    "# concat: 텐서 이어 붙이기\n",
    "t1 = tf.constant([[1, 2], [3, 4]])\n",
    "t2 = tf.constant([[5, 6], [7, 8]])\n",
    "\n",
    "c0 = tf.concat([t1, t2], axis=0)  # 행 방향 (세로로) 이어 붙이기\n",
    "c1 = tf.concat([t1, t2], axis=1)  # 열 방향 (가로로) 이어 붙이기\n",
    "print(f\"t1 shape: {t1.shape}, t2 shape: {t2.shape}\")\n",
    "print(f\"concat(axis=0) shape: {c0.shape}\\n{c0.numpy()}\")\n",
    "print(f\"concat(axis=1) shape: {c1.shape}\\n{c1.numpy()}\")\n",
    "\n",
    "# stack: 새 차원으로 쌓기\n",
    "v1 = tf.constant([1, 2, 3])\n",
    "v2 = tf.constant([4, 5, 6])\n",
    "stacked0 = tf.stack([v1, v2], axis=0)  # (2,3)\n",
    "stacked1 = tf.stack([v1, v2], axis=1)  # (3,2)\n",
    "print(f\"\\nstack(axis=0): {stacked0.shape}\\n{stacked0.numpy()}\")\n",
    "print(f\"stack(axis=1): {stacked1.shape}\\n{stacked1.numpy()}\")\n",
    "\n",
    "# split: 텐서 분할\n",
    "big = tf.constant([[1,2,3,4,5,6]])\n",
    "parts = tf.split(big, num_or_size_splits=3, axis=1)  # 3등분\n",
    "print(f\"\\nsplit (3등분): {[p.numpy() for p in parts]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약 <a name='요약'></a>\n",
    "\n",
    "### 핵심 수식 정리\n",
    "\n",
    "| 연산 | 수식 | TF 코드 |\n",
    "|------|------|----------|\n",
    "| 행렬 곱 | $C_{ij} = \\sum_k A_{ik}B_{kj}$ | `tf.matmul(A, B)` 또는 `A @ B` |\n",
    "| 전치 | $A^T_{ij} = A_{ji}$ | `tf.transpose(A)` |\n",
    "| 원소 합 | $s = \\sum_{i,j} A_{ij}$ | `tf.reduce_sum(A)` |\n",
    "| 원소 평균 | $\\bar{A} = \\frac{1}{mn}\\sum_{i,j} A_{ij}$ | `tf.reduce_mean(A)` |\n",
    "| 브로드캐스팅 | $(1,n) + (m,n) \\to (m,n)$ | 자동 적용 |\n",
    "\n",
    "### 핵심 개념 체크리스트\n",
    "- [ ] Tensor rank = 축(axis)의 개수\n",
    "- [ ] `tf.constant`: 불변 / `tf.Variable`: 가변 (가중치에 사용)\n",
    "- [ ] reshape는 총 원소 수를 유지해야 함\n",
    "- [ ] 브로드캐스팅: 크기 1인 차원은 자동 확장\n",
    "- [ ] `.numpy()`로 TF 텐서 → NumPy 배열 변환\n",
    "\n",
    "### 다음 챕터 예고: 03. 자동 미분 (GradientTape)\n",
    "\n",
    "다음 챕터에서는 딥러닝 학습의 핵심 메커니즘인 **역전파(Backpropagation)**를 학습합니다:\n",
    "- 편미분 $\\frac{\\partial f}{\\partial x}$의 의미\n",
    "- 연쇄 법칙 $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$\n",
    "- `tf.GradientTape`으로 자동 미분 구현\n",
    "- Keras 없이 선형 회귀 수동 학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
