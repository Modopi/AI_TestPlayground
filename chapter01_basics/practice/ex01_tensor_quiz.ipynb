{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: 텐서 연산 완전 정복\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드를 실행하기 전에 종이에 계산해보는 것을 권장합니다\n",
    "- 틀렸다면 해설을 꼼꼼히 읽고, 왜 틀렸는지 이해하세요\n",
    "\n",
    "## 목차\n",
    "- [Q1: 행렬 곱 Shape 계산](#q1)\n",
    "- [Q2: 브로드캐스팅 결과 Shape](#q2)\n",
    "- [Q3: Transpose 결과 Shape](#q3)\n",
    "- [Q4: GradientTape으로 미분 계산](#q4)\n",
    "- [Q5: tf.Variable vs tf.constant](#q5)\n",
    "- [종합 도전: 미니 신경망 구현](#bonus)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 퀴즈 환경 설정\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(\"퀴즈 환경 준비 완료!\")\n",
    "print(\"각 문제를 풀기 전에 먼저 답을 예측해보세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q1: 행렬 곱 Shape 계산 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 두 텐서 `A`와 `B`를 행렬 곱(`tf.matmul`)했을 때, 결과 텐서 `C`의 **shape**은?\n",
    "\n",
    "```python\n",
    "A = tf.zeros([3, 4])   # shape: (3, 4)\n",
    "B = tf.zeros([4, 5])   # shape: (4, 5)\n",
    "C = tf.matmul(A, B)\n",
    "```\n",
    "\n",
    "**힌트:** 행렬 곱 $C = AB$에서\n",
    "- $A \\in \\mathbb{R}^{m \\times k}$, $B \\in \\mathbb{R}^{k \\times n}$이면\n",
    "- $C \\in \\mathbb{R}^{m \\times n}$\n",
    "- 조건: $A$의 **열 수** = $B$의 **행 수**\n",
    "\n",
    "**여러분의 예측:** C의 shape은 `(?, ?)` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────\n",
    "A = tf.zeros([3, 4])   # (3행, 4열)\n",
    "B = tf.zeros([4, 5])   # (4행, 5열)\n",
    "C = tf.matmul(A, B)\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: 행렬 곱 Shape\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"A.shape = {A.shape}  →  (m=3, k=4)\")\n",
    "print(f\"B.shape = {B.shape}  →  (k=4, n=5)\")\n",
    "print(f\"-\" * 45)\n",
    "print(f\"C = A @ B\")\n",
    "print(f\"C.shape = {C.shape}  →  (m=3, n=5)\")\n",
    "print()\n",
    "\n",
    "# 수학적 설명\n",
    "print(\"[해설]\")\n",
    "print(\"  행렬 곱 C_ij = Σ_k A_ik * B_kj\")\n",
    "print(f\"  A의 열 수 ({A.shape[1]}) = B의 행 수 ({B.shape[0]}) → 곱 가능\")\n",
    "print(f\"  결과 행 수 = A의 행 수 = {A.shape[0]}\")\n",
    "print(f\"  결과 열 수 = B의 열 수 = {B.shape[1]}\")\n",
    "print(f\"  ∴ C.shape = ({A.shape[0]}, {B.shape[1]})\")\n",
    "\n",
    "# 오류 케이스도 확인\n",
    "print()\n",
    "print(\"[오류 케이스] A(3,4) @ C(5,4) — 열/행 불일치\")\n",
    "try:\n",
    "    D = tf.zeros([5, 4])   # 내부 차원 불일치\n",
    "    E = tf.matmul(A, D)\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(f\"  예상된 오류 발생: {str(e)[:80]}...\")\n",
    "\n",
    "# 배치 행렬 곱 (3D)\n",
    "print()\n",
    "print(\"[심화] 배치 행렬 곱 (3D Tensor)\")\n",
    "A3 = tf.zeros([8, 3, 4])   # 배치 크기 8\n",
    "B3 = tf.zeros([8, 4, 5])\n",
    "C3 = tf.matmul(A3, B3)\n",
    "print(f\"  A3: {A3.shape}, B3: {B3.shape}\")\n",
    "print(f\"  C3 = A3 @ B3: {C3.shape}  (배치 차원 유지)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q2: 브로드캐스팅 결과 Shape <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 두 텐서를 더할 때 결과 shape은?\n",
    "\n",
    "```python\n",
    "A = tf.zeros([1, 3])   # shape: (1, 3)\n",
    "B = tf.zeros([4, 3])   # shape: (4, 3)\n",
    "C = A + B\n",
    "```\n",
    "\n",
    "**브로드캐스팅 규칙 복습:**\n",
    "1. 오른쪽부터 차원을 비교\n",
    "2. 차원 크기가 같으면 그대로\n",
    "3. 한쪽이 **1**이면 다른 쪽 크기로 확장\n",
    "4. 한쪽이 더 낮은 rank라면 왼쪽에 1을 추가\n",
    "\n",
    "**여러분의 예측:** C의 shape은 `(?, ?)` 입니다.\n",
    "\n",
    "---\n",
    "\n",
    "**추가 문제:** 다음 중 브로드캐스팅이 **불가능**한 경우는?\n",
    "- (a) `(2, 3) + (1, 3)`\n",
    "- (b) `(3, 1) + (1, 4)` \n",
    "- (c) `(2, 3) + (4, 3)` ← 이것은?\n",
    "- (d) `(1,) + (3, 4)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 50)\n",
    "print(\"Q2 풀이: 브로드캐스팅 결과 Shape\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 기본 문제\n",
    "A = tf.zeros([1, 3])\n",
    "B = tf.zeros([4, 3])\n",
    "C = A + B\n",
    "\n",
    "print(f\"A.shape = {A.shape}  (1행, 3열)\")\n",
    "print(f\"B.shape = {B.shape}  (4행, 3열)\")\n",
    "print(f\"C = A + B → C.shape = {C.shape}\")\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(\"  열 차원 비교: 3 == 3  → 그대로 (3)\")\n",
    "print(\"  행 차원 비교: 1 vs 4  → 1이므로 4로 확장\")\n",
    "print(\"  A가 4번 복사되어 더해지는 효과\")\n",
    "print(f\"  ∴ 결과 shape = (4, 3)\")\n",
    "\n",
    "# 실제 값으로 확인\n",
    "A_val = tf.constant([[10, 20, 30]])   # (1,3)\n",
    "B_val = tf.constant([[1, 2, 3],\n",
    "                     [4, 5, 6],\n",
    "                     [7, 8, 9],\n",
    "                     [10,11,12]])     # (4,3)\n",
    "C_val = A_val + B_val\n",
    "print()\n",
    "print(\"실제 값 확인:\")\n",
    "print(f\"  A_val = {A_val.numpy()}\")\n",
    "print(f\"  B_val =\\n{B_val.numpy()}\")\n",
    "print(f\"  C_val = A + B =\\n{C_val.numpy()}\")\n",
    "print(\"  → A의 [10,20,30]이 각 행에 더해짐\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 50)\n",
    "print(\"추가 문제: 브로드캐스팅 가능/불가능 케이스\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cases = [\n",
    "    ([2, 3], [1, 3], \"(a)\"),\n",
    "    ([3, 1], [1, 4], \"(b)\"),\n",
    "    ([2, 3], [4, 3], \"(c) 불가능?\"),\n",
    "    ([1],    [3, 4], \"(d)\"),\n",
    "]\n",
    "\n",
    "for shape1, shape2, label in cases:\n",
    "    try:\n",
    "        t1 = tf.zeros(shape1)\n",
    "        t2 = tf.zeros(shape2)\n",
    "        result = t1 + t2\n",
    "        print(f\"  {label}: {tuple(shape1)} + {tuple(shape2)} → {tuple(result.shape.as_list())} [가능]\")\n",
    "    except (tf.errors.InvalidArgumentError, Exception) as e:\n",
    "        print(f\"  {label}: {tuple(shape1)} + {tuple(shape2)} → 오류! [불가능] ({str(e)[:50]})\")\n",
    "\n",
    "print()\n",
    "print(\"[핵심] (c) (2,3)+(4,3): 2≠4이고 둘 다 1이 아님 → 브로드캐스팅 불가!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 심화: 다양한 브로드캐스팅 패턴\n",
    "\n",
    "print(\"브로드캐스팅 심화 패턴:\")\n",
    "\n",
    "# (3,) + (2,3) → (2,3): 낮은 rank에 왼쪽 1 추가\n",
    "v = tf.constant([1, 2, 3], dtype=tf.float32)          # (3,) → 사실상 (1,3)\n",
    "m = tf.constant([[10,20,30],[40,50,60]], dtype=tf.float32)  # (2,3)\n",
    "r = v + m\n",
    "print(f\"\\n  (3,) + (2,3): {v.shape} + {m.shape} → {r.shape}\")\n",
    "print(f\"  결과:\\n{r.numpy()}\")\n",
    "\n",
    "# (3,1) + (1,4) → (3,4): 외적 덧셈\n",
    "col = tf.constant([[1],[2],[3]], dtype=tf.float32)        # (3,1)\n",
    "row = tf.constant([[10,20,30,40]], dtype=tf.float32)      # (1,4)\n",
    "outer_sum = col + row\n",
    "print(f\"\\n  (3,1) + (1,4): {col.shape} + {row.shape} → {outer_sum.shape}  (외적 덧셈)\")\n",
    "print(f\"  결과:\\n{outer_sum.numpy()}\")\n",
    "\n",
    "# 스칼라 브로드캐스팅\n",
    "big_tensor = tf.zeros([3, 4, 5])\n",
    "scalar = tf.constant(7.0)  # shape ()\n",
    "r2 = big_tensor + scalar\n",
    "print(f\"\\n  () + (3,4,5): {scalar.shape} + {big_tensor.shape} → {r2.shape}  (스칼라는 모든 원소에 적용)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q3: Transpose 결과 Shape <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 3D 텐서에 `transpose`를 적용했을 때 결과 shape은?\n",
    "\n",
    "```python\n",
    "T = tf.zeros([2, 3, 4])   # shape: (2, 3, 4)\n",
    "T_perm = tf.transpose(T, perm=[2, 0, 1])\n",
    "```\n",
    "\n",
    "**힌트:** `perm=[2, 0, 1]`이 의미하는 것:\n",
    "- 새로운 0번 축 ← 기존 **2번** 축 (크기: 4)\n",
    "- 새로운 1번 축 ← 기존 **0번** 축 (크기: 2)\n",
    "- 새로운 2번 축 ← 기존 **1번** 축 (크기: 3)\n",
    "\n",
    "**여러분의 예측:** T_perm의 shape은 `(?, ?, ?)` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 50)\n",
    "print(\"Q3 풀이: Transpose 결과 Shape\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "T = tf.zeros([2, 3, 4])\n",
    "T_perm = tf.transpose(T, perm=[2, 0, 1])\n",
    "\n",
    "print(f\"원본 T.shape = {T.shape}\")\n",
    "print(f\"             축0=2, 축1=3, 축2=4\")\n",
    "print()\n",
    "print(f\"perm = [2, 0, 1]\")\n",
    "print(f\"  새 축0 ← 기존 축2  (크기: {T.shape[2]})\")\n",
    "print(f\"  새 축1 ← 기존 축0  (크기: {T.shape[0]})\")\n",
    "print(f\"  새 축2 ← 기존 축1  (크기: {T.shape[1]})\")\n",
    "print()\n",
    "print(f\"T_perm.shape = {T_perm.shape}\")\n",
    "print()\n",
    "\n",
    "# 실제 데이터로 확인\n",
    "T_val = tf.reshape(tf.range(24), [2, 3, 4])\n",
    "T_perm_val = tf.transpose(T_val, perm=[2, 0, 1])\n",
    "\n",
    "print(\"[해설]\")\n",
    "print(f\"  원본 shape: (2, 3, 4)\")\n",
    "print(f\"  perm[2, 0, 1]: 기존 [축2, 축0, 축1] = [4, 2, 3]\")\n",
    "print(f\"  ∴ 결과 shape = (4, 2, 3)\")\n",
    "\n",
    "print()\n",
    "print(\"다양한 perm 예시:\")\n",
    "perms = {\n",
    "    '[0,1,2]': [0,1,2],   # 변화 없음\n",
    "    '[1,0,2]': [1,0,2],   # 처음 두 축 교환\n",
    "    '[2,1,0]': [2,1,0],   # 완전 역순\n",
    "    '[2,0,1]': [2,0,1],   # 문제의 perm\n",
    "    '[1,2,0]': [1,2,0],   # 순환 이동\n",
    "}\n",
    "\n",
    "for perm_name, perm in perms.items():\n",
    "    result = tf.transpose(T, perm=perm)\n",
    "    print(f\"  perm={perm_name}: (2,3,4) → {tuple(result.shape.as_list())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3 심화: Transpose 원소 위치 변화 추적\n",
    "\n",
    "# 작은 텐서로 원소 위치가 어떻게 바뀌는지 확인\n",
    "small = tf.reshape(tf.range(12), [2, 3, 2])  # shape (2,3,2)\n",
    "print(f\"원본 shape: {small.shape}\")\n",
    "print(f\"원본 값:\")\n",
    "print(small.numpy())\n",
    "\n",
    "# 기본 transpose (역순: perm=[1,0,2] → 처음 두 축 교환)\n",
    "t1 = tf.transpose(small, perm=[1, 0, 2])  # (3,2,2)\n",
    "print(f\"\\ntranspose(perm=[1,0,2]) → shape: {t1.shape}\")\n",
    "print(t1.numpy())\n",
    "\n",
    "print(f\"\\n[검증] 원본 small[0,1,0] = {small[0,1,0].numpy()}\")\n",
    "print(f\"       변환 후  t1[1,0,0] = {t1[1,0,0].numpy()}\")\n",
    "print(\"       perm=[1,0,2] 이므로 축0↔축1 교환, 즉 [i,j,k] → [j,i,k]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q4: GradientTape으로 미분 계산 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "`tf.GradientTape`을 사용하여 $f(x) = 3x^2 + 2x + 1$의 $x = 2$에서의 기울기(미분값)를 구하세요.\n",
    "\n",
    "**수학적 풀이:**\n",
    "$$f(x) = 3x^2 + 2x + 1$$\n",
    "$$f'(x) = 6x + 2$$\n",
    "$$f'(2) = 6 \\times 2 + 2 = 14$$\n",
    "\n",
    "**빈칸을 채우세요:**\n",
    "```python\n",
    "x = tf.Variable(___)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = ___ * x**2 + ___ * x + ___\n",
    "\n",
    "df_dx = tape.gradient(___, ___)\n",
    "print(df_dx.numpy())  # 예상 결과: 14.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 50)\n",
    "print(\"Q4 풀이: GradientTape 미분 계산\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# f(x) = 3x^2 + 2x + 1\n",
    "x = tf.Variable(2.0)   # x = 2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = 3 * x**2 + 2 * x + 1\n",
    "\n",
    "df_dx = tape.gradient(f, x)\n",
    "\n",
    "print(f\"f(x) = 3x² + 2x + 1\")\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"f(2) = 3×{x.numpy()}² + 2×{x.numpy()} + 1 = {f.numpy()}\")\n",
    "print(f\"      (= 3×4 + 4 + 1 = 12 + 4 + 1 = 17)\")\n",
    "print()\n",
    "print(f\"f'(x) = 6x + 2\")\n",
    "print(f\"f'(2) = 6×{x.numpy()} + 2 = {6*2+2}\")\n",
    "print()\n",
    "print(f\"GradientTape 결과: df_dx = {df_dx.numpy()}\")\n",
    "print(f\"수학적 정답:         f'(2) = 14\")\n",
    "print(f\"일치 여부: {abs(df_dx.numpy() - 14.0) < 1e-5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 심화: 여러 x값에서 미분값 계산 및 시각화\n",
    "\n",
    "# f(x) = 3x^2 + 2x + 1, f'(x) = 6x + 2\n",
    "\n",
    "def compute_gradient(x_val):\n",
    "    \"\"\"주어진 x값에서 f(x)와 f'(x)를 반환\"\"\"\n",
    "    x = tf.Variable(float(x_val))\n",
    "    with tf.GradientTape() as tape:\n",
    "        f = 3 * x**2 + 2 * x + 1\n",
    "    df_dx = tape.gradient(f, x)\n",
    "    return f.numpy(), df_dx.numpy()\n",
    "\n",
    "print(\"f(x) = 3x² + 2x + 1  |  f'(x) = 6x + 2\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'x':>5} | {'f(x)':>10} | {'f\\'(x) TF':>12} | {'f\\'(x) 수식':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for x_val in [-3, -2, -1, 0, 1, 2, 3]:\n",
    "    f_val, grad_val = compute_gradient(x_val)\n",
    "    theoretical = 6 * x_val + 2\n",
    "    print(f\"{x_val:>5} | {f_val:>10.1f} | {grad_val:>12.1f} | {theoretical:>12.1f}\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print()\n",
    "print(\"[해설] f'(x) = 6x + 2 = 0 → x = -1/3 ≈ -0.333 에서 최솟값\")\n",
    "f_min, _ = compute_gradient(-1/3)\n",
    "print(f\"       f(-1/3) ≈ {f_min:.4f}  (최솟값)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 응용: 경사하강법으로 f(x)의 최솟값 찾기\n",
    "# GradientTape + 경사하강법 = 딥러닝 학습의 핵심 패턴\n",
    "\n",
    "print(\"경사하강법으로 f(x) = 3x² + 2x + 1의 최솟값 찾기\")\n",
    "print(f\"이론적 최솟값: x = -1/3 ≈ {-1/3:.4f}\")\n",
    "print()\n",
    "\n",
    "x = tf.Variable(3.0)   # 임의의 시작점\n",
    "lr = 0.05              # 학습률\n",
    "\n",
    "print(f\"{'에포크':>6} | {'x':>10} | {'f(x)':>10} | {'f\\'(x)':>10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for epoch in range(31):\n",
    "    with tf.GradientTape() as tape:\n",
    "        f = 3 * x**2 + 2 * x + 1\n",
    "    grad = tape.gradient(f, x)\n",
    "    x.assign_sub(lr * grad)   # x ← x - lr * f'(x)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"{epoch:>6} | {x.numpy():>10.6f} | {f.numpy():>10.6f} | {grad.numpy():>10.6f}\")\n",
    "\n",
    "print(\"-\" * 45)\n",
    "print(f\"\\n수렴된 x = {x.numpy():.6f}\")\n",
    "print(f\"이론적 x = {-1/3:.6f}\")\n",
    "print(f\"오차 = {abs(x.numpy() - (-1/3)):.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Q5: tf.Variable vs tf.constant <a name='q5'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "신경망의 학습 가능한 파라미터를 만들 때 `tf.Variable`과 `tf.constant` 중 어떤 것을 사용해야 할까요?\n",
    "\n",
    "다음 코드 중 **학습이 올바르게 동작하는** 버전은?\n",
    "\n",
    "```python\n",
    "# 버전 A\n",
    "w = tf.constant([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.reduce_sum(w * x)\n",
    "grad = tape.gradient(y, w)  # grad는?\n",
    "\n",
    "# 버전 B\n",
    "w = tf.Variable([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.reduce_sum(w * x)\n",
    "grad = tape.gradient(y, w)  # grad는?\n",
    "```\n",
    "\n",
    "**예측:** 어떤 버전이 None이 아닌 그래디언트를 반환할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q5 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 55)\n",
    "print(\"Q5 풀이: tf.Variable vs tf.constant\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "x = tf.constant([1.0, 2.0, 3.0])\n",
    "\n",
    "# 버전 A: tf.constant 사용\n",
    "w_const = tf.constant([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y_a = tf.reduce_sum(w_const * x)\n",
    "grad_a = tape.gradient(y_a, w_const)\n",
    "\n",
    "print(f\"[버전 A] w = tf.constant\")\n",
    "print(f\"  그래디언트 결과: {grad_a}\")\n",
    "print(f\"  → None! GradientTape은 Variable만 자동 추적함\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 버전 B: tf.Variable 사용\n",
    "w_var = tf.Variable([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    y_b = tf.reduce_sum(w_var * x)\n",
    "grad_b = tape.gradient(y_b, w_var)\n",
    "\n",
    "print(f\"[버전 B] w = tf.Variable\")\n",
    "print(f\"  그래디언트 결과: {grad_b.numpy()}\")\n",
    "print(f\"  → 정상! y = w1*x1 + w2*x2 + w3*x3\")\n",
    "print(f\"  → ∂y/∂w = [x1, x2, x3] = {x.numpy()}\")\n",
    "print()\n",
    "\n",
    "# 예외: constant도 tape.watch()로 추적 가능\n",
    "w_const2 = tf.constant([1.0, 2.0, 3.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(w_const2)   # 명시적 추적\n",
    "    y_c = tf.reduce_sum(w_const2 * x)\n",
    "grad_c = tape.gradient(y_c, w_const2)\n",
    "\n",
    "print(f\"[예외] tf.constant + tape.watch()\")\n",
    "print(f\"  그래디언트 결과: {grad_c.numpy()}\")\n",
    "print(f\"  → tape.watch()로 명시적 추적 시 가능\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 심화: 학습 가능한 파라미터 직접 구현\n",
    "# trainable=False로 특정 파라미터의 학습을 중지할 수 있음\n",
    "\n",
    "print(\"학습 가능/불가능 파라미터 제어:\")\n",
    "print()\n",
    "\n",
    "# 학습 가능 (기본)\n",
    "w_trainable = tf.Variable([1.0, 2.0], name='trainable_weight', trainable=True)\n",
    "# 학습 불가 (예: 고정된 임베딩, BatchNorm 통계)\n",
    "w_frozen    = tf.Variable([3.0, 4.0], name='frozen_weight', trainable=False)\n",
    "\n",
    "print(f\"w_trainable.trainable = {w_trainable.trainable}\")\n",
    "print(f\"w_frozen.trainable    = {w_frozen.trainable}\")\n",
    "\n",
    "x_input = tf.constant([1.0, 1.0])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # trainable=False인 Variable은 기본적으로 추적 안 됨\n",
    "    output = tf.reduce_sum(w_trainable * x_input) + tf.reduce_sum(w_frozen * x_input)\n",
    "\n",
    "grad_trainable = tape.gradient(output, w_trainable)\n",
    "grad_frozen    = tape.gradient(output, w_frozen)   # 주의: 이미 tape 소비됨\n",
    "\n",
    "print(f\"\\n그래디언트 결과:\")\n",
    "print(f\"  ∂output/∂w_trainable = {grad_trainable.numpy() if grad_trainable is not None else 'None'}\")\n",
    "print(f\"  ∂output/∂w_frozen    = {grad_frozen}  (trainable=False이므로 추적 안됨)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Keras 레이어에서 trainable 제어\n",
    "layer = tf.keras.layers.Dense(4, name='my_layer')\n",
    "_ = layer(tf.ones([1, 3]))  # 레이어 빌드\n",
    "\n",
    "print(f\"Dense 레이어 파라미터:\")\n",
    "for var in layer.trainable_variables:\n",
    "    print(f\"  학습 가능 - {var.name}: shape={var.shape}\")\n",
    "\n",
    "# 레이어 동결\n",
    "layer.trainable = False\n",
    "print(f\"\\nlayer.trainable = False 후:\")\n",
    "print(f\"  trainable_variables 수: {len(layer.trainable_variables)}  (0이 되어야 함)\")\n",
    "print(f\"  non_trainable_variables 수: {len(layer.non_trainable_variables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5 최종: tf.Variable의 주요 메서드 총정리\n",
    "\n",
    "print(\"tf.Variable 주요 메서드 및 속성 총정리:\")\n",
    "print()\n",
    "\n",
    "v = tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='demo', trainable=True)\n",
    "\n",
    "print(f\"생성: v = tf.Variable([[1,2],[3,4]])\")\n",
    "print(f\"  v.numpy()      = \\n{v.numpy()}\")\n",
    "print(f\"  v.name         = {v.name}\")\n",
    "print(f\"  v.shape        = {v.shape}\")\n",
    "print(f\"  v.dtype        = {v.dtype}\")\n",
    "print(f\"  v.trainable    = {v.trainable}\")\n",
    "print()\n",
    "\n",
    "print(\"값 수정 메서드:\")\n",
    "v.assign([[10., 20.], [30., 40.]])\n",
    "print(f\"  v.assign([[10,20],[30,40]]):\")\n",
    "print(f\"    {v.numpy()}\")\n",
    "\n",
    "v.assign_add([[1., 1.], [1., 1.]])\n",
    "print(f\"  v.assign_add([[1,1],[1,1]]):\")\n",
    "print(f\"    {v.numpy()}\")\n",
    "\n",
    "v.assign_sub([[1., 1.], [1., 1.]])\n",
    "print(f\"  v.assign_sub([[1,1],[1,1]]):\")\n",
    "print(f\"    {v.numpy()}\")\n",
    "\n",
    "# 특정 인덱스 업데이트\n",
    "v[0, 1].assign(99.)\n",
    "print(f\"  v[0,1].assign(99.):\")\n",
    "print(f\"    {v.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 종합 도전: 미니 신경망 구현 <a name='bonus'></a>\n",
    "\n",
    "지금까지 배운 내용을 종합하여, XOR 문제를 푸는 2층 신경망을 구현하세요.\n",
    "\n",
    "**XOR 진리표:**\n",
    "| x1 | x2 | y |\n",
    "|----|----|----|  \n",
    "| 0  | 0  | 0  |\n",
    "| 0  | 1  | 1  |\n",
    "| 1  | 0  | 1  |\n",
    "| 1  | 1  | 0  |\n",
    "\n",
    "**모델 구조:** Input(2) → Hidden(4, ReLU) → Output(1, Sigmoid)\n",
    "\n",
    "**공식:**\n",
    "$$h = \\text{ReLU}(W_1 x + b_1)$$\n",
    "$$\\hat{y} = \\sigma(W_2 h + b_2)$$\n",
    "$$L = -\\frac{1}{n}\\sum_i \\left[ y_i \\log \\hat{y}_i + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이 ────────────────────────────────────────────\n",
    "print(\"XOR 신경망 — GradientTape으로 수동 구현\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# XOR 데이터\n",
    "X_xor = tf.constant([[0.,0.],[0.,1.],[1.,0.],[1.,1.]], dtype=tf.float32)\n",
    "Y_xor = tf.constant([[0.],[1.],[1.],[0.]], dtype=tf.float32)\n",
    "\n",
    "print(\"XOR 입력 데이터:\")\n",
    "for i in range(4):\n",
    "    print(f\"  x={X_xor[i].numpy()} → y={Y_xor[i].numpy()}\")\n",
    "\n",
    "# 모델 파라미터 초기화 (tf.Variable 사용!)\n",
    "tf.random.set_seed(7)\n",
    "W1 = tf.Variable(tf.random.normal([2, 4], stddev=0.5), name='W1')  # 입력→은닉\n",
    "b1 = tf.Variable(tf.zeros([4]),                         name='b1')\n",
    "W2 = tf.Variable(tf.random.normal([4, 1], stddev=0.5), name='W2')  # 은닉→출력\n",
    "b2 = tf.Variable(tf.zeros([1]),                         name='b2')\n",
    "\n",
    "params = [W1, b1, W2, b2]\n",
    "print(f\"\\n파라미터 수: {sum(p.numpy().size for p in params)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파 함수 정의\n",
    "def forward(X):\n",
    "    \"\"\"순전파: Input → ReLU Hidden → Sigmoid Output\"\"\"\n",
    "    # 1층: h = ReLU(W1 @ X^T + b1)\n",
    "    z1 = tf.matmul(X, W1) + b1        # (4,2)@(2,4) + (4,) = (4,4)\n",
    "    h  = tf.nn.relu(z1)               # ReLU 활성화\n",
    "    # 2층: y_hat = Sigmoid(W2 @ h^T + b2)\n",
    "    z2 = tf.matmul(h, W2) + b2        # (4,4)@(4,1) + (1,) = (4,1)\n",
    "    y_hat = tf.nn.sigmoid(z2)         # Sigmoid 활성화\n",
    "    return y_hat\n",
    "\n",
    "def bce_loss(y_true, y_pred):\n",
    "    \"\"\"이진 교차 엔트로피 손실\"\"\"\n",
    "    eps = 1e-7\n",
    "    y_pred = tf.clip_by_value(y_pred, eps, 1.0 - eps)  # 수치 안정성\n",
    "    return -tf.reduce_mean(\n",
    "        y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "    )\n",
    "\n",
    "# 학습 루프\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "loss_history = []\n",
    "print(f\"{'에포크':>8} | {'BCE 손실':>12} | {'예측 (반올림)':>20}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for epoch in range(2001):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = forward(X_xor)\n",
    "        loss = bce_loss(Y_xor, y_pred)\n",
    "    \n",
    "    grads = tape.gradient(loss, params)\n",
    "    optimizer.apply_gradients(zip(grads, params))\n",
    "    \n",
    "    loss_history.append(loss.numpy())\n",
    "    \n",
    "    if epoch % 500 == 0:\n",
    "        preds = tf.round(y_pred).numpy().flatten()\n",
    "        print(f\"{epoch:>8} | {loss.numpy():>12.6f} | {preds}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n학습 완료!\")\n",
    "print(f\"최종 손실: {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 예측 결과 확인\n",
    "print(\"XOR 신경망 최종 예측 결과:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'입력 x':>12} | {'실제 y':>8} | {'예측 확률':>12} | {'예측 (반올림)':>12} | {'정답':>6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "final_preds = forward(X_xor)\n",
    "all_correct = True\n",
    "\n",
    "for i in range(4):\n",
    "    x_i = X_xor[i].numpy()\n",
    "    y_true_i = Y_xor[i].numpy()[0]\n",
    "    y_prob_i = final_preds[i].numpy()[0]\n",
    "    y_pred_i = round(y_prob_i)\n",
    "    correct = y_pred_i == y_true_i\n",
    "    if not correct:\n",
    "        all_correct = False\n",
    "    status = \"O\" if correct else \"X\"\n",
    "    print(f\"  {str(x_i):>10} | {y_true_i:>8.0f} | {y_prob_i:>12.6f} | {y_pred_i:>12.0f} | {status:>6}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\n결과: {'모든 경우 정답!' if all_correct else 'XOR 아직 미완성 (더 학습 필요)'}\")\n",
    "print()\n",
    "print(\"[요약] 이 XOR 구현에서 사용한 개념들:\")\n",
    "print(\"  - tf.Variable: 학습 가능한 파라미터 W1, b1, W2, b2\")\n",
    "print(\"  - tf.matmul:   행렬 곱으로 레이어 계산\")\n",
    "print(\"  - GradientTape: 자동 미분으로 역전파 계산\")\n",
    "print(\"  - Adam optimizer: 그래디언트 기반 파라미터 업데이트\")\n",
    "print(\"  - 브로드캐스팅: + b 편향 덧셈에 자동 적용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 전체 요약 및 핵심 공식 정리\n",
    "\n",
    "### Q1~Q5 핵심 정리\n",
    "\n",
    "| 문제 | 핵심 개념 | 결론 |\n",
    "|------|-----------|------|\n",
    "| **Q1** | 행렬 곱 shape | $(m, k) \\times (k, n) \\to (m, n)$ — 내부 차원 제거 |\n",
    "| **Q2** | 브로드캐스팅 | 크기 1인 차원만 확장 가능. 1도 아니고 다르면 오류 |\n",
    "| **Q3** | Transpose perm | `perm[i]` = 새 i번 축이 기존 몇 번 축인지 |\n",
    "| **Q4** | GradientTape | `tape.gradient(y, x)` = $\\frac{\\partial y}{\\partial x}$ |\n",
    "| **Q5** | Variable vs Constant | 학습 파라미터는 반드시 `tf.Variable`로 선언 |\n",
    "\n",
    "### 자주 쓰는 공식 모음\n",
    "\n",
    "$$\\text{행렬 곱: } C_{ij} = \\sum_k A_{ik}B_{kj}$$\n",
    "\n",
    "$$\\text{편미분: } \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(\\ldots, x_i + \\epsilon, \\ldots) - f(\\ldots, x_i, \\ldots)}{\\epsilon}$$\n",
    "\n",
    "$$\\text{연쇄 법칙: } \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$$\n",
    "\n",
    "$$\\text{경사하강법: } w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}$$\n",
    "\n",
    "$$\\text{MSE: } L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "$$\\text{BCE: } L = -\\frac{1}{n}\\sum_i \\left[ y_i \\log \\hat{y}_i + (1-y_i)\\log(1-\\hat{y}_i) \\right]$$\n",
    "\n",
    "---\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "Chapter 01의 기초를 완료했습니다. 다음 챕터들에서는:\n",
    "- **Chapter 02:** Keras API로 신경망 구조화\n",
    "- **Chapter 03:** 활성화 함수, 손실 함수, 옵티마이저 심화\n",
    "- **Chapter 04:** CNN, RNN 등 특화 레이어\n",
    "\n",
    "수고하셨습니다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
