{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03: ìë™ ë¯¸ë¶„ (Automatic Differentiation)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ê²½ì‚¬í•˜ê°•ë²•ì„ ìœ„í•œ ë¯¸ë¶„ì˜ í•„ìš”ì„±ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•œë‹¤\n",
    "- `tf.GradientTape`ì˜ ë™ì‘ ì›ë¦¬ì™€ ì‚¬ìš©ë²•ì„ ìµíŒë‹¤\n",
    "- ê³ ì°¨ ë¯¸ë¶„(Hessian)ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤\n",
    "- GradientTapeìœ¼ë¡œ Keras ì—†ì´ ì„ í˜• íšŒê·€ë¥¼ ìˆ˜ë™ êµ¬í˜„í•œë‹¤\n",
    "- `@tf.function`ì˜ ì—­í• ê³¼ ì‚¬ìš© ì‹œì ì„ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: í¸ë¯¸ë¶„ê³¼ ì—°ì‡„ ë²•ì¹™](#ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ì™œ ë¯¸ë¶„ì´ í•„ìš”í•œê°€?](#ì™œ-ë¯¸ë¶„ì¸ê°€)\n",
    "3. [tf.GradientTape ê¸°ë³¸ ì‚¬ìš©](#gradienttape-ê¸°ì´ˆ)\n",
    "4. [ê³ ì°¨ ë¯¸ë¶„](#ê³ ì°¨-ë¯¸ë¶„)\n",
    "5. [ì„ í˜• íšŒê·€ ìˆ˜ë™ êµ¬í˜„](#ì„ í˜•-íšŒê·€)\n",
    "6. [@tf.function ì†Œê°œ](#tf-function)\n",
    "7. [ìš”ì•½](#ìš”ì•½)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜í•™ì  ê¸°ì´ˆ <a name='ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### 1. í¸ë¯¸ë¶„ (Partial Derivative)\n",
    "\n",
    "ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f(x_1, x_2, \\ldots, x_n)$ì—ì„œ íŠ¹ì • ë³€ìˆ˜ $x_i$ì— ëŒ€í•œ í¸ë¯¸ë¶„ì€  \n",
    "ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë¥¼ **ìƒìˆ˜ë¡œ ê³ ì •**í–ˆì„ ë•Œ $x_i$ê°€ ë³€í™”í•  ë•Œ $f$ì˜ ë³€í™”ìœ¨ì…ë‹ˆë‹¤:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} = \\lim_{\\Delta x_i \\to 0} \\frac{f(x_1, \\ldots, x_i + \\Delta x_i, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{\\Delta x_i}$$\n",
    "\n",
    "**ê·¸ë˜ë””ì–¸íŠ¸(Gradient):** ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ ë²¡í„°\n",
    "\n",
    "$$\\nabla f = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ì—°ì‡„ ë²•ì¹™ (Chain Rule)\n",
    "\n",
    "ë³µí•© í•¨ìˆ˜ $L = L(y)$, $y = y(w)$ê°€ ìˆì„ ë•Œ, ì—°ì‡„ ë²•ì¹™:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$$\n",
    "\n",
    "ì‹ ê²½ë§ì—ì„œëŠ” ì´ ê·œì¹™ì´ ì¸µì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ë°˜ë³µì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤ (ì—­ì „íŒŒ):\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_n} \\cdot \\frac{\\partial a_n}{\\partial a_{n-1}} \\cdots \\frac{\\partial a_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial w_1}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)\n",
    "\n",
    "íŒŒë¼ë¯¸í„° $w$ë¥¼ ì†ì‹¤ $L$ì´ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°˜ë³µ ì—…ë°ì´íŠ¸:\n",
    "\n",
    "$$w_{t+1} = w_t - \\eta \\cdot \\frac{\\partial L}{\\partial w_t}$$\n",
    "\n",
    "ì—¬ê¸°ì„œ $\\eta$ëŠ” í•™ìŠµë¥ (learning rate)ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ë¯¸ë¶„Â·ê²½ì‚¬í•˜ê°•ë²• ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ“ ë¯¸ë¶„(Derivative)ì´ ë­ì˜ˆìš”?\n",
    "\n",
    "ë¯¸ë¶„ì€ **'ì§€ê¸ˆ ì´ ìˆœê°„ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ë³€í•˜ê³  ìˆëŠ”ì§€'** ì•Œë ¤ì¤˜ìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìë™ì°¨ ì†ë„ê³„ì²˜ëŸ¼ìš”!\n",
    "> - ì†ë„ê³„ = ê±°ë¦¬ì˜ ë¯¸ë¶„ (ê±°ë¦¬ê°€ ì‹œê°„ì— ë”°ë¼ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ë³€í•˜ëŠ”ì§€)\n",
    "> - ë”¥ëŸ¬ë‹ì—ì„œ ë¯¸ë¶„ = ì†ì‹¤ì´ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€\n",
    "\n",
    "#### ğŸ”ï¸ ê²½ì‚¬í•˜ê°•ë²•ì´ ë­ì˜ˆìš”?\n",
    "\n",
    "ì†ì‹¤ í•¨ìˆ˜(Loss)ë¥¼ **ì‚°(mountain)**ì´ë¼ê³  ìƒê°í•´ë³´ì„¸ìš”!\n",
    "ìš°ë¦¬ì˜ ëª©í‘œëŠ” **ê°€ì¥ ë‚®ì€ ê³¨ì§œê¸°**(ìµœì†Ÿê°’)ë¥¼ ì°¾ëŠ” ê²ƒì´ì—ìš”.\n",
    "\n",
    "```\n",
    "    ì†ì‹¤\n",
    "     |\n",
    "     |   â†˜ ë‚´ë¦¬ë§‰ê¸¸ (ê¸°ìš¸ê¸°)\n",
    "     |        â†˜\n",
    "     |              â˜… ìµœì†Ÿê°’ ëª©í‘œ!\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒë¼ë¯¸í„°(w)\n",
    "```\n",
    "\n",
    "**ê²½ì‚¬í•˜ê°•ë²•ì˜ ì›ë¦¬:**\n",
    "1. í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê²½ì‚¬(ê¸°ìš¸ê¸°, Gradient)ë¥¼ ì¸¡ì •\n",
    "2. ê²½ì‚¬ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ í•œ ê±¸ìŒ ì´ë™\n",
    "3. 1~2 ë°˜ë³µ â†’ ì ì  ë‚®ì€ ê³³ìœ¼ë¡œ!\n",
    "\n",
    "$$w_{ìƒˆ} = w_{í˜„ì¬} - \\eta \\times \\text{ê¸°ìš¸ê¸°}$$\n",
    "\n",
    "- $\\eta$ (ì—íƒ€): **í•™ìŠµë¥ ** â€” í•œ ê±¸ìŒì˜ í¬ê¸° (ë„ˆë¬´ í¬ë©´ ì™”ë‹¤ê°”ë‹¤, ë„ˆë¬´ ì‘ìœ¼ë©´ ë„ˆë¬´ ëŠë¦¼!)\n",
    "\n",
    "#### ğŸ¬ GradientTape â€” ì—°ì‚° 'ë…¹í™”' í›„ 'ì¬ìƒ'\n",
    "\n",
    "GradientTapeì€ ì´ë¦„ ê·¸ëŒ€ë¡œ **'ê¸°ìš¸ê¸° ì¸¡ì • í…Œì´í”„'**ì˜ˆìš”!\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:  # ğŸ¬ ë…¹í™” ì‹œì‘\n",
    "    y = f(x)                     # ì—°ì‚° ê¸°ë¡ë¨\n",
    "grad = tape.gradient(y, x)       # âª ì—­ë°©í–¥ìœ¼ë¡œ ì¬ìƒ â†’ ê¸°ìš¸ê¸° ê³„ì‚°!\n",
    "```\n",
    "\n",
    "> ğŸ’¡ ë§ˆì¹˜ ë¸”ë™ë°•ìŠ¤ì²˜ëŸ¼ ì „ë°©í–¥ ì£¼í–‰ì„ ê¸°ë¡í•˜ê³ ,\n",
    "> ì—­ë°©í–¥ìœ¼ë¡œ ëŒë ¤ì„œ 'ì–´ë””ì„œ ë¬´ìŠ¨ ì¼ì´ ìˆì—ˆëŠ”ì§€' ë¶„ì„í•˜ëŠ” ê±°ì˜ˆìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ì„ ìœ„í•œ ë°±ì—”ë“œ ì„¤ì •\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì™œ ë¯¸ë¶„ì´ í•„ìš”í•œê°€? <a name='ì™œ-ë¯¸ë¶„ì¸ê°€'></a>\n",
    "\n",
    "ë”¥ëŸ¬ë‹ í•™ìŠµì˜ í•µì‹¬ì€ **ì†ì‹¤ í•¨ìˆ˜(Loss Function)ë¥¼ ìµœì†Œí™”**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  \n",
    "ì´ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜)ë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì›€ì§ì—¬ì•¼ í•˜ëŠ”ì§€ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.  \n",
    "ê·¸ ë°©í–¥ì„ ì•Œë ¤ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ **ê·¸ë˜ë””ì–¸íŠ¸(ê¸°ìš¸ê¸°)**ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ê´€ì  ì´í•´: 1D í•¨ìˆ˜ì—ì„œ ìµœì†Ÿê°’ ì°¾ê¸°\n",
    "# f(x) = x^2 - 4x + 5 â†’ ìµœì†Ÿê°’: x=2 ì—ì„œ f(2)=1\n",
    "\n",
    "def loss_fn(x):\n",
    "    return x**2 - 4*x + 5\n",
    "\n",
    "# x ë²”ìœ„ì—ì„œ í•¨ìˆ˜ê°’ ê³„ì‚°\n",
    "x_vals = tf.linspace(-1.0, 5.0, 100)\n",
    "y_vals = loss_fn(x_vals)\n",
    "\n",
    "print(\"f(x) = xÂ² - 4x + 5\")\n",
    "print(f\"f(-1) = {loss_fn(tf.constant(-1.0)).numpy():.2f}\")\n",
    "print(f\"f( 0) = {loss_fn(tf.constant(0.0)).numpy():.2f}\")\n",
    "print(f\"f( 2) = {loss_fn(tf.constant(2.0)).numpy():.2f}  â† ìµœì†Ÿê°’ (f'(2)=0)\")\n",
    "print(f\"f( 4) = {loss_fn(tf.constant(4.0)).numpy():.2f}\")\n",
    "\n",
    "# ë¯¸ë¶„: f'(x) = 2x - 4 â†’ f'(2) = 0 (ìµœì†Ÿê°’ ì¡°ê±´)\n",
    "print(\"\\në¯¸ë¶„ f'(x) = 2x - 4\")\n",
    "for x in [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]:\n",
    "    grad = 2*x - 4\n",
    "    direction = \"â† ì™¼ìª½ìœ¼ë¡œ ì´ë™ í•„ìš”\" if grad > 0 else (\"â†’ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì´ë™ í•„ìš”\" if grad < 0 else \"â˜… ìµœì†Ÿê°’!\")\n",
    "    print(f\"  f'({x:.0f}) = {grad:+.1f}  {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.GradientTape ê¸°ë³¸ ì‚¬ìš© <a name='gradienttape-ê¸°ì´ˆ'></a>\n",
    "\n",
    "`tf.GradientTape`ì€ **ìë™ ë¯¸ë¶„(Automatic Differentiation)** ì—”ì§„ì…ë‹ˆë‹¤.  \n",
    "í…Œì´í”„(Tape) ë¹„ìœ : ì—°ì‚°ì„ í…Œì´í”„ì— ë…¹í™”í•˜ë“¯ ê¸°ë¡í•œ ë’¤, ì—­ë°©í–¥ìœ¼ë¡œ ì¬ìƒí•˜ë©° ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "with tf.GradientTape() as tape:\n",
    "    y = f(x)   # ì—°ì‚° ê¸°ë¡\n",
    "dy_dx = tape.gradient(y, x)   # ì—­ë°©í–¥ìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ì‚¬ìš©: ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸\n",
    "# f(x) = x^2, f'(x) = 2x\n",
    "\n",
    "x = tf.Variable(3.0)   # Variableì´ì–´ì•¼ ìë™ìœ¼ë¡œ ì¶”ì ë¨\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # GradientTape ì»¨í…ìŠ¤íŠ¸ ë‚´ì˜ Variable ì—°ì‚°ì´ ê¸°ë¡ë¨\n",
    "    y = x ** 2\n",
    "\n",
    "# í…Œì´í”„ë¥¼ ì—­ë°©í–¥ìœ¼ë¡œ ì¬ìƒí•˜ì—¬ dy/dx ê³„ì‚°\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "print(f\"f(x) = xÂ²\")\n",
    "print(f\"x = {x.numpy()}\")\n",
    "print(f\"f(x) = {y.numpy()}\")\n",
    "print(f\"f'(x) = dy/dx = {dy_dx.numpy()}  (ìˆ˜í•™ì  ì •ë‹µ: 2Ã—3 = 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ í¸ë¯¸ë¶„\n",
    "# f(x, y) = x^2 + 2xy + y^3\n",
    "# âˆ‚f/âˆ‚x = 2x + 2y\n",
    "# âˆ‚f/âˆ‚y = 2x + 3y^2\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    f = x**2 + 2*x*y + y**3\n",
    "\n",
    "# ë‘ ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ì„ í•œ ë²ˆì— ê³„ì‚°\n",
    "grads = tape.gradient(f, [x, y])\n",
    "df_dx, df_dy = grads\n",
    "\n",
    "print(f\"f(x, y) = xÂ² + 2xy + yÂ³\")\n",
    "print(f\"x={x.numpy()}, y={y.numpy()}\")\n",
    "print(f\"f({x.numpy()}, {y.numpy()}) = {f.numpy():.1f}\")\n",
    "print(f\"\\ní¸ë¯¸ë¶„ ê²°ê³¼:\")\n",
    "print(f\"  âˆ‚f/âˆ‚x = 2x + 2y  = 2Ã—{x.numpy()}+2Ã—{y.numpy()} = {2*x+2*y:.1f}\")\n",
    "print(f\"  GradientTape ê²°ê³¼: {df_dx.numpy():.1f}\")\n",
    "print(f\"\\n  âˆ‚f/âˆ‚y = 2x + 3yÂ² = 2Ã—{x.numpy()}+3Ã—{y.numpy()}Â² = {2*x+3*y**2:.1f}\")\n",
    "print(f\"  GradientTape ê²°ê³¼: {df_dy.numpy():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constantëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì¶”ì ë˜ì§€ ì•ŠìŒ â†’ tape.watch()ë¡œ ëª…ì‹œì  ì¶”ì \n",
    "\n",
    "x_const = tf.constant(2.0)   # constant\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x_const)   # constantë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì \n",
    "    y = x_const ** 3      # f(x) = x^3, f'(x) = 3x^2\n",
    "\n",
    "dy_dx = tape.gradient(y, x_const)\n",
    "print(f\"f(x) = xÂ³, x = {x_const.numpy()}\")\n",
    "print(f\"f'(x) = 3xÂ² = 3Ã—{x_const.numpy()}Â² = {3*x_const.numpy()**2:.1f}\")\n",
    "print(f\"GradientTape ê²°ê³¼: {dy_dx.numpy():.1f}\")\n",
    "\n",
    "# persistent=True: ì—¬ëŸ¬ ë²ˆ gradient() í˜¸ì¶œ í—ˆìš©\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = x ** 2      # f1(x) = xÂ²\n",
    "    y2 = x ** 3      # f2(x) = xÂ³\n",
    "\n",
    "grad_y1 = tape.gradient(y1, x)   # 2x = 4\n",
    "grad_y2 = tape.gradient(y2, x)   # 3xÂ² = 12\n",
    "del tape   # persistent í…Œì´í”„ëŠ” ì‚¬ìš© í›„ ëª…ì‹œì ìœ¼ë¡œ ì‚­ì œ\n",
    "\n",
    "print(f\"\\npersistent=True ì˜ˆì‹œ:\")\n",
    "print(f\"  d(xÂ²)/dx at x=2: {grad_y1.numpy()}  (ì˜ˆìƒ: 4)\")\n",
    "print(f\"  d(xÂ³)/dx at x=2: {grad_y2.numpy()}  (ì˜ˆìƒ: 12)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê³ ì°¨ ë¯¸ë¶„ <a name='ê³ ì°¨-ë¯¸ë¶„'></a>\n",
    "\n",
    "GradientTapeì„ ì¤‘ì²©í•˜ë©´ **2ì°¨ ë¯¸ë¶„(Hessian)** ë“± ê³ ì°¨ ë¯¸ë¶„ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$f(x) = x^4 \\Rightarrow f'(x) = 4x^3 \\Rightarrow f''(x) = 12x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ì°¨ ë¯¸ë¶„ (ì´ì¤‘ GradientTape)\n",
    "# f(x) = x^4\n",
    "# f'(x) = 4xÂ³  â†’ x=2: 4Ã—8 = 32\n",
    "# f''(x) = 12xÂ² â†’ x=2: 12Ã—4 = 48\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape2:      # ì™¸ë¶€ í…Œì´í”„: 2ì°¨ ë¯¸ë¶„ìš©\n",
    "    with tf.GradientTape() as tape1:  # ë‚´ë¶€ í…Œì´í”„: 1ì°¨ ë¯¸ë¶„ìš©\n",
    "        y = x ** 4\n",
    "    dy_dx = tape1.gradient(y, x)      # 1ì°¨ ë¯¸ë¶„: 4xÂ³\n",
    "\n",
    "d2y_dx2 = tape2.gradient(dy_dx, x)   # 2ì°¨ ë¯¸ë¶„: 12xÂ²\n",
    "\n",
    "print(f\"f(x) = xâ´, x = {x.numpy()}\")\n",
    "print(f\"f(x)   = {y.numpy():.1f}        (= 2â´ = 16)\")\n",
    "print(f\"f'(x)  = {dy_dx.numpy():.1f}       (= 4Ã—2Â³ = 32)\")\n",
    "print(f\"f''(x) = {d2y_dx2.numpy():.1f}       (= 12Ã—2Â² = 48)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hessian í–‰ë ¬ ê³„ì‚°\n",
    "# f(x, y) = x^2 + xy + y^2\n",
    "# Hessian H = [[âˆ‚Â²f/âˆ‚xÂ², âˆ‚Â²f/âˆ‚xâˆ‚y], [âˆ‚Â²f/âˆ‚yâˆ‚x, âˆ‚Â²f/âˆ‚yÂ²]]\n",
    "#           = [[2, 1], [1, 2]]\n",
    "\n",
    "x = tf.Variable(1.0)\n",
    "y = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "    with tf.GradientTape() as t1:\n",
    "        f = x**2 + x*y + y**2\n",
    "    # 1ì°¨ í¸ë¯¸ë¶„\n",
    "    grads = t1.gradient(f, [x, y])   # [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y]\n",
    "\n",
    "# 2ì°¨ í¸ë¯¸ë¶„ (Hessian ëŒ€ê° ì›ì†Œ)\n",
    "# ì£¼ì˜: ë¹„ëŒ€ê° ì›ì†ŒëŠ” ë³„ë„ì˜ persistent tape í•„ìš”\n",
    "grad_x, grad_y = grads\n",
    "print(f\"f(x,y) = xÂ² + xy + yÂ² at x=1, y=1\")\n",
    "print(f\"f(1,1) = {f.numpy():.1f}\")\n",
    "print(f\"\\n1ì°¨ í¸ë¯¸ë¶„:\")\n",
    "print(f\"  âˆ‚f/âˆ‚x = 2x + y = {grad_x.numpy():.1f}  (ì˜ˆìƒ: 3)\")\n",
    "print(f\"  âˆ‚f/âˆ‚y = x + 2y = {grad_y.numpy():.1f}  (ì˜ˆìƒ: 3)\")\n",
    "\n",
    "# ì´ë¡ ì  Hessian\n",
    "print(f\"\\nì´ë¡ ì  Hessian í–‰ë ¬:\")\n",
    "print(f\"  H = [[âˆ‚Â²f/âˆ‚xÂ², âˆ‚Â²f/âˆ‚xâˆ‚y], [âˆ‚Â²f/âˆ‚yâˆ‚x, âˆ‚Â²f/âˆ‚yÂ²]]\")\n",
    "print(f\"    = [[2, 1], [1, 2]]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì„ í˜• íšŒê·€ ìˆ˜ë™ êµ¬í˜„ <a name='ì„ í˜•-íšŒê·€'></a>\n",
    "\n",
    "ì´ì œ GradientTapeì„ ì´ìš©í•´ Keras ì—†ì´ ì„ í˜• íšŒê·€ë¥¼ ì²˜ìŒë¶€í„° êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ëª¨ë¸:** $\\hat{y} = wx + b$\n",
    "\n",
    "**ì†ì‹¤ í•¨ìˆ˜ (MSE):** $L = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - wx_i - b)^2$\n",
    "\n",
    "**ê·¸ë˜ë””ì–¸íŠ¸:**\n",
    "$$\\frac{\\partial L}{\\partial w} = -\\frac{2}{n}\\sum_{i=1}^{n} x_i(y_i - \\hat{y}_i)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$\n",
    "\n",
    "**íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (Gradient Descent):**\n",
    "$$w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}, \\quad b \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ë‹¨ê³„: í•™ìŠµ ë°ì´í„° ìƒì„±\n",
    "# ì‹¤ì œ ê´€ê³„: y = 2x + 1 + ë…¸ì´ì¦ˆ\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 100  # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜\n",
    "\n",
    "# ì…ë ¥ x: ê· ë“±ë¶„í¬ì—ì„œ ìƒ˜í”Œë§\n",
    "X = tf.cast(tf.random.uniform([N, 1], -3, 3), tf.float32)\n",
    "\n",
    "# ëª©í‘œê°’ y: y = 2x + 1 + ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ\n",
    "TRUE_W = 2.0\n",
    "TRUE_B = 1.0\n",
    "noise = tf.random.normal([N, 1], stddev=0.5)\n",
    "Y = TRUE_W * X + TRUE_B + noise\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  X shape: {X.shape}, ë²”ìœ„: [{X.numpy().min():.2f}, {X.numpy().max():.2f}]\")\n",
    "print(f\"  Y shape: {Y.shape}, ë²”ìœ„: [{Y.numpy().min():.2f}, {Y.numpy().max():.2f}]\")\n",
    "print(f\"  ì‹¤ì œ íŒŒë¼ë¯¸í„°: w={TRUE_W}, b={TRUE_B}\")\n",
    "print(f\"  í•™ìŠµ ëª©í‘œ: ì´ ê°’ì— ê°€ê¹ê²Œ ìˆ˜ë ´í•˜ëŠ”ì§€ í™•ì¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ë‹¨ê³„: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
    "# tf.Variableë¡œ ì„ ì–¸í•´ì•¼ GradientTapeì´ ì¶”ì  ê°€ëŠ¥\n",
    "\n",
    "w = tf.Variable(tf.random.normal([1, 1]), name='weight')   # ë¬´ì‘ìœ„ ì´ˆê¸°í™”\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')                # 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "\n",
    "print(f\"íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”:\")\n",
    "print(f\"  w ì´ˆê¸°ê°’: {w.numpy().flatten()[0]:.4f}  (ëª©í‘œ: {TRUE_W})\")\n",
    "print(f\"  b ì´ˆê¸°ê°’: {b.numpy()[0]:.4f}  (ëª©í‘œ: {TRUE_B})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3ë‹¨ê³„: í•™ìŠµ ë£¨í”„ êµ¬í˜„\n",
    "# GradientTapeìœ¼ë¡œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° â†’ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ë°˜ë³µ\n",
    "\n",
    "learning_rate = 0.05\n",
    "num_epochs = 200\n",
    "\n",
    "loss_history = []\n",
    "w_history = []\n",
    "b_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # â”€â”€ ìˆœì „íŒŒ & ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    with tf.GradientTape() as tape:\n",
    "        # ìˆœì „íŒŒ: y_hat = w*x + b\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°: MSE = (1/n) * sum((y - y_hat)^2)\n",
    "        loss = tf.reduce_mean(tf.square(Y - y_pred))\n",
    "    \n",
    "    # â”€â”€ ì—­ì „íŒŒ: ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # dL/dw, dL/db ìë™ ê³„ì‚° (ì—°ì‡„ ë²•ì¹™ ìë™ ì ìš©)\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "    dL_dw, dL_db = gradients\n",
    "    \n",
    "    # â”€â”€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ê²½ì‚¬í•˜ê°•ë²•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # w â† w - Î· * dL/dw\n",
    "    # b â† b - Î· * dL/db\n",
    "    w.assign_sub(learning_rate * dL_dw)\n",
    "    b.assign_sub(learning_rate * dL_db)\n",
    "    \n",
    "    # ê¸°ë¡\n",
    "    loss_history.append(loss.numpy())\n",
    "    w_history.append(w.numpy().flatten()[0])\n",
    "    b_history.append(b.numpy()[0])\n",
    "    \n",
    "    # ì§„í–‰ ìƒí™© ì¶œë ¥ (ë§¤ 50 ì—í¬í¬)\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs}  | \"\n",
    "              f\"Loss: {loss.numpy():.6f} | \"\n",
    "              f\"w: {w.numpy().flatten()[0]:.4f} | \"\n",
    "              f\"b: {b.numpy()[0]:.4f}\")\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"  ìµœì¢… w: {w.numpy().flatten()[0]:.4f}  (ì‹¤ì œ: {TRUE_W})\")\n",
    "print(f\"  ìµœì¢… b: {b.numpy()[0]:.4f}  (ì‹¤ì œ: {TRUE_B})\")\n",
    "print(f\"  ìµœì¢… ì†ì‹¤: {loss_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. ì†ì‹¤ ê³¡ì„ \n",
    "axes[0].plot(loss_history, 'b-', linewidth=1.5)\n",
    "axes[0].set_xlabel('ì—í¬í¬', fontsize=11)\n",
    "axes[0].set_ylabel('MSE ì†ì‹¤', fontsize=11)\n",
    "axes[0].set_title('í•™ìŠµ ì†ì‹¤ ê³¡ì„ ', fontsize=12)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ìˆ˜ë ´ í™•ì¸\n",
    "\n",
    "# 2. ê°€ì¤‘ì¹˜ w ìˆ˜ë ´\n",
    "axes[1].plot(w_history, 'r-', linewidth=1.5, label='í•™ìŠµëœ w')\n",
    "axes[1].axhline(y=TRUE_W, color='g', linestyle='--', label=f'ì‹¤ì œ w={TRUE_W}')\n",
    "axes[1].set_xlabel('ì—í¬í¬', fontsize=11)\n",
    "axes[1].set_ylabel('w ê°’', fontsize=11)\n",
    "axes[1].set_title('ê°€ì¤‘ì¹˜ w ìˆ˜ë ´', fontsize=12)\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ë°ì´í„°ì™€ í•™ìŠµëœ ì§ì„ \n",
    "x_line = np.linspace(-3, 3, 100)\n",
    "y_line = w.numpy().flatten()[0] * x_line + b.numpy()[0]\n",
    "y_true_line = TRUE_W * x_line + TRUE_B\n",
    "\n",
    "axes[2].scatter(X.numpy(), Y.numpy(), alpha=0.4, s=15, label='ë°ì´í„°', color='steelblue')\n",
    "axes[2].plot(x_line, y_line, 'r-', linewidth=2, label=f'í•™ìŠµ ê²°ê³¼: y={w.numpy().flatten()[0]:.2f}x+{b.numpy()[0]:.2f}')\n",
    "axes[2].plot(x_line, y_true_line, 'g--', linewidth=1.5, label=f'ì‹¤ì œ: y={TRUE_W}x+{TRUE_B}')\n",
    "axes[2].set_xlabel('x', fontsize=11)\n",
    "axes[2].set_ylabel('y', fontsize=11)\n",
    "axes[2].set_title('ì„ í˜• íšŒê·€ ê²°ê³¼', fontsize=12)\n",
    "axes[2].legend(fontsize=8)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/Users/alex/AI_TestPlayground/chapter01_basics/linear_regression_result.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter01_basics/linear_regression_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¹„êµ: Keras optimizer ì‚¬ìš© ë²„ì „\n",
    "# ìœ„ì˜ ìˆ˜ë™ ì—…ë°ì´íŠ¸ë¥¼ Keras optimizerë¡œ ë‹¨ìˆœí™”\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì¬ì´ˆê¸°í™”\n",
    "tf.random.set_seed(42)\n",
    "w2 = tf.Variable(tf.random.normal([1, 1]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# Adam optimizer ì‚¬ìš© (í˜„ëŒ€ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "for epoch in range(200):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, w2) + b2\n",
    "        loss2 = tf.reduce_mean(tf.square(Y - y_pred))\n",
    "    \n",
    "    grads2 = tape.gradient(loss2, [w2, b2])\n",
    "    \n",
    "    # optimizer.apply_gradients: ìˆ˜ë™ assign_sub ëŒ€ì‹  optimizerê°€ ì²˜ë¦¬\n",
    "    optimizer.apply_gradients(zip(grads2, [w2, b2]))\n",
    "\n",
    "print(f\"Keras Adam optimizer ê²°ê³¼:\")\n",
    "print(f\"  w: {w2.numpy().flatten()[0]:.4f}  (ì‹¤ì œ: {TRUE_W})\")\n",
    "print(f\"  b: {b2.numpy()[0]:.4f}  (ì‹¤ì œ: {TRUE_B})\")\n",
    "print(f\"  ìµœì¢… ì†ì‹¤: {loss2.numpy():.6f}\")\n",
    "print(f\"\\nìˆ˜ë™ êµ¬í˜„ê³¼ ê²°ê³¼ ë¹„êµ:\")\n",
    "print(f\"  ìˆ˜ë™ w: {w.numpy().flatten()[0]:.4f} vs Adam w: {w2.numpy().flatten()[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @tf.function ì†Œê°œ <a name='tf-function'></a>\n",
    "\n",
    "`@tf.function`ì€ Python í•¨ìˆ˜ë¥¼ **TensorFlow ê³„ì‚° ê·¸ë˜í”„ë¡œ ì»´íŒŒì¼**í•˜ëŠ” ë°ì½”ë ˆì´í„°ì…ë‹ˆë‹¤.\n",
    "\n",
    "- **Eager Execution:** Python ì½”ë“œê°€ ì¤„ ë‹¨ìœ„ë¡œ ì¦‰ì‹œ ì‹¤í–‰ â†’ ë””ë²„ê¹… í¸ë¦¬\n",
    "- **@tf.function:** í•¨ìˆ˜ ì „ì²´ë¥¼ ê·¸ë˜í”„ë¡œ ì»´íŒŒì¼ â†’ ì‹¤í–‰ ì†ë„ ìµœì í™”\n",
    "\n",
    "**íŠ¸ë ˆì´ì‹±(Tracing):** ì²« í˜¸ì¶œ ì‹œ í•¨ìˆ˜ë¥¼ ê·¸ë˜í”„ë¡œ ë³€í™˜, ì´í›„ í˜¸ì¶œì—ì„œ ê·¸ë˜í”„ ì¬ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function ê¸°ë³¸ ì‚¬ìš©\n",
    "\n",
    "# Eager ë²„ì „\n",
    "def train_step_eager(X, Y, w, b, lr=0.01):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        loss = tf.reduce_mean(tf.square(Y - y_pred))\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "    w.assign_sub(lr * grads[0])\n",
    "    b.assign_sub(lr * grads[1])\n",
    "    return loss\n",
    "\n",
    "# @tf.function ë²„ì „ â€” ë™ì¼í•œ ì½”ë“œì— ë°ì½”ë ˆì´í„°ë§Œ ì¶”ê°€\n",
    "@tf.function\n",
    "def train_step_graph(X, Y, w, b, lr=0.01):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = tf.matmul(X, w) + b\n",
    "        loss = tf.reduce_mean(tf.square(Y - y_pred))\n",
    "    grads = tape.gradient(loss, [w, b])\n",
    "    w.assign_sub(lr * grads[0])\n",
    "    b.assign_sub(lr * grads[1])\n",
    "    return loss\n",
    "\n",
    "# ì„±ëŠ¥ ë¹„êµ\n",
    "import time\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "w_e = tf.Variable(tf.random.normal([1, 1]))\n",
    "b_e = tf.Variable(tf.zeros([1]))\n",
    "w_g = tf.Variable(w_e.numpy())  # ë™ì¼í•œ ì´ˆê¸°ê°’ìœ¼ë¡œ ì‹œì‘\n",
    "b_g = tf.Variable(b_e.numpy())\n",
    "\n",
    "# ì›Œë°ì—… (JIT ì»´íŒŒì¼)\n",
    "_ = train_step_eager(X, Y, w_e, b_e)\n",
    "_ = train_step_graph(X, Y, w_g, b_g)\n",
    "\n",
    "N = 500\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    train_step_eager(X, Y, w_e, b_e)\n",
    "t_eager = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "for _ in range(N):\n",
    "    train_step_graph(X, Y, w_g, b_g)\n",
    "t_graph = time.time() - t0\n",
    "\n",
    "print(f\"í•™ìŠµ ìŠ¤í… {N}íšŒ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(f\"  Eager (ì¼ë°˜ í•¨ìˆ˜):    {t_eager:.4f}ì´ˆ\")\n",
    "print(f\"  @tf.function (ê·¸ë˜í”„): {t_graph:.4f}ì´ˆ\")\n",
    "speedup = t_eager / t_graph\n",
    "print(f\"  ì†ë„ ë¹„ìœ¨: {speedup:.2f}x {'(ê·¸ë˜í”„ê°€ ë¹ ë¦„)' if speedup > 1 else '(ìœ ì‚¬)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function ì£¼ì˜ì‚¬í•­: Python ì‚¬ì´ë“œ ì´í™íŠ¸\n",
    "\n",
    "# íŠ¸ë ˆì´ì‹±(tracing) íšŸìˆ˜ í™•ì¸\n",
    "trace_count = 0\n",
    "\n",
    "@tf.function\n",
    "def traced_fn(x):\n",
    "    global trace_count\n",
    "    trace_count += 1  # íŠ¸ë ˆì´ì‹± ì‹œì—ë§Œ ì‹¤í–‰ë¨ (ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì—ëŠ” ë¬´ì‹œ)\n",
    "    print(f\"Python ì½”ë“œ ì‹¤í–‰ (íŠ¸ë ˆì´ì‹±): trace_count={trace_count}\")\n",
    "    return x * 2\n",
    "\n",
    "# ë™ì¼ íƒ€ì…/shape: ì¬ì‚¬ìš©\n",
    "print(\"ì²« ë²ˆì§¸ í˜¸ì¶œ (float32):\")\n",
    "r1 = traced_fn(tf.constant(1.0))\n",
    "print(\"ë‘ ë²ˆì§¸ í˜¸ì¶œ (ë™ì¼ íƒ€ì… â€” ì¬ì¶”ì  ì—†ìŒ):\")\n",
    "r2 = traced_fn(tf.constant(2.0))\n",
    "print(\"ì„¸ ë²ˆì§¸ í˜¸ì¶œ (int32 â€” ìƒˆë¡œìš´ íƒ€ì…ì´ë¯€ë¡œ ì¬ì¶”ì ):\")\n",
    "r3 = traced_fn(tf.constant(3))\n",
    "\n",
    "print(f\"\\nê²°ë¡ : trace_count={trace_count} (íƒ€ì…ì´ ë°”ë€” ë•Œë§Œ ì¬ì¶”ì )\")\n",
    "print(\"tf.printë¥¼ ì‚¬ìš©í•˜ë©´ ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ì—ë„ ì¶œë ¥ ê°€ëŠ¥:\")\n",
    "\n",
    "@tf.function\n",
    "def fn_with_tfprint(x):\n",
    "    tf.print(\"ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ x =\", x)  # ê·¸ë˜í”„ ì‹¤í–‰ ì‹œì—ë„ ì‹¤í–‰ë¨\n",
    "    return x ** 2\n",
    "\n",
    "fn_with_tfprint(tf.constant(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìš”ì•½ <a name='ìš”ì•½'></a>\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹ ì •ë¦¬\n",
    "\n",
    "| ê°œë… | ìˆ˜ì‹ | ì˜ë¯¸ |\n",
    "|------|------|------|\n",
    "| í¸ë¯¸ë¶„ | $\\frac{\\partial f}{\\partial x}$ | ë‹¤ë¥¸ ë³€ìˆ˜ ê³ ì • í›„ $x$ ë³€í™”ì— ëŒ€í•œ $f$ì˜ ë³€í™”ìœ¨ |\n",
    "| ê·¸ë˜ë””ì–¸íŠ¸ | $\\nabla f = (\\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n})$ | í•¨ìˆ˜ì˜ ìµœëŒ€ ì¦ê°€ ë°©í–¥ |\n",
    "| ì—°ì‡„ ë²•ì¹™ | $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$ | ì—­ì „íŒŒì˜ ìˆ˜í•™ì  ê·¼ê±° |\n",
    "| ê²½ì‚¬í•˜ê°•ë²• | $w \\leftarrow w - \\eta \\nabla_w L$ | ì†ì‹¤ ê°ì†Œ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ |\n",
    "| MSE ì†ì‹¤ | $L = \\frac{1}{n}\\sum_i(y_i - \\hat{y}_i)^2$ | ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ì˜ í‰ê·  ì œê³± ì˜¤ì°¨ |\n",
    "\n",
    "### GradientTape í•µì‹¬ íŒ¨í„´\n",
    "\n",
    "```python\n",
    "# ê¸°ë³¸ íŒ¨í„´\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = forward_pass(x, w)\n",
    "grads = tape.gradient(loss, w)\n",
    "w.assign_sub(learning_rate * grads)\n",
    "```\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„°: ì‹¤ìŠµ í€´ì¦ˆ\n",
    "\n",
    "`practice/ex01_tensor_quiz.ipynb`ì—ì„œ ë‹¤ìŒì„ ë³µìŠµí•©ë‹ˆë‹¤:\n",
    "- í…ì„œ shape ê³„ì‚° ë¬¸ì œ\n",
    "- ë¸Œë¡œë“œìºìŠ¤íŒ… ê²°ê³¼ ì˜ˆì¸¡\n",
    "- transpose ê²°ê³¼ ê³„ì‚°\n",
    "- GradientTapeìœ¼ë¡œ ì„ì˜ í•¨ìˆ˜ ë¯¸ë¶„\n",
    "- `tf.Variable` vs `tf.constant` ì‹¤ìŠµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}