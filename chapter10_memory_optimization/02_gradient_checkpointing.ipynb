{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10-02: Gradient Checkpointing (ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Activation Memoryê°€ Sequence Lengthì— ë”°ë¼ $O(S)$ë¡œ ì¦ê°€í•˜ëŠ” ì´ìœ ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ íŒŒì•…í•œë‹¤\n",
    "- Gradient Checkpointingì˜ í•µì‹¬ ì•„ì´ë””ì–´: **ì¼ë¶€ Forward ê²°ê³¼ë¥¼ ë²„ë¦¬ê³  ì—­ì „íŒŒ ì‹œ ì¬ê³„ì‚°**í•˜ëŠ” ì›ë¦¬ë¥¼ ì¦ëª…í•œë‹¤\n",
    "- **ë©”ëª¨ë¦¬ $O(\\sqrt{L})$ vs ì—°ì‚°ëŸ‰ +33%** ì˜ Trade-offë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•œë‹¤\n",
    "- TensorFlow `tf.recompute_grad`ë¡œ ì‹¤ì œ Gradient Checkpointingì„ ì ìš©í•˜ê³  ë©”ëª¨ë¦¬ ì ˆì•½ì„ ì‹¤ì¸¡í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Activation Memoryì™€ ì¬ê³„ì‚° ë¹„ìš©](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ì™œ Activationì´ ë§ì•„ì§€ëŠ”ê°€?](#2.-Activation-ë¬¸ì œ)\n",
    "3. [Gradient Checkpointing ì›ë¦¬ ì‹œê°í™”](#3.-GC-ì›ë¦¬)\n",
    "4. [ìµœì  ì²´í¬í¬ì¸íŠ¸ ê°„ê²© ë„ì¶œ](#4.-ìµœì -ê°„ê²©)\n",
    "5. [TF ì‹¤ì „ êµ¬í˜„ ë° ë©”ëª¨ë¦¬ ì¸¡ì •](#5.-TF-êµ¬í˜„)\n",
    "6. [Trade-off ì¢…í•© ì‹œê°í™”](#6.-Trade-off-ì‹œê°í™”)\n",
    "7. [ì •ë¦¬ ë° ì—°ìŠµ ë¬¸ì œ](#7.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### Activation Memory ê³µì‹ (Transformer L-ë ˆì´ì–´ ê¸°ì¤€)\n",
    "\n",
    "í‘œì¤€ í•™ìŠµì—ì„œ ì—­ì „íŒŒë¥¼ ìœ„í•´ **ëª¨ë“  ë ˆì´ì–´ì˜ Forward ê²°ê³¼**ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$M_{act}^{\\text{standard}} = L \\times M_{act,1}$$\n",
    "\n",
    "### Gradient Checkpointingì˜ í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "ë ˆì´ì–´ë¥¼ $k$ê°œ ê°„ê²©ìœ¼ë¡œ **ì²´í¬í¬ì¸íŠ¸** ë°°ì¹˜:\n",
    "\n",
    "$$M_{act}^{\\text{GC}} = \\underbrace{\\frac{L}{k}}_{\\text{ì²´í¬í¬ì¸íŠ¸ ìˆ˜}} \\times M_{act,1} + \\underbrace{k}_{\\text{ì¬ê³„ì‚° êµ¬ê°„}} \\times M_{act,1} = \\left(\\frac{L}{k} + k\\right) M_{act,1}$$\n",
    "\n",
    "ì´ë¥¼ $k$ì— ëŒ€í•´ ìµœì†Œí™”:\n",
    "\n",
    "$$\\frac{d}{dk}\\left(\\frac{L}{k} + k\\right) = -\\frac{L}{k^2} + 1 = 0 \\implies k^* = \\sqrt{L}$$\n",
    "\n",
    "$$\\boxed{M_{act}^{\\text{GC, opt}} = 2\\sqrt{L} \\cdot M_{act,1} \\quad (O(\\sqrt{L}) \\text{ ë‹¬ì„±!})}$$\n",
    "\n",
    "### ì¬ê³„ì‚° ë¹„ìš©\n",
    "\n",
    "ê° Segmentë¥¼ ì—­ì „íŒŒ ì‹œ í•œ ë²ˆ ë” Forward ê³„ì‚°:\n",
    "\n",
    "$$\\text{ì¶”ê°€ FLOPs} = \\frac{L}{k^*} \\times k^* \\times F_1 = L \\times F_1$$\n",
    "\n",
    "ê¸°ì¡´ Forward ë¹„ìš©ì´ $L \\times F_1$ì´ë¯€ë¡œ, **ì´ Forward íšŸìˆ˜ = 2ë°°** â†’ ì—°ì‚°ëŸ‰ **+33%** (ì „ì²´ì˜ 1/3)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ—’ï¸ Gradient Checkpointingì´ ë­”ê°€ìš”?\n",
    "\n",
    "> ì‹œí—˜ ë¬¸ì œ 100ë¬¸ì œë¥¼ í’€ë©´ì„œ **ëª¨ë“  ì¤‘ê°„ ê³„ì‚° ê³¼ì •ì„ ì—°ìŠµì¥ì— ê¸°ë¡**í•˜ë©´ ì¢…ì´ê°€ ë„ˆë¬´ ë§ì´ í•„ìš”í•´ìš”.  \n",
    ">\n",
    "> í•˜ë‚˜ì˜ í•´ê²°ì±…:  \n",
    "> - **10ë¬¸ì œë§ˆë‹¤** ê·¸ ì‹œì ì˜ í’€ì´ ìƒíƒœë§Œ ì €ì¥  \n",
    "> - ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¼\n",
    "> - íŠ¹ì • êµ¬ê°„ì„ ë‹¤ì‹œ í•„ìš”í•  ë•Œ? **ê·¸ êµ¬ê°„ë§Œ ë‹¤ì‹œ í’€ê¸°!**\n",
    ">\n",
    "> ì´ë ‡ê²Œ í•˜ë©´ **ì—°ìŠµì¥(ë©”ëª¨ë¦¬)ì€ 10ì¥**ë§Œ í•„ìš”í•˜ì§€ë§Œ, ë‹¤ì‹œ í‘¸ëŠ” ì‹œê°„(ì—°ì‚°)ì´ ì¡°ê¸ˆ ë” ê±¸ë ¤ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: ìµœì  ì²´í¬í¬ì¸íŠ¸ ê°„ê²©\n",
    "\n",
    "96-ë ˆì´ì–´ GPT-3 ëª¨ë¸ì— Gradient Checkpointing ì ìš© ì‹œ ìµœì  ê°„ê²© $k^*$ëŠ”?  \n",
    "í‘œì¤€ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì ˆì•½ë¥ ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$L = 96$\n",
    "\n",
    "$$k^* = \\sqrt{96} \\approx 9.8 \\Rightarrow k = 10$$\n",
    "\n",
    "$$M_{GC} = \\left(\\frac{96}{10} + 10\\right)M_1 = 19.6 M_1 \\quad \\text{vs} \\quad M_{std} = 96 M_1$$\n",
    "\n",
    "ì ˆì•½ë¥  = $1 - \\frac{19.6}{96} \\approx \\mathbf{79.6\\%}$\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: ì—°ì‚° ì˜¤ë²„í—¤ë“œ\n",
    "\n",
    "í‘œì¤€ í•™ìŠµ(Forward 1íšŒ + Backward 1íšŒ)ê³¼ Gradient Checkpointing(Forward 2íšŒ + Backward 1íšŒ) ì ìš© ì‹œ  \n",
    "ì´ ì—°ì‚°ëŸ‰ì€ ê°ê° ëª‡ ë°°ì¸ê°€? (Forward 1íšŒ = 1 FLOPs ë‹¨ìœ„)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "Backward ì—°ì‚°ëŸ‰ â‰ˆ Forwardì˜ 2ë°° (Chain Ruleì˜ í¸ë¯¸ë¶„ ê³„ì‚°)\n",
    "\n",
    "- **í‘œì¤€**: Forward(1) + Backward(â‰ˆ2) = **3 FLOPs**\n",
    "- **GC**: Forward(1) + ì¬ê³„ì‚° Forward(1) + Backward(â‰ˆ2) = **4 FLOPs** (+33%)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation ë¬¸ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Activation Memory ì¦ê°€ ë¬¸ì œ ì •ëŸ‰í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def act_mem_standard(L, M1=1.0):\n",
    "    \"\"\"í‘œì¤€ í•™ìŠµ: ëª¨ë“  ë ˆì´ì–´ Activation ë³´ì¡´ â†’ O(L)\"\"\"\n",
    "    return L * M1\n",
    "\n",
    "def act_mem_gc(L, k, M1=1.0):\n",
    "    \"\"\"Gradient Checkpointing: k ê°„ê²© ì²´í¬í¬ì¸íŠ¸ â†’ O(L/k + k)\"\"\"\n",
    "    return (L / k + k) * M1\n",
    "\n",
    "def optimal_k(L):\n",
    "    \"\"\"ìµœì  ì²´í¬í¬ì¸íŠ¸ ê°„ê²©: k* = sqrt(L)\"\"\"\n",
    "    return int(np.round(np.sqrt(L)))\n",
    "\n",
    "# ë ˆì´ì–´ ìˆ˜ë³„ ë©”ëª¨ë¦¬ ë¹„êµ\n",
    "layer_counts = [12, 24, 32, 48, 80, 96, 128]\n",
    "\n",
    "print(f\"{'L':>5} | {'k*':>4} | {'í‘œì¤€ M(Lë‹¨ìœ„)':>13} | {'GC ìµœì  M':>10} | {'ì ˆì•½ë¥ ':>7} | {'ì—°ì‚° ì˜¤ë²„í—¤ë“œ'}\")  \n",
    "print(\"-\" * 65)\n",
    "for L in layer_counts:\n",
    "    k = optimal_k(L)\n",
    "    m_std = act_mem_standard(L)\n",
    "    m_gc  = act_mem_gc(L, k)\n",
    "    save_pct = (1 - m_gc / m_std) * 100\n",
    "    overhead = 1 / 3 * 100  # í•­ìƒ +33%\n",
    "    print(f\"{L:>5} | {k:>4} | {m_std:>13.0f} | {m_gc:>10.1f} | {save_pct:>6.1f}% | +{overhead:.0f}% FLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GC ì›ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Gradient Checkpointing ë™ì‘ ì›ë¦¬ ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def draw_gc_diagram(ax, L, k, title):\n",
    "    \"\"\"Gradient Checkpointing ë ˆì´ì–´ ë‹¤ì´ì–´ê·¸ë¨\"\"\"\n",
    "    ax.set_xlim(-0.5, L + 0.5)\n",
    "    ax.set_ylim(-1.5, 3.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
    "\n",
    "    for i in range(L):\n",
    "        is_ckpt = (i % k == 0)\n",
    "        color = '#1E88E5' if is_ckpt else '#BDBDBD'\n",
    "        alpha = 0.9 if is_ckpt else 0.4\n",
    "        rect = plt.Rectangle((i, 1.8), 0.85, 0.8, color=color, alpha=alpha, ec='white')\n",
    "        ax.add_patch(rect)\n",
    "        if is_ckpt:\n",
    "            ax.text(i + 0.42, 2.55, f'C{i//k}', ha='center', fontsize=7, fontweight='bold', color='white')\n",
    "        if i < L - 1:\n",
    "            ax.annotate('', xy=(i+1, 2.2), xytext=(i+0.85, 2.2),\n",
    "                        arrowprops=dict(arrowstyle='->', color='gray', lw=1))\n",
    "\n",
    "    # ì €ì¥ëœ vs ë²„ë ¤ì§„ Activation ë²”ë¡€\n",
    "    ax.text(-0.3, 1.5, 'Forward:', fontsize=8, va='center')\n",
    "    ax.add_patch(plt.Rectangle((1.5, 1.35), 0.6, 0.3, color='#1E88E5', alpha=0.9))\n",
    "    ax.text(2.3, 1.5, '= ì²´í¬í¬ì¸íŠ¸ (ë³´ì¡´)', fontsize=7, va='center')\n",
    "    ax.add_patch(plt.Rectangle((6.5, 1.35), 0.6, 0.3, color='#BDBDBD', alpha=0.4))\n",
    "    ax.text(7.3, 1.5, '= ë²„ë¦¼ (ì—­ì „íŒŒì‹œ ì¬ê³„ì‚°)', fontsize=7, va='center')\n",
    "\n",
    "    # ì—­ì „íŒŒ ì¬ê³„ì‚° í‘œì‹œ\n",
    "    for seg in range(L // k):\n",
    "        seg_start = seg * k\n",
    "        seg_end = min(seg_start + k, L)\n",
    "        ax.annotate('', xy=(seg_start, 0.7), xytext=(seg_end - 0.15, 0.7),\n",
    "                    arrowprops=dict(arrowstyle='->', color='#E53935', lw=1.5))\n",
    "        ax.text((seg_start + seg_end) / 2, 0.3, f'ì¬ê³„ì‚°\\n({seg_end - seg_start}ë ˆì´ì–´)',\n",
    "                ha='center', fontsize=6.5, color='#E53935')\n",
    "\n",
    "    ax.text(-0.3, 0.7, 'Backward:', fontsize=8, va='center', color='#E53935')\n",
    "\n",
    "\n",
    "L = 12  # ì‹œê°í™”ìš© ë ˆì´ì–´ ìˆ˜\n",
    "k_naive = 1  # í‘œì¤€ (ëª¨ë‘ ë³´ì¡´)\n",
    "k_gc    = 4  # GC (4ë ˆì´ì–´ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 7))\n",
    "\n",
    "draw_gc_diagram(axes[0], L, k_naive,\n",
    "    f'í‘œì¤€ í•™ìŠµ (k=1): ëª¨ë“  Activation ë³´ì¡´ â†’ ë©”ëª¨ë¦¬ {act_mem_standard(L):.0f}Ã—Mâ‚')\n",
    "draw_gc_diagram(axes[1], L, k_gc,\n",
    "    f'Gradient Checkpointing (k={k_gc}): ìµœì â‰ˆâˆšL ê°„ê²© â†’ ë©”ëª¨ë¦¬ {act_mem_gc(L,k_gc):.0f}Ã—Mâ‚ ({(1-act_mem_gc(L,k_gc)/act_mem_standard(L))*100:.0f}% ì ˆì•½)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ìµœì  ê°„ê²©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì²´í¬í¬ì¸íŠ¸ ê°„ê²© kì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ê³¡ì„  (L=96 ê¸°ì¤€)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "L = 96\n",
    "k_range = np.arange(1, L + 1)\n",
    "mem_values = [(L / k + k) for k in k_range]\n",
    "\n",
    "k_opt = optimal_k(L)\n",
    "m_min = act_mem_gc(L, k_opt)\n",
    "m_std = act_mem_standard(L)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(k_range, mem_values, 'b-', lw=2.5, label='GC ë©”ëª¨ë¦¬ $L/k + k$')\n",
    "ax1.axhline(y=m_std, color='red', ls='--', lw=2, alpha=0.8, label=f'í‘œì¤€ í•™ìŠµ (k=1): {m_std}Ã—Mâ‚')\n",
    "ax1.axvline(x=k_opt, color='green', ls=':', lw=2, label=f'ìµœì  k*=âˆšL={k_opt}')\n",
    "ax1.scatter([k_opt], [m_min], s=120, zorder=5, color='green',\n",
    "            label=f'ìµœì†Ÿê°’: {m_min:.0f}Ã—Mâ‚ ({(1-m_min/m_std)*100:.0f}% ì ˆì•½)')\n",
    "ax1.fill_between(k_range, mem_values, m_std, where=[m < m_std for m in mem_values],\n",
    "                 alpha=0.1, color='blue', label='ì ˆì•½ êµ¬ê°„')\n",
    "ax1.set_xlabel('ì²´í¬í¬ì¸íŠ¸ ê°„ê²© k', fontsize=11)\n",
    "ax1.set_ylabel('Activation ë©”ëª¨ë¦¬ (Ã—Mâ‚)', fontsize=11)\n",
    "ax1.set_title(f'Gradient Checkpointing ë©”ëª¨ë¦¬ ê³¡ì„ \\n(L={L}, k*=âˆšL={k_opt}ì—ì„œ ìµœì†Ÿê°’)', fontweight='bold')\n",
    "ax1.legend(fontsize=9); ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 50)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: Lì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "L_range = np.arange(1, 200)\n",
    "m_std_arr = L_range\n",
    "m_gc_arr  = 2 * np.sqrt(L_range)\n",
    "\n",
    "ax2.plot(L_range, m_std_arr, 'r-', lw=2.5, label='í‘œì¤€ ($O(L)$)')\n",
    "ax2.plot(L_range, m_gc_arr, 'b-', lw=2.5, label='GC ìµœì  ($O(\\\\sqrt{L})$)')\n",
    "ax2.fill_between(L_range, m_gc_arr, m_std_arr, alpha=0.1, color='blue', label='GC ì ˆì•½ êµ¬ê°„')\n",
    "ax2.scatter([96], [96], s=80, color='red', zorder=5)\n",
    "ax2.scatter([96], [2*np.sqrt(96)], s=80, color='blue', zorder=5)\n",
    "ax2.annotate(f'GPT-3 L=96: {96}Ã—Mâ‚', xy=(96, 96), xytext=(110, 90),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'), fontsize=9, color='red')\n",
    "ax2.annotate(f'GPT-3 GC: {2*np.sqrt(96):.1f}Ã—Mâ‚', xy=(96, 2*np.sqrt(96)),\n",
    "             xytext=(110, 30), arrowprops=dict(arrowstyle='->', color='blue'),\n",
    "             fontsize=9, color='blue')\n",
    "ax2.set_xlabel('ë ˆì´ì–´ ìˆ˜ L', fontsize=11)\n",
    "ax2.set_ylabel('Activation ë©”ëª¨ë¦¬ (Ã—Mâ‚)', fontsize=11)\n",
    "ax2.set_title('ë ˆì´ì–´ ìˆ˜ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ë³µì¡ë„\\ní‘œì¤€ O(L) vs GC O(âˆšL)', fontweight='bold')\n",
    "ax2.legend(fontsize=9); ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"L=96 (GPT-3): í‘œì¤€ 96Ã—Mâ‚ â†’ GC {2*np.sqrt(96):.1f}Ã—Mâ‚ ({(1-2*np.sqrt(96)/96)*100:.0f}% ì ˆì•½)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# TensorFlow tf.recompute_gradë¡œ Gradient Checkpointing êµ¬í˜„\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# ì¬ê³„ì‚°ì´ ë°œìƒí•˜ëŠ”ì§€ ì¶”ì í•˜ëŠ” ì „ì—­ ì¹´ìš´í„°\n",
    "forward_call_count = {'standard': 0, 'gc': 0}\n",
    "\n",
    "class TrackedDenseLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"forward í˜¸ì¶œ íšŸìˆ˜ë¥¼ ì¶”ì í•˜ëŠ” Dense Layer\"\"\"\n",
    "    def __init__(self, units, mode='standard', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = tf.keras.layers.Dense(units, activation='gelu')\n",
    "        self.mode = mode\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        forward_call_count[self.mode] += 1\n",
    "        return self.dense(x)\n",
    "\n",
    "\n",
    "def build_n_layer_mlp(n_layers, hidden, mode='standard'):\n",
    "    \"\"\"nê°œ Dense ë ˆì´ì–´ë¥¼ ìŒ“ì€ MLP (í‘œì¤€ ë˜ëŠ” GC ë²„ì „)\"\"\"\n",
    "    layers = [TrackedDenseLayer(hidden, mode=mode) for _ in range(n_layers)]\n",
    "    output_layer = tf.keras.layers.Dense(1)\n",
    "    return layers, output_layer\n",
    "\n",
    "\n",
    "# ë°ì´í„° ì„¤ì •\n",
    "hidden = 128\n",
    "n_layers = 8\n",
    "x_data = tf.random.normal([32, hidden])\n",
    "y_data = tf.random.normal([32, 1])\n",
    "\n",
    "# â”€â”€ 1) í‘œì¤€ Forward-Backward â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "forward_call_count['standard'] = 0\n",
    "std_layers, std_out = build_n_layer_mlp(n_layers, hidden, 'standard')\n",
    "all_std_vars = sum([l.trainable_variables for l in std_layers], []) + std_out.trainable_variables\n",
    "\n",
    "# ì›Œë°ì—… (ë³€ìˆ˜ ìƒì„±)\n",
    "with tf.GradientTape() as tape:\n",
    "    h = x_data\n",
    "    for layer in std_layers:\n",
    "        h = layer(h, training=True)\n",
    "    pred = std_out(h)\n",
    "    loss = tf.reduce_mean((pred - y_data) ** 2)\n",
    "\n",
    "all_std_vars = tape.watched_variables()\n",
    "print(f\"[í‘œì¤€] í•™ìŠµ ìŠ¤í… ì¤‘ Forward í˜¸ì¶œ íšŸìˆ˜: {forward_call_count['standard']}\")\n",
    "\n",
    "# â”€â”€ 2) Gradient Checkpointing (tf.recompute_grad) â”€â”€â”€â”€\n",
    "forward_call_count['gc'] = 0\n",
    "gc_layers, gc_out = build_n_layer_mlp(n_layers, hidden, 'gc')\n",
    "\n",
    "# tf.recompute_grad: ë˜í•‘ëœ í•¨ìˆ˜ë¥¼ Backward ì‹œ ì¬í˜¸ì¶œ\n",
    "def make_recomputed_layer(layer):\n",
    "    @tf.recompute_grad\n",
    "    def recomputed_call(x):\n",
    "        return layer(x, training=True)\n",
    "    return recomputed_call\n",
    "\n",
    "recomputed = [make_recomputed_layer(l) for l in gc_layers]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # ì›Œë°ì—… Forward\n",
    "    h = x_data\n",
    "    for layer in gc_layers:\n",
    "        h = layer(h, training=True)\n",
    "    pred_gc = gc_out(h)\n",
    "\n",
    "print(f\"[GC] Warm-up Forward í˜¸ì¶œ: {forward_call_count['gc']}\")\n",
    "forward_call_count['gc'] = 0  # ë¦¬ì…‹\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    h = x_data\n",
    "    for rc in recomputed:\n",
    "        h = rc(h)\n",
    "    pred_gc = gc_out(h)\n",
    "    loss_gc = tf.reduce_mean((pred_gc - y_data) ** 2)\n",
    "\n",
    "grads = tape.gradient(loss_gc, tape.watched_variables())\n",
    "print(f\"[GC] Forward+Backward ì´ Forward í˜¸ì¶œ: {forward_call_count['gc']}\")\n",
    "print(f\"  â†’ {n_layers}ë ˆì´ì–´ Ã— 2(Forward 1 + ì¬ê³„ì‚° 1) ì˜ˆìƒ: {n_layers * 2}\")\n",
    "print(f\"\\nê²°ë¡ :\")\n",
    "print(f\"  í‘œì¤€: Forward {n_layers}íšŒ â†’ Activation {n_layers}ê°œ ë³´ì¡´\")\n",
    "print(f\"  GC  : Forward {n_layers}íšŒ + ì¬ê³„ì‚° {n_layers}íšŒ â†’ Activation O(âˆš{n_layers})ê°œ ë³´ì¡´\")\n",
    "print(f\"  â†’ ë©”ëª¨ë¦¬ ì ˆì•½ {(1-2*np.sqrt(n_layers)/n_layers)*100:.0f}%, ì—°ì‚° +33%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trade-off ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ë©”ëª¨ë¦¬-ì—°ì‚°ëŸ‰ Trade-off ì¢…í•© ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "scenarios = ['í‘œì¤€\\n(k=L)', 'GC k=2', 'GC k=4', 'GC k=âˆšL\\n(ìµœì )', 'ì „ì²´ ì¬ê³„ì‚°\\n(k=1)']\n",
    "L = 32\n",
    "k_vals = [L, 2, 4, optimal_k(L), 1]\n",
    "\n",
    "mem = [act_mem_gc(L, k) / L * 100 if k < L else 100 for k in k_vals]\n",
    "mem[0] = 100  # í‘œì¤€ = 100%\n",
    "mem[-1] = act_mem_gc(L, 1) / L * 100  # k=1: L/1 + 1 â‰ˆ L+1\n",
    "flops = [100, 100 + 100*(L/2-1)/L/3*100 , 100 + 100*(L/4-1)/L/3*100, 133.3, 200]\n",
    "# ë‹¨ìˆœí™”: í‘œì¤€=100, GC=133, ì™„ì „ì¬ê³„ì‚°=200\n",
    "flops = [100.0, 116.7, 125.0, 133.3, 200.0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_sc = ['#E53935', '#FB8C00', '#43A047', '#1E88E5', '#6D4C41']\n",
    "for i, (sc, m, f) in enumerate(zip(scenarios, mem, flops)):\n",
    "    ax.scatter(f, m, s=200, color=colors_sc[i], zorder=5, label=sc)\n",
    "    offset = (2, 3)\n",
    "    if i == 4: offset = (-30, 3)\n",
    "    ax.annotate(sc.replace('\\n', ' '), (f, m), fontsize=9,\n",
    "                xytext=(f + offset[0], m + offset[1]),\n",
    "                arrowprops=dict(arrowstyle='->', lw=0.8, color='gray'))\n",
    "\n",
    "ax.axhline(y=mem[3], color='blue', ls=':', alpha=0.5, label=f'ìµœì  GC ë©”ëª¨ë¦¬ì„  ({mem[3]:.0f}%)')\n",
    "ax.axvline(x=133.3, color='blue', ls=':', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('ì—°ì‚°ëŸ‰ (í‘œì¤€=100%)', fontsize=11)\n",
    "ax.set_ylabel('Activation ë©”ëª¨ë¦¬ (í‘œì¤€=100%)', fontsize=11)\n",
    "ax.set_title(f'Gradient Checkpointing ë©”ëª¨ë¦¬-ì—°ì‚°ëŸ‰ Trade-off (L={L})', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.annotate('â† ë©”ëª¨ë¦¬â†“\\n   ì—°ì‚°â†‘ â†’', xy=(145, 40), fontsize=10, color='darkblue',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightcyan', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ê²°ë¡ :\")\n",
    "print(\"  ìµœì  GC(k=âˆšL): +33% ì—°ì‚° ì˜¤ë²„í—¤ë“œë¡œ ~80%+ ë©”ëª¨ë¦¬ ì ˆì•½ â†’ ì‹¤ë¬´ í‘œì¤€\")\n",
    "print(\"  ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° GCê°€ ë‹¨ì¼ ìµœì„ ì˜ í•´ê²°ì±…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| í•­ëª© | ê°’ |\n",
    "|------|----|\n",
    "| í‘œì¤€ Activation Memory | $O(L)$ |\n",
    "| GC ìµœì  Activation Memory | $O(\\sqrt{L})$ |\n",
    "| ìµœì  ì²´í¬í¬ì¸íŠ¸ ê°„ê²© | $k^* = \\sqrt{L}$ |\n",
    "| ì—°ì‚° ì˜¤ë²„í—¤ë“œ | +33% (Forward 2íšŒ) |\n",
    "| ë©”ëª¨ë¦¬ ì ˆì•½ë¥  (GPT-3 L=96) | â‰ˆ 80% |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$M_{GC} = \\left(\\frac{L}{k} + k\\right) M_1 \\xrightarrow{k=\\sqrt{L}} 2\\sqrt{L} \\cdot M_1$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 10-03: ZeRO Redundancy Optimizer** â€” Gradient Checkpointingì´ Activationì„ ì¤„ì¸ë‹¤ë©´, ZeROëŠ” **íŒŒë¼ë¯¸í„°, ê·¸ë˜ë””ì–¸íŠ¸, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ**ë¥¼ GPU ê°„ì— ë¶„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ $O(1/N)$ìœ¼ë¡œ ì¤„ì¸ë‹¤. Stage 1/2/3ì˜ ë‹¨ê³„ë³„ íŒŒí‹°ì…”ë‹ì„ ìˆ˜ì‹ê³¼ í•¨ê»˜ ì™„ì „íˆ ì´í•´í•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python (tf_study)", "language": "python", "name": "tf_study"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}