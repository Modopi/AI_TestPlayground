{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10-01: GPU ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ (Memory Profiling)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- GPU VRAMì„ ì°¨ì§€í•˜ëŠ” **4ê°€ì§€ êµ¬ì„±ìš”ì†Œ**ë¥¼ ìˆ˜ì‹ ë ˆë²¨ì—ì„œ ì •í™•íˆ íŒŒì•…í•˜ê³ , ëª¨ë¸ ì•„í‚¤í…ì²˜ë§Œ ë³´ê³  í•„ìš” ë©”ëª¨ë¦¬ë¥¼ ìŠ¤ìŠ¤ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤\n",
    "- **Mixed Precision í•™ìŠµ** (FP16/BF16 + FP32 ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜)ì˜ í•„ìš”ì„±ê³¼ ë©”ëª¨ë¦¬-ì†ë„ Trade-offë¥¼ ë¶„ì„í•œë‹¤\n",
    "- FP8 / BF16 / FP16 / FP32 ë°ì´í„°íƒ€ì…ì˜ **ìˆ˜ì¹˜ ë²”ìœ„ì™€ ì •ë°€ë„ ì°¨ì´**ë¥¼ ì½”ë“œë¡œ ì§ì ‘ í™•ì¸í•œë‹¤\n",
    "- ì‹¤ì œ Transformer ëª¨ë¸ì˜ **Activation Memory**ë¥¼ ì¶”ì •í•˜ëŠ” ê³µì‹ì„ ë„ì¶œí•˜ê³ , Sequence Lengthì— ë”°ë¥¸ ë³€í™”ë¥¼ ì‹œê°í™”í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ë©”ëª¨ë¦¬ ë¶„ë¥˜ì™€ ê³µì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ë°ì´í„°íƒ€ì…ë³„ ë²”ìœ„ì™€ ì •ë°€ë„ ì‹¤í—˜](#2.-ë°ì´í„°íƒ€ì…-ì‹¤í—˜)\n",
    "3. [ëª¨ë¸ ì„¤ì •ë³„ ë©”ëª¨ë¦¬ ê³„ì‚°ê¸°](#3.-ë©”ëª¨ë¦¬-ê³„ì‚°ê¸°)\n",
    "4. [Activation Memory ì¶”ì • ë° ì‹œê°í™”](#4.-Activation-Memory)\n",
    "5. [ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ ë¹„êµ](#5.-ìµœì í™”-ì˜µì…˜-ë¹„êµ)\n",
    "6. [ì •ë¦¬ ë° ì—°ìŠµ ë¬¸ì œ](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### GPU VRAM 4ê°€ì§€ êµ¬ì„±ìš”ì†Œ\n",
    "\n",
    "$$M_{VRAM} = M_{params} + M_{grad} + M_{optim} + M_{act}$$\n",
    "\n",
    "| êµ¬ì„±ìš”ì†Œ | ë°ì´í„°íƒ€ì… | í¬ê¸° | ì„¤ëª… |\n",
    "|---------|-----------|------|------|\n",
    "| ëª¨ë¸ íŒŒë¼ë¯¸í„° $M_{params}$ | FP16 (Mixed) | $2P$ | ì¶”ë¡  ë° Forward ì— ì‚¬ìš© |\n",
    "| ê·¸ë˜ë””ì–¸íŠ¸ $M_{grad}$ | FP16 (Mixed) | $2P$ | Backward ì‹œ ê³„ì‚° |\n",
    "| ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ (Mixedë§Œ) | FP32 | $4P$ | ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸ ì •ë°€ë„ ë³´ì¥ |\n",
    "| Adam 1ì°¨ ëª¨ë©˜íŠ¸ $m$ | FP32 | $4P$ | ì§€ìˆ˜ì´ë™í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸ |\n",
    "| Adam 2ì°¨ ëª¨ë©˜íŠ¸ $v$ | FP32 | $4P$ | ì§€ìˆ˜ì´ë™í‰ê·  ê·¸ë˜ë””ì–¸íŠ¸Â² |\n",
    "| **í•©ê³„ (Mixed+Adam)** | | $\\mathbf{16P}$ | |\n",
    "| **Activation** $M_{act}$ | FP16 | $O(B \\cdot S \\cdot H \\cdot L)$ | Batch, Seq, Hidden, Layer í¬ê¸° ë¹„ë¡€ |\n",
    "\n",
    "### Mixed Precisionì˜ í•µì‹¬ íšŒë¡œ\n",
    "\n",
    "```\n",
    "FP32 ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜\n",
    "       â”‚  (ì—…ë°ì´íŠ¸ Î´W ì ìš©)       â† ì •ë°€ë„ ë³´ì¥\n",
    "       â–¼\n",
    "    FP32 ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜\n",
    "       â”‚  (FP32â†’FP16 ìºìŠ¤íŒ…)\n",
    "       â–¼\n",
    "  FP16 ê°€ì¤‘ì¹˜ â”€â”€â†’ Forward (FP16: 2ë°° ë¹ ë¦„) â”€â”€â†’ FP16 í™œì„±í™”\n",
    "                                                      â”‚\n",
    "                                                      â–¼\n",
    "                 Backward (FP16) â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ FP16 ê·¸ë˜ë””ì–¸íŠ¸\n",
    "                      â”‚  (Loss Scale: NaN ë°©ì§€)\n",
    "                      â–¼\n",
    "               FP32ë¡œ ëˆ„ì  â†’ Adam ì—…ë°ì´íŠ¸ â†’ FP32 ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜\n",
    "```\n",
    "\n",
    "### Transformer Activation Memory (ë ˆì´ì–´ë‹¹)\n",
    "\n",
    "$$M_{act, layer} \\approx B \\cdot S \\cdot H \\cdot \\underbrace{(4 + 2h/H + 2S/H)}_{\\text{attention í•­}} \\cdot \\text{dtype bytes}$$\n",
    "\n",
    "ë‹¨ìˆœ ê·¼ì‚¬: $M_{act} \\approx B \\cdot S \\cdot H \\cdot 12 \\cdot L \\cdot \\text{bytes}$  \n",
    "(ì—¬ê¸°ì„œ $B$=ë°°ì¹˜, $S$=ì‹œí€€ìŠ¤, $H$=íˆë“ , $L$=ë ˆì´ì–´)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ’¾ GPU ë©”ëª¨ë¦¬ê°€ ì™œ ê½‰ ì°¨ë‚˜ìš”?\n",
    "\n",
    "> AI í›ˆë ¨ ì‹œ GPUì— ì˜¬ë¼ê°€ëŠ” ê²ƒë“¤:\n",
    "> - ğŸ“š **êµê³¼ì„œ(ëª¨ë¸ ê°€ì¤‘ì¹˜)**: ê°€ì¥ ê¸°ë³¸ í•„ìˆ˜í’ˆ\n",
    "> - ğŸ“ **ì˜¤ë‹µ ë…¸íŠ¸(ê·¸ë˜ë””ì–¸íŠ¸)**: ì–´ë””ì„œ í‹€ë ¸ëŠ”ì§€ ê¸°ë¡\n",
    "> - ğŸ“Š **ì„±ì  ê¸°ë¡ë¶€(Adam ìƒíƒœ)**: ê³¼ê±° í•™ìŠµ íë¦„ ê¸°ì–µ\n",
    "> - ğŸ–ï¸ **ì—°ìŠµì¥(Activation)**: ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ (ë°°ì¹˜ê°€ í´ìˆ˜ë¡ ë§ì•„ì§!)\n",
    ">\n",
    "> ì´ 4ê°€ì§€ ì¤‘ **Adam ìƒíƒœ(ì˜µí‹°ë§ˆì´ì €)ê°€ 50%** ì´ìƒ ì°¨ì§€í•´ìš”!\n",
    "\n",
    "#### ğŸ­ FP16ê³¼ FP32ì˜ ì°¨ì´ëŠ”?\n",
    "\n",
    "> - **FP32(4ë°”ì´íŠ¸)**: ì•„ì£¼ ì •ë°€í•œ ê³„ì‚° (ì†Œìˆ˜ì  ì•„ë˜ 7ìë¦¬)\n",
    "> - **FP16(2ë°”ì´íŠ¸)**: ëœ ì •ë°€í•˜ì§€ë§Œ ì ˆë°˜ ë©”ëª¨ë¦¬ (ì†Œìˆ˜ì  ì•„ë˜ 3ìë¦¬)\n",
    "> - **ê¼¼ìˆ˜(Mixed Precision)**: ë¹ ë¥¸ ê³„ì‚°ì€ FP16ìœ¼ë¡œ, ì¤‘ìš”í•œ ì—…ë°ì´íŠ¸ëŠ” FP32ë¡œ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: í•™ìŠµ ë©”ëª¨ë¦¬ ê³„ì‚°\n",
    "\n",
    "Llama-2 13B (13,000,000,000 íŒŒë¼ë¯¸í„°)ë¥¼ Mixed Precision + Adamìœ¼ë¡œ í•™ìŠµí•  ë•Œ:\n",
    "- ëª¨ë¸ ê°€ì¤‘ì¹˜ë§Œì˜ FP16 ë©”ëª¨ë¦¬ëŠ”?\n",
    "- ì „ì²´ í•™ìŠµ ë©”ëª¨ë¦¬ëŠ”?\n",
    "- A100 80GB GPU ëª‡ ëŒ€ê°€ í•„ìš”í•œê°€? (70% í™œìš© ê°€ì •)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$P = 13 \\times 10^9$\n",
    "\n",
    "- FP16 ê°€ì¤‘ì¹˜: $2 \\times 13 \\times 10^9 = 26$ GB\n",
    "- ì „ì²´ í•™ìŠµ: $16 \\times 13 = 208$ GB\n",
    "- A100 80GB 70% = 56GB â†’ $\\lceil 208/56 \\rceil = \\mathbf{4}$ëŒ€\n",
    "\n",
    "(Activation ì œì™¸. ì‹¤ì œë¡œëŠ” ë” í•„ìš”)\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: FP16 ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜\n",
    "\n",
    "FP16ì˜ ìµœëŒ€ê°’ì€ 65504ì…ë‹ˆë‹¤.  \n",
    "`attention_scores = Q @ K.T / sqrt(d_k)`ì—ì„œ $d_k=64$ì¼ ë•Œ,  \n",
    "Q, K ê° ì›ì†Œê°€ 10.0ì´ë©´ attention scoreì˜ ìµœëŒ€ê°’ì€ ì–¼ë§ˆì¸ê°€?  \n",
    "ì´ê²ƒì´ FP16ì—ì„œ ì•ˆì „í•œê°€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$Q = K = 10.0$, $d_k = 64$, ë²¡í„° ê¸¸ì´ $d_k$  \n",
    "$\\text{score} = \\frac{Q \\cdot K}{\\sqrt{64}} = \\frac{10 \\times 10 \\times 64}{8} = \\frac{6400}{8} = 800$\n",
    "\n",
    "800 < 65504 â†’ ì´ ê²½ìš°ëŠ” ì•ˆì „í•˜ì§€ë§Œ, $d_k$ê°€ í¬ê±°ë‚˜ ê°’ì´ í¬ë©´ ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜!  \n",
    "â†’ ì´ê²ƒì´ **BF16(FP32ì™€ ë™ì¼í•œ ì§€ìˆ˜ ë²”ìœ„)**ê°€ ì„ í˜¸ë˜ëŠ” ì´ìœ \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„°íƒ€ì… ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ë°ì´í„°íƒ€ì…ë³„ ìˆ˜ì¹˜ ë²”ìœ„, ì •ë°€ë„, ë©”ëª¨ë¦¬ ë¹„êµ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "dtype_info = {\n",
    "    'FP32 (float32)': {\n",
    "        'tf_dtype': tf.float32,\n",
    "        'bytes': 4, 'exp_bits': 8, 'mantissa_bits': 23,\n",
    "        'max': 3.4e38, 'min_pos': 1.18e-38\n",
    "    },\n",
    "    'BF16 (bfloat16)': {\n",
    "        'tf_dtype': tf.bfloat16,\n",
    "        'bytes': 2, 'exp_bits': 8, 'mantissa_bits': 7,\n",
    "        'max': 3.4e38, 'min_pos': 1.18e-38\n",
    "    },\n",
    "    'FP16 (float16)': {\n",
    "        'tf_dtype': tf.float16,\n",
    "        'bytes': 2, 'exp_bits': 5, 'mantissa_bits': 10,\n",
    "        'max': 65504, 'min_pos': 6.1e-5\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"{'íƒ€ì…':<20} {'ë°”ì´íŠ¸':>5} {'ì§€ìˆ˜ë¹„íŠ¸':>8} {'ê°€ìˆ˜ë¹„íŠ¸':>8} {'ìµœëŒ“ê°’':>12} {'ìµœì†Ÿê°’(+)':>12} {'FP32 ëŒ€ë¹„ ë©”ëª¨ë¦¬'}\")  \n",
    "print(\"-\" * 85)\n",
    "for name, info in dtype_info.items():\n",
    "    savings = (1 - info['bytes'] / 4) * 100\n",
    "    print(f\"{name:<20} {info['bytes']:>5} {info['exp_bits']:>8} {info['mantissa_bits']:>8}\"\n",
    "          f\" {info['max']:>12.2e} {info['min_pos']:>12.2e} \"\n",
    "          f\"{'ë™ì¼' if savings==0 else f'-{savings:.0f}%'}\")\n",
    "\n",
    "print(\"\\n=== ì •ë°€ë„ ì†ì‹¤ ì‹¤í—˜ ===\")\n",
    "test_values = [1.0, 1.0 + 1e-3, 1.0 + 1e-4, 3.14159265358979, 65504.0, 65505.0]\n",
    "\n",
    "print(f\"{'ì›ë³¸ FP32ê°’':>22} | {'BF16ë¡œ ë³€í™˜':>15} | {'FP16ë¡œ ë³€í™˜':>15} | {'ì„¤ëª…'}\")\n",
    "print(\"-\" * 80)\n",
    "for v in test_values:\n",
    "    orig    = tf.constant(v, dtype=tf.float32)\n",
    "    in_bf16 = tf.cast(tf.cast(orig, tf.bfloat16), tf.float32).numpy()\n",
    "    in_fp16 = tf.cast(tf.cast(orig, tf.float16), tf.float32).numpy()\n",
    "    note = ''\n",
    "    if v == 65505.0: note = 'â† FP16 ìµœëŒ“ê°’ ì´ˆê³¼!'\n",
    "    elif abs(v - in_fp16) > 1e-2: note = 'â† FP16 ì •ë°€ë„ ì†ì‹¤'\n",
    "    print(f\"{v:>22.10f} | {in_bf16:>15.10f} | {in_fp16:>15.6f} | {note}\")\n",
    "\n",
    "print(\"\\nâ†’ BF16ì€ FP32ì™€ ë™ì¼í•œ ì§€ìˆ˜ ë²”ìœ„ â†’ ì˜¤ë²„í”Œë¡œìš° ì—†ìŒ\")\n",
    "print(\"â†’ FP16ì€ ìµœëŒ“ê°’ì´ 65504ë¡œ ì œí•œ â†’ ì˜¤ë²„í”Œë¡œìš° ìœ„í—˜ (Loss Scaling í•„ìš”)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# FP16 ì˜¤ë²„í”Œë¡œìš° ì‹œë®¬ë ˆì´ì…˜ (Loss Scalingì˜ í•„ìš”ì„±)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ê·¹ë‹¨ì ìœ¼ë¡œ ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ â†’ FP16ì—ì„œ ì–¸ë”í”Œë¡œìš°\n",
    "tiny_grad = tf.constant(1e-8, dtype=tf.float32)\n",
    "tiny_in_fp16 = tf.cast(tiny_grad, tf.float16).numpy()\n",
    "print(f\"ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ {1e-8} â†’ FP16: {tiny_in_fp16}  â† ì–¸ë”í”Œë¡œìš°! (0ì´ ë¨)\")\n",
    "\n",
    "# Loss Scaling: ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ scale ë°° í‚¤ì›Œì„œ ì €ì¥, ë‚˜ì¤‘ì— ë‚˜ëˆ”\n",
    "loss_scale = 2048.0\n",
    "scaled_grad = tiny_grad * loss_scale\n",
    "scaled_in_fp16 = tf.cast(scaled_grad, tf.float16).numpy()\n",
    "restored = scaled_in_fp16 / loss_scale\n",
    "\n",
    "print(f\"\\n[Loss Scaling ì ìš©] scale = {loss_scale}\")\n",
    "print(f\"  ì›ë³¸: {tiny_grad.numpy():.2e}\")\n",
    "print(f\"  ìŠ¤ì¼€ì¼ í›„ FP16 ì €ì¥: {scaled_in_fp16}\")\n",
    "print(f\"  ìŠ¤ì¼€ì¼ ì œê±° ë³µì›:    {restored:.2e}\")\n",
    "print(f\"  ë³µì› ì„±ê³µ: {'âœ…' if abs(restored - tiny_grad.numpy()) < 1e-10 else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nâ†’ Loss Scalingì´ FP16 í•™ìŠµì˜ í•µì‹¬ ê¸°ë²•!\")\n",
    "print(\"   TFì—ì„œëŠ” tf.keras.mixed_precision.LossScaleOptimizer ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë©”ëª¨ë¦¬ ê³„ì‚°ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì¢…í•© í•™ìŠµ ë©”ëª¨ë¦¬ ê³„ì‚°ê¸°\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def compute_memory_breakdown(params_B, batch_size, seq_len, hidden_dim,\n",
    "                             n_layers, n_heads, optimizer='adam',\n",
    "                             dtype='fp16', act_checkpoint=False):\n",
    "    \"\"\"\n",
    "    Transformer ëª¨ë¸ì˜ ì „ì²´ í•™ìŠµ ë©”ëª¨ë¦¬ë¥¼ í•­ëª©ë³„ë¡œ ë¶„í•´.\n",
    "    \n",
    "    Returns: í•­ëª©ë³„ ë©”ëª¨ë¦¬ ë”•ì…”ë„ˆë¦¬ (GB ë‹¨ìœ„)\n",
    "    \"\"\"\n",
    "    P = params_B * 1e9\n",
    "    dty = 2 if dtype in ('fp16', 'bf16') else 4\n",
    "\n",
    "    # â”€â”€ ì •ì  ë©”ëª¨ë¦¬ (Param, Grad, Optimizer) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    param_mem  = P * dty / 1e9\n",
    "    grad_mem   = P * dty / 1e9\n",
    "    master_mem = (P * 4 / 1e9) if dtype in ('fp16', 'bf16') else 0.0\n",
    "\n",
    "    optim_cfg = {'sgd': 0, 'momentum': P*4/1e9, 'adam': P*8/1e9}\n",
    "    optim_mem = optim_cfg.get(optimizer, P*8/1e9)\n",
    "\n",
    "    # â”€â”€ Activation Memory (Transformer ê·¼ì‚¬) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # í•µì‹¬ í•­: MHA Q,K,V í”„ë¡œì ì…˜ + Attention Score + FFN\n",
    "    # ë ˆì´ì–´ë‹¹: B Ã— S Ã— (12H) Ã— bytes (ê·¼ì‚¬)\n",
    "    act_per_layer = batch_size * seq_len * hidden_dim * 12 * dty / 1e9\n",
    "    if act_checkpoint:\n",
    "        # Gradient Checkpointing: âˆšL ë ˆì´ì–´ë§Œ ë³´ì¡´ (ë‚˜ë¨¸ì§€ ì¬ê³„ì‚°)\n",
    "        act_mem = act_per_layer * int(np.sqrt(n_layers))\n",
    "    else:\n",
    "        act_mem = act_per_layer * n_layers\n",
    "\n",
    "    total = param_mem + grad_mem + master_mem + optim_mem + act_mem\n",
    "\n",
    "    return {\n",
    "        'param':   param_mem,\n",
    "        'grad':    grad_mem,\n",
    "        'master':  master_mem,\n",
    "        'optim':   optim_mem,\n",
    "        'act':     act_mem,\n",
    "        'total':   total\n",
    "    }\n",
    "\n",
    "\n",
    "# â”€â”€ ì£¼ìš” LLM ë©”ëª¨ë¦¬ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "llms = [\n",
    "    # (ì´ë¦„,   params_B, batch, seq,  H,     L,  heads)\n",
    "    ('Llama-2 7B',   7,   4, 4096,  4096, 32,  32),\n",
    "    ('Llama-2 13B', 13,   4, 4096,  5120, 40,  40),\n",
    "    ('Llama-2 70B', 70,   2, 4096,  8192, 80,  64),\n",
    "]\n",
    "\n",
    "print(f\"{'ëª¨ë¸':<15} {'Param':>7} {'Grad':>6} {'Master':>7} {'Adam':>6} {'Act':>6} {'í•©ê³„':>7} | H100 í•„ìš”\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, p, b, s, h, l, nh in llms:\n",
    "    mb = compute_memory_breakdown(p, b, s, h, l, nh)\n",
    "    n_gpu = max(1, int(np.ceil(mb['total'] / (80 * 0.7))))\n",
    "    print(f\"{name:<15} {mb['param']:>7.1f} {mb['grad']:>6.1f} {mb['master']:>7.1f}\"\n",
    "          f\" {mb['optim']:>6.1f} {mb['act']:>6.1f} {mb['total']:>7.1f} GB | {n_gpu}ëŒ€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Activation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Activation Memory vs Sequence Length ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "seq_lens = np.array([512, 1024, 2048, 4096, 8192, 16384, 32768])\n",
    "params_B, hidden, layers, heads = 7, 4096, 32, 32\n",
    "batch = 1\n",
    "\n",
    "act_mems = []\n",
    "for sl in seq_lens:\n",
    "    mb = compute_memory_breakdown(params_B, batch, int(sl), hidden, layers, heads)\n",
    "    act_mems.append(mb['act'])\n",
    "\n",
    "static_mem = compute_memory_breakdown(params_B, batch, 512, hidden, layers, heads)['total'] \\\n",
    "           - compute_memory_breakdown(params_B, batch, 512, hidden, layers, heads)['act']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ì™¼ìª½: Activation vs ì´ ë©”ëª¨ë¦¬\n",
    "ax1 = axes[0]\n",
    "total_mems = [static_mem + a for a in act_mems]\n",
    "ax1.fill_between(seq_lens, 0, [static_mem]*len(seq_lens), alpha=0.6, color='#1E88E5',\n",
    "                 label=f'ì •ì  ë©”ëª¨ë¦¬ (Param+Grad+Optim: {static_mem:.0f}GB)')\n",
    "ax1.fill_between(seq_lens, [static_mem]*len(seq_lens), total_mems, alpha=0.6, color='#E53935',\n",
    "                 label='Activation Memory (âˆ Seq Length)')\n",
    "ax1.axhline(y=80, color='k', ls='--', lw=2, label='H100 80GB')\n",
    "ax1.axhline(y=80*0.7, color='gray', ls=':', lw=1.5, label='H100 56GB (70%)')\n",
    "\n",
    "for sl, tm in zip(seq_lens, total_mems):\n",
    "    ax1.annotate(f'{tm:.0f}GB', (sl, tm + 3), ha='center', fontsize=8, color='darkred')\n",
    "\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_xticks(seq_lens)\n",
    "ax1.set_xticklabels([f'{sl//1024}K' if sl>=1024 else str(sl) for sl in seq_lens])\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (Sequence Length)', fontsize=11)\n",
    "ax1.set_ylabel('ë©”ëª¨ë¦¬ (GB)', fontsize=11)\n",
    "ax1.set_title(f'Llama-2 7B: ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬\\n(batch=1)', fontweight='bold')\n",
    "ax1.legend(fontsize=8, loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: Activation Memory ë¹„ìœ¨ ì¶”ì„¸\n",
    "ax2 = axes[1]\n",
    "act_ratios = [a / t * 100 for a, t in zip(act_mems, total_mems)]\n",
    "\n",
    "ax2.bar(range(len(seq_lens)), act_ratios, color='#E53935', alpha=0.75, label='Activation ë¹„ìœ¨')\n",
    "ax2.plot(range(len(seq_lens)), act_ratios, 'ko-', lw=2, ms=6)\n",
    "ax2.set_xticks(range(len(seq_lens)))\n",
    "ax2.set_xticklabels([f'{sl//1024}K' if sl>=1024 else str(sl) for sl in seq_lens])\n",
    "ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (Sequence Length)', fontsize=11)\n",
    "ax2.set_ylabel('Activationì´ ì´ ë©”ëª¨ë¦¬ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ (%)', fontsize=11)\n",
    "ax2.set_title('ê¸´ ì‹œí€€ìŠ¤ì¼ìˆ˜ë¡\\nActivationì´ ë©”ëª¨ë¦¬ ì§€ë°°', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (pct, sl) in enumerate(zip(act_ratios, seq_lens)):\n",
    "    ax2.text(i, pct + 1.5, f'{pct:.0f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ê²°ë¡ :\")\n",
    "print(f\"  â€¢ ì§§ì€ ì‹œí€€ìŠ¤: ì •ì  ë©”ëª¨ë¦¬(íŒŒë¼ë¯¸í„° ë“±) ì§€ë°°\")\n",
    "print(f\"  â€¢ ê¸´ ì‹œí€€ìŠ¤(4K+): Activationì´ ì ˆë°˜ ì´ìƒ â†’ Gradient Checkpointing í•„ìˆ˜!\")\n",
    "print(f\"  â€¢ Attentionì˜ O(SÂ²) í•­ ë•Œë¬¸ì— ì‹œí€€ìŠ¤ ê¸¸ì´ 4ë°° â†’ Activation â‰ˆ 4ë°° ì¦ê°€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ìµœì í™” ì˜µì…˜ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•ë³„ íš¨ê³¼ ë¹„êµ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def compute_scenario(params_B, b, s, h, l, nh, scenario):\n",
    "    \"\"\"ë‹¤ì–‘í•œ ë©”ëª¨ë¦¬ ìµœì í™” ì‹œë‚˜ë¦¬ì˜¤ ê³„ì‚°\"\"\"\n",
    "    cfg = {\n",
    "        'â‘  FP32 + SGD':          {'dtype': 'fp32', 'optimizer': 'sgd', 'act_ckpt': False},\n",
    "        'â‘¡ FP32 + Adam':         {'dtype': 'fp32', 'optimizer': 'adam', 'act_ckpt': False},\n",
    "        'â‘¢ FP16 + Adam':         {'dtype': 'fp16', 'optimizer': 'adam', 'act_ckpt': False},\n",
    "        'â‘£ FP16 + Adam + GC':    {'dtype': 'fp16', 'optimizer': 'adam', 'act_ckpt': True},\n",
    "        'â‘¤ BF16 + Adam':         {'dtype': 'bf16', 'optimizer': 'adam', 'act_ckpt': False},\n",
    "        'â‘¥ BF16 + Adam + GC':    {'dtype': 'bf16', 'optimizer': 'adam', 'act_ckpt': True},\n",
    "    }\n",
    "    kw = cfg[scenario]\n",
    "    return compute_memory_breakdown(params_B, b, s, h, l, nh,\n",
    "                                    optimizer=kw['optimizer'],\n",
    "                                    dtype=kw['dtype'],\n",
    "                                    act_checkpoint=kw['act_ckpt'])\n",
    "\n",
    "# Llama-2 7B ê¸°ì¤€\n",
    "p, b, s, h, l, nh = 7, 2, 2048, 4096, 32, 32\n",
    "scenarios = ['â‘  FP32 + SGD', 'â‘¡ FP32 + Adam', 'â‘¢ FP16 + Adam',\n",
    "             'â‘£ FP16 + Adam + GC', 'â‘¤ BF16 + Adam', 'â‘¥ BF16 + Adam + GC']\n",
    "totals = [compute_scenario(p, b, s, h, l, nh, sc)['total'] for sc in scenarios]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 5))\n",
    "sc_colors = ['#C62828', '#E53935', '#1E88E5', '#42A5F5', '#43A047', '#81C784']\n",
    "bars = ax.barh(range(len(scenarios)), totals, color=sc_colors, alpha=0.85, height=0.6)\n",
    "\n",
    "ax.axvline(x=80, color='red', ls='--', lw=2, alpha=0.8, label='H100 80GB')\n",
    "ax.axvline(x=80*0.7, color='orange', ls=':', lw=1.5, label='H100 56GB (70%)')\n",
    "\n",
    "for i, (bar, tot) in enumerate(zip(bars, totals)):\n",
    "    reduction = (1 - tot / totals[0]) * 100\n",
    "    label = f'{tot:.0f} GB (ê¸°ì¤€)' if i == 0 else f'{tot:.0f} GB (-{reduction:.0f}%)'\n",
    "    ax.text(tot + 0.5, i, label, va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.set_yticks(range(len(scenarios)))\n",
    "ax.set_yticklabels(scenarios, fontsize=10)\n",
    "ax.set_xlabel('ì´ í•™ìŠµ ë©”ëª¨ë¦¬ (GB)', fontsize=11)\n",
    "ax.set_title(f'Llama-2 7B ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ ë¹„êµ\\n(Batch=2, Seq=2048)', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\në©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ ì •ë¦¬:\")\n",
    "for sc, tot in zip(scenarios, totals):\n",
    "    red = (1 - tot/totals[0]) * 100\n",
    "    fits = 'ë‹¨ì¼ H100 âœ…' if tot <= 80*0.7 else f'H100 {int(np.ceil(tot/(80*0.7)))}ëŒ€ í•„ìš”'\n",
    "    print(f\"  {sc:<24}: {tot:>6.0f} GB (-{red:>3.0f}%) â†’ {fits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| í•­ëª© | FP32 í•™ìŠµ | Mixed Precision (FP16+FP32) |\n",
    "|------|----------|----------------------------|\n",
    "| íŒŒë¼ë¯¸í„° | FP32 ($4P$) | FP16 ($2P$) + FP32 ë§ˆìŠ¤í„° ($4P$) |\n",
    "| ê·¸ë˜ë””ì–¸íŠ¸ | FP32 ($4P$) | FP16 ($2P$) |\n",
    "| Adam ìƒíƒœ | FP32 ($8P$) | FP32 ($8P$) |\n",
    "| **í•©ê³„** | **$16P$** | **$16P$** â† ë™ì¼! |\n",
    "| **ì—°ì‚° ì†ë„** | ê¸°ì¤€ | 1.5~2ë°° ë¹ ë¦„ (Tensor Core í™œìš©) |\n",
    "\n",
    "> ğŸ’¡ Mixed Precisionì€ **ë©”ëª¨ë¦¬ ì ˆì•½ ëª©ì ì´ ì•„ë‹ˆë¼ ì—°ì‚° ì†ë„ í–¥ìƒ**ì´ ì£¼ ëª©ì !  \n",
    "> ë©”ëª¨ë¦¬ ì ˆì•½ì€ ZeRO, Gradient Checkpointing ë“±ì´ ë‹´ë‹¹í•œë‹¤.\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$M_{total} = \\underbrace{(2+2+4+4+4)P}_{=16P \\text{ bytes}} + \\underbrace{B \\cdot S \\cdot H \\cdot 12L \\cdot \\text{bytes}}_{\\text{Activation}}$$\n",
    "\n",
    "### ë°ì´í„°íƒ€ì… ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| íƒ€ì… | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ìš©ë„ |\n",
    "|------|------|------|----------|\n",
    "| **FP32** | ìµœê³  ì •ë°€ë„ | ë©”ëª¨ë¦¬ 2ë°°, ì†ë„ ì ˆë°˜ | ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ |\n",
    "| **BF16** | FP32 ë²”ìœ„ ë™ì¼, ì†ë„ 2ë°° | FP16ë³´ë‹¤ ì •ë°€ë„ ë‚®ìŒ | **LLM í•™ìŠµ ê¶Œì¥ (A100/H100)** |\n",
    "| **FP16** | ì†ë„ 2ë°° | Overflow ìœ„í—˜ (Loss Scaling í•„ìš”) | GPUê°€ BF16 ë¯¸ì§€ì› ì‹œ |\n",
    "| **FP8** | ì†ë„ 4ë°°, ë©”ëª¨ë¦¬ ì ˆë°˜ | ì‹¤í—˜ì , FP32 ë§ˆìŠ¤í„° í•„ìˆ˜ | DeepSeek-V3, H100 ì „ìš© |\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 10-02: Gradient Checkpointing** â€” Activation Memoryë¥¼ ì œê³±ê·¼ ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì´ëŠ” í•µì‹¬ ê¸°ë²•. Forward ê²°ê³¼ ì¼ë¶€ë¥¼ ë²„ë¦¬ê³  Backward ì‹œ ì„ íƒì ìœ¼ë¡œ ì¬ê³„ì‚°í•˜ëŠ” ì›ë¦¬ì™€ ë©”ëª¨ë¦¬-ì—°ì‚°ëŸ‰ Trade-off ($O(\\sqrt{L})$ vs $O(L)$)ë¥¼ í•™ìŠµí•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python (tf_study)", "language": "python", "name": "tf_study"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}