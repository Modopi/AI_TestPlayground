{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: 클래스 조건부 MNIST CFG 생성\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "- Classifier-Free Guidance(CFG)를 사용한 조건부 생성과 Guidance Scale에 따른 품질-다양성 Trade-off를 실험합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: 조건부/비조건부 Noise Prediction](#q1)\n",
    "- [Q2: CFG 공식 적용](#q2)\n",
    "- [Q3: Guidance Scale 실험](#q3)\n",
    "- [Q4: FID/IS 개념 이해](#q4)\n",
    "- [종합 도전: Mini CFG Generation Pipeline](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 라이브러리 임포트 ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"NumPy 버전: {np.__version__}\")\n",
    "\n",
    "# DDPM 관련 상수\n",
    "T = 200\n",
    "beta_min, beta_max = 1e-4, 0.02\n",
    "betas = np.linspace(beta_min, beta_max, T).astype(np.float32)\n",
    "alphas = 1.0 - betas\n",
    "alpha_bars = np.cumprod(alphas).astype(np.float32)\n",
    "\n",
    "print(f\"타임스텝 수: {T}\")\n",
    "print(f\"alpha_bar[0] = {alpha_bars[0]:.6f}, alpha_bar[-1] = {alpha_bars[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q1: 조건부/비조건부 Noise Prediction <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "CFG를 사용하려면 **하나의 모델**이 다음 두 가지를 예측할 수 있어야 합니다:\n",
    "\n",
    "1. **비조건부** $\\epsilon_\\theta(x_t, t, \\varnothing)$: 조건 없이 노이즈 예측\n",
    "2. **조건부** $\\epsilon_\\theta(x_t, t, c)$: 클래스 $c$를 받아 노이즈 예측\n",
    "\n",
    "학습 시 일정 확률 $p_{uncond}$로 조건을 드롭(null로 대체)합니다.\n",
    "\n",
    "간단한 MLP 기반 조건부 노이즈 예측 모델을 구현하세요.\n",
    "\n",
    "**여러분의 예측:**\n",
    "- $p_{uncond} = 0.1$이면 학습 데이터의 `?`%는 비조건부로 학습됩니다\n",
    "- 비조건부 학습이 없으면 ($p_{uncond} = 0$) CFG가 작동할까요? `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: 조건부/비조건부 Noise Prediction\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 간단한 CFG 모델 (2D 데이터용)\n",
    "class CFGNoiseModel(tf.keras.Model):\n",
    "    # 조건부/비조건부 통합 노이즈 예측 모델\n",
    "    def __init__(self, data_dim=2, n_classes=4, time_dim=16, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # 시간 임베딩\n",
    "        self.time_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(time_dim, activation='swish'),\n",
    "            tf.keras.layers.Dense(time_dim)\n",
    "        ])\n",
    "\n",
    "        # 클래스 임베딩 (null class = n_classes)\n",
    "        self.class_embed = tf.keras.layers.Embedding(n_classes + 1, time_dim)\n",
    "\n",
    "        # 메인 네트워크\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='swish'),\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='swish'),\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='swish'),\n",
    "            tf.keras.layers.Dense(data_dim)\n",
    "        ])\n",
    "\n",
    "    def call(self, x_t, t, class_label=None, p_uncond=0.0, training=False):\n",
    "        # 시간 임베딩\n",
    "        t_emb = self.time_mlp(tf.cast(tf.reshape(t, [-1, 1]), tf.float32))\n",
    "\n",
    "        # 조건 드롭 (학습 시)\n",
    "        if class_label is not None:\n",
    "            if training and p_uncond > 0:\n",
    "                mask = tf.random.uniform([tf.shape(class_label)[0]]) < p_uncond\n",
    "                null_label = tf.fill(tf.shape(class_label), self.n_classes)\n",
    "                class_label = tf.where(mask, null_label, class_label)\n",
    "            c_emb = self.class_embed(class_label)\n",
    "        else:\n",
    "            batch = tf.shape(x_t)[0]\n",
    "            null_label = tf.fill([batch], self.n_classes)\n",
    "            c_emb = self.class_embed(null_label)\n",
    "\n",
    "        combined = tf.concat([x_t, t_emb + c_emb], axis=-1)\n",
    "        return self.net(combined)\n",
    "\n",
    "# 모델 생성 및 테스트\n",
    "model = CFGNoiseModel(data_dim=2, n_classes=4)\n",
    "\n",
    "# 더미 데이터\n",
    "batch = 8\n",
    "x_dummy = tf.random.normal((batch, 2))\n",
    "t_dummy = tf.constant([50] * batch, dtype=tf.int32)\n",
    "c_dummy = tf.constant([0, 1, 2, 3, 0, 1, 2, 3], dtype=tf.int32)\n",
    "\n",
    "# 비조건부 예측\n",
    "eps_uncond = model(x_dummy, t_dummy, class_label=None)\n",
    "print(f\"비조건부 예측 shape: {eps_uncond.shape}\")\n",
    "\n",
    "# 조건부 예측\n",
    "eps_cond = model(x_dummy, t_dummy, class_label=c_dummy)\n",
    "print(f\"조건부 예측 shape: {eps_cond.shape}\")\n",
    "\n",
    "# 학습 시 드롭 시뮬레이션\n",
    "eps_train = model(x_dummy, t_dummy, class_label=c_dummy, p_uncond=0.1, training=True)\n",
    "print(f\"학습 시 예측 shape: {eps_train.shape}\")\n",
    "\n",
    "total_params = sum(np.prod(w.shape) for w in model.trainable_weights)\n",
    "print(f\"\\n총 파라미터 수: {total_params:,}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  p_uncond = 0.1 → 학습 데이터의 10%는 비조건부로 학습됩니다.\")\n",
    "print(\"  비조건부 학습 없으면 (p_uncond=0), 모델이 epsilon(x_t, ∅)을 학습하지 않아\")\n",
    "print(\"  CFG 공식의 비조건부 항을 계산할 수 없습니다 → CFG 불가!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q2: CFG 공식 적용 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 CFG 공식을 구현하세요:\n",
    "\n",
    "$$\\tilde\\epsilon_\\theta(x_t, c) = \\epsilon_\\theta(x_t, \\varnothing) + w \\cdot [\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing)]$$\n",
    "\n",
    "주어진 모델에서 조건부와 비조건부 예측을 뽑고, guidance scale $w$로 결합하는 함수를 작성하세요.\n",
    "\n",
    "**여러분의 예측:**\n",
    "- $w = 1$일 때 결과는 `?`과 같다\n",
    "- $w = 0$일 때 결과는 `?`과 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: CFG 공식 적용\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def cfg_predict(model, x_t, t, class_label, guidance_scale):\n",
    "    # CFG 노이즈 예측\n",
    "    # 1. 비조건부 예측\n",
    "    eps_uncond = model(x_t, t, class_label=None)\n",
    "\n",
    "    # 2. 조건부 예측\n",
    "    eps_cond = model(x_t, t, class_label=class_label)\n",
    "\n",
    "    # 3. CFG 결합\n",
    "    eps_cfg = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "    return eps_cfg, eps_uncond, eps_cond\n",
    "\n",
    "# 테스트\n",
    "x_test = tf.random.normal((4, 2))\n",
    "t_test = tf.constant([100, 100, 100, 100], dtype=tf.int32)\n",
    "c_test = tf.constant([0, 1, 2, 3], dtype=tf.int32)\n",
    "\n",
    "print(f\"{'w':<5} | {'eps_cfg norm':>12} | {'eps_uncond와 같나':>18} | {'eps_cond와 같나':>16}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for w in [0.0, 0.5, 1.0, 3.0, 7.5]:\n",
    "    eps_cfg, eps_u, eps_c = cfg_predict(model, x_test, t_test, c_test, w)\n",
    "    norm = tf.reduce_mean(tf.norm(eps_cfg, axis=-1)).numpy()\n",
    "    same_as_uncond = tf.reduce_all(tf.abs(eps_cfg - eps_u) < 1e-5).numpy()\n",
    "    same_as_cond = tf.reduce_all(tf.abs(eps_cfg - eps_c) < 1e-5).numpy()\n",
    "    print(f\"w={w:<3.1f} | {norm:>12.4f} | {str(same_as_uncond):>18} | {str(same_as_cond):>16}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  w=0: CFG = eps_uncond (비조건부와 동일)\")\n",
    "print(\"  w=1: CFG = eps_cond (조건부와 동일)\")\n",
    "print(\"  w>1: 조건 방향을 과강조 (외삽)\")\n",
    "print(\"  CFG = (1-w)*uncond + w*cond 로 재작성 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q3: Guidance Scale 실험 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "2D 가우시안 혼합 데이터를 사용하여, 다양한 guidance scale에서 CFG 생성 결과를 비교하세요.\n",
    "\n",
    "4개의 클래스 중심:\n",
    "- 클래스 0: $(-2, -2)$\n",
    "- 클래스 1: $(2, -2)$\n",
    "- 클래스 2: $(-2, 2)$\n",
    "- 클래스 3: $(2, 2)$\n",
    "\n",
    "$w = [0, 1, 3, 7, 15]$에서 생성 결과를 시각화하고, 다양성과 정확도를 측정하세요.\n",
    "\n",
    "**여러분의 예측:** $w$가 커지면 생성된 점들이 클래스 중심에 `?` 해질 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: Guidance Scale 실험\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 4클래스 GMM 데이터\n",
    "class_centers = np.array([[-2, -2], [2, -2], [-2, 2], [2, 2]], dtype=np.float32)\n",
    "class_std = 0.5\n",
    "n_per_class = 200\n",
    "\n",
    "# 학습 데이터 생성\n",
    "np.random.seed(42)\n",
    "data_all = []\n",
    "labels_all = []\n",
    "for c in range(4):\n",
    "    pts = class_centers[c] + np.random.randn(n_per_class, 2).astype(np.float32) * class_std\n",
    "    data_all.append(pts)\n",
    "    labels_all.append(np.full(n_per_class, c))\n",
    "data_all = np.concatenate(data_all, axis=0)\n",
    "labels_all = np.concatenate(labels_all, axis=0)\n",
    "\n",
    "# Oracle CFG 시뮬레이션\n",
    "def oracle_cfg_sample(n_samples, target_class, guidance_scale, alpha_bars, class_centers, n_steps=100):\n",
    "    T_steps = len(alpha_bars)\n",
    "    step_indices = np.linspace(0, T_steps - 1, n_steps + 1, dtype=int)[::-1]\n",
    "\n",
    "    x = np.random.randn(n_samples, 2).astype(np.float32)\n",
    "\n",
    "    for i in range(len(step_indices) - 1):\n",
    "        t_c = step_indices[i]\n",
    "        t_p = step_indices[i + 1]\n",
    "        ab_c = alpha_bars[t_c]\n",
    "        ab_p = alpha_bars[t_p]\n",
    "\n",
    "        # 비조건부: 모든 클래스의 가중 평균 스코어\n",
    "        eps_uncond = np.zeros_like(x)\n",
    "        for ctr in class_centers:\n",
    "            eff_mu = np.sqrt(ab_c) * ctr\n",
    "            eff_var = ab_c * class_std**2 + (1 - ab_c)\n",
    "            eps_uncond += -(np.sqrt(1 - ab_c)) * (-(x - eff_mu) / eff_var) / len(class_centers)\n",
    "\n",
    "        # 조건부: 타겟 클래스의 스코어\n",
    "        ctr = class_centers[target_class]\n",
    "        eff_mu = np.sqrt(ab_c) * ctr\n",
    "        eff_var = ab_c * class_std**2 + (1 - ab_c)\n",
    "        eps_cond = -(np.sqrt(1 - ab_c)) * (-(x - eff_mu) / eff_var)\n",
    "\n",
    "        # CFG\n",
    "        eps_cfg = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        # DDIM step\n",
    "        x0_hat = (x - np.sqrt(1 - ab_c) * eps_cfg) / (np.sqrt(ab_c) + 1e-8)\n",
    "        x0_hat = np.clip(x0_hat, -8, 8)\n",
    "        x = np.sqrt(ab_p) * x0_hat + np.sqrt(1 - ab_p) * eps_cfg\n",
    "\n",
    "    return x\n",
    "\n",
    "# 다양한 w에서 실험\n",
    "w_values = [0.0, 1.0, 3.0, 7.0, 15.0]\n",
    "target_class = 0\n",
    "\n",
    "fig, axes = plt.subplots(1, len(w_values), figsize=(20, 4))\n",
    "\n",
    "diversity_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for idx, w in enumerate(w_values):\n",
    "    np.random.seed(42)\n",
    "    samples = oracle_cfg_sample(200, target_class, w, alpha_bars, class_centers, n_steps=50)\n",
    "\n",
    "    ax = axes[idx]\n",
    "    # 모든 클래스 중심 표시\n",
    "    for c, ctr in enumerate(class_centers):\n",
    "        color = 'red' if c == target_class else 'gray'\n",
    "        marker = '*' if c == target_class else 'o'\n",
    "        ax.scatter(*ctr, c=color, s=150, marker=marker, zorder=5, edgecolors='black')\n",
    "    ax.scatter(samples[:, 0], samples[:, 1], s=8, alpha=0.4, c='blue')\n",
    "\n",
    "    # 다양성: 생성 샘플의 표준편차\n",
    "    diversity = np.mean(np.std(samples, axis=0))\n",
    "    diversity_scores.append(diversity)\n",
    "\n",
    "    # 정확도: 타겟 클래스 중심과의 평균 거리\n",
    "    dist_to_target = np.mean(np.linalg.norm(samples - class_centers[target_class], axis=1))\n",
    "    accuracy_scores.append(dist_to_target)\n",
    "\n",
    "    ax.set_title(f'w = {w}', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'CFG 생성 결과 (클래스 {target_class} 타겟)', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/q3_cfg_guidance_experiment.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"그래프 저장됨: chapter13_genai_diffusion/practice/q3_cfg_guidance_experiment.png\")\n",
    "\n",
    "print(f\"\\n{'w':>5} | {'다양성(std)':>12} | {'타겟 거리':>12} | {'판정':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for w, div, acc in zip(w_values, diversity_scores, accuracy_scores):\n",
    "    verdict = \"비조건부\" if w == 0 else (\"균형\" if 1 <= w <= 5 else (\"고품질\" if acc < 1.5 else \"과강조\"))\n",
    "    print(f\"w={w:>3.0f} | {div:>12.3f} | {acc:>12.3f} | {verdict:>12}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  w 증가 → 타겟 클래스 중심으로 집중 (다양성↓, 정확도↑)\")\n",
    "print(\"  w가 너무 크면 생성 분포가 축소되어 모드 붕괴 위험\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q4: FID/IS 개념 이해 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "생성 모델의 품질 평가에 사용되는 두 가지 주요 지표를 이해하세요:\n",
    "\n",
    "**Inception Score (IS):**\n",
    "\n",
    "$$IS = \\exp\\!\\left(\\mathbb{E}_x\\left[D_{KL}(p(y|x) \\| p(y))\\right]\\right)$$\n",
    "\n",
    "- $p(y|x)$: 생성 이미지의 클래스 확신도 (sharp할수록 좋음)\n",
    "- $p(y)$: 전체 생성 이미지의 클래스 분포 (uniform할수록 좋음)\n",
    "\n",
    "**Fréchet Inception Distance (FID):**\n",
    "\n",
    "$$FID = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}\\!\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\right)$$\n",
    "\n",
    "- $(\\mu_r, \\Sigma_r)$: 실제 데이터의 Inception 특성 통계\n",
    "- $(\\mu_g, \\Sigma_g)$: 생성 데이터의 Inception 특성 통계\n",
    "\n",
    "**여러분의 예측:** FID는 낮을수록 좋나요, 높을수록 좋나요? `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: FID/IS 개념 이해\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 간소화된 FID 계산 (2D 가우시안으로 시뮬레이션)\n",
    "def compute_simple_fid(real_data, gen_data):\n",
    "    # 2D 데이터에서의 FID 근사\n",
    "    mu_r = np.mean(real_data, axis=0)\n",
    "    mu_g = np.mean(gen_data, axis=0)\n",
    "    sigma_r = np.cov(real_data.T)\n",
    "    sigma_g = np.cov(gen_data.T)\n",
    "\n",
    "    diff = mu_r - mu_g\n",
    "    mean_term = np.sum(diff ** 2)\n",
    "\n",
    "    # sqrt(Sigma_r @ Sigma_g) 계산\n",
    "    product = sigma_r @ sigma_g\n",
    "    eigenvalues = np.linalg.eigvals(product)\n",
    "    eigenvalues = np.maximum(eigenvalues.real, 0)\n",
    "    sqrt_product_trace = np.sum(np.sqrt(eigenvalues))\n",
    "\n",
    "    cov_term = np.trace(sigma_r) + np.trace(sigma_g) - 2 * sqrt_product_trace\n",
    "\n",
    "    return mean_term + cov_term\n",
    "\n",
    "def compute_simple_is(class_probs):\n",
    "    # 간소화된 IS\n",
    "    p_y_given_x = class_probs\n",
    "    p_y = np.mean(p_y_given_x, axis=0, keepdims=True)\n",
    "    kl_div = np.sum(p_y_given_x * (np.log(p_y_given_x + 1e-10) - np.log(p_y + 1e-10)), axis=1)\n",
    "    return np.exp(np.mean(kl_div))\n",
    "\n",
    "# 실제 데이터 (4클래스)\n",
    "real_data = data_all.copy()\n",
    "\n",
    "# 다양한 w에서 FID/IS 계산\n",
    "print(f\"\\n{'w':>5} | {'FID (↓좋음)':>12} | {'IS (↑좋음)':>10} | {'평가':>10}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "fid_values = []\n",
    "is_values = []\n",
    "\n",
    "for w in [0.0, 1.0, 3.0, 5.0, 7.0, 10.0, 15.0]:\n",
    "    np.random.seed(42)\n",
    "    # 각 클래스에서 균등하게 생성\n",
    "    gen_samples = []\n",
    "    class_probs_list = []\n",
    "    for c in range(4):\n",
    "        samples = oracle_cfg_sample(n_per_class, c, w, alpha_bars, class_centers, n_steps=50)\n",
    "        gen_samples.append(samples)\n",
    "\n",
    "        # 간소화된 클래스 확률 (거리 기반)\n",
    "        for s in samples:\n",
    "            dists = np.array([np.linalg.norm(s - ctr) for ctr in class_centers])\n",
    "            probs = np.exp(-dists) / np.sum(np.exp(-dists))\n",
    "            class_probs_list.append(probs)\n",
    "\n",
    "    gen_all = np.concatenate(gen_samples, axis=0)\n",
    "    class_probs_arr = np.array(class_probs_list)\n",
    "\n",
    "    fid = compute_simple_fid(real_data, gen_all)\n",
    "    is_score = compute_simple_is(class_probs_arr)\n",
    "\n",
    "    fid_values.append(fid)\n",
    "    is_values.append(is_score)\n",
    "\n",
    "    quality = \"낮음\" if fid > 5 else (\"보통\" if fid > 1 else \"높음\")\n",
    "    print(f\"w={w:>3.0f} | {fid:>12.3f} | {is_score:>10.3f} | {quality:>10}\")\n",
    "\n",
    "# 시각화\n",
    "w_plot = [0.0, 1.0, 3.0, 5.0, 7.0, 10.0, 15.0]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(w_plot, fid_values, 'bo-', lw=2.5, ms=8, label='FID (↓ 좋음)')\n",
    "ax1.set_xlabel('Guidance Scale (w)', fontsize=11)\n",
    "ax1.set_ylabel('FID', fontsize=11)\n",
    "ax1.set_title('FID vs Guidance Scale', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(w_plot, is_values, 'ro-', lw=2.5, ms=8, label='IS (↑ 좋음)')\n",
    "ax2.set_xlabel('Guidance Scale (w)', fontsize=11)\n",
    "ax2.set_ylabel('Inception Score', fontsize=11)\n",
    "ax2.set_title('IS vs Guidance Scale', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/q4_fid_is_metrics.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter13_genai_diffusion/practice/q4_fid_is_metrics.png\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  FID는 낮을수록 좋습니다 (생성≈실제)\")\n",
    "print(\"  IS는 높을수록 좋습니다 (다양하면서도 뚜렷한 클래스)\")\n",
    "print(\"  w↑ → IS↑ (클래스 확신도 증가) but 과도하면 다양성↓\")\n",
    "print(\"  실무에서는 FID를 더 신뢰합니다 (IS는 모드 붕괴 감지 못함)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 종합 도전: Mini CFG Generation Pipeline <a name='bonus'></a>\n",
    "\n",
    "### 미니 구현\n",
    "\n",
    "완전한 CFG 파이프라인을 구축하세요:\n",
    "1. 조건부/비조건부 통합 모델 학습 (2D GMM 데이터)\n",
    "2. CFG를 적용한 DDIM 샘플러로 생성\n",
    "3. 각 클래스별 생성 결과 + 다양한 $w$의 효과를 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이 ──────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"종합 도전: Mini CFG Generation Pipeline\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 간단한 학습 루프\n",
    "cfg_model = CFGNoiseModel(data_dim=2, n_classes=4, time_dim=32, hidden_dim=128)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# 학습 데이터 텐서\n",
    "x0_tf = tf.constant(data_all, dtype=tf.float32)\n",
    "labels_tf = tf.constant(labels_all, dtype=tf.int32)\n",
    "alpha_bars_tf = tf.constant(alpha_bars, dtype=tf.float32)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 256\n",
    "p_uncond = 0.1\n",
    "losses = []\n",
    "\n",
    "print(\"모델 학습 시작...\")\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.random.permutation(len(data_all))[:batch_size]\n",
    "    x0_batch = tf.gather(x0_tf, indices)\n",
    "    c_batch = tf.gather(labels_tf, indices)\n",
    "\n",
    "    t_batch = tf.random.uniform([batch_size], 0, T, dtype=tf.int32)\n",
    "    ab_batch = tf.gather(alpha_bars_tf, t_batch)\n",
    "\n",
    "    eps = tf.random.normal(tf.shape(x0_batch))\n",
    "    x_t = tf.sqrt(tf.reshape(ab_batch, [-1, 1])) * x0_batch + \\\n",
    "          tf.sqrt(1 - tf.reshape(ab_batch, [-1, 1])) * eps\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        eps_pred = cfg_model(x_t, t_batch, class_label=c_batch, p_uncond=p_uncond, training=True)\n",
    "        loss = tf.reduce_mean((eps - eps_pred) ** 2)\n",
    "\n",
    "    grads = tape.gradient(loss, cfg_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, cfg_model.trainable_variables))\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f\"  Epoch {epoch+1:>3}/{n_epochs}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"학습 완료! 최종 Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "# CFG + DDIM 샘플링\n",
    "def cfg_ddim_sample(model, n_samples, target_class, guidance_scale, alpha_bars_np, n_steps=50):\n",
    "    step_indices = np.linspace(0, len(alpha_bars_np) - 1, n_steps + 1, dtype=int)[::-1]\n",
    "    x = tf.random.normal((n_samples, 2))\n",
    "    c_label = tf.fill([n_samples], target_class)\n",
    "\n",
    "    for i in range(len(step_indices) - 1):\n",
    "        tc, tp = step_indices[i], step_indices[i + 1]\n",
    "        abc = alpha_bars_np[tc]\n",
    "        abp = alpha_bars_np[tp]\n",
    "\n",
    "        t_tensor = tf.fill([n_samples], tc)\n",
    "\n",
    "        eps_uncond = model(x, t_tensor, class_label=None)\n",
    "        eps_cond = model(x, t_tensor, class_label=c_label)\n",
    "        eps_cfg = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        x0_hat = (x - tf.sqrt(1 - abc) * eps_cfg) / (tf.sqrt(abc) + 1e-8)\n",
    "        x0_hat = tf.clip_by_value(x0_hat, -8, 8)\n",
    "        x = tf.sqrt(abp) * x0_hat + tf.sqrt(1 - abp) * eps_cfg\n",
    "\n",
    "    return x.numpy()\n",
    "\n",
    "# 전체 결과 시각화\n",
    "fig, axes = plt.subplots(2, 5, figsize=(22, 8))\n",
    "\n",
    "w_test_values = [0.0, 1.0, 3.0, 7.0, 15.0]\n",
    "\n",
    "for row, target_cls in enumerate([0, 3]):\n",
    "    for col, w_val in enumerate(w_test_values):\n",
    "        tf.random.set_seed(42)\n",
    "        samples = cfg_ddim_sample(cfg_model, 200, target_cls, w_val, alpha_bars, n_steps=50)\n",
    "\n",
    "        ax = axes[row, col]\n",
    "        for c, ctr in enumerate(class_centers):\n",
    "            color = 'red' if c == target_cls else 'lightgray'\n",
    "            ax.scatter(*ctr, c=color, s=120, marker='*', zorder=5, edgecolors='black')\n",
    "        ax.scatter(samples[:, 0], samples[:, 1], s=5, alpha=0.4, c='blue')\n",
    "        ax.set_xlim(-5, 5)\n",
    "        ax.set_ylim(-5, 5)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        if row == 0:\n",
    "            ax.set_title(f'w = {w_val}', fontweight='bold', fontsize=12)\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f'클래스 {target_cls}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Mini CFG Pipeline: 클래스별 × Guidance Scale별 생성 결과',\n",
    "             fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/bonus_cfg_pipeline.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"그래프 저장됨: chapter13_genai_diffusion/practice/bonus_cfg_pipeline.png\")\n",
    "\n",
    "# 학습 곡선\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "ax.plot(losses, 'b-', alpha=0.3, lw=0.5)\n",
    "window = 10\n",
    "smoothed = np.convolve(losses, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(losses)), smoothed, 'r-', lw=2, label=f'{window}-epoch 이동평균')\n",
    "ax.set_xlabel('Epoch', fontsize=11)\n",
    "ax.set_ylabel('MSE Loss', fontsize=11)\n",
    "ax.set_title('CFG 모델 학습 곡선', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/bonus_training_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"그래프 저장됨: chapter13_genai_diffusion/practice/bonus_training_curve.png\")\n",
    "\n",
    "print(\"\\n[결론]\")\n",
    "print(\"  1. 조건 드롭(p_uncond=0.1)이 CFG의 핵심 학습 전략입니다\")\n",
    "print(\"  2. w=0: 비조건부 → w=1: 표준 조건부 → w>1: 조건 과강조\")\n",
    "print(\"  3. 실제 Stable Diffusion은 w=7.5, DALL-E 3는 동적 w를 사용합니다\")\n",
    "print(\"  4. CFG는 추가 파라미터 없이 단순한 추론 기법으로 생성 품질을 크게 향상시킵니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}