{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-02-01",
   "metadata": {},
   "source": [
    "# Chapter 08-02: TensorBoard 시각화\n",
    "\n",
    "## 학습 목표\n",
    "- TensorBoard를 설정하고 실행하는 방법을 익힌다\n",
    "- 콜백을 활용한 자동 학습 로깅을 구현한다\n",
    "- `tf.summary` API로 스칼라, 이미지, 히스토그램을 직접 기록한다\n",
    "- 커스텀 학습 루프에서 TensorBoard를 활용한다\n",
    "\n",
    "## 목차\n",
    "1. TensorBoard 실행 방법\n",
    "2. TensorBoard 콜백 자동 로깅\n",
    "3. `tf.summary.scalar` 직접 사용\n",
    "4. `tf.summary.image` - 이미지 로깅\n",
    "5. `tf.summary.histogram` - 가중치 분포\n",
    "6. 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "\n",
    "# 로그 디렉토리 기본 경로 설정\n",
    "LOG_BASE_DIR = '/tmp/tb_logs'\n",
    "os.makedirs(LOG_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# MNIST 데이터셋 로드 (실습 공통 사용)\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test  = X_test.astype('float32') / 255.0\n",
    "# 채널 차원 추가\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test  = X_test[..., np.newaxis]\n",
    "print(f\"학습 데이터: {X_train.shape}, 레이블: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-03",
   "metadata": {},
   "source": [
    "## 1. TensorBoard 실행 방법\n",
    "\n",
    "### Jupyter Notebook에서 인라인 실행\n",
    "\n",
    "```python\n",
    "# Jupyter 매직 명령어로 TensorBoard 로드 및 실행\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "```\n",
    "\n",
    "### 터미널에서 별도 실행\n",
    "\n",
    "```bash\n",
    "# 터미널에서 TensorBoard 서버 시작 (기본 포트 6006)\n",
    "tensorboard --logdir ./logs\n",
    "\n",
    "# 포트 지정\n",
    "tensorboard --logdir ./logs --port 6007\n",
    "```\n",
    "\n",
    "브라우저에서 `http://localhost:6006` 접속하면 대시보드를 확인할 수 있다.\n",
    "\n",
    "### 로그 디렉토리 구조 권장 패턴\n",
    "\n",
    "```\n",
    "logs/\n",
    "├── run1/           ← 첫 번째 실험\n",
    "│   ├── train/\n",
    "│   └── validation/\n",
    "└── run2/           ← 두 번째 실험 (하이퍼파라미터 변경)\n",
    "    ├── train/\n",
    "    └── validation/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-04-md",
   "metadata": {},
   "source": [
    "## 2. TensorBoard 콜백으로 자동 로깅\n",
    "\n",
    "`tf.keras.callbacks.TensorBoard`는 학습 루프에 자동으로 통합되어\n",
    "손실, 정확도, 그래프, 가중치 히스토그램 등을 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 CNN 모델 생성\n",
    "def build_cnn():\n",
    "    \"\"\"MNIST 분류용 간단한 CNN 모델\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = build_cnn()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 타임스탬프로 고유한 로그 디렉토리 생성\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = os.path.join(LOG_BASE_DIR, f'callback_{timestamp}')\n",
    "\n",
    "# TensorBoard 콜백 설정\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1,      # 매 에포크마다 가중치 히스토그램 기록\n",
    "    write_graph=True,      # 모델 그래프 기록\n",
    "    write_images=True,     # 가중치를 이미지로 시각화\n",
    "    update_freq='epoch',   # 에포크 단위로 로그 업데이트\n",
    "    profile_batch=0,       # 프로파일링 비활성화 (성능 오버헤드 방지)\n",
    ")\n",
    "\n",
    "# 학습 실행 (소량 데이터로 빠른 실습)\n",
    "history = model.fit(\n",
    "    X_train[:5000], y_train[:5000],\n",
    "    validation_data=(X_test[:1000], y_test[:1000]),\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    callbacks=[tb_callback],\n",
    "    verbose=1\n",
    ")\n",
    "print(f\"\\nTensorBoard 로그 위치: {log_dir}\")\n",
    "print(\"터미널에서 실행: tensorboard --logdir\", LOG_BASE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-05-md",
   "metadata": {},
   "source": [
    "## 3. `tf.summary.scalar` - 커스텀 학습 루프에서 직접 기록\n",
    "\n",
    "`model.fit`을 사용하지 않는 커스텀 학습 루프에서 메트릭을 직접 기록한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 학습 루프용 모델 및 옵티마이저 초기화\n",
    "custom_model = build_cnn()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# summary writer 생성 (train / validation 분리)\n",
    "custom_log_dir = os.path.join(LOG_BASE_DIR, f'custom_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "train_writer = tf.summary.create_file_writer(os.path.join(custom_log_dir, 'train'))\n",
    "val_writer   = tf.summary.create_file_writer(os.path.join(custom_log_dir, 'validation'))\n",
    "\n",
    "# tf.data 데이터셋 구성\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train[:2000], y_train[:2000]))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(64)\n",
    "\n",
    "# 커스텀 학습 루프 (2 에포크)\n",
    "for epoch in range(2):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = custom_model(x_batch, training=True)\n",
    "            loss = loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, custom_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, custom_model.trainable_variables))\n",
    "        train_acc_metric.update_state(y_batch, logits)\n",
    "        epoch_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    train_acc = train_acc_metric.result().numpy()\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # tf.summary.scalar로 학습 메트릭 기록\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_acc, step=epoch)\n",
    "\n",
    "    # 검증 메트릭 계산 및 기록\n",
    "    val_logits = custom_model(X_test[:500], training=False)\n",
    "    val_loss = loss_fn(y_test[:500], val_logits).numpy()\n",
    "    val_acc = tf.keras.metrics.sparse_categorical_accuracy(y_test[:500], val_logits)\n",
    "    val_acc = tf.reduce_mean(val_acc).numpy()\n",
    "\n",
    "    with val_writer.as_default():\n",
    "        tf.summary.scalar('loss', val_loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', val_acc, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}, acc={train_acc:.4f} | val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "print(f\"\\n커스텀 루프 로그 위치: {custom_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-06-md",
   "metadata": {},
   "source": [
    "## 4. `tf.summary.image` - 예측 결과 이미지 로깅\n",
    "\n",
    "예측 결과나 중간 특징 맵을 이미지로 TensorBoard에 기록하면\n",
    "모델이 무엇을 학습하고 있는지 직관적으로 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')  # 비대화형 백엔드 (서버 환경)\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "# 이미지 로거용 summary writer 생성\n",
    "img_log_dir = os.path.join(LOG_BASE_DIR, f'images_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "img_writer = tf.summary.create_file_writer(img_log_dir)\n",
    "\n",
    "def plot_predictions_to_image(images, true_labels, pred_labels, n=6):\n",
    "    \"\"\"예측 결과를 matplotlib 그림으로 그린 뒤 PNG 바이트로 반환\"\"\"\n",
    "    fig, axes = plt.subplots(1, n, figsize=(12, 2))\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(images[i, :, :, 0], cmap='gray')\n",
    "        color = 'green' if true_labels[i] == pred_labels[i] else 'red'\n",
    "        axes[i].set_title(f\"실제:{true_labels[i]}\\n예측:{pred_labels[i]}\", color=color, fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 그림을 PNG 바이트로 변환\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight')\n",
    "    buf.seek(0)\n",
    "    plt.close(fig)\n",
    "    return buf\n",
    "\n",
    "# 샘플 이미지로 예측 수행\n",
    "sample_images = X_test[:6]\n",
    "sample_labels = y_test[:6]\n",
    "preds = model.predict(sample_images, verbose=0)\n",
    "pred_labels = np.argmax(preds, axis=1)\n",
    "\n",
    "# matplotlib 그림 → TensorBoard 이미지 텐서 변환\n",
    "buf = plot_predictions_to_image(sample_images, sample_labels, pred_labels)\n",
    "image_tensor = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "image_tensor = tf.expand_dims(image_tensor, 0)  # 배치 차원 추가\n",
    "\n",
    "# TensorBoard에 이미지 기록\n",
    "with img_writer.as_default():\n",
    "    tf.summary.image('예측_결과_샘플', image_tensor, step=0)\n",
    "\n",
    "# 원시 입력 이미지를 직접 기록 (배치 형태여야 함)\n",
    "with img_writer.as_default():\n",
    "    tf.summary.image('입력_이미지', sample_images[:4], step=0, max_outputs=4)\n",
    "\n",
    "print(f\"이미지 로그 저장 완료: {img_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-07-md",
   "metadata": {},
   "source": [
    "## 5. `tf.summary.histogram` - 가중치 분포 로깅\n",
    "\n",
    "가중치와 그래디언트의 분포를 에포크별로 기록하면\n",
    "기울기 소실(Vanishing Gradient)이나 폭발(Exploding Gradient) 문제를 조기에 발견할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02-07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히스토그램 로거용 summary writer\n",
    "hist_log_dir = os.path.join(LOG_BASE_DIR, f'histogram_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "hist_writer = tf.summary.create_file_writer(hist_log_dir)\n",
    "\n",
    "# 간단한 Dense 모델로 히스토그램 시연\n",
    "hist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,), name='dense_1'),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='dense_2'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='output')\n",
    "])\n",
    "hist_optimizer = tf.keras.optimizers.Adam()\n",
    "hist_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# 데이터 평탄화 (Dense 모델용)\n",
    "X_flat_train = X_train[:2000].reshape(-1, 784)\n",
    "X_flat_test  = X_test[:500].reshape(-1, 784)\n",
    "\n",
    "# 히스토그램을 기록하는 커스텀 학습 루프\n",
    "for epoch in range(3):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_flat_train, y_train[:2000]))\n",
    "    dataset = dataset.batch(128)\n",
    "\n",
    "    for x_batch, y_batch in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = hist_model(x_batch, training=True)\n",
    "            loss = hist_loss_fn(y_batch, logits)\n",
    "        grads = tape.gradient(loss, hist_model.trainable_variables)\n",
    "        hist_optimizer.apply_gradients(zip(grads, hist_model.trainable_variables))\n",
    "\n",
    "    # 에포크마다 가중치 및 그래디언트 히스토그램 기록\n",
    "    with hist_writer.as_default():\n",
    "        for var, grad in zip(hist_model.trainable_variables, grads):\n",
    "            # 가중치 분포\n",
    "            tf.summary.histogram(f'weights/{var.name}', var, step=epoch)\n",
    "            # 그래디언트 분포 (None이 아닌 경우에만)\n",
    "            if grad is not None:\n",
    "                tf.summary.histogram(f'gradients/{var.name}', grad, step=epoch)\n",
    "        # 에포크 손실도 함께 기록\n",
    "        tf.summary.scalar('epoch_loss', loss, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/3 완료 | 마지막 배치 손실: {loss.numpy():.4f}\")\n",
    "\n",
    "print(f\"\\n히스토그램 로그 저장 완료: {hist_log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-02-08",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### TensorBoard 주요 기능 요약\n",
    "\n",
    "| 기능 | API | 확인 탭 |\n",
    "|------|-----|----------|\n",
    "| 손실/정확도 곡선 | 콜백 or `tf.summary.scalar` | Scalars |\n",
    "| 모델 구조 그래프 | 콜백 `write_graph=True` | Graphs |\n",
    "| 가중치 분포 | 콜백 `histogram_freq=1` or `tf.summary.histogram` | Histograms, Distributions |\n",
    "| 이미지 시각화 | `tf.summary.image` | Images |\n",
    "| 텍스트 로깅 | `tf.summary.text` | Text |\n",
    "| 하이퍼파라미터 비교 | `tf.summary.experimental.set_step` + HParams 플러그인 | HParams |\n",
    "\n",
    "### 권장 워크플로우\n",
    "\n",
    "```python\n",
    "# 1. 타임스탬프 기반 로그 디렉토리 (실험 구분)\n",
    "log_dir = f'logs/{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\n",
    "# 2. 콜백 설정\n",
    "tb_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# 3. 학습\n",
    "model.fit(..., callbacks=[tb_cb])\n",
    "\n",
    "# 4. 터미널에서 실행\n",
    "# tensorboard --logdir logs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
