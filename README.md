# TensorFlow í•™ìŠµ ì»¤ë¦¬í˜ëŸ¼

> **í™˜ê²½**: Python 3.11 + TensorFlow 2.x (Apple Silicon: tensorflow-macos + tensorflow-metal)
> **í˜•ì‹**: Jupyter Notebook (.ipynb) â€” ì´ë¡  + ìˆ˜í•™ + ì½”ë“œ + ì¶œë ¥ í†µí•©
> **ìˆ˜í•™ ì›ì¹™**: ê° ì±•í„° í•´ë‹¹ ìˆ˜ì¤€ì˜ ì—°ì‚° ë‹¨ìœ„ ìˆ˜ì‹ í¬í•¨. ì¦ëª…ì€ ìƒëµí•˜ê³  ìˆ˜ì‹ì˜ ì˜ë¯¸Â·íë¦„ì— ì§‘ì¤‘.

---

## í™˜ê²½ ì„¤ì •

### Conda í™˜ê²½ ìƒì„± (Apple Silicon Mac)

```bash
# í™˜ê²½ ìƒì„±
conda env create -f environment.yml

# í™˜ê²½ í™œì„±í™”
conda activate tf_study

# VSCode Jupyter ì»¤ë„ ë“±ë¡
python -m ipykernel install --user --name tf_study --display-name "Python (tf_study)"
```

### ì„¤ì¹˜ í™•ì¸

```bash
python main.py
# ì¶œë ¥ ì˜ˆì‹œ: tensorflow_version : 2.x.x
```

VSCodeì—ì„œ ë…¸íŠ¸ë¶ ì—´ê¸° â†’ ìš°ì¸¡ ìƒë‹¨ ì»¤ë„ ì„ íƒ â†’ **Python (tf_study)** ì„ íƒ

---

## ì „ì²´ ì»¤ë¦¬í˜ëŸ¼ ì§€ë„

```
chapter01_basics/              â† ì‹œì‘ì : Tensorì™€ ìë™ë¯¸ë¶„
chapter02_keras_api/           â† ëª¨ë¸ êµ¬ì„± 3ê°€ì§€ ë°©ì‹
chapter03_training_mechanics/  â† í•™ìŠµ ì›ë¦¬ì™€ ì œì–´
chapter04_data_pipeline/       â† ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬
chapter05_computer_vision/     â† CNNê³¼ ì´ë¯¸ì§€ í•™ìŠµ
chapter06_nlp/                 â† í…ìŠ¤íŠ¸ì™€ ì‹œê³„ì—´ í•™ìŠµ
chapter07_advanced_architectures/ â† Transformer, VAE, GAN
chapter08_production/          â† ëª¨ë¸ ë°°í¬ì™€ ìµœì í™”
chapter09_distributed_systems/ â† [Advanced] ë¶„ì‚° ì‹œìŠ¤í…œê³¼ ë³‘ë ¬ ì²˜ë¦¬
chapter10_memory_optimization/ â† [Advanced] ëŒ€ê·œëª¨ ëª¨ë¸ ë©”ëª¨ë¦¬ ìµœì í™” (ZeRO/Offload)
chapter11_custom_kernels/      â† [Advanced] ì»¤ìŠ¤í…€ GPU ì»¤ë„ í”„ë¡œê·¸ë˜ë° (CUDA/Triton)
chapter12_modern_llms/         â† [Advanced] ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Llama/MoE)
chapter13_genai_diffusion/     â† [Advanced] ìƒì„± AI ì‹¬í™” (Diffusion/SDE)
chapter14_extreme_inference/   â† [Advanced] ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” (PagedAttention/AWQ)
chapter15_alignment_rlhf/      â† [Advanced] AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ (RLHF/DPO)
chapter16_sparse_attention/    â† [State-of-the-Art] ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± (DeepSeek/Qwen)
chapter17_diffusion_transformers/ â† [State-of-the-Art] ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT (Sora/Hunyuan)
projects/                      â† ì¢…í•© ì‹¤ì „ í”„ë¡œì íŠ¸ (Basic ~ SOTA)
```

---

## Chapter 01 â€” TensorFlow ê¸°ì´ˆ

**ë””ë ‰í† ë¦¬**: `chapter01_basics/`

**í•™ìŠµ ëª©í‘œ**

- TensorFlow 2.xì˜ í•µì‹¬ ìë£Œêµ¬ì¡° Tensorë¥¼ ì´í•´í•œë‹¤
- ìë™ ë¯¸ë¶„(Automatic Differentiation)ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  í™œìš©í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| í–‰ë ¬ ê³± | $C_{ij} = \sum_k A_{ik} B_{kj}$ |
| í¸ë¯¸ë¶„ | $\frac{\partial f}{\partial x}$ |
| ì—°ì‡„ ë²•ì¹™ | $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w}$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_tensorflow_intro.ipynb` | TF 2.x ì² í•™, Eager Execution, ë””ë°”ì´ìŠ¤ í™•ì¸ |
| `02_tensors_operations.ipynb` | Tensor rank/shape/dtype, ì¸ë±ì‹±, ìˆ˜í•™ ì—°ì‚°, NumPy ì—°ë™ |
| `03_automatic_differentiation.ipynb` | GradientTape, ì„ í˜•íšŒê·€ ìˆ˜ë™ êµ¬í˜„ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_tensor_quiz.ipynb` | Tensor ì¡°ì‘ í€´ì¦ˆ 5ë¬¸ì œ, GradientTape ê¸°ìš¸ê¸° ê³„ì‚° |

**ì£¼ìš” í‚¤ì›Œë“œ**: `tf.Tensor`, `tf.Variable`, `tf.constant`, `tf.GradientTape`, `@tf.function`

---

## Chapter 02 â€” Keras API 3ì¢…

**ë””ë ‰í† ë¦¬**: `chapter02_keras_api/`

**í•™ìŠµ ëª©í‘œ**

- Sequential, Functional, Subclassing ì„¸ ê°€ì§€ ëª¨ë¸ êµ¬ì„± ë°©ì‹ê³¼ ê°ê°ì˜ ì‚¬ìš© ì‹œì ì„ ì´í•´í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| ì™„ì „ ì—°ê²° ë ˆì´ì–´ | $\mathbf{y} = f(\mathbf{W}\mathbf{x} + \mathbf{b})$ |
| íŒŒë¼ë¯¸í„° ìˆ˜ (Dense) | $n_{in} \times n_{out} + n_{out}$ |
| ReLU | $f(x) = \max(0, x)$ |
| Sigmoid | $\sigma(x) = \frac{1}{1 + e^{-x}}$ |
| Softmax | $\sigma(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$ |
| Tanh | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_sequential_api.ipynb` | Sequential ëª¨ë¸, MNIST ì†ê¸€ì”¨ ë¶„ë¥˜ |
| `02_functional_api.ipynb` | ë‹¤ì¤‘ ì…ì¶œë ¥, ì”ì°¨ ì—°ê²° (Residual Connection) |
| `03_subclassing_api.ipynb` | `tf.keras.Model` ìƒì†, ì»¤ìŠ¤í…€ ë ˆì´ì–´ |
| `04_layers_and_activations.ipynb` | Dense/Dropout/BN/LN, í™œì„±í™” í•¨ìˆ˜ ë¹„êµ ì‹œê°í™” |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_build_your_first_model.ipynb` | Fashion MNISTë¥¼ 3ê°€ì§€ APIë¡œ êµ¬í˜„, summary ë¹„êµ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `keras.Sequential`, `keras.Model`, `keras.Input`, `model.summary()`

---

## Chapter 03 â€” í•™ìŠµ ë©”ì¹´ë‹ˆì¦˜

**ë””ë ‰í† ë¦¬**: `chapter03_training_mechanics/`

**í•™ìŠµ ëª©í‘œ**

- ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ì½œë°±ì„ ì´í•´í•˜ê³  ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„ë¥¼ ì§ì ‘ êµ¬í˜„í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| MSE | $L = \frac{1}{N}\sum_{i=1}^N (\hat{y}_i - y_i)^2$ |
| Binary Cross-Entropy | $L = -[y \log p + (1-y)\log(1-p)]$ |
| Categorical Cross-Entropy | $L = -\sum_i y_i \log p_i$ |
| SGD ì—…ë°ì´íŠ¸ | $\theta \leftarrow \theta - \eta \nabla_\theta L(\theta)$ |
| Adam (1ì°¨ ëª¨ë©˜íŠ¸) | $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$ |
| Adam (2ì°¨ ëª¨ë©˜íŠ¸) | $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$ |
| Adam ì—…ë°ì´íŠ¸ | $\theta \leftarrow \theta - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ |
| L2 ì •ê·œí™” | $L_{total} = L_{task} + \lambda \sum_j w_j^2$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_loss_functions.ipynb` | MSE/MAE/Huber/Cross-Entropy, ì»¤ìŠ¤í…€ ì†ì‹¤ |
| `02_optimizers.ipynb` | SGD/Adam/AdamW, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬, ìˆ˜ë ´ ë¹„êµ ì‹œê°í™” |
| `03_metrics_and_evaluation.ipynb` | Accuracy/Precision/Recall/F1, ROC, Confusion Matrix |
| `04_callbacks.ipynb` | ModelCheckpoint, EarlyStopping, TensorBoard, ì»¤ìŠ¤í…€ ì½œë°± |
| `05_custom_training_loop.ipynb` | GradientTape ìˆ˜ë™ ë£¨í”„, `@tf.function` ìµœì í™” |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_optimizer_comparison.ipynb` | ë™ì¼ ëª¨ë¸ì— SGD/Adam/RMSprop ì ìš© í›„ ìˆ˜ë ´ ë¹„êµ |
| `practice/ex02_custom_callback.ipynb` | í•™ìŠµë¥  ì‹œê°í™” ì»¤ìŠ¤í…€ ì½œë°± êµ¬í˜„ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `compile`, `fit`, `GradientTape`, `EarlyStopping`, `@tf.function`

---

## Chapter 04 â€” ë°ì´í„° íŒŒì´í”„ë¼ì¸

**ë””ë ‰í† ë¦¬**: `chapter04_data_pipeline/`

**í•™ìŠµ ëª©í‘œ**

- `tf.data` APIë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| Min-Max ì •ê·œí™” | $x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}$ |
| Z-score í‘œì¤€í™” | $x' = \frac{x - \mu}{\sigma}$ |
| 2D íšŒì „ í–‰ë ¬ | $R(\theta) = \begin{bmatrix}\cos\theta & -\sin\theta \\ \sin\theta & \cos\theta\end{bmatrix}$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_tf_data_api.ipynb` | Dataset ìƒì„±, map/filter/batch/shuffle/prefetch, AUTOTUNE |
| `02_data_augmentation.ipynb` | RandomFlip/Rotation/Zoom, ì¦ê°• ê²°ê³¼ ì‹œê°í™” |
| `03_tfrecord_format.ipynb` | TFRecord ìƒì„±Â·ì½ê¸°, `tf.train.Example` |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_build_data_pipeline.ipynb` | ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ â†’ ì „ì²˜ë¦¬ â†’ ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ ì™„ì„± |

**ì£¼ìš” í‚¤ì›Œë“œ**: `tf.data.Dataset`, `.map()`, `.batch()`, `.prefetch()`, `tf.data.AUTOTUNE`

---

## Chapter 05 â€” ì»´í“¨í„° ë¹„ì „

**ë””ë ‰í† ë¦¬**: `chapter05_computer_vision/`

**í•™ìŠµ ëª©í‘œ**

- CNNì˜ í•©ì„±ê³± ì—°ì‚° ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ , ì „ì´í•™ìŠµìœ¼ë¡œ ì‹¤ì œ ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| 2D í•©ì„±ê³± | $(I * K)[i,j] = \sum_m \sum_n I[i+m,\, j+n] \cdot K[m,n]$ |
| ì¶œë ¥ í¬ê¸° | $O = \left\lfloor \frac{I - K + 2P}{S} \right\rfloor + 1$ |
| íŒŒë¼ë¯¸í„° ìˆ˜ (Conv2D) | $K_h \times K_w \times C_{in} \times C_{out} + C_{out}$ |
| FLOPs (Conv2D) | $2 \times K_h K_w \times C_{in} \times C_{out} \times H_{out} \times W_{out}$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_cnn_basics.ipynb` | í•©ì„±ê³± ì›ë¦¬Â·ì‹œê°í™”, Padding/Stride, Pooling, íŠ¹ì§• ë§µ ì‹œê°í™” |
| `02_cnn_architectures.ipynb` | LeNet â†’ VGG â†’ ResNet (Skip Connection) â†’ ConvNeXt, `tf.keras.applications` |
| `03_transfer_learning.ipynb` | Feature Extraction vs Fine-Tuning, ë‹¨ê³„ë³„ ë™ê²° í•´ì œ |
| `04_object_detection_intro.ipynb` | ë¶„ë¥˜/íƒì§€/ë¶„í•  ê°œë…, IoU/mAP, TF Hub ì¶”ë¡  ì˜ˆì‹œ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_cifar10_classifier.ipynb` | CIFAR-10 CNN êµ¬í˜„, í•™ìŠµ ê³¡ì„  ì‹œê°í™” |
| `practice/ex02_transfer_learning_flowers.ipynb` | Flowers ë°ì´í„°ì…‹, **EfficientNetV2** ì „ì´í•™ìŠµ _(êµ¬ë²„ì „ EfficientNetB0 ëŒ€ì²´; 2026-02-25 ê¸°ì¤€)_ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `Conv2D`, `MaxPooling2D`, `GlobalAveragePooling2D`, `tf.keras.applications`, `EfficientNetV2`, `ConvNeXt`, `layer.trainable`

---

## Chapter 06 â€” ìì—°ì–´ ì²˜ë¦¬ (NLP)

**ë””ë ‰í† ë¦¬**: `chapter06_nlp/`

**í•™ìŠµ ëª©í‘œ**

- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬, ì„ë² ë”©, RNN ê³„ì—´ ëª¨ë¸ì„ ì´í•´í•˜ê³  í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ë¥¼ êµ¬í˜„í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | $\cos(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}$ |
| RNN ìƒíƒœ ì—…ë°ì´íŠ¸ | $h_t = \tanh(W_h h_{t-1} + W_x x_t + b)$ |
| LSTM forget gate | $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$ |
| LSTM input gate | $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$ |
| LSTM cell update | $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$ |
| LSTM output | $h_t = o_t \odot \tanh(C_t)$ |
| GRU update gate | $z_t = \sigma(W_z [h_{t-1}, x_t])$ |
| GRU ìµœì¢… ì¶œë ¥ | $h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_text_preprocessing.ipynb` | í† í°í™”, TextVectorization, íŒ¨ë”©Â·ë§ˆìŠ¤í‚¹, í•œêµ­ì–´ í˜•íƒœì†Œ ê°œìš” |
| `02_word_embeddings.ipynb` | One-Hot í•œê³„, Embedding ë ˆì´ì–´, GloVe/FastText ë¡œë“œ, t-SNE ì‹œê°í™” |
| `03_rnn_lstm_gru.ipynb` | RNN ê¸°ìš¸ê¸° ì†Œì‹¤, LSTM ê²Œì´íŠ¸, GRU, ì–‘ë°©í–¥ RNN |
| `04_text_classification.ipynb` | ê°ì„± ë¶„ì„: CNN/LSTM/GRU/BiLSTM ë¹„êµ |
| `05_sequence_to_sequence.ipynb` | Encoder-Decoder êµ¬ì¡°, Attention ì†Œê°œ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_sentiment_analysis.ipynb` | IMDB ë°ì´í„°, Embedding+LSTM ê°ì„± ë¶„ë¥˜ |
| `practice/ex02_korean_text_classification.ipynb` | NSMC(ë„¤ì´ë²„ ì˜í™” ë¦¬ë·°), KoNLPy í˜•íƒœì†Œ ë¶„ì„ + ë¶„ë¥˜ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `TextVectorization`, `Embedding`, `LSTM`, `GRU`, `Bidirectional`, `return_sequences`

---

## Chapter 07 â€” ê³ ê¸‰ ì•„í‚¤í…ì²˜

**ë””ë ‰í† ë¦¬**: `chapter07_advanced_architectures/`

**í•™ìŠµ ëª©í‘œ**

- Transformerì˜ Attention ë©”ì¹´ë‹ˆì¦˜ì„ ì´í•´í•˜ê³ , ìƒì„± ëª¨ë¸(VAE, GAN)ì˜ ì›ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| Scaled Dot-Product Attention | $\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^T}{\sqrt{d_k}}\right)V$ |
| Multi-Head Attention | $\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\ldots,\text{head}_h)\,W^O$ |
| Positional Encoding (sin) | $PE_{(pos,\,2i)} = \sin\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)$ |
| Positional Encoding (cos) | $PE_{(pos,\,2i+1)} = \cos\!\left(\frac{pos}{10000^{2i/d_{model}}}\right)$ |
| VAE ELBO | $\mathcal{L} = \mathbb{E}[\log p(x\|z)] - D_{KL}(q(z\|x) \| p(z))$ |
| GAN ëª©ì  í•¨ìˆ˜ | $\min_G \max_D\; \mathbb{E}[\log D(x)] + \mathbb{E}[\log(1 - D(G(z)))]$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_attention_mechanism.ipynb` | Bahdanau Attention, Self-Attention, Attention Weight ì‹œê°í™” |
| `02_transformer_basics.ipynb` | Positional Encoding, Encoder/Decoder Block êµ¬í˜„ |
| `03_bert_fine_tuning.ipynb` | BERT ì‚¬ì „í•™ìŠµ ëª©í‘œ, **KerasHub** (`keras_hub.models.BertClassifier`) + í•œêµ­ì–´ BERT Fine-Tuning _(HuggingFace TF ë°±ì—”ë“œëŠ” 2025ë…„ 9ì›” deprecated â†’ KerasHub ì „í™˜ ê¶Œì¥; 2026-02-25 ê¸°ì¤€)_ |
| `04_generative_models_vae.ipynb` | Autoencoder â†’ VAE, ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­, MNIST ìƒì„± |
| `05_generative_models_gan.ipynb` | DCGAN êµ¬í˜„, Wasserstein Loss, ìƒì„± ì´ë¯¸ì§€ ì‹œê°í™” |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_transformer_text_classification.ipynb` | Transformer Encoderë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ |
| `practice/ex02_simple_gan.ipynb` | ê°„ë‹¨í•œ GANìœ¼ë¡œ ì†ê¸€ì”¨ ì´ë¯¸ì§€ ìƒì„± |

**ì£¼ìš” í‚¤ì›Œë“œ**: `MultiHeadAttention`, `LayerNormalization`, `KerasHub`, `keras_hub`, `VAE`, `GAN`

---

## Chapter 08 â€” í”„ë¡œë•ì…˜ ë°°í¬

**ë””ë ‰í† ë¦¬**: `chapter08_production/`

**í•™ìŠµ ëª©í‘œ**

- í•™ìŠµëœ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ë°°í¬í•˜ê¸° ìœ„í•œ ì €ì¥Â·ë³€í™˜Â·ìµœì í™” ì›Œí¬í”Œë¡œìš°ë¥¼ ìµíŒë‹¤

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| Int8 ì–‘ìí™” | $x_{int} = \text{round}\!\left(\frac{x_{float}}{s}\right) + z$ |
| ì—­ì–‘ìí™” | $x_{float} \approx s \cdot (x_{int} - z)$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_model_saving_formats.ipynb` | `.keras` / SavedModel / HDF5 í˜•ì‹ ë¹„êµ, ê°€ì¤‘ì¹˜ ì €ì¥ |
| `02_tensorboard.ipynb` | ìŠ¤ì¹¼ë¼/ì´ë¯¸ì§€/íˆìŠ¤í† ê·¸ë¨ ë¡œê¹…, í”„ë¡œíŒŒì¼ëŸ¬ |
| `03_tensorflow_lite.ipynb` | TFLite ë³€í™˜, Post-Training Quantization, ì¶”ë¡  |
| `04_performance_optimization.ipynb` | Mixed Precision, `@tf.function`, ë¶„ì‚°í•™ìŠµ ê°œìš” |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_export_and_serve.ipynb` | ëª¨ë¸ í•™ìŠµ â†’ ì €ì¥ â†’ TFLite ë³€í™˜ â†’ ì¶”ë¡  ì „ì²´ íŒŒì´í”„ë¼ì¸ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `tf.saved_model`, `TFLiteConverter`, `tf.summary`, `MirroredStrategy`

---

## Chapter 09 â€” [Advanced] ë¶„ì‚° ì‹œìŠ¤í…œê³¼ ë³‘ë ¬ ì²˜ë¦¬ (Distributed Systems)

**ë””ë ‰í† ë¦¬**: `chapter09_distributed_systems/`

**í•™ìŠµ ëª©í‘œ**

- ë³µìˆ˜ì˜ GPU ë° ë©€í‹° ë…¸ë“œ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°, í…ì„œ, íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ì˜ í•µì‹¬ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| í†µì‹  ë¹„ìš© (All-Reduce) | $T_{comm} = 2(N-1)\frac{K}{N} \cdot t_{byte}$ (Ring All-Reduce) |
| í…ì„œ ë³‘ë ¬í™” ì—°ì‚° | $Y = \text{GeLU}(X A_1) A_2$ (ì—´ë°©í–¥ ë¶„í•  ë° í–‰ë°©í–¥ ë¶„í•  í–‰ë ¬ ê³±) |
| íŒŒì´í”„ë¼ì¸ ë²„ë¸” (Bubble) | $B = \frac{p-1}{m}$ (p: ìŠ¤í…Œì´ì§€ ìˆ˜, m: ë§ˆì´í¬ë¡œë°°ì¹˜ ìˆ˜) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_distributed_training_basics.ipynb` | Data Parallelism(DP), DistributedDataParallel(DDP), í†µì‹  í† í´ë¡œì§€(All-Reduce/All-Gather) |
| `02_tensor_parallelism.ipynb` | Megatron-LM ë°©ì‹ì˜ 1D í…ì„œ ë³‘ë ¬í™”, Column/Row ë³‘ë ¬ Linear ë ˆì´ì–´ êµ¬í˜„ |
| `03_pipeline_parallelism.ipynb` | ë§ˆì´í¬ë¡œë°°ì¹˜ ë¶„í• , Gpipe ë° 1F1B ìŠ¤ì¼€ì¤„ë§ ê¸°ë²•ì„ í†µí•œ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™” |
| `04_3d_parallelism.ipynb` | DP + TP + PPë¥¼ ê²°í•©í•œ 3D ë³‘ë ¬ ì²˜ë¦¬ (**Megatron-Core** ê¸°ë°˜ ì•„í‚¤í…ì²˜ ë¦¬ë·° _(êµ¬ë²„ì „ Megatron-Turing NLG 530BëŠ” 2021ë…„ ëª¨ë¸ â†’ í˜„í–‰ Megatron-Coreë¡œ êµì²´; 2026-02-25 ê¸°ì¤€)_) |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_implement_ddp_scratch.ipynb` | NCCL ë°±ì—”ë“œë¥¼ ëª¨ì‚¬í•˜ì—¬ ë‹¨ìˆœ íŒŒì´ì¬ìœ¼ë¡œ Ring All-Reduce ë¡œì§ êµ¬í˜„ |
| `practice/ex02_1d_tensor_parallel_llm.ipynb` | ì†Œí˜• Transformer ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ 2ê°œì˜ GPUë¡œ ë¶„ë¦¬í•˜ì—¬ Forward Pass ê³„ì‚° í›„ ê²€ì¦ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `All-Reduce`, `Megatron-Core`, `Megatron-LM`, `Tensor Parallelism`, `Pipeline Parallelism`, `Microbatch`

---

## Chapter 10 â€” [Advanced] ëŒ€ê·œëª¨ ëª¨ë¸ ë©”ëª¨ë¦¬ ìµœì í™” (Memory Optimization & ZeRO)

**ë””ë ‰í† ë¦¬**: `chapter10_memory_optimization/`

**í•™ìŠµ ëª©í‘œ**

- ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì € ìƒíƒœ, ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ê³  ZeRO ë° CPU Offloading ê¸°ë²•ì„ ì ìš©í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| Adam ìµœì í™”ê¸° ìƒíƒœ ë©”ëª¨ë¦¬ | $M_{adam} = 2 \times 4\text{B(fp32)} \times P_{count} = 8 \cdot P_{count}$ ë°”ì´íŠ¸ |
| ì´ í•™ìŠµ ë©”ëª¨ë¦¬ ë³µì¡ë„ | $M_{total} = P_{fp16} + G_{fp16} + P_{fp32} + M_{fp32} + V_{fp32}$ |
| ZeRO íŒŒí‹°ì…”ë‹ íš¨ìœ¨ | Stage 3 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ $\approx \frac{M_{total}}{N}$ (N: GPU ê°œìˆ˜) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_memory_profiling.ipynb` | ëª¨ë¸ íŒŒë¼ë¯¸í„°, í™œì„±í™” ì—°ì‚°(Activation), ì˜µí‹°ë§ˆì´ì €ê°€ ì°¨ì§€í•˜ëŠ” VRAM ì •í™•íˆ ê³„ì‚°í•˜ê¸° |
| `02_gradient_checkpointing.ipynb` | Activation Recomputation ì›ë¦¬ (ë©”ëª¨ë¦¬ì™€ ì—°ì‚°ëŸ‰ì˜ Trade-off) |
| `03_zero_redundancy_optimizer.ipynb` | ZeRO Stage 1(Optimizer ìƒíƒœ), Stage 2(Gradient), Stage 3(Parameter) ë¶„í•  ì›ë¦¬ |
| `04_cpu_and_nvme_offloading.ipynb` | DeepSpeedì˜ Zero-Offload ë©”ì»¤ë‹ˆì¦˜, PCIe ëŒ€ì—­í­ í•œê³„ ê·¹ë³µ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_memory_calculator.ipynb` | ëª¨ë¸ ì•„í‚¤í…ì²˜ ë° ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥í•˜ë©´ í•„ìš”í•œ VRAMì„ ì¶œë ¥í•˜ëŠ” ê³„ì‚°ê¸° ë§Œë“¤ê¸° |
| `practice/ex02_deepspeed_zero3_training.ipynb` | ë‹¨ì¼ GPUì—ì„œ OOMì´ ë°œìƒí•˜ëŠ” 14B ì´ìƒ ëª¨ë¸ì„ ZeRO-3+Offloadë¡œ í›ˆë ¨ ì„±ê³µì‹œí‚¤ê¸° |

**ì£¼ìš” í‚¤ì›Œë“œ**: `ZeRO Stage 3`, `Gradient Checkpointing`, `CPU Offload`, `DeepSpeed`, `OOM`

---

## Chapter 11 â€” [Advanced] ì»¤ìŠ¤í…€ GPU ì»¤ë„ í”„ë¡œê·¸ë˜ë° (CUDA & Triton)

**ë””ë ‰í† ë¦¬**: `chapter11_custom_kernels/`

**í•™ìŠµ ëª©í‘œ**

- íŒŒì´ì¬ í”„ë ˆì„ì›Œí¬ì˜ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì´ê³  í…ì„œ ì½”ì–´ë¥¼ ìµœëŒ€ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ ì‚¬ìš©ì ì •ì˜ ì»¤ë„ì„ ì§ì ‘ ê°œë°œí•˜ê³  ì ìš©í•œë‹¤.

> **ğŸ“… ë¼ì´ë¸ŒëŸ¬ë¦¬Â·í•˜ë“œì›¨ì–´ ë²„ì „ ê¸°ì¤€ (2026-02-25)**
>
> - **CUDA Toolkit**: 13.1.1 (2026ë…„ 1ì›” ì¶œì‹œ, NVIDIA ê³µì‹ ìµœì‹  ì•ˆì • ë²„ì „)
> - **OpenAI Triton**: 3.6.0 (2026ë…„ 1ì›” 20ì¼ ì¶œì‹œ, Blackwell ì•„í‚¤í…ì²˜ ì§€ì›)
> - **GPU ì„¸ëŒ€**: H100(Hopper) â†’ **H200**(HBM3e 141GB, 4.8TB/s) â†’ **B200/Blackwell**(HBM3e 192GB, 8TB/s, FP4 ì§€ì›)

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| ë©”ëª¨ë¦¬ ëŒ€ì—­í­ í•œê³„ | $\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{Memory Access (Bytes)}}$ |
| íƒ€ì¼ë§ (Tiling) ì—°ì‚° | ë¸”ë¡ í–‰ë ¬ ê³±: $C_{i,j} = \sum_k A_{i,k} \cdot B_{k,j}$ ë¥¼ Shared Memory ìœ„ì—ì„œ ì²˜ë¦¬ |
| Softmax ì•ˆì •í™” í“¨ì „ | $m = \max(x)$, $y = \frac{\exp(x-m)}{\sum \exp(x-m)}$ ì—°ì‚°ì„ ë©”ëª¨ë¦¬ ì™•ë³µ í•œ ë²ˆì— ì²˜ë¦¬(Fusion) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_gpu_architecture_basics.ipynb` | SMÂ·WarpÂ·ë©”ëª¨ë¦¬ ê³„ì¸µ; H100/H200/B200(Blackwell) ì„¸ëŒ€ë³„ ìŠ¤í™ ë¹„êµ ë° Roofline ë¶„ì„ |
| `02_cuda_cpp_extensions.ipynb` | C++ê³¼ pybind11ì„ í™œìš©í•˜ì—¬ PyTorch/TensorFlowì— ì»¤ìŠ¤í…€ C++(CUDA 13.x) ì—°ì‚° ì—°ë™ |
| `03_triton_kernel_programming.ipynb` | Triton 3.x ë¬¸ë²•ìœ¼ë¡œ GPU ì»¤ë„ ì‘ì„±: ë¸”ë¡ í¬ì¸í„°Â·íƒ€ì¼ ë§¤í•‘Â·Blackwell FP4 ì§€ì› ê°œìš” |
| `04_kernel_profiling_and_fusion.ipynb` | Nsight Systems ë° í”„ë¡œíŒŒì¼ëŸ¬ í™œìš©, ì—¬ëŸ¬ ì—°ì‚°ì„ ë¬¶ëŠ” Kernel Fusion ì „ëµ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_vector_add_cuda.ipynb` | ë²¡í„° ë§ì…ˆ ì—°ì‚°ì„ ìˆœìˆ˜ CUDA C++ ì»¤ë„ë¡œ ì‘ì„±í•˜ê³  íŒŒì´ì¬ì—ì„œ í˜¸ì¶œ |
| `practice/ex02_triton_fused_softmax_dropout.ipynb` | Triton 3.xë¡œ Softmaxì™€ Dropout ì—°ì‚°ì„ í•˜ë‚˜ì˜ ì»¤ë„ë¡œ í“¨ì „í•˜ì—¬ ì†ë„ ì¸¡ì • |

**ì£¼ìš” í‚¤ì›Œë“œ**: `CUDA 13.x`, `Triton 3.x`, `Kernel Fusion`, `Shared Memory`, `Arithmetic Intensity`, `H200`, `Blackwell B200`

---

## Chapter 12 â€” [Advanced] ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì•„í‚¤í…ì²˜ (Modern LLMs & MoE)

**ë””ë ‰í† ë¦¬**: `chapter12_modern_llms/`

**í•™ìŠµ ëª©í‘œ**

- Llama-3, GPT-4 ë“± ìµœì‹  SOTA ëª¨ë¸ì— ì ìš©ëœ Attention ë³€í˜•ê³¼ ì •ê·œí™” ê¸°ë²•ì˜ ìˆ˜í•™ì  ë°°ê²½ì„ ì´í•´í•˜ê³ , KV CacheÂ·GQAÂ·MoEë¥¼ í¬í•¨í•œ ê±°ëŒ€ ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ë°‘ë°”ë‹¥ë¶€í„° ì‘ì„±í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| RMSNorm | $\bar{a}_i = \frac{a_i}{\sqrt{\frac{1}{n}\sum_{j=1}^n a_j^2 + \epsilon}} g_i$ |
| RoPE (Rotary PE) | $f(q, m) = (q_1\cos(m\theta) - q_2\sin(m\theta), q_2\cos(m\theta) + q_1\sin(m\theta), \dots)$ |
| SwiGLU | $\text{SwiGLU}(x) = \text{Swish}(xW) \otimes (xV)$ |
| GQA KV ì ˆê° | $\text{KV Heads} = G \ll H_Q$, íŒŒë¼ë¯¸í„° ì ˆê°ë¥  $= 1 - G/H_Q$ |
| KV Cache í¬ê¸° | $M_{KV} = 2 \times L \times H_{kv} \times d_{head} \times B \times S_{max} \times \text{bytes}$ |
| MoE Router Loss | $L_{aux} = \alpha \cdot N \sum_{i=1}^N f_i \cdot P_i$ (ë¶€í•˜ ê· í˜•ì„ ìœ„í•œ ë³´ì¡° ì†ì‹¤) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_llama_architecture_deepdive.ipynb` | Pre-Norm + RMSNorm Â· SwiGLU ìˆ˜ì‹; **GQA í¬í•¨**: MHAâ†’MQAâ†’GQA ì§„í™”ì™€ QÂ·KÂ·V í—¤ë“œ ë¹„ìœ¨ì— ë”°ë¥¸ ë©”ëª¨ë¦¬Â·ì†ë„ ë¹„êµ |
| `02_kv_cache_and_memory.ipynb` | **[ì‹ ì„¤]** KV Cache í¬ê¸° ê³µì‹, Rolling Buffer, Multi-Turn ëŒ€í™”ì˜ ë©”ëª¨ë¦¬ ì¦ê°€ íŒ¨í„´, Prefix Caching ê°œìš” |
| `03_rotary_position_embedding.ipynb` | ë³µì†Œìˆ˜ í‰ë©´ì—ì„œì˜ RoPE ìˆ˜ì‹ ë„ì¶œ, ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë³´ì¡´ ì¦ëª…, YaRN / LongRoPEë¡œ ì»¨í…ìŠ¤íŠ¸ ì°½ í™•ì¥í•˜ëŠ” ì›ë¦¬ |
| `04_moe_routing_and_load_balancing.ipynb` | Top-k ë¼ìš°í„° ìˆ˜ì‹, Softmax ê²Œì´íŒ… vs Linear ê²Œì´íŒ…, Auxiliary Loss ë„ì¶œ, Expert Capacity Factor |
| `05_deepseek_moe_architecture.ipynb` | **[ì‹ ì„¤]** DeepSeekMoE: Shared Expert + Routed Expert ë¶„ë¦¬ ì„¤ê³„, Multi-Token Prediction(MTP) ìˆ˜ì‹, Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_implement_llama_scratch.ipynb` | RMSNorm Â· RoPE Â· SwiGLU Â· GQAë¥¼ ì ìš©í•œ ì†Œí˜• Llama ëª¨ë¸ ë¸”ë¡ì„ ëª¨ë“ˆë³„ë¡œ ì§ì ‘ ì‘ì„±í•˜ê¸° |
| `practice/ex02_custom_moe_layer.ipynb` | Shared Expert 1ê°œ + Routed Expert 4ê°œ ì¤‘ Top-2ë¥¼ ì„ íƒí•˜ëŠ” DeepSeekMoE ìŠ¤íƒ€ì¼ ë ˆì´ì–´ êµ¬í˜„ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `RoPE`, `YaRN`, `SwiGLU`, `RMSNorm`, `GQA`, `KV Cache`, `MoE`, `Load Balancing Loss`, `DeepSeekMoE`, `MTP`

---

## Chapter 13 â€” [Advanced] ìƒì„± AI ì‹¬í™” (Diffusion Models & SDE)

**ë””ë ‰í† ë¦¬**: `chapter13_genai_diffusion/`

**í•™ìŠµ ëª©í‘œ**

- í™•ì‚° ëª¨ë¸ì´ ì–´ë–»ê²Œ ìˆ˜í•™ì  ë…¸ì´ì¦ˆ í™•ë¥  ë³€í™˜ì—ì„œ ì‹œì‘ë˜ëŠ”ì§€ ì´í•´í•˜ê³ (DDPM), ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„Â·ê³ ì† ìƒ˜í”ŒëŸ¬Â·CFGÂ·Score MatchingÂ·SDEê¹Œì§€ ë”¥ëŸ¬ë‹ ìƒì„± ëª¨ë¸ì˜ ì „ ì´ë¡  ì²´ê³„ë¥¼ ë‹¤ë£¬ë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| DDPM Forward Process | $q(x_t \| x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)$ |
| DDPM Reverse Process | $p_\theta(x_{t-1} \| x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$ |
| ELBO (ë‹¨ìˆœí™”) | $\mathcal{L}_{simple}(\theta) = \mathbb{E}_{t,x_0,\epsilon}[\|\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)\|^2]$ |
| DDIM Sampler | $x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\hat{x}_0 + \sqrt{1-\bar{\alpha}_{t-1}-\sigma_t^2}\cdot\epsilon_\theta + \sigma_t\epsilon$ |
| CFG ê°€ì´ë˜ìŠ¤ | $\tilde{\epsilon}_\theta(x_t, c) = \epsilon_\theta(x_t) + w \cdot [\epsilon_\theta(x_t, c) - \epsilon_\theta(x_t)]$ |
| SDE (í™•ë¥ ë¯¸ë¶„ë°©ì •ì‹) | $dx = f(x,t)dt + g(t)dw$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_ddpm_theory_and_math.ipynb` | ë§ˆë¥´ì½”í”„ ì²´ì¸ ê¸°ë°˜ì˜ í™•ì‚°. Forward/Reverse process ìˆ˜ì‹ ë„ì¶œ, ELBO ìœ ë„ê³¼ì • ì™„ì „ ì „ê°œ |
| `02_noise_schedules_and_samplers.ipynb` | **[ì‹ ì„¤]** Linear/Cosine/EDM ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ ë¹„êµ; DDIM(ë¹„ë§ˆë¥´ì½”í”„ ì—­ë°©í–¥), DPM-Solver++ ì›ë¦¬ ë° ìŠ¤í… ìˆ˜ vs í’ˆì§ˆ Trade-off |
| `03_unet_for_diffusion.ipynb` | ë…¸ì´ì¦ˆ ì˜ˆì¸¡ê¸° ì—­í• ì˜ DDPM ì „ìš© UNet: ì”ì°¨ ë¸”ë¡, Cross-Attention, ì‹œê°„ ì„ë² ë”©(Sinusoidal) |
| `04_conditional_diffusion_cfg.ipynb` | Classifier-Free Guidance(CFG) ìˆ˜ì‹ ë„ì¶œ; Guidance Scale ì¡°ì ˆ, ControlNet ì¡°ê±´ë¶€ ì œì–´ ê°œìš” |
| `05_score_matching_and_sde.ipynb` | **[ë¶„ë¦¬]** Score Matching â†’ Langevin Dynamics â†’ ì—°ì† ì‹œê°„ SDE/ODE í†µí•© í”„ë ˆì„ì›Œí¬ (Song et al. 2021) |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_ddpm_forward_reverse.ipynb` | Linear/Cosine ìŠ¤ì¼€ì¤„ë¡œ ë…¸ì´ì¦ˆë¥¼ ì…íˆê³ (Forward), DDIM Samplerë¡œ ì œê±°í•˜ëŠ”(Reverse) ì‹œë®¬ë ˆì´ì…˜ ë¹„êµ |
| `practice/ex02_implement_cfg_generation.ipynb` | í´ë˜ìŠ¤ ì¡°ê±´ë¶€ MNIST CFG ìƒì„± + Guidance Scaleì— ë”°ë¥¸ í’ˆì§ˆ-ë‹¤ì–‘ì„± Trade-off ì‹¤í—˜ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `DDPM`, `ELBO`, `DDIM`, `DPM-Solver++`, `UNet`, `CFG`, `Score Matching`, `SDE`, `Langevin Dynamics`

---

## Chapter 14 â€” [Advanced] ê·¹ë‹¨ì  ì¶”ë¡  ìµœì í™” (Extreme Inference Optimization)

**ë””ë ‰í† ë¦¬**: `chapter14_extreme_inference/`

**í•™ìŠµ ëª©í‘œ**

- ëŒ€ê·œëª¨ ëª¨ë¸ ì„œë¹™ì—ì„œ ë³‘ëª©(Latency, Throughput)ì´ ë°œìƒí•˜ëŠ” ë¬¼ë¦¬ì  ì›ì¸ì„ íŒŒì•…í•˜ê³ , FlashAttention Â· Speculative Decoding Â· PagedAttention Â· ìµœì‹  ì–‘ìí™”ë¥¼ ì´ìš©í•œ ì¢…í•©ì  í•´ê²°ì±…ì„ êµ¬ì¶•í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| FlashAttention IO ë³µì¡ë„ | $O\left(\frac{N^2 d}{M}\right)$ HBM ì ‘ê·¼ëŸ‰, $M$ = SRAM í¬ê¸° |
| Speculative Decoding ê¸°ëŒ“ê°’ ì†ë„ | $E[\text{accepted tokens}] = \frac{1 - \beta^{k+1}}{1-\beta}$ ($\beta$ = ë“œë˜í”„íŠ¸ ìˆ˜ìš©ë¥ , $k$ = ë“œë˜í”„íŠ¸ ê¸¸ì´) |
| GPTQ ìµœì í™” (Hessian ê¸°ë°˜) | $\min_{\hat{W}} \|WX - \hat{W}X\|_F^2$ w.r.t. $\hat{W} = \text{Round}(W - \delta W)$, $\delta W$ from Hessian |
| AWQ (Activation-aware) | $\min_Q \| WX - Q(W \cdot S)S^{-1}X \|^2$ (ì±„ë„ë³„ ìŠ¤ì¼€ì¼ $S$ë¡œ ì•„ì›ƒë¼ì´ì–´ ë³´í˜¸) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_inference_bottlenecks.ipynb` | Prefill(Compute-bound) vs Decode(Memory-bound) ë¬¼ë¦¬ ì›ì¸ ë¶„ì„; Rooflineìœ¼ë¡œ KV Cache ì—°ì‚° AI ê³„ì‚° |
| `02_flash_attention_deepdive.ipynb` | IO Complexity ìˆ˜ì‹, Tiling + Recomputation ì›ë¦¬, FlashAttention v1â†’v2â†’v3 ì„±ëŠ¥ ë°œì „ì‚¬ |
| `03_speculative_decoding.ipynb` | **[ì‹ ì„¤]** Draft-Verify íŒ¨ëŸ¬ë‹¤ì„; ìˆ˜ìš©ë¥  $\beta$ì™€ ê¸°ëŒ“ í† í° ìˆ˜ ìœ ë„; MedusaÂ·EAGLE ë“± ë‹¤ì¤‘ í—¤ë“œ ë°©ì‹ ë¹„êµ |
| `04_vllm_and_paged_attention.ipynb` | OS Page Table ì°¨ìš© PagedAttention ë©”ì»¤ë‹ˆì¦˜, Continuous Batching, ë™ì  KV Block ìŠ¤ì¼€ì¤„ë§ |
| `05_quantization_gptq_awq.ipynb` | **[ë¶„ë¦¬]** PTQ ê¸°ì´ˆ â†’ GPTQ(Hessian 2ì°¨ ìµœì í™”) â†’ AWQ(Scale ì±„ë„ ë³´í˜¸) â†’ W4A16/INT8/FP8 ë¹„êµ ë²¤ì¹˜ë§ˆí¬ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_paged_attention_sim.ipynb` | PagedAttention ë©”ëª¨ë¦¬ ë¸”ë¡ í• ë‹¹ ì‹œë®¬ë ˆì´ì…˜ â€” ë¬¼ë¦¬/ê°€ìƒ ë¸”ë¡ ë§¤í•‘ ë° KV Copy-on-Write êµ¬í˜„ |
| `practice/ex02_awq_quantization_eval.ipynb` | Llama ê°€ì¤‘ì¹˜ë¥¼ AWQ ë°©ì‹ W4A16ìœ¼ë¡œ ì–‘ìí™”í•œ í›„ Perplexity ë° ì†ë„ ë¹„êµ í‰ê°€ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `FlashAttention`, `Speculative Decoding`, `EAGLE`, `PagedAttention`, `Continuous Batching`, `AWQ`, `GPTQ`, `KV Cache`, `W4A16`

---

## Chapter 15 â€” [Advanced] AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ (Alignment & RLHF)

**ë””ë ‰í† ë¦¬**: `chapter15_alignment_rlhf/`

**í•™ìŠµ ëª©í‘œ**

- ëª¨ë¸ ì¶œë ¥ì„ ì¸ê°„ì˜ ìœ¤ë¦¬ì Â·ë…¼ë¦¬ì  ì˜ë„ì— ë§ê²Œ ì •ë ¬(Align)í•˜ê¸° ìœ„í•´ Policy GradientÂ·PPO ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³ , RLHF íŒŒì´í”„ë¼ì¸ê³¼ DPOÂ·Constitutional AIê¹Œì§€ Alignment ì „ ê¸°ë²• ì²´ê³„ë¥¼ ë§ˆìŠ¤í„°í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| REINFORCE (Policy Gradient) | $\nabla_\theta J(\theta) = \mathbb{E}[G_t \nabla_\theta \log \pi_\theta(a_t\|s_t)]$ |
| Advantage Function | $A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)$ |
| PPO Clip ëª©ì  í•¨ìˆ˜ | $L^{CLIP}(\theta) = \mathbb{E}\left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right]$ |
| RLHF Reward Model Loss | $\mathcal{L}(\theta) = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) \right]$ |
| DPO ëª©ì  í•¨ìˆ˜ | $\mathbb{E} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w\|x)}{\pi_{ref}(y_w\|x)} - \beta \log \frac{\pi_\theta(y_l\|x)}{\pi_{ref}(y_l\|x)} \right) \right]$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_rl_fundamentals_mdp_policy.ipynb` | MDP ì •ì˜, Bellman ë°©ì •ì‹, REINFORCE(Policy Gradient) ìˆ˜ì‹ ë„ì¶œ, ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„ |
| `02_actor_critic_and_ppo.ipynb` | **[ì‹ ì„¤]** Advantage Function ìœ ë„, A2C â†’ PPO-Clip ìˆ˜ì‹ ì™„ì „ ì „ê°œ, KL í˜ë„í‹° vs Clip ë¹„êµ |
| `03_rlhf_pipeline_overview.ipynb` | InstructGPT: SFT â†’ Reward Model â†’ PPO 3ë‹¨ê³„ ì•„í‚¤í…ì²˜ + Bradley-Terry ëª¨ë¸ ìˆ˜ì‹ |
| `04_dpo_and_preference_learning.ipynb` | DPO ë² ì´ì¦ˆ ë„ì¶œ, RLHFì™€ ì„±ëŠ¥ ë¹„êµ, ORPOÂ·KTOÂ·SimPO íŒŒìƒ ê¸°ë²• ê°œìš” |
| `05_constitutional_ai_and_rlaif.ipynb` | **[ì‹ ì„¤]** Anthropic CAI ì›ì¹™, AI-í”¼ë“œë°±(RLAIF) ìë™í™” íŒŒì´í”„ë¼ì¸, Red TeamingÂ·Jailbreak ë°©ì–´ ê¸°ë²• |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_train_reward_model.ipynb` | Preference Pairë¡œ Bradley-Terry ê¸°ë°˜ Reward Model í›ˆë ¨; ì„ í˜¸ë„ ì •í™•ë„ ë° Calibration í‰ê°€ |
| `practice/ex02_dpo_fine_tuning_lora.ipynb` | TRL + LoRA(PEFT)ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ì„ DPO ì§€ì‹œí•™ìŠµ ë´‡ìœ¼ë¡œ ì „í™˜; RLHF vs DPO ìˆ˜ë ´ ì†ë„ ë¹„êµ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `MDP`, `Policy Gradient`, `PPO`, `Advantage`, `RLHF`, `DPO`, `ORPO`, `Bradley-Terry`, `Constitutional AI`, `RLAIF`

---

## Chapter 16 â€” [State-of-the-Art] ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± (Sparse Attention & DeepSeek/Qwen)

**ë””ë ‰í† ë¦¬**: `chapter16_sparse_attention/`

**í•™ìŠµ ëª©í‘œ**

- DeepSeek-V3ì˜ FP8 í›ˆë ¨Â·MLAÂ·Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ê¸°ë²•ì„ ìˆ˜ì‹ ìˆ˜ì¤€ì—ì„œ ë¶„ì„í•˜ê³ , Linear Attention(GLA/RetNet/Mamba)ê³¼ Qwen Hybrid êµ¬ì¡°, Long-context Sparse Attentionì˜ ìµœì‹  íë¦„ì„ í†µí•©ì ìœ¼ë¡œ ì´í•´í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| MLA KV ì••ì¶• | $\mathbf{c}_t^{KV} = W_d^{KV} \mathbf{h}_t \in \mathbb{R}^{d_c}$, $d_c \ll d_{KV}$ (KV Cache $d_c/d_{KV}$ë°° ì¶•ì†Œ) |
| FP8 ìŠ¤ì¼€ì¼ë§ | $Q(x) = \text{round}(x / s) \ cdot s_{max} / 127$ (FP8 E4M3, ì±„ë„ë³„ ìŠ¤ì¼€ì¼) |
| Linear Attention | $O(x) = \phi(Q)\left(\sum_i \phi(K_i)^T V_i\right) / \sum_i \phi(Q)\phi(K_i)^T$ (ì‹œí€€ìŠ¤ ê¸¸ì´ $O(1)$ ë©”ëª¨ë¦¬) |
| YaRN ì»¨í…ìŠ¤íŠ¸ í™•ì¥ | $\theta_i' = \theta_i \cdot (s \cdot d_{model} / \pi)^{2i/d}$ (ìŠ¤ì¼€ì¼ ì¸ì $s$ë¡œ RoPE ì£¼íŒŒìˆ˜ ì¬ì¡°ì •) |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|---|
| `01_deepseek_v3_fp8_training.ipynb` | **[ì¬ì„¤ê³„]** FP8 E4M3 í˜¼í•© ì •ë°€ë„ ì›ë¦¬, Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±(í¸í–¥ ë³´ì •), MTP ìˆ˜ì‹ ë„ì¶œ |
| `02_multi_head_latent_attention.ipynb` | MLA KV ì••ì¶• ìˆ˜ì‹ ì™„ì „ ë„ì¶œ, Up-projection ë³µì›, GQA ëŒ€ë¹„ KV Cache ì ˆê°ë¥  ì •ëŸ‰ ë¹„êµ |
| `03_linear_attention_and_hybrids.ipynb` | **[ì¬ì„¤ê³„]** GLAÂ·RetNetÂ·Mamba ë“± Linear Attention ê³„ì—´; Qwenì˜ SWA+Full+Linear Hybrid êµ¬ì¡° |
| `04_long_context_and_sparse_attn.ipynb` | **[ì¬í¸]** YaRN + DSA(DeepSeek Sparse Attention) + Sliding Window ë°©ë²•ë¡  í†µí•©; >50% ë¹„ìš© ì ˆê° ê¸°ë²• |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_mla_from_scratch.ipynb` | KV ì°¨ì› ì••ì¶•(down-projection) â†’ ì €ì¥ â†’ ë³µì›(up-projection) MLA ë ˆì´ì–´ êµ¬í˜„ + GQA ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì¸¡ì • |
| `practice/ex02_linear_attention_layer.ipynb` | **[êµì²´]** ì¸ê³¼ì  GLA(Gated Linear Attention) ë ˆì´ì–´ êµ¬í˜„ ë° ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì†ë„Â·ë©”ëª¨ë¦¬ ë¹„êµ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `DeepSeek-V3`, `FP8 Training`, `MLA`, `Auxiliary-Loss-Free`, `Linear Attention`, `GLA`, `Mamba`, `Qwen Hybrid`, `YaRN`, `DSA`

---

## Chapter 17 â€” [State-of-the-Art] ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT (Diffusion Transformers & Sora/Hunyuan)

**ë””ë ‰í† ë¦¬**: `chapter17_diffusion_transformers/`

**í•™ìŠµ ëª©í‘œ**

- DiT ì•„í‚¤í…ì²˜ì˜ ìˆ˜ì‹ ê¸°ì´ˆ(adaLN-Zero)ë¶€í„° í˜„ëŒ€ ë¹„ë””ì˜¤ ìƒì„± í›ˆë ¨ íŒ¨ëŸ¬ë‹¤ì„ì¸ Flow Matching, ê·¸ë¦¬ê³  SoraÂ·HunyuanVideoì˜ 3D ë¹„ë””ì˜¤ ì¸ì½”ë”©/ë””ì½”ë”© ì•„í‚¤í…ì²˜ê¹Œì§€ SOTA Video Generation ì „ ê³¼ì •ì„ ì´í•´í•˜ê³  êµ¬í˜„í•œë‹¤.

**ìˆ˜í•™ì  ê¸°ì´ˆ**
| ê°œë… | ìˆ˜ì‹ |
|------|------|
| adaLN-Zero ì¡°ê±´ë¶€ ì‚½ì… | $\mathbf{h} = \alpha_c \odot \text{LayerNorm}(\mathbf{h}) \odot (1 + \gamma_c) + \beta_c$, ì´ˆê¸°ê°’ $\alpha_c=0$ (í•™ìŠµ ì•ˆì •í™”) |
| 3D Causal VAE ì••ì¶• | $z \in \mathbb{R}^{C \times (T/M_t) \times (H/M_h) \times (W/M_w)}$ ($M$ = ì••ì¶• ë¹„ìœ¨) |
| Flow Matching (ODE) | $\frac{dx}{dt} = v_\theta(x, t)$, $\mathcal{L}_{FM} = \mathbb{E}_{t,x_0,x_1}[\|v_\theta(x_t, t) - (x_1 - x_0)\|^2]$ |
| Rectified Flow | $x_t = (1-t)x_0 + t x_1$, ì§ì„  ê²½ë¡œ ODE (DDPMì˜ ë…¸ì´ì¦ˆ ê²½ë¡œ vs ì§ì„  ë¹„êµ) |
| Spatiotemporal Patch | 3D íŒ¨ì¹˜ $(p_t, p_h, p_w)$ â†’ ì‹œí€€ìŠ¤ ê¸¸ì´ $= (T/p_t)(H/p_h)(W/p_w)$ |

**ê°•ì˜ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `01_from_unet_to_dit.ipynb` | U-Net í•œê³„ â†’ DiT ìŠ¤ì¼€ì¼ë§ ë²•ì¹™ ê²€ì¦(Peebles & Xie 2023); íŒ¨ì¹˜ í¬ê¸°ì™€ FID ê´€ê³„ ë¶„ì„ |
| `02_spatiotemporal_vae.ipynb` | 3D Causal VAE êµ¬ì¡°; ì‹œê°„ì  Flickering ì–µì œ ì›ë¦¬; ê³µê°„/ì‹œê°„ ì••ì¶• ë¹„ìœ¨ì— ë”°ë¥¸ í’ˆì§ˆ Trade-off |
| `03_dit_conditioning_and_adaln.ipynb` | **[ì‹ ì„¤]** adaLN-Zero ìˆ˜ì‹ ë„ì¶œ; ì‹œê°„ $t$Â·í´ë˜ìŠ¤Â·í…ìŠ¤íŠ¸ ì¡°ê±´ ì£¼ì… ë°©ì‹; CFG in DiT ì„¤ê³„ |
| `04_flow_matching_and_rectified_flow.ipynb` | **[ì‹ ì„¤]** Flow Matching ODE ìˆ˜ì‹; Rectified Flow (ì§ì„  ê²½ë¡œ); SD3Â·Fluxì™€ DDPM í›ˆë ¨ ë°©ì‹ ë¹„êµ |
| `05_sora_and_hunyuan_architecture.ipynb` | **[ì¬í¸]** Sora ìŠ¤ì¼€ì¼ë§ + NaViT ê°€ë³€ í•´ìƒë„; HunyuanVideo Dualâ†’Single-stream ë©€í‹°ëª¨ë‹¬ í“¨ì „ ë¹„êµ |

**ì‹¤ìŠµ íŒŒì¼**
| íŒŒì¼ | ë‚´ìš© |
|------|------|
| `practice/ex01_spatiotemporal_patcher.ipynb` | 3D ë¹„ë””ì˜¤ í…ì„œë¥¼ ì…ë ¥ë°›ì•„ ì‹œê³µê°„ 3D RoPEê°€ ì¶”ê°€ëœ DiT íŒ¨ì¹˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ êµ¬í˜„ |
| `practice/ex02_dit_block_with_adaln.ipynb` | adaLN-Zero ëª¨ë“ˆ êµ¬í˜„ + Flow Matching Lossë¡œ Moving MNISTë¥¼ ë…¸ì´ì¦ˆ ì˜ˆì¸¡í•˜ëŠ” ì†Œí˜• DiT ì¡°ë¦½ |

**ì£¼ìš” í‚¤ì›Œë“œ**: `DiT`, `adaLN-Zero`, `Flow Matching`, `Rectified Flow`, `Sora`, `HunyuanVideo`, `3D Causal VAE`, `NaViT`, `Flux`

---

## ìµœì¢… ì‹¤ì „ í”„ë¡œì íŠ¸

| í”„ë¡œì íŠ¸                             | ì„¤ëª…                                                                                   | ì£¼ìš” ê¸°ìˆ                                      |
| ------------------------------------ | -------------------------------------------------------------------------------------- | --------------------------------------------- |
| `project01_image_classifier/`        | ì»¤ìŠ¤í…€ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° (EfficientNetB0 ì „ì´í•™ìŠµ + TFLite ë³€í™˜)                           | CNN, ì „ì´í•™ìŠµ, tf.data                        |
| `project02_sentiment_analysis/`      | í•œêµ­ì–´ ê°ì„± ë¶„ì„ API (KoBERT Fine-Tuning + FastAPI ì„œë¹™)                               | BERT, HuggingFace, SavedModel                 |
| `project03_korean_nlp/`              | í•œêµ­ì–´ NLP íŒŒì´í”„ë¼ì¸ (í˜•íƒœì†Œ ë¶„ì„ + ì£¼ì œ ë¶„ë¥˜)                                        | KoNLPy, Bi-LSTM, TFRecord                     |
| **`project04_custom_llm_server/`**   | **[Advanced] vLLM ê¸°ë°˜ ì»¤ìŠ¤í…€ LLM ì¶”ë¡  ì„œë²„ (AWQ ì–‘ìí™” + PagedAttention)**            | **AWQ, vLLM, FastAPI, XLA**                   |
| **`project05_diffusion_image_gen/`** | **[Advanced] ì¡°ê±´ë¶€(Conditional) Diffusion ìŠ¤í¬ë˜ì¹˜ êµ¬í˜„**                             | **DDPM, UNet, Classifier-Free Guidance**      |
| **`project06_dpo_aligned_bot/`**     | **[Advanced] DPO/LoRA ê¸°ë°˜ ë‚˜ë§Œì˜ ì•ˆì „í•œ ì§€ì‹œí•™ìŠµ(Instruct) AI ë³´ì¡°ë´‡**                | **DPO, PEFT(LoRA), TRL**                      |
| **`project07_mini_sora_dit/`**       | **[SOTA] ì›€ì§ì´ëŠ” MNIST(Moving MNIST) ë°ì´í„°ë¥¼ í™œìš©í•œ ì†Œí˜• DiT ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ í›ˆë ¨** | **DiT, 3D Conv, adaLN, Spatiotemporal Patch** |

---

## ì°¸ê³  ë¬¸í—Œ ë° ë…¼ë¬¸ (References)

**ê¸°ì´ˆ ë° ê³ ì „ ë…¼ë¬¸/ìë£Œ**

- [TensorFlow ê³µì‹ ë¬¸ì„œ](https://www.tensorflow.org/guide)
- [Keras API ë ˆí¼ëŸ°ìŠ¤](https://keras.io/api/)
- [TensorFlow íŠœí† ë¦¬ì–¼](https://www.tensorflow.org/tutorials)
- [HuggingFace Transformers + TF](https://huggingface.co/docs/transformers/keras_callbacks)
- Goodfellow et al., _Deep Learning_ (2016) â€” ìˆ˜í•™ì  ê¸°ì´ˆ
- Vaswani et al., _Attention Is All You Need_ (2017) â€” Transformer
- Kingma & Welling, _Auto-Encoding Variational Bayes_ (2013) â€” VAE
- Goodfellow et al., _Generative Adversarial Networks_ (2014) â€” GAN

**ì‹œìŠ¤í…œ ë° Advanced ë…¼ë¬¸**

- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054) (DeepSpeed)
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [PagedAttention: Efficient Memory Management for Large Language Model Serving](https://arxiv.org/abs/2309.06180) (vLLM)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) (DPO)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)

**SOTA ì•„í‚¤í…ì²˜ (DeepSeek, DiT, Sora)**

- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) (MLA, DeepSeekMoE, FP8 Training)
- [Scalable Diffusion Models with Transformers (DiT)](https://arxiv.org/abs/2212.09748)
- [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators) (OpenAI Sora Technical Report)
- [HunyuanVideo: A Systematic Framework For Large Video Generation Models](https://arxiv.org/abs/2412.17601) (Tencent)
