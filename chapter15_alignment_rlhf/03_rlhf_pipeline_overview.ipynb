{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” RLHF íŒŒì´í”„ë¼ì¸ ê°œìš”\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- InstructGPTì˜ SFT â†’ Reward Model â†’ PPO 3ë‹¨ê³„ ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤\n",
    "- Bradley-Terry ì„ í˜¸ ëª¨ë¸ì˜ ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  êµ¬í˜„í•œë‹¤\n",
    "- Reward Modelì˜ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìœ ë„í•˜ê³  í›ˆë ¨ ê³¼ì •ì„ êµ¬í˜„í•œë‹¤\n",
    "- RLHFì˜ PPO ë‹¨ê³„ì—ì„œ KL í˜ë„í‹°ê°€ í•˜ëŠ” ì—­í• ì„ ìˆ˜ì‹ìœ¼ë¡œ ì„¤ëª…í•œë‹¤\n",
    "- ì „ì²´ RLHF íŒŒì´í”„ë¼ì¸ì„ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ êµ¬í˜„í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Bradley-Terry ëª¨ë¸ê³¼ RLHF ìˆ˜ì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [SFT â†’ RM â†’ PPO íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜](#2.-RLHF-íŒŒì´í”„ë¼ì¸)\n",
    "3. [Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„](#3.-Bradley-Terry-ëª¨ë¸)\n",
    "4. [Reward Model í•™ìŠµ](#4.-Reward-Model-í•™ìŠµ)\n",
    "5. [KL Divergence í˜ë„í‹° ì‹œê°í™”](#5.-KL-Divergence-í˜ë„í‹°)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### Bradley-Terry ì„ í˜¸ ëª¨ë¸\n",
    "\n",
    "ë‘ ì‘ë‹µ $y_w$(ì„ í˜¸), $y_l$(ë¹„ì„ í˜¸)ì— ëŒ€í•œ ì„ í˜¸ í™•ë¥ :\n",
    "\n",
    "$$P(y_w \\succ y_l \\mid x) = \\sigma\\left(r(x, y_w) - r(x, y_l)\\right)$$\n",
    "\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜\n",
    "- $r(x, y)$: ë³´ìƒ ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ìŠ¤ì¹¼ë¼ ì ìˆ˜\n",
    "- $y_w \\succ y_l$: $y_w$ê°€ $y_l$ë³´ë‹¤ ì„ í˜¸ë¨\n",
    "\n",
    "### Reward Model í•™ìŠµ ì†ì‹¤\n",
    "\n",
    "$$\\mathcal{L}_{RM}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l)}\\left[\\log \\sigma\\left(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\right)\\right]$$\n",
    "\n",
    "- ì„ í˜¸ ì‘ë‹µì˜ ë³´ìƒì„ ë¹„ì„ í˜¸ ì‘ë‹µë³´ë‹¤ **ë†’ì´ëŠ”** ë°©í–¥ìœ¼ë¡œ í•™ìŠµ\n",
    "\n",
    "### RLHF ëª©ì í•¨ìˆ˜ (PPO ë‹¨ê³„)\n",
    "\n",
    "$$\\max_\\theta \\; \\mathbb{E}_{x \\sim D,\\; y \\sim \\pi_\\theta(\\cdot|x)} \\left[r_\\phi(x, y)\\right] - \\beta \\, D_{KL}\\left[\\pi_\\theta(\\cdot|x) \\;\\|\\; \\pi_{ref}(\\cdot|x)\\right]$$\n",
    "\n",
    "- $r_\\phi(x, y)$: í•™ìŠµëœ Reward Modelì˜ ì ìˆ˜\n",
    "- $\\pi_{ref}$: SFT ëª¨ë¸ (ê¸°ì¤€ ì •ì±…)\n",
    "- $\\beta$: KL í˜ë„í‹° ê³„ìˆ˜\n",
    "\n",
    "### InstructGPT 3ë‹¨ê³„\n",
    "\n",
    "| ë‹¨ê³„ | ì…ë ¥ | ì¶œë ¥ | ì†ì‹¤í•¨ìˆ˜ |\n",
    "|------|------|------|---------|\n",
    "| **Step 1: SFT** | $(x, y^*)$ ì‹œë²” ë°ì´í„° | $\\pi_{SFT}$ | $-\\log P(y^* \\mid x)$ (êµì°¨ ì—”íŠ¸ë¡œí”¼) |\n",
    "| **Step 2: RM** | $(x, y_w, y_l)$ ë¹„êµ ìŒ | $r_\\theta$ | $-\\log\\sigma(r_\\theta(y_w) - r_\\theta(y_l))$ |\n",
    "| **Step 3: PPO** | $x$ í”„ë¡¬í”„íŠ¸ | $\\pi_\\theta$ | $\\mathbb{E}[r_\\phi(y)] - \\beta D_{KL}$ |\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| Bradley-Terry | $P(y_w \\succ y_l) = \\sigma(r(y_w) - r(y_l))$ | ìŒë³„ ë¹„êµ ì„ í˜¸ ëª¨ë¸ |\n",
    "| RM Loss | $-\\mathbb{E}[\\log\\sigma(\\Delta r)]$ | ì„ í˜¸ ì ìˆ˜ ì°¨ì´ ìµœëŒ€í™” |\n",
    "| RLHF Objective | $\\mathbb{E}[r(y)] - \\beta D_{KL}$ | ë³´ìƒ ìµœëŒ€í™” + ì•ˆì •ì„± |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ RLHF ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ RLHFê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: AIë¥¼ ê°•ì•„ì§€ í›ˆë ¨ì‹œí‚¤ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•´ìš”!\n",
    "\n",
    "**1ë‹¨ê³„ - SFT (ë”°ë¼í•˜ê¸° í›ˆë ¨):** ë¨¼ì € ê°•ì•„ì§€ì—ê²Œ \"ì•‰ì•„\"ë¥¼ ì—¬ëŸ¬ ë²ˆ ë³´ì—¬ì£¼ë©´ì„œ ë”°ë¼í•˜ê²Œ í•´ìš”.\n",
    "ì´ê²ƒì€ ì‚¬ëŒì´ ì“´ ì¢‹ì€ ë‹µë³€ì„ AIì—ê²Œ ë³´ì—¬ì£¼ê³  ë”°ë¼ì“°ê²Œ í•˜ëŠ” ê±°ì˜ˆìš”.\n",
    "\n",
    "**2ë‹¨ê³„ - Reward Model (ì ìˆ˜íŒ ë§Œë“¤ê¸°):** ê°•ì•„ì§€ê°€ í•œ í–‰ë™ ì¤‘ì—ì„œ \"ì´ê²Œ ë” ì¢‹ì•„!\"ë¼ê³  \n",
    "ì‚¬ëŒì´ ê³¨ë¼ì£¼ë©´, AIê°€ \"ë¬´ì—‡ì´ ì¢‹ì€ í–‰ë™ì¸ì§€\" ë°°ì›Œìš”. ì´ê²ƒì´ ë³´ìƒ ëª¨ë¸ì´ì—ìš”.\n",
    "\n",
    "**3ë‹¨ê³„ - PPO (ììœ  ì—°ìŠµ):** ê°•ì•„ì§€ê°€ ììœ ë¡­ê²Œ í–‰ë™í•˜ë©´ì„œ, ì ìˆ˜íŒ(ë³´ìƒ ëª¨ë¸)ì„ ë³´ê³ \n",
    "ìŠ¤ìŠ¤ë¡œ ë” ì¢‹ì€ í–‰ë™ì„ ì°¾ì•„ê°€ìš”. ë‹¨, ë„ˆë¬´ ì—‰ëš±í•œ í–‰ë™ì€ ì•ˆ ë˜ë„ë¡ ì œí•œí•´ìš”(KL í˜ë„í‹°).\n",
    "\n",
    "#### ğŸ† Bradley-Terryê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì¶•êµ¬ íŒ€ ë­í‚¹ì„ ìƒê°í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "ë‘ íŒ€ì´ ì‹œí•©í•˜ë©´ ì ìˆ˜ê°€ ë†’ì€ íŒ€ì´ ì´ê¸¸ í™•ë¥ ì´ ë†’ì£ ? Bradley-Terry ëª¨ë¸ì€:\n",
    "- \"AíŒ€ ì ìˆ˜ê°€ 100ì´ê³  BíŒ€ ì ìˆ˜ê°€ 80ì´ë©´, Aê°€ ì´ê¸¸ í™•ë¥ ì€ ì–¼ë§ˆ?\"ë¥¼ ê³„ì‚°í•´ìš”\n",
    "- ë§ˆì°¬ê°€ì§€ë¡œ \"ë‹µë³€ Aì˜ ì ìˆ˜ê°€ ë†’ê³  ë‹µë³€ Bì˜ ì ìˆ˜ê°€ ë‚®ìœ¼ë©´, Aê°€ ì„ í˜¸ë  í™•ë¥ ì€?\"ì„ ê³„ì‚°í•´ìš”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: Bradley-Terry í™•ë¥  ê³„ì‚°\n",
    "\n",
    "ë³´ìƒ ëª¨ë¸ì´ ë‘ ì‘ë‹µì— ëŒ€í•´ $r(x, y_w) = 2.0$, $r(x, y_l) = 0.5$ë¥¼ ì¶œë ¥í–ˆìŠµë‹ˆë‹¤.\n",
    "$y_w$ê°€ ì„ í˜¸ë  í™•ë¥  $P(y_w \\succ y_l)$ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$P(y_w \\succ y_l) = \\sigma(r(y_w) - r(y_l)) = \\sigma(2.0 - 0.5) = \\sigma(1.5)$$\n",
    "\n",
    "$$= \\frac{1}{1 + e^{-1.5}} = \\frac{1}{1 + 0.2231} = \\frac{1}{1.2231} \\approx 0.8176$$\n",
    "\n",
    "â†’ ì•½ 81.8%ì˜ í™•ë¥ ë¡œ $y_w$ê°€ ì„ í˜¸ë©ë‹ˆë‹¤. ë³´ìƒ ì°¨ì´ê°€ í´ìˆ˜ë¡ í™•ë¥ ì´ 1ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: Reward Model Loss ê³„ì‚°\n",
    "\n",
    "$r_\\theta(y_w) = 1.0$, $r_\\theta(y_l) = 0.8$ì¼ ë•Œ ì†ì‹¤ê°’ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\mathcal{L} = -\\log\\sigma(r_\\theta(y_w) - r_\\theta(y_l)) = -\\log\\sigma(1.0 - 0.8) = -\\log\\sigma(0.2)$$\n",
    "\n",
    "$$\\sigma(0.2) = \\frac{1}{1+e^{-0.2}} = \\frac{1}{1.8187} \\approx 0.5498$$\n",
    "\n",
    "$$\\mathcal{L} = -\\log(0.5498) \\approx 0.5981$$\n",
    "\n",
    "â†’ ë³´ìƒ ì°¨ì´ê°€ ì‘ì•„ì„œ ëª¨ë¸ì˜ í™•ì‹ ë„ê°€ ë‚®ê³  ì†ì‹¤ì´ ë¹„êµì  í½ë‹ˆë‹¤. í•™ìŠµì„ í†µí•´ ì°¨ì´ë¥¼ ë” ë²Œë ¤ì•¼ í•©ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SFT â†’ RM â†’ PPO íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ <a name='2.-RLHF-íŒŒì´í”„ë¼ì¸'></a>\n",
    "\n",
    "InstructGPTì˜ 3ë‹¨ê³„ RLHF íŒŒì´í”„ë¼ì¸ì„ ê°„ì†Œí™”ëœ í˜•íƒœë¡œ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "ì‹¤ì œ LLM ëŒ€ì‹  ì‘ì€ MLP ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ì›ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Step 1: SFT â”‚ â”€â”€â†’ â”‚  Step 2: RM  â”‚ â”€â”€â†’ â”‚  Step 3: PPOâ”‚\n",
    "â”‚  ì§€ë„í•™ìŠµ     â”‚     â”‚  ì„ í˜¸ í•™ìŠµ    â”‚     â”‚  ê°•í™”í•™ìŠµ     â”‚\n",
    "â”‚  (x, y*) ìŒ  â”‚     â”‚  (x, y_w>y_l)â”‚     â”‚  r(x,y) ìµœëŒ€í™”â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RLHF 3ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê°„ì†Œí™”: ì…ë ¥ xëŠ” 5ì°¨ì› ë²¡í„°, ì¶œë ¥ yëŠ” 3ì°¨ì› ë²¡í„°\n",
    "\n",
    "input_dim = 5\n",
    "output_dim = 3\n",
    "hidden_dim = 16\n",
    "n_samples = 200\n",
    "\n",
    "# ì‹œë²” ë°ì´í„° ìƒì„± (SFTìš©)\n",
    "np.random.seed(42)\n",
    "X_demo = np.random.randn(n_samples, input_dim).astype(np.float32)\n",
    "# \"ì¢‹ì€\" ì‘ë‹µ: ì…ë ¥ì˜ ë¹„ì„ í˜• ë³€í™˜\n",
    "true_transform = lambda x: np.tanh(x @ np.random.randn(input_dim, output_dim) * 0.5)\n",
    "np.random.seed(42)\n",
    "Y_demo = true_transform(X_demo).astype(np.float32)\n",
    "\n",
    "# ============================================================\n",
    "# Step 1: SFT (Supervised Fine-Tuning)\n",
    "# ============================================================\n",
    "print(\"=\" * 55)\n",
    "print(\"  Step 1: SFT (ì§€ë„í•™ìŠµ ë¯¸ì„¸ì¡°ì •)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "sft_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_dim, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(hidden_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_dim)\n",
    "])\n",
    "sft_model.compile(optimizer=tf.keras.optimizers.Adam(0.01), loss='mse')\n",
    "\n",
    "sft_history = sft_model.fit(X_demo, Y_demo, epochs=50, batch_size=32,\n",
    "                             verbose=0, validation_split=0.2)\n",
    "\n",
    "sft_loss = sft_history.history['loss'][-1]\n",
    "sft_val_loss = sft_history.history['val_loss'][-1]\n",
    "print(f\"  SFT ìµœì¢… ì†ì‹¤: {sft_loss:.4f}\")\n",
    "print(f\"  SFT ê²€ì¦ ì†ì‹¤: {sft_val_loss:.4f}\")\n",
    "\n",
    "# SFT ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ê¸°ì¤€ ì •ì±…)\n",
    "ref_weights = [w.numpy().copy() for w in sft_model.trainable_variables]\n",
    "\n",
    "# ì˜ˆì¸¡ ì˜ˆì‹œ\n",
    "sample_x = X_demo[:3]\n",
    "sample_pred = sft_model.predict(sample_x, verbose=0)\n",
    "print(f\"\\n  ì˜ˆì¸¡ ì˜ˆì‹œ (ì…ë ¥ 3ê°œ):\")\n",
    "for i in range(3):\n",
    "    print(f\"    y_pred[{i}] = [{', '.join(f'{v:.3f}' for v in sample_pred[i])}]\")\n",
    "    print(f\"    y_true[{i}] = [{', '.join(f'{v:.3f}' for v in Y_demo[i])}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„ <a name='3.-Bradley-Terry-ëª¨ë¸'></a>\n",
    "\n",
    "ì„ í˜¸ ìŒ ë°ì´í„° $(x, y_w, y_l)$ì„ ìƒì„±í•˜ê³ , Bradley-Terry í™•ë¥  ëª¨ë¸ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$P(y_w \\succ y_l \\mid x) = \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$$\n",
    "\n",
    "ë³´ìƒ ì°¨ì´ì— ë”°ë¥¸ ì„ í˜¸ í™•ë¥  ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Bradley-Terry ì„ í˜¸ ëª¨ë¸ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì„ í˜¸ ìŒ ë°ì´í„° ìƒì„±\n",
    "n_pairs = 500\n",
    "np.random.seed(42)\n",
    "\n",
    "X_pairs = np.random.randn(n_pairs, input_dim).astype(np.float32)\n",
    "\n",
    "# ì„ í˜¸ ì‘ë‹µ: SFT ëª¨ë¸ì˜ ì¶œë ¥ + ì•½ê°„ì˜ ê°œì„ \n",
    "Y_preferred = sft_model.predict(X_pairs, verbose=0) + np.random.randn(n_pairs, output_dim).astype(np.float32) * 0.1\n",
    "# ë¹„ì„ í˜¸ ì‘ë‹µ: ëœë¤ ë…¸ì´ì¦ˆê°€ í° ì¶œë ¥\n",
    "Y_rejected = sft_model.predict(X_pairs, verbose=0) + np.random.randn(n_pairs, output_dim).astype(np.float32) * 0.8\n",
    "\n",
    "# Bradley-Terry í™•ë¥  ì‹œê°í™”\n",
    "reward_diffs = np.linspace(-5, 5, 200)\n",
    "bt_probs = 1.0 / (1.0 + np.exp(-reward_diffs))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) Bradley-Terry í™•ë¥  ê³¡ì„ \n",
    "ax1 = axes[0]\n",
    "ax1.plot(reward_diffs, bt_probs, 'b-', lw=2.5, label=r'$\\sigma(\\Delta r)$')\n",
    "ax1.axhline(y=0.5, color='gray', ls='--', lw=1.5, alpha=0.5)\n",
    "ax1.axvline(x=0, color='gray', ls='--', lw=1.5, alpha=0.5)\n",
    "ax1.fill_between(reward_diffs, bt_probs, 0.5,\n",
    "                 where=(reward_diffs > 0), alpha=0.15, color='green',\n",
    "                 label=r'$r(y_w) > r(y_l)$')\n",
    "ax1.fill_between(reward_diffs, bt_probs, 0.5,\n",
    "                 where=(reward_diffs < 0), alpha=0.15, color='red',\n",
    "                 label=r'$r(y_w) < r(y_l)$')\n",
    "ax1.set_xlabel(r'ë³´ìƒ ì°¨ì´ $r(y_w) - r(y_l)$', fontsize=11)\n",
    "ax1.set_ylabel(r'$P(y_w \\succ y_l)$', fontsize=11)\n",
    "ax1.set_title('Bradley-Terry ì„ í˜¸ í™•ë¥ ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) Reward Model Loss ê³¡ì„ \n",
    "ax2 = axes[1]\n",
    "rm_loss_vals = -np.log(bt_probs + 1e-8)\n",
    "ax2.plot(reward_diffs, rm_loss_vals, 'r-', lw=2.5, label=r'$-\\log\\sigma(\\Delta r)$')\n",
    "ax2.set_xlabel(r'ë³´ìƒ ì°¨ì´ $\\Delta r$', fontsize=11)\n",
    "ax2.set_ylabel('Loss', fontsize=11)\n",
    "ax2.set_title('Reward Model ì†ì‹¤ í•¨ìˆ˜', fontweight='bold')\n",
    "ax2.set_ylim(0, 5)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bradley_terry_model.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/bradley_terry_model.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ ì˜ˆì‹œ\n",
    "print(f\"\\nBradley-Terry í™•ë¥  ì˜ˆì‹œ:\")\n",
    "print(f\"{'Î”r':>6} | {'P(y_w â‰» y_l)':>14} | {'RM Loss':>10}\")\n",
    "print(f\"{'-'*36}\")\n",
    "for dr in [-2.0, -1.0, 0.0, 0.5, 1.0, 2.0, 3.0]:\n",
    "    p = 1 / (1 + np.exp(-dr))\n",
    "    loss = -np.log(p)\n",
    "    print(f\"{dr:>+6.1f} | {p:>14.4f} | {loss:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. Reward Model í•™ìŠµ <a name='4.-Reward-Model-í•™ìŠµ'></a>\n",
    "\n",
    "Reward Modelì€ ì…ë ¥ $(x, y)$ë¥¼ ë°›ì•„ ìŠ¤ì¹¼ë¼ ì ìˆ˜ $r_\\theta(x, y)$ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì†ì‹¤ í•¨ìˆ˜:\n",
    "\n",
    "$$\\mathcal{L}_{RM}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\sigma\\left(r_\\theta(x_i, y_w^i) - r_\\theta(x_i, y_l^i)\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Reward Model í•™ìŠµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Reward Model: (x, y)ë¥¼ concatí•˜ì—¬ ìŠ¤ì¹¼ë¼ ë³´ìƒ ì¶œë ¥\n",
    "reward_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu',\n",
    "                          input_shape=(input_dim + output_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)  # ìŠ¤ì¹¼ë¼ ë³´ìƒ\n",
    "])\n",
    "\n",
    "optimizer_rm = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„° ì¤€ë¹„\n",
    "X_w = np.concatenate([X_pairs, Y_preferred], axis=1)\n",
    "X_l = np.concatenate([X_pairs, Y_rejected], axis=1)\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "rm_losses = []\n",
    "rm_accuracies = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    epoch_correct = 0\n",
    "\n",
    "    indices = np.random.permutation(n_pairs)\n",
    "    for start in range(0, n_pairs, batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            r_w = reward_model(X_w[batch_idx], training=True)\n",
    "            r_l = reward_model(X_l[batch_idx], training=True)\n",
    "\n",
    "            # Bradley-Terry loss\n",
    "            diff = r_w - r_l\n",
    "            loss = -tf.reduce_mean(tf.math.log_sigmoid(diff))\n",
    "\n",
    "        grads = tape.gradient(loss, reward_model.trainable_variables)\n",
    "        optimizer_rm.apply_gradients(zip(grads, reward_model.trainable_variables))\n",
    "\n",
    "        epoch_losses.append(loss.numpy())\n",
    "        epoch_correct += tf.reduce_sum(\n",
    "            tf.cast(diff > 0, tf.float32)).numpy()\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    accuracy = epoch_correct / n_pairs\n",
    "    rm_losses.append(avg_loss)\n",
    "    rm_accuracies.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, \"\n",
    "              f\"ì„ í˜¸ ì •í™•ë„={accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nReward Model í•™ìŠµ ì™„ë£Œ\")\n",
    "print(f\"  ìµœì¢… ì†ì‹¤: {rm_losses[-1]:.4f}\")\n",
    "print(f\"  ìµœì¢… ì„ í˜¸ ì •í™•ë„: {rm_accuracies[-1]:.4f}\")\n",
    "\n",
    "# ë³´ìƒ ë¶„í¬ í™•ì¸\n",
    "r_w_all = reward_model(X_w).numpy().flatten()\n",
    "r_l_all = reward_model(X_l).numpy().flatten()\n",
    "print(f\"\\në³´ìƒ ë¶„í¬:\")\n",
    "print(f\"  ì„ í˜¸ ì‘ë‹µ r(y_w): í‰ê· ={r_w_all.mean():.3f}, í‘œì¤€í¸ì°¨={r_w_all.std():.3f}\")\n",
    "print(f\"  ë¹„ì„ í˜¸ ì‘ë‹µ r(y_l): í‰ê· ={r_l_all.mean():.3f}, í‘œì¤€í¸ì°¨={r_l_all.std():.3f}\")\n",
    "print(f\"  í‰ê·  ë³´ìƒ ì°¨ì´: {(r_w_all - r_l_all).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Reward Model í•™ìŠµ ê³¼ì • ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# (1) í•™ìŠµ ì†ì‹¤\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rm_losses, 'b-', lw=2, label='RM Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Reward Model í•™ìŠµ ì†ì‹¤', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ì„ í˜¸ ì •í™•ë„\n",
    "ax2 = axes[1]\n",
    "ax2.plot(rm_accuracies, 'g-', lw=2, label='ì„ í˜¸ ì •í™•ë„')\n",
    "ax2.axhline(y=0.5, color='red', ls='--', lw=1.5, label='ëœë¤ ê¸°ì¤€ì„ ')\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Accuracy', fontsize=11)\n",
    "ax2.set_title('ì„ í˜¸ ìŒ ë¶„ë¥˜ ì •í™•ë„', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "# (3) ë³´ìƒ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨\n",
    "ax3 = axes[2]\n",
    "ax3.hist(r_w_all, bins=30, alpha=0.6, color='green', label=r'$r(y_w)$ ì„ í˜¸', density=True)\n",
    "ax3.hist(r_l_all, bins=30, alpha=0.6, color='red', label=r'$r(y_l)$ ë¹„ì„ í˜¸', density=True)\n",
    "ax3.axvline(x=r_w_all.mean(), color='darkgreen', ls='--', lw=2)\n",
    "ax3.axvline(x=r_l_all.mean(), color='darkred', ls='--', lw=2)\n",
    "ax3.set_xlabel('ë³´ìƒ ì ìˆ˜', fontsize=11)\n",
    "ax3.set_ylabel('ë°€ë„', fontsize=11)\n",
    "ax3.set_title('í•™ìŠµëœ ë³´ìƒ ë¶„í¬', fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_model_training.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/reward_model_training.png\")\n",
    "print(f\"ì„ í˜¸ ì‘ë‹µ(ë…¹ìƒ‰)ê³¼ ë¹„ì„ í˜¸ ì‘ë‹µ(ë¹¨ê°„ìƒ‰)ì˜ ë³´ìƒ ë¶„í¬ê°€ ì˜ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. KL Divergence í˜ë„í‹° ì‹œê°í™” <a name='5.-KL-Divergence-í˜ë„í‹°'></a>\n",
    "\n",
    "RLHFì˜ PPO ë‹¨ê³„ì—ì„œ KL í˜ë„í‹°ëŠ” í•™ìŠµ ì •ì±…ì´ ê¸°ì¤€ ì •ì±…(SFT)ì—ì„œ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\max_\\theta \\; \\mathbb{E}_{y \\sim \\pi_\\theta}\\left[r_\\phi(x, y)\\right] - \\beta \\, D_{KL}\\left[\\pi_\\theta \\| \\pi_{ref}\\right]$$\n",
    "\n",
    "$$D_{KL}\\left[\\pi_\\theta \\| \\pi_{ref}\\right] = \\sum_y \\pi_\\theta(y|x) \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)}$$\n",
    "\n",
    "$\\beta$ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ â†’ Reward Hacking (ë³´ìƒ ëª¨ë¸ì˜ í—ˆì ì„ ì•…ìš©)  \n",
    "$\\beta$ê°€ ë„ˆë¬´ í¬ë©´ â†’ SFT ëª¨ë¸ì—ì„œ ê±°ì˜ ë²—ì–´ë‚˜ì§€ ëª»í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ KL Divergence í˜ë„í‹° ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë‹¤ì–‘í•œ Î² ê°’ì— ë”°ë¥¸ RLHF ëª©ì í•¨ìˆ˜ ë¶„ì„\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜: ì •ì±…ì´ ê¸°ì¤€ì—ì„œ ì ì§„ì ìœ¼ë¡œ ë²—ì–´ë‚˜ëŠ” ê²½ìš°\n",
    "n_vocab = 20\n",
    "ref_logits = np.random.randn(n_vocab) * 0.5\n",
    "ref_probs = np.exp(ref_logits) / np.exp(ref_logits).sum()\n",
    "\n",
    "# ë³´ìƒì´ ë†’ì€ í† í°ìœ¼ë¡œ ì •ì±…ì´ ì´ë™í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤\n",
    "high_reward_tokens = [3, 7, 12]\n",
    "shift_amounts = np.linspace(0, 3, 50)\n",
    "\n",
    "betas = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
    "results = {b: {'kl': [], 'reward': [], 'objective': []} for b in betas}\n",
    "\n",
    "for shift in shift_amounts:\n",
    "    # ì •ì±… ì´ë™: ì„ í˜¸ í† í°ì— ë³´ë„ˆìŠ¤\n",
    "    shifted_logits = ref_logits.copy()\n",
    "    for t in high_reward_tokens:\n",
    "        shifted_logits[t] += shift\n",
    "    shifted_probs = np.exp(shifted_logits) / np.exp(shifted_logits).sum()\n",
    "\n",
    "    # KL divergence ê³„ì‚°\n",
    "    kl = np.sum(shifted_probs * np.log(shifted_probs / (ref_probs + 1e-10) + 1e-10))\n",
    "\n",
    "    # ë³´ìƒ ê³„ì‚° (ì„ í˜¸ í† í° í™•ë¥  í•©)\n",
    "    reward = sum(shifted_probs[t] for t in high_reward_tokens)\n",
    "\n",
    "    for beta in betas:\n",
    "        results[beta]['kl'].append(kl)\n",
    "        results[beta]['reward'].append(reward)\n",
    "        results[beta]['objective'].append(reward - beta * kl)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# (1) ë³´ìƒ vs KL íŠ¸ë ˆì´ë“œì˜¤í”„\n",
    "ax1 = axes[0]\n",
    "kls = results[0.01]['kl']\n",
    "rewards = results[0.01]['reward']\n",
    "ax1.plot(kls, rewards, 'b-o', lw=2, ms=3, label='ë³´ìƒ vs KL')\n",
    "ax1.set_xlabel(r'$D_{KL}[\\pi_\theta \\| \\pi_{ref}]$', fontsize=11)\n",
    "ax1.set_ylabel('Expected Reward', fontsize=11)\n",
    "ax1.set_title('ë³´ìƒ-KL íŠ¸ë ˆì´ë“œì˜¤í”„', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ë‹¤ì–‘í•œ Î²ì— ë”°ë¥¸ ëª©ì í•¨ìˆ˜\n",
    "ax2 = axes[1]\n",
    "colors_beta = ['#2196F3', '#FF9800', '#E91E63', '#9C27B0', '#4CAF50']\n",
    "for beta, color in zip(betas, colors_beta):\n",
    "    ax2.plot(shift_amounts, results[beta]['objective'],\n",
    "             lw=2, color=color, label=f'Î²={beta}')\n",
    "    # ìµœì ì  í‘œì‹œ\n",
    "    opt_idx = np.argmax(results[beta]['objective'])\n",
    "    ax2.plot(shift_amounts[opt_idx], results[beta]['objective'][opt_idx],\n",
    "             '*', ms=15, color=color)\n",
    "ax2.set_xlabel('ì •ì±… ì´ë™ëŸ‰', fontsize=11)\n",
    "ax2.set_ylabel(r'$\\mathbb{E}[r] - \beta D_{KL}$', fontsize=11)\n",
    "ax2.set_title(r'RLHF ëª©ì í•¨ìˆ˜ ($\beta$ ë³€í™”)', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# (3) KL ë°œì‚° ê³¡ì„ \n",
    "ax3 = axes[2]\n",
    "ax3.plot(shift_amounts, results[0.01]['kl'], 'r-', lw=2.5,\n",
    "         label=r'$D_{KL}[\\pi_\theta \\| \\pi_{ref}]$')\n",
    "ax3.fill_between(shift_amounts, 0, results[0.01]['kl'],\n",
    "                 alpha=0.1, color='red')\n",
    "ax3.set_xlabel('ì •ì±… ì´ë™ëŸ‰', fontsize=11)\n",
    "ax3.set_ylabel(r'$D_{KL}$', fontsize=11)\n",
    "ax3.set_title('KL Divergence ì¦ê°€', fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kl_divergence_penalty.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/kl_divergence_penalty.png\")\n",
    "\n",
    "# ìµœì ì  ë¶„ì„\n",
    "print(f\"\\nÎ²ë³„ ìµœì  ì •ì±… ì´ë™ ì§€ì :\")\n",
    "print(f\"{'Î²':>6} | {'ìµœì  ì´ë™ëŸ‰':>10} | {'ë³´ìƒ':>8} | {'KL':>8} | {'ëª©ì í•¨ìˆ˜':>10}\")\n",
    "print(f\"{'-'*52}\")\n",
    "for beta in betas:\n",
    "    opt_idx = np.argmax(results[beta]['objective'])\n",
    "    print(f\"{beta:>6.2f} | {shift_amounts[opt_idx]:>10.2f} | \"\n",
    "          f\"{results[beta]['reward'][opt_idx]:>8.4f} | \"\n",
    "          f\"{results[beta]['kl'][opt_idx]:>8.4f} | \"\n",
    "          f\"{results[beta]['objective'][opt_idx]:>+10.4f}\")\n",
    "print(f\"\\n  â†’ Î²ê°€ í´ìˆ˜ë¡ ì •ì±…ì´ ê¸°ì¤€ì— ê°€ê¹Œì´ ë¨¸ë¬´ë¦„ (ë³´ìˆ˜ì )\")\n",
    "print(f\"  â†’ Î²ê°€ ì‘ì„ìˆ˜ë¡ ë³´ìƒ ê·¹ëŒ€í™”ì— ì§‘ì¤‘ (ê³µê²©ì  â†’ Reward Hacking ìœ„í—˜)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| InstructGPT 3ë‹¨ê³„ | SFT â†’ Reward Model â†’ PPO | â­â­â­ |\n",
    "| Bradley-Terry | $P(y_w \\succ y_l) = \\sigma(\\Delta r)$ â€” ìŒë³„ ì„ í˜¸ í™•ë¥  | â­â­â­ |\n",
    "| RM Loss | $-\\mathbb{E}[\\log\\sigma(r(y_w) - r(y_l))]$ | â­â­â­ |\n",
    "| RLHF Objective | $\\mathbb{E}[r(y)] - \\beta D_{KL}[\\pi_\\theta \\| \\pi_{ref}]$ | â­â­â­ |\n",
    "| KL í˜ë„í‹° | $\\beta D_{KL}$ â€” ê¸°ì¤€ ì •ì±…ì—ì„œ ì´íƒˆ ì œí•œ | â­â­â­ |\n",
    "| Reward Hacking | Î²ê°€ ì‘ì„ ë•Œ ë³´ìƒ ëª¨ë¸ì˜ í—ˆì ì„ ì•…ìš©í•˜ëŠ” í˜„ìƒ | â­â­ |\n",
    "| SFT | ì‹œë²” ë°ì´í„°ë¡œ ì§€ë„í•™ìŠµ â€” RLHFì˜ ì²« ë²ˆì§¸ ë‹¨ê³„ | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$P(y_w \\succ y_l \\mid x) = \\sigma\\left(r(x, y_w) - r(x, y_l)\\right)$$\n",
    "\n",
    "$$\\mathcal{L}_{RM}(\\theta) = -\\mathbb{E}\\left[\\log \\sigma\\left(r_\\theta(x, y_w) - r_\\theta(x, y_l)\\right)\\right]$$\n",
    "\n",
    "$$\\max_\\theta \\; \\mathbb{E}_{y \\sim \\pi_\\theta}\\left[r_\\phi(x, y)\\right] - \\beta \\, D_{KL}\\left[\\pi_\\theta \\| \\pi_{ref}\\right]$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**04_dpo_and_preference_learning.ipynb** â€” Reward Model ì—†ì´ë„ ì„ í˜¸ í•™ìŠµì´ ê°€ëŠ¥í•œ DPO(Direct Preference Optimization)ì˜ ë² ì´ì¦ˆ ë„ì¶œê³¼ RLHF ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}