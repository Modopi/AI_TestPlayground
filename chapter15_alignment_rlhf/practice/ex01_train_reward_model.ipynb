{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: Reward Model 훈련\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: Bradley-Terry 확률 계산](#q1)\n",
    "- [Q2: Reward Model 손실 함수 구현](#q2)\n",
    "- [Q3: Preference Pair 데이터 생성](#q3)\n",
    "- [Q4: Reward Model 학습 및 평가](#q4)\n",
    "- [종합 도전: Full RM Training Pipeline](#bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q1: Bradley-Terry 확률 계산 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Bradley-Terry 선호 모델에서, Reward Model이 두 응답에 대해 다음 점수를 출력했습니다:\n",
    "\n",
    "$$r_\\theta(x, y_w) = 3.2, \\quad r_\\theta(x, y_l) = 1.8$$\n",
    "\n",
    "1. 선호 확률 $P(y_w \\succ y_l)$을 계산하세요\n",
    "2. 보상 차이가 2배($r(y_w)=6.4, r(y_l)=3.6$)가 되면 확률은?\n",
    "3. 두 결과의 차이가 의미하는 바를 설명하세요\n",
    "\n",
    "**Bradley-Terry 수식:**\n",
    "$$P(y_w \\succ y_l) = \\sigma(r(y_w) - r(y_l)) = \\frac{1}{1 + e^{-(r(y_w) - r(y_l))}}$$\n",
    "\n",
    "**여러분의 예측:** $P(y_w \\succ y_l)$ 은 `?` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: Bradley-Terry 확률 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def bt_probability(r_w, r_l):\n",
    "    diff = r_w - r_l\n",
    "    return 1.0 / (1.0 + np.exp(-diff))\n",
    "\n",
    "# Case 1: r(y_w) = 3.2, r(y_l) = 1.8\n",
    "r_w1, r_l1 = 3.2, 1.8\n",
    "p1 = bt_probability(r_w1, r_l1)\n",
    "print(f\"\\n[Case 1] r(y_w)={r_w1}, r(y_l)={r_l1}\")\n",
    "print(f\"  보상 차이: {r_w1 - r_l1:.1f}\")\n",
    "print(f\"  P(y_w > y_l) = sigma({r_w1 - r_l1:.1f}) = {p1:.6f}\")\n",
    "print(f\"  = 1 / (1 + exp(-{r_w1 - r_l1:.1f})) = 1 / (1 + {np.exp(-(r_w1-r_l1)):.4f}) = {p1:.6f}\")\n",
    "\n",
    "# Case 2: 2배 보상\n",
    "r_w2, r_l2 = 6.4, 3.6\n",
    "p2 = bt_probability(r_w2, r_l2)\n",
    "print(f\"\\n[Case 2] r(y_w)={r_w2}, r(y_l)={r_l2}\")\n",
    "print(f\"  보상 차이: {r_w2 - r_l2:.1f}\")\n",
    "print(f\"  P(y_w > y_l) = sigma({r_w2 - r_l2:.1f}) = {p2:.6f}\")\n",
    "\n",
    "# 해설\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  Case 1 확률: {p1:.4f} (약 {p1*100:.1f}%)\")\n",
    "print(f\"  Case 2 확률: {p2:.4f} (약 {p2*100:.1f}%)\")\n",
    "print(f\"  차이: {(p2-p1)*100:.2f}%p 증가\")\n",
    "print(f\"  → 보상 값 자체가 아닌 '차이'가 확률을 결정합니다.\")\n",
    "print(f\"  → 차이가 같으면(1.4 vs 2.8) 확률이 달라집니다.\")\n",
    "print(f\"  → 시그모이드 함수의 비선형성 때문입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q2: Reward Model 손실 함수 구현 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Bradley-Terry 기반 Reward Model의 손실 함수를 TensorFlow로 구현하세요:\n",
    "\n",
    "$$\\mathcal{L}_{RM}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\log \\sigma\\left(r_\\theta(x_i, y_w^i) - r_\\theta(x_i, y_l^i)\\right)$$\n",
    "\n",
    "다음 배치에 대해 손실을 계산하세요:\n",
    "- 배치 크기: 4\n",
    "- 선호 보상: `[2.1, 1.5, 3.0, 0.8]`\n",
    "- 비선호 보상: `[1.0, 1.2, 0.5, 0.9]`\n",
    "\n",
    "**여러분의 예측:** 손실값은 `?`, 정확도는 `?/4` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: Reward Model 손실 함수\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def reward_model_loss(r_chosen, r_rejected):\n",
    "    # Bradley-Terry 기반 RM 손실\n",
    "    diff = r_chosen - r_rejected\n",
    "    loss = -tf.reduce_mean(tf.math.log_sigmoid(diff))\n",
    "    accuracy = tf.reduce_mean(tf.cast(diff > 0, tf.float32))\n",
    "    return loss, accuracy, diff\n",
    "\n",
    "r_chosen = tf.constant([2.1, 1.5, 3.0, 0.8])\n",
    "r_rejected = tf.constant([1.0, 1.2, 0.5, 0.9])\n",
    "\n",
    "loss, acc, diffs = reward_model_loss(r_chosen, r_rejected)\n",
    "\n",
    "print(f\"\\n입력 데이터:\")\n",
    "print(f\"  r_chosen:   {r_chosen.numpy()}\")\n",
    "print(f\"  r_rejected: {r_rejected.numpy()}\")\n",
    "print(f\"  차이 (Δr):  {diffs.numpy()}\")\n",
    "\n",
    "print(f\"\\n단계별 계산:\")\n",
    "for i in range(4):\n",
    "    d = diffs.numpy()[i]\n",
    "    sig = 1 / (1 + np.exp(-d))\n",
    "    log_sig = np.log(sig)\n",
    "    print(f\"  [{i}] Δr={d:+.1f} → σ(Δr)={sig:.4f} → log σ={log_sig:.4f} \"\n",
    "          f\"{'✅' if d > 0 else '❌'}\")\n",
    "\n",
    "print(f\"\\n결과:\")\n",
    "print(f\"  RM Loss = -mean(log_sigmoid) = {loss.numpy():.4f}\")\n",
    "print(f\"  정확도 = {acc.numpy():.2f} ({int(acc.numpy()*4)}/4)\")\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  샘플 4번째: r_chosen(0.8) < r_rejected(0.9) → 순서 역전!\")\n",
    "print(f\"  이 역전된 샘플이 손실을 크게 만듭니다.\")\n",
    "print(f\"  학습을 통해 chosen의 보상을 높이고 rejected를 낮춰야 합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Preference Pair 데이터 생성 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "실제 RLHF/DPO 학습에서는 선호 쌍(Preference Pair) 데이터가 필요합니다.\n",
    "다음 조건으로 합성 데이터를 생성하세요:\n",
    "\n",
    "1. 프롬프트: 10차원 랜덤 벡터 (300개)\n",
    "2. 선호 응답: 프롬프트의 선형 변환 + 작은 노이즈\n",
    "3. 비선호 응답: 프롬프트의 선형 변환 + 큰 노이즈 + 약간의 편향\n",
    "4. 품질 점수: 프롬프트와 응답의 코사인 유사도로 계산\n",
    "\n",
    "**여러분의 예측:** 선호 응답의 평균 품질 점수는 비선호보다 `?`만큼 높습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: Preference Pair 데이터 생성\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_pairs = 300\n",
    "prompt_dim = 10\n",
    "response_dim = 10\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompts = np.random.randn(n_pairs, prompt_dim).astype(np.float32)\n",
    "\n",
    "# 이상적 변환 행렬\n",
    "W_ideal = np.random.randn(prompt_dim, response_dim).astype(np.float32) * 0.3\n",
    "\n",
    "# 선호 응답: 이상적 변환 + 작은 노이즈\n",
    "chosen_responses = prompts @ W_ideal + np.random.randn(n_pairs, response_dim).astype(np.float32) * 0.1\n",
    "# 비선호 응답: 이상적 변환 + 큰 노이즈 + 편향\n",
    "rejected_responses = prompts @ W_ideal + np.random.randn(n_pairs, response_dim).astype(np.float32) * 0.8 + 0.5\n",
    "\n",
    "# 품질 점수: 이상적 응답과의 코사인 유사도\n",
    "ideal_responses = prompts @ W_ideal\n",
    "def cosine_similarity(a, b):\n",
    "    dot = np.sum(a * b, axis=1)\n",
    "    norm_a = np.linalg.norm(a, axis=1)\n",
    "    norm_b = np.linalg.norm(b, axis=1)\n",
    "    return dot / (norm_a * norm_b + 1e-8)\n",
    "\n",
    "chosen_quality = cosine_similarity(chosen_responses, ideal_responses)\n",
    "rejected_quality = cosine_similarity(rejected_responses, ideal_responses)\n",
    "\n",
    "print(f\"\\n데이터셋 통계:\")\n",
    "print(f\"  프롬프트 수: {n_pairs}\")\n",
    "print(f\"  프롬프트 차원: {prompt_dim}\")\n",
    "print(f\"  응답 차원: {response_dim}\")\n",
    "\n",
    "print(f\"\\n품질 점수 (코사인 유사도):\")\n",
    "print(f\"  선호 응답:   평균={chosen_quality.mean():.4f}, 표준편차={chosen_quality.std():.4f}\")\n",
    "print(f\"  비선호 응답: 평균={rejected_quality.mean():.4f}, 표준편차={rejected_quality.std():.4f}\")\n",
    "print(f\"  평균 차이:   {(chosen_quality - rejected_quality).mean():.4f}\")\n",
    "\n",
    "# 올바른 순서 비율 (chosen > rejected)\n",
    "correct_order = (chosen_quality > rejected_quality).mean()\n",
    "print(f\"\\n순서 정확도: {correct_order:.1%} (chosen > rejected)\")\n",
    "print(f\"  → {correct_order:.1%}의 쌍에서 선호 응답이 실제로 더 높은 품질\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  노이즈가 작은 chosen이 이상적 응답에 더 가깝습니다.\")\n",
    "print(f\"  하지만 {(1-correct_order)*100:.1f}%의 쌍은 역전되어 있어 노이지한 라벨입니다.\")\n",
    "print(f\"  실제 RLHF에서도 인간 평가자 간 불일치율이 20-30%에 달합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q4: Reward Model 학습 및 평가 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Q3에서 생성한 데이터로 Reward Model을 학습하고 평가하세요:\n",
    "\n",
    "1. MLP 기반 Reward Model 구축 (입력: prompt+response → 스칼라 보상)\n",
    "2. Bradley-Terry 손실로 30 에폭 학습\n",
    "3. 선호 정확도(Pairwise Accuracy)와 Calibration을 측정\n",
    "\n",
    "$$\\text{Pairwise Accuracy} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{1}\\left[r_\\theta(x_i, y_w^i) > r_\\theta(x_i, y_l^i)\\right]$$\n",
    "\n",
    "**여러분의 예측:** 학습 후 선호 정확도는 `?%` 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: Reward Model 학습 및 평가\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 데이터 준비: (prompt, response) → concat\n",
    "X_chosen = np.concatenate([prompts, chosen_responses], axis=1)\n",
    "X_rejected = np.concatenate([prompts, rejected_responses], axis=1)\n",
    "\n",
    "# 학습/검증 분리\n",
    "n_train = int(n_pairs * 0.8)\n",
    "train_chosen, val_chosen = X_chosen[:n_train], X_chosen[n_train:]\n",
    "train_rejected, val_rejected = X_rejected[:n_train], X_rejected[n_train:]\n",
    "\n",
    "# Reward Model 구축\n",
    "reward_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu',\n",
    "                          input_shape=(prompt_dim + response_dim,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "n_epochs = 30\n",
    "batch_size = 32\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    epoch_correct = 0\n",
    "\n",
    "    indices = np.random.permutation(n_train)\n",
    "    for start in range(0, n_train, batch_size):\n",
    "        batch_idx = indices[start:start + batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            r_w = reward_model(train_chosen[batch_idx], training=True)[:, 0]\n",
    "            r_l = reward_model(train_rejected[batch_idx], training=True)[:, 0]\n",
    "            diff = r_w - r_l\n",
    "            loss = -tf.reduce_mean(tf.math.log_sigmoid(diff))\n",
    "\n",
    "        grads = tape.gradient(loss, reward_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, reward_model.trainable_variables))\n",
    "        epoch_losses.append(loss.numpy())\n",
    "        epoch_correct += tf.reduce_sum(tf.cast(diff > 0, tf.float32)).numpy()\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    train_acc = epoch_correct / n_train\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # 검증 정확도\n",
    "    r_w_val = reward_model(val_chosen, training=False)[:, 0]\n",
    "    r_l_val = reward_model(val_rejected, training=False)[:, 0]\n",
    "    val_acc = tf.reduce_mean(tf.cast((r_w_val - r_l_val) > 0, tf.float32)).numpy()\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: Loss={avg_loss:.4f}, \"\n",
    "              f\"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "# Calibration 평가\n",
    "r_w_all = reward_model(X_chosen).numpy().flatten()\n",
    "r_l_all = reward_model(X_rejected).numpy().flatten()\n",
    "pred_probs = 1 / (1 + np.exp(-(r_w_all - r_l_all)))\n",
    "\n",
    "# 확률 구간별 실제 정확도\n",
    "n_bins = 5\n",
    "bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "calibration_data = []\n",
    "for i in range(n_bins):\n",
    "    mask = (pred_probs >= bin_edges[i]) & (pred_probs < bin_edges[i + 1])\n",
    "    if mask.sum() > 0:\n",
    "        actual_acc = (r_w_all[mask] > r_l_all[mask]).mean()\n",
    "        predicted_prob = pred_probs[mask].mean()\n",
    "        calibration_data.append((predicted_prob, actual_acc, mask.sum()))\n",
    "\n",
    "print(f\"\\n최종 결과:\")\n",
    "print(f\"  학습 정확도: {train_accs[-1]:.4f}\")\n",
    "print(f\"  검증 정확도: {val_accs[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\nCalibration 평가:\")\n",
    "print(f\"{'예측 확률':>12} | {'실제 정확도':>12} | {'샘플 수':>8}\")\n",
    "print(\"-\" * 38)\n",
    "for pred, actual, n in calibration_data:\n",
    "    print(f\"{pred:>12.3f} | {actual:>12.3f} | {n:>8d}\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  Reward Model이 선호 응답에 더 높은 보상을 줄수록 정확도가 높습니다.\")\n",
    "print(f\"  Calibration은 '예측 확률과 실제 정확도가 얼마나 일치하는가'를 측정합니다.\")\n",
    "print(f\"  이상적으로는 예측 80% → 실제 80%가 되어야 합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 종합 도전: Full RM Training Pipeline <a name='bonus'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Q1-Q4의 모든 구성 요소를 통합하여 완전한 Reward Model 학습 파이프라인을 구현하세요:\n",
    "\n",
    "1. **데이터 생성**: 500개의 선호 쌍 + 학습/검증/테스트 분리 (70/15/15)\n",
    "2. **모델 학습**: Bradley-Terry 손실 + 학습률 스케줄링\n",
    "3. **평가**: Pairwise Accuracy + Calibration 플롯\n",
    "4. **분석**: 보상 분포 히스토그램 + 학습 곡선\n",
    "\n",
    "이 파이프라인이 실제 RLHF에서 어떻게 사용되는지 설명하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이: Full RM Training Pipeline ────────────────────\n",
    "print(\"=\" * 55)\n",
    "print(\"종합 도전: Full Reward Model Training Pipeline\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. 데이터 생성\n",
    "n_total = 500\n",
    "prompt_d = 10\n",
    "response_d = 10\n",
    "prompts_full = np.random.randn(n_total, prompt_d).astype(np.float32)\n",
    "W_transform = np.random.randn(prompt_d, response_d).astype(np.float32) * 0.3\n",
    "\n",
    "chosen_full = prompts_full @ W_transform + np.random.randn(n_total, response_d).astype(np.float32) * 0.1\n",
    "rejected_full = prompts_full @ W_transform + np.random.randn(n_total, response_d).astype(np.float32) * 0.8 + 0.3\n",
    "\n",
    "X_c = np.concatenate([prompts_full, chosen_full], axis=1)\n",
    "X_r = np.concatenate([prompts_full, rejected_full], axis=1)\n",
    "\n",
    "# 학습/검증/테스트 분리 (70/15/15)\n",
    "n1 = int(n_total * 0.7)\n",
    "n2 = int(n_total * 0.85)\n",
    "tr_c, va_c, te_c = X_c[:n1], X_c[n1:n2], X_c[n2:]\n",
    "tr_r, va_r, te_r = X_r[:n1], X_r[n1:n2], X_r[n2:]\n",
    "print(f\"\\n데이터 분할: 학습={n1}, 검증={n2-n1}, 테스트={n_total-n2}\")\n",
    "\n",
    "# 2. 모델 + 학습률 스케줄링\n",
    "rm_full = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(prompt_d + response_d,)),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.002, decay_steps=50 * (n1 // 32 + 1))\n",
    "opt_full = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# 학습\n",
    "epochs_full = 50\n",
    "batch_sz = 32\n",
    "full_train_losses, full_val_losses = [], []\n",
    "full_train_accs, full_val_accs = [], []\n",
    "\n",
    "for epoch in range(epochs_full):\n",
    "    idx = np.random.permutation(n1)\n",
    "    e_losses = []\n",
    "    e_correct = 0\n",
    "    for s in range(0, n1, batch_sz):\n",
    "        bi = idx[s:s+batch_sz]\n",
    "        with tf.GradientTape() as tape:\n",
    "            rw = rm_full(tr_c[bi], training=True)[:, 0]\n",
    "            rl = rm_full(tr_r[bi], training=True)[:, 0]\n",
    "            loss = -tf.reduce_mean(tf.math.log_sigmoid(rw - rl))\n",
    "        grads = tape.gradient(loss, rm_full.trainable_variables)\n",
    "        opt_full.apply_gradients(zip(grads, rm_full.trainable_variables))\n",
    "        e_losses.append(loss.numpy())\n",
    "        e_correct += tf.reduce_sum(tf.cast((rw-rl) > 0, tf.float32)).numpy()\n",
    "\n",
    "    # 검증\n",
    "    vw = rm_full(va_c, training=False)[:, 0]\n",
    "    vl = rm_full(va_r, training=False)[:, 0]\n",
    "    v_loss = -tf.reduce_mean(tf.math.log_sigmoid(vw - vl)).numpy()\n",
    "    v_acc = tf.reduce_mean(tf.cast((vw-vl) > 0, tf.float32)).numpy()\n",
    "\n",
    "    full_train_losses.append(np.mean(e_losses))\n",
    "    full_val_losses.append(v_loss)\n",
    "    full_train_accs.append(e_correct / n1)\n",
    "    full_val_accs.append(v_acc)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"  Epoch {epoch+1:3d}: TrLoss={np.mean(e_losses):.4f}, \"\n",
    "              f\"VaLoss={v_loss:.4f}, TrAcc={e_correct/n1:.4f}, VaAcc={v_acc:.4f}\")\n",
    "\n",
    "# 3. 테스트 평가\n",
    "tw = rm_full(te_c, training=False).numpy().flatten()\n",
    "tl = rm_full(te_r, training=False).numpy().flatten()\n",
    "test_acc = (tw > tl).mean()\n",
    "print(f\"\\n테스트 셋 Pairwise Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# 4. 시각화\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 10))\n",
    "\n",
    "# (1) 학습 곡선\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(full_train_losses, 'b-', lw=2, label='Train Loss')\n",
    "ax1.plot(full_val_losses, 'r--', lw=2, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('Reward Model 학습 곡선', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) 정확도 곡선\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(full_train_accs, 'b-', lw=2, label='Train Acc')\n",
    "ax2.plot(full_val_accs, 'r--', lw=2, label='Val Acc')\n",
    "ax2.axhline(y=0.5, color='gray', ls=':', lw=1.5, label='Random')\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Accuracy', fontsize=11)\n",
    "ax2.set_title('Pairwise Accuracy', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "# (3) 보상 분포\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(tw, bins=25, alpha=0.6, color='green', label='Chosen', density=True)\n",
    "ax3.hist(tl, bins=25, alpha=0.6, color='red', label='Rejected', density=True)\n",
    "ax3.axvline(x=tw.mean(), color='darkgreen', ls='--', lw=2)\n",
    "ax3.axvline(x=tl.mean(), color='darkred', ls='--', lw=2)\n",
    "ax3.set_xlabel('보상 점수', fontsize=11)\n",
    "ax3.set_ylabel('밀도', fontsize=11)\n",
    "ax3.set_title('테스트 셋 보상 분포', fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# (4) Calibration 플롯\n",
    "ax4 = axes[1, 1]\n",
    "pred_p = 1 / (1 + np.exp(-(tw - tl)))\n",
    "n_cal_bins = 10\n",
    "cal_predicted, cal_actual = [], []\n",
    "for i in range(n_cal_bins):\n",
    "    lo = i / n_cal_bins\n",
    "    hi = (i + 1) / n_cal_bins\n",
    "    mask = (pred_p >= lo) & (pred_p < hi)\n",
    "    if mask.sum() > 2:\n",
    "        cal_predicted.append(pred_p[mask].mean())\n",
    "        cal_actual.append((tw[mask] > tl[mask]).mean())\n",
    "\n",
    "ax4.plot([0, 1], [0, 1], 'k--', lw=1.5, label='완벽한 Calibration')\n",
    "ax4.plot(cal_predicted, cal_actual, 'bo-', lw=2, ms=8, label='RM Calibration')\n",
    "ax4.fill_between([0, 1], [0, 1], alpha=0.05, color='gray')\n",
    "ax4.set_xlabel('예측 확률', fontsize=11)\n",
    "ax4.set_ylabel('실제 정확도', fontsize=11)\n",
    "ax4.set_title('Calibration Plot', fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/rm_training_pipeline.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"그래프 저장됨: chapter15_alignment_rlhf/practice/rm_training_pipeline.png\")\n",
    "\n",
    "print(f\"\\n파이프라인 최종 요약:\")\n",
    "print(f\"{'메트릭':<20} | {'값':>10}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"{'학습 최종 Loss':<20} | {full_train_losses[-1]:>10.4f}\")\n",
    "print(f\"{'검증 최종 Loss':<20} | {full_val_losses[-1]:>10.4f}\")\n",
    "print(f\"{'학습 Accuracy':<20} | {full_train_accs[-1]:>10.4f}\")\n",
    "print(f\"{'검증 Accuracy':<20} | {full_val_accs[-1]:>10.4f}\")\n",
    "print(f\"{'테스트 Accuracy':<20} | {test_acc:>10.4f}\")\n",
    "print(f\"{'보상 마진':<20} | {(tw-tl).mean():>10.4f}\")\n",
    "\n",
    "print(f\"\\n[해설]\")\n",
    "print(f\"  이 Reward Model은 RLHF의 2단계에서 사용됩니다:\")\n",
    "print(f\"  1. 인간 평가자가 만든 선호 쌍으로 RM을 학습\")\n",
    "print(f\"  2. 학습된 RM이 새로운 응답에 점수를 매김\")\n",
    "print(f\"  3. PPO가 RM 점수를 최대화하도록 정책을 학습\")\n",
    "print(f\"  Calibration이 좋을수록 PPO의 학습 신호가 신뢰할 수 있습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "\n",
    "이 실습에서 다룬 핵심 개념:\n",
    "\n",
    "| 개념 | 구현 | 핵심 포인트 |\n",
    "|------|------|-----------|\n",
    "| Bradley-Terry | `sigma(r_w - r_l)` | 보상 '차이'가 확률 결정 |\n",
    "| RM Loss | `-log_sigmoid(Δr)` | 선호 보상 > 비선호 보상으로 학습 |\n",
    "| 선호 데이터 | 노이즈 차이로 품질 구분 | 실제로는 20-30% 라벨 노이즈 존재 |\n",
    "| Calibration | 예측 확률 vs 실제 정확도 | PPO 학습 신호의 신뢰성 결정 |\n",
    "\n",
    "**다음 실습 →** `ex02_dpo_fine_tuning_lora.ipynb`에서 DPO + LoRA를 사용한 직접 선호 최적화를 구현합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}