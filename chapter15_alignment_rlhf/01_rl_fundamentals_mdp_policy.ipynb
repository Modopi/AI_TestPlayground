{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” RL ê¸°ì´ˆì™€ MDP/Policy Gradient\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- MDP(Markov Decision Process)ì˜ ìˆ˜í•™ì  ì •ì˜ì™€ êµ¬ì„± ìš”ì†Œë¥¼ ì´í•´í•œë‹¤\n",
    "- Bellman ë°©ì •ì‹ì˜ ìœ ë„ ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ì „ê°œí•˜ê³  Value Iterationì„ êµ¬í˜„í•œë‹¤\n",
    "- REINFORCE(Policy Gradient) ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³  êµ¬í˜„í•œë‹¤\n",
    "- ë¦¬ì›Œë“œ ì‹ í˜¸(Reward Signal)ê°€ NLP ì •ë ¬ ë¬¸ì œì—ì„œ ì–´ë–»ê²Œ ì„¤ê³„ë˜ëŠ”ì§€ ì´í•´í•œë‹¤\n",
    "- ê°•í™”í•™ìŠµê³¼ ì§€ë„í•™ìŠµì˜ ì°¨ì´ë¥¼ êµ¬ë³„í•˜ê³ , LLM ì •ë ¬ì—ì„œì˜ RL í•„ìš”ì„±ì„ ì„¤ëª…í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: MDPì™€ Bellman ë°©ì •ì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [GridWorld MDP êµ¬í˜„](#2.-GridWorld-MDP-êµ¬í˜„)\n",
    "3. [Value Iteration ì‹œê°í™”](#3.-Value-Iteration-ì‹œê°í™”)\n",
    "4. [REINFORCE Policy Gradient](#4.-REINFORCE-Policy-Gradient)\n",
    "5. [NLPë¥¼ ìœ„í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„](#5.-NLP-ë¦¬ì›Œë“œ-ì‹ í˜¸-ì„¤ê³„)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### MDP (Markov Decision Process)\n",
    "\n",
    "MDPëŠ” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤:\n",
    "\n",
    "$$\\mathcal{M} = (S, A, P, R, \\gamma)$$\n",
    "\n",
    "- $S$: ìƒíƒœ ê³µê°„ (State space)\n",
    "- $A$: í–‰ë™ ê³µê°„ (Action space)\n",
    "- $P(s' | s, a)$: ìƒíƒœ ì „ì´ í™•ë¥  (Transition probability)\n",
    "- $R(s, a)$: ë³´ìƒ í•¨ìˆ˜ (Reward function)\n",
    "- $\\gamma \\in [0, 1)$: í• ì¸ ì¸ì (Discount factor)\n",
    "\n",
    "### ì •ì±…ê³¼ ê°€ì¹˜ í•¨ìˆ˜\n",
    "\n",
    "**ì •ì±… (Policy):**\n",
    "\n",
    "$$\\pi(a | s) = P(A_t = a \\mid S_t = s)$$\n",
    "\n",
    "**ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ (State Value Function):**\n",
    "\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]$$\n",
    "\n",
    "**í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ (Action Value Function):**\n",
    "\n",
    "$$Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]$$\n",
    "\n",
    "### Bellman ë°©ì •ì‹\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s')\\right]$$\n",
    "\n",
    "**Bellman ìµœì  ë°©ì •ì‹:**\n",
    "\n",
    "$$V^*(s) = \\max_{a} \\left[R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^*(s')\\right]$$\n",
    "\n",
    "### REINFORCE (Policy Gradient)\n",
    "\n",
    "ëª©ì  í•¨ìˆ˜:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T} R(s_t, a_t)\\right]$$\n",
    "\n",
    "**Policy Gradient Theorem:**\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[G_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right]$$\n",
    "\n",
    "- $G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k+1}$: ëˆ„ì  ë³´ìƒ (Return)\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$: Score function (ë¡œê·¸ ì •ì±…ì˜ ê¸°ìš¸ê¸°)\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| MDP 5-íŠœí”Œ | $(S, A, P, R, \\gamma)$ | ìˆœì°¨ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬ |\n",
    "| Bellman ë°©ì •ì‹ | $V^\\pi(s) = \\sum_a \\pi(a|s)[R + \\gamma \\sum_{s'} PV^\\pi]$ | í˜„ì¬ ê°€ì¹˜ = ì¦‰ê° ë³´ìƒ + ë¯¸ë˜ ê°€ì¹˜ |\n",
    "| Policy Gradient | $\\nabla_\\theta J = \\mathbb{E}[G_t \\nabla_\\theta \\log \\pi_\\theta]$ | ë†’ì€ ë³´ìƒ â†’ í•´ë‹¹ í–‰ë™ í™•ë¥  ì¦ê°€ |\n",
    "| Return | $G_t = \\sum_k \\gamma^k R_{t+k+1}$ | ë¯¸ë˜ ë³´ìƒì˜ í• ì¸ í•© |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ê°•í™”í•™ìŠµ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ MDPê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ë³´ë“œê²Œì„ì„ ìƒê°í•´ ë³´ì„¸ìš”! ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ì„œ(í–‰ë™) ì¹¸ì„ ì´ë™í•˜ê³ (ìƒíƒœ ì „ì´), \n",
    "> ì–´ë–¤ ì¹¸ì— ë„ì°©í•˜ë©´ ìš©ëˆì„ ë°›ê³ (ë³´ìƒ), ì–´ë–¤ ì¹¸ì— ê°€ë©´ ë²Œê¸ˆì„ ë‚´ìš”(ìŒì˜ ë³´ìƒ).\n",
    "\n",
    "MDPëŠ” ì´ëŸ° ê²Œì„ì˜ ê·œì¹™ì„ ìˆ˜í•™ìœ¼ë¡œ ì •ë¦¬í•œ ê±°ì˜ˆìš”:\n",
    "- **ìƒíƒœ(S)**: ì§€ê¸ˆ ë‚´ê°€ ì–´ë””ì— ìˆëŠ”ì§€ (ê²Œì„íŒ ìœ„ì˜ ë‚´ ìœ„ì¹˜)\n",
    "- **í–‰ë™(A)**: ë‚´ê°€ í•  ìˆ˜ ìˆëŠ” ì„ íƒ (ìœ„/ì•„ë˜/ì™¼ìª½/ì˜¤ë¥¸ìª½ ì´ë™)\n",
    "- **ë³´ìƒ(R)**: í–‰ë™ì˜ ê²°ê³¼ë¡œ ë°›ëŠ” ì ìˆ˜ (ìš©ëˆ ë˜ëŠ” ë²Œê¸ˆ)\n",
    "- **í• ì¸ì¸ì(Î³)**: ë‚˜ì¤‘ì— ë°›ì„ ë³´ìƒì€ ì¡°ê¸ˆ ëœ ì¤‘ìš”í•´ìš” (ì§€ê¸ˆ 100ì› > ë‚´ì¼ 100ì›)\n",
    "\n",
    "#### ğŸ¯ Policy Gradientê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ëˆˆì„ ê°ê³  ë‹¤íŠ¸ë¥¼ ë˜ì§€ëŠ” ì—°ìŠµì„ í•œë‹¤ê³  í•´ ë´ìš”!\n",
    "\n",
    "ì²˜ìŒì—ëŠ” ì•„ë¬´ ë°ë‚˜ ë˜ì§€ì§€ë§Œ, **ê³¼ë…ì— ë§ì„ ë•Œë§ˆë‹¤ ê·¸ë•Œ ë˜ì§„ ë°©ë²•ì„ ë” ë§ì´ ì¨ìš”**.\n",
    "ì´ê²ƒì´ ë°”ë¡œ Policy Gradientì˜ í•µì‹¬ì´ì—ìš”:\n",
    "- ì¢‹ì€ ê²°ê³¼($G_t$ê°€ í¼) â†’ ê·¸ í–‰ë™ì„ **ë” ìì£¼** í•˜ë„ë¡ ì •ì±… ì—…ë°ì´íŠ¸\n",
    "- ë‚˜ìœ ê²°ê³¼($G_t$ê°€ ì‘ìŒ) â†’ ê·¸ í–‰ë™ì„ **ëœ ìì£¼** í•˜ë„ë¡ ì •ì±… ì—…ë°ì´íŠ¸\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: Bellman ë°©ì •ì‹ ê³„ì‚°\n",
    "\n",
    "2ê°œì˜ ìƒíƒœ $s_1, s_2$ì™€ ê²°ì •ì  ì •ì±… $\\pi(s_1) = a_R$ (ì˜¤ë¥¸ìª½)ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "- $R(s_1, a_R) = 5$, $P(s_2 | s_1, a_R) = 1.0$\n",
    "- $R(s_2, a_R) = 10$ (ì¢…ë£Œ ìƒíƒœ), $\\gamma = 0.9$\n",
    "\n",
    "$V^\\pi(s_1)$ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$V^\\pi(s_2) = 10 \\quad \\text{(ì¢…ë£Œ ìƒíƒœ, ì´í›„ ë³´ìƒ ì—†ìŒ)}$$\n",
    "\n",
    "$$V^\\pi(s_1) = R(s_1, a_R) + \\gamma \\cdot P(s_2|s_1, a_R) \\cdot V^\\pi(s_2)$$\n",
    "\n",
    "$$= 5 + 0.9 \\times 1.0 \\times 10 = 5 + 9 = 14$$\n",
    "\n",
    "â†’ $V^\\pi(s_1) = 14$. í˜„ì¬ ë³´ìƒ(5)ê³¼ í• ì¸ëœ ë¯¸ë˜ ë³´ìƒ(9)ì˜ í•©ì…ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: Return ê³„ì‚°\n",
    "\n",
    "ë³´ìƒ ì‹œí€€ìŠ¤ $R_1=1, R_2=2, R_3=3$ì´ê³  $\\gamma=0.5$ì¼ ë•Œ $G_0$ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3 = 1 + 0.5 \\times 2 + 0.25 \\times 3 = 1 + 1 + 0.75 = 2.75$$\n",
    "\n",
    "â†’ í• ì¸ ì¸ìê°€ ì‘ì„ìˆ˜ë¡ ê·¼ì‹œì•ˆì (ê°€ê¹Œìš´ ë³´ìƒ ì„ í˜¸), í´ìˆ˜ë¡ ì›ì‹œì•ˆì (ë¯¸ë˜ ë³´ìƒë„ ì¤‘ì‹œ)ì…ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GridWorld MDP êµ¬í˜„ <a name='2.-GridWorld-MDP-êµ¬í˜„'></a>\n",
    "\n",
    "4Ã—4 ê²©ì ì„¸ê³„(GridWorld)ë¥¼ MDPë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ìƒ/í•˜/ì¢Œ/ìš°ë¡œ ì´ë™í•˜ë©°, \n",
    "ëª©í‘œ ì§€ì ì— ë„ë‹¬í•˜ë©´ +1 ë³´ìƒ, í•¨ì •ì— ë¹ ì§€ë©´ -1 ë³´ìƒì„ ë°›ìŠµë‹ˆë‹¤.\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ | ì„¤ì • |\n",
    "|-----------|------|\n",
    "| ìƒíƒœ ê³µê°„ | 4Ã—4 = 16ê°œ ìƒíƒœ |\n",
    "| í–‰ë™ ê³µê°„ | {ìƒ, í•˜, ì¢Œ, ìš°} = 4ê°œ |\n",
    "| ë³´ìƒ | ëª©í‘œ(+1), í•¨ì •(-1), ì´ë™(-0.04) |\n",
    "| í• ì¸ ì¸ì | $\\gamma = 0.99$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ GridWorld MDP êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class GridWorldMDP:\n",
    "    # 4x4 ê²©ì ì„¸ê³„ MDP êµ¬í˜„\n",
    "    # ìƒíƒœ: 0~15 (4x4 ê²©ì), í–‰ë™: 0=ìƒ, 1=í•˜, 2=ì¢Œ, 3=ìš°\n",
    "\n",
    "    def __init__(self, size=4, gamma=0.99):\n",
    "        self.size = size\n",
    "        self.n_states = size * size\n",
    "        self.n_actions = 4\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # íŠ¹ìˆ˜ ìƒíƒœ ì„¤ì •\n",
    "        self.goal = 15       # ìš°í•˜ë‹¨ = ëª©í‘œ\n",
    "        self.trap = 11       # í•¨ì •\n",
    "        self.terminal = {self.goal, self.trap}\n",
    "\n",
    "        # ë³´ìƒ ì„¤ì •\n",
    "        self.rewards = np.full(self.n_states, -0.04)  # ì´ë™ ë¹„ìš©\n",
    "        self.rewards[self.goal] = 1.0   # ëª©í‘œ ë„ë‹¬\n",
    "        self.rewards[self.trap] = -1.0  # í•¨ì •\n",
    "\n",
    "        # í–‰ë™ ë°©í–¥: (í–‰ ë³€í™”, ì—´ ë³€í™”)\n",
    "        self.action_effects = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "        self.action_names = ['â†‘', 'â†“', 'â†', 'â†’']\n",
    "\n",
    "    def _to_rc(self, s):\n",
    "        return s // self.size, s % self.size\n",
    "\n",
    "    def _to_s(self, r, c):\n",
    "        return r * self.size + c\n",
    "\n",
    "    def step(self, state, action):\n",
    "        # ì¢…ë£Œ ìƒíƒœë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "        if state in self.terminal:\n",
    "            return state, 0.0\n",
    "\n",
    "        r, c = self._to_rc(state)\n",
    "        dr, dc = self.action_effects[action]\n",
    "        nr, nc = r + dr, c + dc\n",
    "\n",
    "        # ê²©ì ë²½ ì²´í¬\n",
    "        if 0 <= nr < self.size and 0 <= nc < self.size:\n",
    "            next_state = self._to_s(nr, nc)\n",
    "        else:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state, self.rewards[next_state]\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        # ê²°ì •ì  ì „ì´ (í™•ë¥  1.0)\n",
    "        next_state, reward = self.step(state, action)\n",
    "        return [(next_state, reward, 1.0)]\n",
    "\n",
    "\n",
    "env = GridWorldMDP()\n",
    "print(f\"GridWorld MDP ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"  ìƒíƒœ ê³µê°„: {env.n_states}ê°œ (4Ã—4 ê²©ì)\")\n",
    "print(f\"  í–‰ë™ ê³µê°„: {env.n_actions}ê°œ ({', '.join(env.action_names)})\")\n",
    "print(f\"  í• ì¸ ì¸ì: Î³ = {env.gamma}\")\n",
    "print(f\"  ëª©í‘œ ìƒíƒœ: {env.goal} (ë³´ìƒ: +{env.rewards[env.goal]})\")\n",
    "print(f\"  í•¨ì • ìƒíƒœ: {env.trap} (ë³´ìƒ: {env.rewards[env.trap]})\")\n",
    "print(f\"  ì´ë™ ë¹„ìš©: {env.rewards[0]}\")\n",
    "\n",
    "# ê²©ì ë³´ìƒ ë§µ ì¶œë ¥\n",
    "print(f\"\\në³´ìƒ ë§µ (4Ã—4):\")\n",
    "reward_grid = env.rewards.reshape(4, 4)\n",
    "for r in range(4):\n",
    "    row_str = \"\"\n",
    "    for c in range(4):\n",
    "        s = r * 4 + c\n",
    "        if s == env.goal:\n",
    "            row_str += \" [+1.0 G] \"\n",
    "        elif s == env.trap:\n",
    "            row_str += \" [-1.0 T] \"\n",
    "        else:\n",
    "            row_str += f\" [{env.rewards[s]:+.2f}] \"\n",
    "    print(f\"  {row_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. Value Iteration ì‹œê°í™” <a name='3.-Value-Iteration-ì‹œê°í™”'></a>\n",
    "\n",
    "Bellman ìµœì  ë°©ì •ì‹ì„ ë°˜ë³µì ìœ¼ë¡œ ì ìš©í•˜ì—¬ ìµœì  ê°€ì¹˜ í•¨ìˆ˜ $V^*$ë¥¼ êµ¬í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_{a} \\left[R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V_k(s')\\right]$$\n",
    "\n",
    "ìˆ˜ë ´ ì¡°ê±´: $\\max_s |V_{k+1}(s) - V_k(s)| < \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Value Iteration êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def value_iteration(env, theta=1e-6, max_iter=1000):\n",
    "    V = np.zeros(env.n_states)\n",
    "    history = [V.copy()]\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        V_new = np.zeros(env.n_states)\n",
    "\n",
    "        for s in range(env.n_states):\n",
    "            if s in env.terminal:\n",
    "                V_new[s] = env.rewards[s]\n",
    "                continue\n",
    "\n",
    "            q_values = []\n",
    "            for a in range(env.n_actions):\n",
    "                transitions = env.get_transition_prob(s, a)\n",
    "                q = sum(prob * (reward + env.gamma * V[ns])\n",
    "                        for ns, reward, prob in transitions)\n",
    "                q_values.append(q)\n",
    "            V_new[s] = max(q_values)\n",
    "\n",
    "        delta = np.max(np.abs(V_new - V))\n",
    "        V = V_new\n",
    "        history.append(V.copy())\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Value Iteration ìˆ˜ë ´: {iteration + 1}íšŒ ë°˜ë³µ\")\n",
    "            break\n",
    "\n",
    "    # ìµœì  ì •ì±… ì¶”ì¶œ\n",
    "    policy = np.zeros(env.n_states, dtype=int)\n",
    "    for s in range(env.n_states):\n",
    "        if s in env.terminal:\n",
    "            continue\n",
    "        q_values = []\n",
    "        for a in range(env.n_actions):\n",
    "            transitions = env.get_transition_prob(s, a)\n",
    "            q = sum(prob * (reward + env.gamma * V[ns])\n",
    "                    for ns, reward, prob in transitions)\n",
    "            q_values.append(q)\n",
    "        policy[s] = np.argmax(q_values)\n",
    "\n",
    "    return V, policy, history\n",
    "\n",
    "\n",
    "V_star, pi_star, vi_history = value_iteration(env)\n",
    "\n",
    "print(f\"\\nìµœì  ê°€ì¹˜ í•¨ìˆ˜ V* (4Ã—4):\")\n",
    "V_grid = V_star.reshape(4, 4)\n",
    "for r in range(4):\n",
    "    row_str = \"  \"\n",
    "    for c in range(4):\n",
    "        row_str += f\"{V_grid[r, c]:+7.3f} \"\n",
    "    print(row_str)\n",
    "\n",
    "print(f\"\\nìµœì  ì •ì±… Ï€* (4Ã—4):\")\n",
    "for r in range(4):\n",
    "    row_str = \"  \"\n",
    "    for c in range(4):\n",
    "        s = r * 4 + c\n",
    "        if s == env.goal:\n",
    "            row_str += \"   G    \"\n",
    "        elif s == env.trap:\n",
    "            row_str += \"   T    \"\n",
    "        else:\n",
    "            row_str += f\"   {env.action_names[pi_star[s]]}    \"\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Value Iteration ìˆ˜ë ´ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# (1) Value ìˆ˜ë ´ ê³¼ì •\n",
    "ax1 = axes[0]\n",
    "iterations_to_show = [0, 1, 5, 10, 20, len(vi_history)-1]\n",
    "for it in iterations_to_show:\n",
    "    if it < len(vi_history):\n",
    "        ax1.plot(range(env.n_states), vi_history[it],\n",
    "                 'o-', ms=5, lw=1.5, label=f'iter={it}')\n",
    "ax1.set_xlabel('State', fontsize=11)\n",
    "ax1.set_ylabel('V(s)', fontsize=11)\n",
    "ax1.set_title('Value Iteration ìˆ˜ë ´ ê³¼ì •', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ìµœì  ê°€ì¹˜ í•¨ìˆ˜ íˆíŠ¸ë§µ\n",
    "ax2 = axes[1]\n",
    "V_grid = V_star.reshape(4, 4)\n",
    "im = ax2.imshow(V_grid, cmap='RdYlGn', interpolation='nearest')\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        s = r * 4 + c\n",
    "        label = f'{V_grid[r,c]:.2f}'\n",
    "        if s == env.goal:\n",
    "            label += '\\n(G)'\n",
    "        elif s == env.trap:\n",
    "            label += '\\n(T)'\n",
    "        ax2.text(c, r, label, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "fig.colorbar(im, ax=ax2, shrink=0.8)\n",
    "ax2.set_title('ìµœì  ê°€ì¹˜ í•¨ìˆ˜ V*', fontweight='bold')\n",
    "\n",
    "# (3) ìµœì  ì •ì±… ì‹œê°í™”\n",
    "ax3 = axes[2]\n",
    "arrow_map = {0: (0, 0.3), 1: (0, -0.3), 2: (-0.3, 0), 3: (0.3, 0)}\n",
    "ax3.set_xlim(-0.5, 3.5)\n",
    "ax3.set_ylim(3.5, -0.5)\n",
    "for r in range(4):\n",
    "    for c in range(4):\n",
    "        s = r * 4 + c\n",
    "        if s == env.goal:\n",
    "            ax3.add_patch(plt.Rectangle((c-0.4, r-0.4), 0.8, 0.8,\n",
    "                                        color='green', alpha=0.3))\n",
    "            ax3.text(c, r, 'G', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        elif s == env.trap:\n",
    "            ax3.add_patch(plt.Rectangle((c-0.4, r-0.4), 0.8, 0.8,\n",
    "                                        color='red', alpha=0.3))\n",
    "            ax3.text(c, r, 'T', ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            dx, dy = arrow_map[pi_star[s]]\n",
    "            ax3.annotate('', xy=(c+dx, r-dy), xytext=(c, r),\n",
    "                        arrowprops=dict(arrowstyle='->', color='blue', lw=2.5))\n",
    "ax3.set_title('ìµœì  ì •ì±… Ï€*', fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xticks(range(4))\n",
    "ax3.set_yticks(range(4))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter15_alignment_rlhf/value_iteration_gridworld.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/value_iteration_gridworld.png\")\n",
    "print(f\"ì´ ìˆ˜ë ´ ë°˜ë³µ ìˆ˜: {len(vi_history) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. REINFORCE Policy Gradient <a name='4.-REINFORCE-Policy-Gradient'></a>\n",
    "\n",
    "REINFORCE ì•Œê³ ë¦¬ì¦˜ì€ ì •ì±…ì„ ì§ì ‘ ë§¤ê°œë³€ìˆ˜í™”í•˜ì—¬ ê¸°ìš¸ê¸° ìƒìŠ¹ë²•ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì•Œê³ ë¦¬ì¦˜ ë‹¨ê³„:**\n",
    "1. í˜„ì¬ ì •ì±… $\\pi_\\theta$ë¡œ ì—í”¼ì†Œë“œ ìƒì„±: $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, \\ldots)$\n",
    "2. ê° ì‹œê°„ ë‹¨ê³„ì˜ Return ê³„ì‚°: $G_t = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k+1}$\n",
    "3. ì •ì±… ê¸°ìš¸ê¸° ê³„ì‚°: $\\nabla_\\theta J(\\theta) = \\sum_t G_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$\n",
    "4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "ê°„ë‹¨í•œ **K-Armed Bandit** ë¬¸ì œì— REINFORCEë¥¼ ì ìš©í•´ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ REINFORCE: K-Armed Bandit ë¬¸ì œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5ê°œì˜ ìŠ¬ë¡¯ë¨¸ì‹ (arm), ê°ê° ë‹¤ë¥¸ í‰ê·  ë³´ìƒì„ ê°€ì§\n",
    "n_arms = 5\n",
    "true_rewards = np.array([0.2, -0.5, 1.5, 0.8, -0.2])\n",
    "\n",
    "# TFë¡œ ì •ì±… ë„¤íŠ¸ì›Œí¬ êµ¬í˜„ (Softmax ì •ì±…)\n",
    "policy_logits = tf.Variable(tf.zeros(n_arms), dtype=tf.float32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "n_episodes = 500\n",
    "reward_history = []\n",
    "action_probs_history = []\n",
    "\n",
    "print(f\"K-Armed Bandit REINFORCE\")\n",
    "print(f\"  íŒ” ê°œìˆ˜: {n_arms}\")\n",
    "print(f\"  ì‹¤ì œ ë³´ìƒ í‰ê· : {true_rewards}\")\n",
    "print(f\"  ìµœì  íŒ”: arm {np.argmax(true_rewards)} (ë³´ìƒ={true_rewards.max()})\")\n",
    "print()\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # í˜„ì¬ ì •ì±…ìœ¼ë¡œ í–‰ë™ ì„ íƒ\n",
    "        probs = tf.nn.softmax(policy_logits)\n",
    "        action = tf.random.categorical(tf.math.log(probs[tf.newaxis, :]), 1)[0, 0]\n",
    "\n",
    "        # ë³´ìƒ ìˆ˜ì§‘ (ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ í¬í•¨)\n",
    "        reward = true_rewards[action.numpy()] + np.random.randn() * 0.5\n",
    "\n",
    "        # REINFORCE loss: -G * log Ï€(a|s)\n",
    "        log_prob = tf.math.log(probs[action] + 1e-8)\n",
    "        loss = -reward * log_prob\n",
    "\n",
    "    grads = tape.gradient(loss, [policy_logits])\n",
    "    optimizer.apply_gradients(zip(grads, [policy_logits]))\n",
    "\n",
    "    reward_history.append(reward)\n",
    "    action_probs_history.append(tf.nn.softmax(policy_logits).numpy().copy())\n",
    "\n",
    "    if (ep + 1) % 100 == 0:\n",
    "        curr_probs = tf.nn.softmax(policy_logits).numpy()\n",
    "        avg_reward = np.mean(reward_history[-50:])\n",
    "        best_arm = np.argmax(curr_probs)\n",
    "        print(f\"  ì—í”¼ì†Œë“œ {ep+1:4d}: í‰ê·  ë³´ìƒ(ìµœê·¼ 50)={avg_reward:+.3f}, \"\n",
    "              f\"ìµœì„  arm={best_arm} (P={curr_probs[best_arm]:.3f})\")\n",
    "\n",
    "final_probs = tf.nn.softmax(policy_logits).numpy()\n",
    "print(f\"\\nìµœì¢… ì •ì±… ë¶„í¬:\")\n",
    "for i in range(n_arms):\n",
    "    bar = 'â–ˆ' * int(final_probs[i] * 40)\n",
    "    print(f\"  Arm {i} (ì‹¤ì œ r={true_rewards[i]:+.1f}): P={final_probs[i]:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ REINFORCE í•™ìŠµ ê³¼ì • ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ë³´ìƒ ì´ë™ í‰ê· \n",
    "ax1 = axes[0]\n",
    "window = 20\n",
    "rewards_smooth = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(range(len(rewards_smooth)), rewards_smooth, 'b-', lw=2, label='ì´ë™ í‰ê·  (w=20)')\n",
    "ax1.axhline(y=true_rewards.max(), color='red', ls='--', lw=1.5,\n",
    "            label=f'ìµœì  ë³´ìƒ ({true_rewards.max()})')\n",
    "ax1.fill_between(range(len(rewards_smooth)),\n",
    "                 rewards_smooth - 0.3, rewards_smooth + 0.3,\n",
    "                 alpha=0.1, color='blue')\n",
    "ax1.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)\n",
    "ax1.set_ylabel('ë³´ìƒ', fontsize=11)\n",
    "ax1.set_title('REINFORCE í•™ìŠµ ê³¡ì„ ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ì •ì±… í™•ë¥  ë³€í™”\n",
    "ax2 = axes[1]\n",
    "probs_arr = np.array(action_probs_history)\n",
    "for i in range(n_arms):\n",
    "    ax2.plot(probs_arr[:, i], lw=2, label=f'Arm {i} (r={true_rewards[i]:+.1f})')\n",
    "ax2.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)\n",
    "ax2.set_ylabel('ì„ íƒ í™•ë¥ ', fontsize=11)\n",
    "ax2.set_title('ì •ì±… í™•ë¥  ë³€í™” (Softmax)', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter15_alignment_rlhf/reinforce_bandit.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/reinforce_bandit.png\")\n",
    "print(f\"ìµœì¢… ì„ íƒ í™•ë¥ : Arm {np.argmax(final_probs)} = {final_probs.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. NLPë¥¼ ìœ„í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„ <a name='5.-NLP-ë¦¬ì›Œë“œ-ì‹ í˜¸-ì„¤ê³„'></a>\n",
    "\n",
    "LLM ì •ë ¬(Alignment)ì—ì„œ RLì„ ì‚¬ìš©í•  ë•Œ, ë¦¬ì›Œë“œ ì‹ í˜¸ë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í•˜ëŠ”ì§€ê°€ í•µì‹¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "| ë¦¬ì›Œë“œ ìœ í˜• | ìˆ˜ì‹/ì„¤ëª… | ì˜ˆì‹œ |\n",
    "|-------------|-----------|------|\n",
    "| ì¸ê°„ ì„ í˜¸ ê¸°ë°˜ | $r(x, y) = \\text{RewardModel}(x, y)$ | RLHF |\n",
    "| ê·œì¹™ ê¸°ë°˜ | $r = \\mathbb{1}[\\text{ì¡°ê±´ ì¶©ì¡±}]$ | ê¸¸ì´ ì œí•œ, ì•ˆì „ì„± |\n",
    "| ìë™ ë©”íŠ¸ë¦­ | $r = \\text{BLEU}(y, y^*)$ ë˜ëŠ” $\\text{ROUGE}$ | ë²ˆì—­ í’ˆì§ˆ |\n",
    "| KL í˜ë„í‹° | $r_{total} = r(x,y) - \\beta D_{KL}[\\pi_\\theta \\| \\pi_{ref}]$ | ì •ì±… ì´íƒˆ ë°©ì§€ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ NLP ë¦¬ì›Œë“œ ì‹ í˜¸ ì„¤ê³„ ì˜ˆì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê°„ë‹¨í•œ ì‹œí€€ìŠ¤ ìƒì„± ë¬¸ì œ: í† í° ìƒì„± ì‹œ ë‹¤ì–‘í•œ ë¦¬ì›Œë“œ ì‹ í˜¸ë¥¼ ì ìš©\n",
    "\n",
    "vocab_size = 100\n",
    "seq_len = 10\n",
    "n_samples = 1000\n",
    "\n",
    "# 1. ê¸¸ì´ ê¸°ë°˜ ë¦¬ì›Œë“œ: ì§§ì€ ì‘ë‹µì— ë³´ë„ˆìŠ¤\n",
    "def length_reward(seq, target_len=5):\n",
    "    actual_len = len(seq)\n",
    "    return -abs(actual_len - target_len) / target_len\n",
    "\n",
    "# 2. ë‹¤ì–‘ì„± ë¦¬ì›Œë“œ: ë°˜ë³µ í† í° íŒ¨ë„í‹°\n",
    "def diversity_reward(seq):\n",
    "    unique_ratio = len(set(seq)) / len(seq)\n",
    "    return unique_ratio\n",
    "\n",
    "# 3. ì•ˆì „ì„± ë¦¬ì›Œë“œ: ê¸ˆì§€ í† í°(ID 0~9)ì´ ì—†ìœ¼ë©´ ë³´ìƒ\n",
    "def safety_reward(seq, forbidden=set(range(10))):\n",
    "    has_forbidden = any(t in forbidden for t in seq)\n",
    "    return 0.0 if has_forbidden else 1.0\n",
    "\n",
    "# 4. KL í˜ë„í‹°: ê¸°ì¤€ ì •ì±…ì—ì„œ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡\n",
    "def kl_penalty(pi_logits, ref_logits, beta=0.1):\n",
    "    pi_probs = tf.nn.softmax(pi_logits)\n",
    "    ref_probs = tf.nn.softmax(ref_logits)\n",
    "    kl = tf.reduce_sum(pi_probs * tf.math.log(pi_probs / (ref_probs + 1e-8) + 1e-8))\n",
    "    return -beta * kl.numpy()\n",
    "\n",
    "# ëœë¤ ì‹œí€€ìŠ¤ì— ëŒ€í•´ ë¦¬ì›Œë“œ ë¶„í¬ ì‹œê°í™”\n",
    "np.random.seed(42)\n",
    "length_rewards = []\n",
    "diversity_rewards = []\n",
    "safety_rewards = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    seq = np.random.randint(0, vocab_size, size=seq_len).tolist()\n",
    "    length_rewards.append(length_reward(seq))\n",
    "    diversity_rewards.append(diversity_reward(seq))\n",
    "    safety_rewards.append(safety_reward(seq))\n",
    "\n",
    "print(f\"ë¦¬ì›Œë“œ ì‹ í˜¸ í†µê³„ (ëœë¤ ì‹œí€€ìŠ¤ {n_samples}ê°œ):\")\n",
    "print(f\"{'ë¦¬ì›Œë“œ ìœ í˜•':<20} | {'í‰ê· ':>8} | {'í‘œì¤€í¸ì°¨':>8} | {'ìµœì†Œ':>8} | {'ìµœëŒ€':>8}\")\n",
    "print(f\"{'-'*62}\")\n",
    "for name, values in [(\"ê¸¸ì´ ê¸°ë°˜\", length_rewards),\n",
    "                      (\"ë‹¤ì–‘ì„± ê¸°ë°˜\", diversity_rewards),\n",
    "                      (\"ì•ˆì „ì„± ê¸°ë°˜\", safety_rewards)]:\n",
    "    vals = np.array(values)\n",
    "    print(f\"{name:<20} | {vals.mean():>8.3f} | {vals.std():>8.3f} | {vals.min():>8.3f} | {vals.max():>8.3f}\")\n",
    "\n",
    "# KL í˜ë„í‹° ì˜ˆì‹œ\n",
    "pi_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32))\n",
    "ref_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32))\n",
    "kl_val = kl_penalty(pi_logits, ref_logits, beta=0.1)\n",
    "print(f\"\\nKL í˜ë„í‹° ì˜ˆì‹œ (Î²=0.1): {kl_val:.4f}\")\n",
    "print(f\"  â†’ ì •ì±…ì´ ê¸°ì¤€ ì •ì±…ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ìŒì˜ ë³´ìƒì´ ì»¤ì§\")\n",
    "print(f\"  â†’ ì´ëŠ” RLHFì—ì„œ ëª¨ë¸ì´ reward hackingí•˜ëŠ” ê²ƒì„ ë°©ì§€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| MDP | $(S, A, P, R, \\gamma)$ â€” ìˆœì°¨ì  ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬ | â­â­â­ |\n",
    "| Bellman ë°©ì •ì‹ | $V^\\pi(s) = \\sum_a \\pi(a|s)[R + \\gamma \\sum_{s'} PV^\\pi]$ | â­â­â­ |\n",
    "| Value Iteration | Bellman ìµœì  ë°©ì •ì‹ ë°˜ë³µ â†’ ìµœì  ì •ì±… | â­â­ |\n",
    "| REINFORCE | $\\nabla_\\theta J = \\mathbb{E}[G_t \\nabla_\\theta \\log \\pi_\\theta]$ | â­â­â­ |\n",
    "| Return ($G_t$) | ë¯¸ë˜ ë³´ìƒì˜ í• ì¸ í•©: $\\sum_k \\gamma^k R_{t+k+1}$ | â­â­ |\n",
    "| KL í˜ë„í‹° | $-\\beta D_{KL}[\\pi_\\theta \\| \\pi_{ref}]$ â€” ì •ì±… ì´íƒˆ ë°©ì§€ | â­â­â­ |\n",
    "| ë¦¬ì›Œë“œ ì„¤ê³„ | ì¸ê°„ ì„ í˜¸ / ê·œì¹™ / ë©”íŠ¸ë¦­ ê¸°ë°˜ ë³´ìƒ ì‹ í˜¸ | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\left[R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V^\\pi(s')\\right]$$\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[G_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t)\\right]$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**02_actor_critic_and_ppo.ipynb** â€” Advantage Functionì„ ë„ì…í•˜ì—¬ REINFORCEì˜ ë†’ì€ ë¶„ì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , A2Cì—ì„œ PPO-Clipê¹Œì§€ ìˆ˜ì‹ì„ ì™„ì „ ì „ê°œí•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}