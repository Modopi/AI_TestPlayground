{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ì™€ ê°•í™”í•™ìŠµ â€” DPOì™€ ì„ í˜¸ í•™ìŠµ\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- DPO(Direct Preference Optimization)ì˜ ìˆ˜í•™ì  ë„ì¶œ ê³¼ì •ì„ RLHF ëª©ì í•¨ìˆ˜ë¡œë¶€í„° ìœ ë„í•œë‹¤\n",
    "- DPOê°€ ì•”ë¬µì  ë³´ìƒ ëª¨ë¸(implicit reward model)ì„ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•œë‹¤\n",
    "- DPO ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê³  ì˜¨ë„ íŒŒë¼ë¯¸í„° Î²ì˜ íš¨ê³¼ë¥¼ ë¶„ì„í•œë‹¤\n",
    "- DPOì™€ RLHFì˜ í•™ìŠµ ê³¡ì„ ì„ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ë¹„êµí•œë‹¤\n",
    "- ORPO, KTO, SimPO ë“± DPO íŒŒìƒ ê¸°ë²•ì˜ í•µì‹¬ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: DPO ë„ì¶œê³¼ ì•”ë¬µì  ë³´ìƒ](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„](#2.-DPO-ì†ì‹¤-í•¨ìˆ˜)\n",
    "3. [Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™”](#3.-Î²-íš¨ê³¼-ì‹œê°í™”)\n",
    "4. [DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ](#4.-DPO-vs-RLHF-ë¹„êµ)\n",
    "5. [ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²•](#5.-íŒŒìƒ-ê¸°ë²•)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### RLHFì—ì„œ DPOë¡œì˜ ìœ ë„\n",
    "\n",
    "RLHF ëª©ì í•¨ìˆ˜ì˜ ìµœì  ì •ì±…ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‹«íŒ í˜•íƒœ(closed-form)ë¡œ êµ¬í•´ì§‘ë‹ˆë‹¤:\n",
    "\n",
    "$$\\pi^*(y \\mid x) = \\frac{1}{Z(x)} \\pi_{ref}(y \\mid x) \\exp\\!\\left(\\frac{1}{\\beta} r^*(y, x)\\right)$$\n",
    "\n",
    "- $Z(x) = \\sum_y \\pi_{ref}(y|x) \\exp\\!\\left(\\frac{1}{\\beta}r^*(y,x)\\right)$: ì •ê·œí™” ìƒìˆ˜ (partition function)\n",
    "- $\\pi_{ref}$: ê¸°ì¤€ ì •ì±… (SFT ëª¨ë¸)\n",
    "- $\\beta$: KL í˜ë„í‹° ê³„ìˆ˜ (ì˜¨ë„ íŒŒë¼ë¯¸í„°)\n",
    "\n",
    "### ì•”ë¬µì  ë³´ìƒ (Implicit Reward)\n",
    "\n",
    "ìœ„ ìµœì  ì •ì±…ì„ ë³´ìƒì— ëŒ€í•´ ì—­ìœ¼ë¡œ í’€ë©´:\n",
    "\n",
    "$$r^*(y \\mid x) = \\beta \\log \\frac{\\pi^*(y \\mid x)}{\\pi_{ref}(y \\mid x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "- $r^*(y \\mid x)$: ìµœì  ì •ì±…ì´ ë‚´í¬í•˜ëŠ” ì•”ë¬µì  ë³´ìƒ\n",
    "- ì •ì±…ì˜ log-ratioê°€ ê³§ ë³´ìƒ í•¨ìˆ˜ ì—­í•  â†’ **ë³„ë„ì˜ Reward Modelì´ ë¶ˆí•„ìš”**\n",
    "\n",
    "### DPO ëª©ì í•¨ìˆ˜\n",
    "\n",
    "Bradley-Terry ì„ í˜¸ ëª¨ë¸ì— ì•”ë¬µì  ë³´ìƒì„ ëŒ€ì…í•˜ë©´:\n",
    "\n",
    "$$\\mathcal{L}_{DPO}(\\pi_\\theta) = -\\mathbb{E}_{(x, y_w, y_l)}\\left[\\log \\sigma\\!\\left(\\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{ref}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{ref}(y_l \\mid x)}\\right)\\right]$$\n",
    "\n",
    "- $y_w$: ì„ í˜¸ ì‘ë‹µ (chosen)\n",
    "- $y_l$: ë¹„ì„ í˜¸ ì‘ë‹µ (rejected)\n",
    "- $\\sigma$: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜\n",
    "\n",
    "### RLHF vs DPO ë“±ê°€ì„±\n",
    "\n",
    "| í•­ëª© | RLHF | DPO |\n",
    "|------|------|-----|\n",
    "| Reward Model | ëª…ì‹œì  í•™ìŠµ í•„ìš” ($r_\\theta$) | **ë¶ˆí•„ìš”** (ì •ì±…ì´ ì•”ë¬µì  ë³´ìƒ) |\n",
    "| RL ì•Œê³ ë¦¬ì¦˜ | PPO (ë³µì¡í•œ ê°•í™”í•™ìŠµ) | **ë¶ˆí•„ìš”** (ì§€ë„í•™ìŠµê³¼ ë™ì¼) |\n",
    "| í•™ìŠµ ëª©í‘œ | $\\mathbb{E}[r(y)] - \\beta D_{KL}$ | $-\\mathbb{E}[\\log\\sigma(\\beta\\Delta\\log\\pi)]$ |\n",
    "| ì•ˆì •ì„± | PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê° | **ì•ˆì •ì ** (ë‹¨ìˆœ cross-entropy í˜•íƒœ) |\n",
    "| ê³„ì‚° ë¹„ìš© | 4ê°œ ëª¨ë¸ (actor, critic, ref, RM) | **2ê°œ ëª¨ë¸** (actor, ref) |\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| ìµœì  ì •ì±… | $\\pi^* \\propto \\pi_{ref}\\exp(r/\\beta)$ | KL-ì œì•½ í•˜ ìµœì í•´ |\n",
    "| ì•”ë¬µì  ë³´ìƒ | $r^* = \\beta\\log(\\pi^*/\\pi_{ref}) + \\beta\\log Z$ | ì •ì±… = ë³´ìƒ ëª¨ë¸ |\n",
    "| DPO Loss | $-\\mathbb{E}[\\log\\sigma(\\beta\\Delta\\log\\pi)]$ | ì„ í˜¸ í•™ìŠµ ì§ì ‘ ìµœì í™” |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ DPO ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ DPOê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: RLHFê°€ \"ì‹œí—˜ â†’ ì±„ì  â†’ ê³µë¶€ â†’ ì‹œí—˜ â†’ ì±„ì  â†’ ...\"ì˜ ë³µì¡í•œ ë°˜ë³µì´ë¼ë©´,\n",
    "> DPOëŠ” \"ì •ë‹µê³¼ ì˜¤ë‹µì„ ë³´ê³  ë°”ë¡œ ê³µë¶€í•˜ê¸°\"ì™€ ê°™ì•„ìš”!\n",
    "\n",
    "**RLHFì˜ ë¬¸ì œì :** ê°•ì•„ì§€ í›ˆë ¨ì— ë¹„ìœ í•˜ë©´, RLHFëŠ”\n",
    "1. ë¨¼ì € \"ì´ê²ƒì´ ì¢‹ì€ í–‰ë™\" ì ìˆ˜íŒ(Reward Model)ì„ ë§Œë“¤ê³ \n",
    "2. ê·¸ ì ìˆ˜íŒì„ ë³´ë©´ì„œ ê°•ì•„ì§€ë¥¼ ë”°ë¡œ í›ˆë ¨(PPO)í•´ì•¼ í•´ìš”\n",
    "3. ì ìˆ˜íŒì´ ì˜ëª»ë˜ë©´ ê°•ì•„ì§€ê°€ ì—‰ëš±í•œ í–‰ë™ì„ ë°°ìš¸ ìˆ˜ë„ ìˆì–´ìš”!\n",
    "\n",
    "**DPOì˜ í•´ê²°:** DPOëŠ” ì ìˆ˜íŒì„ ë”°ë¡œ ë§Œë“¤ì§€ ì•Šì•„ìš”!\n",
    "- \"ì´ í–‰ë™ì´ ì € í–‰ë™ë³´ë‹¤ ì¢‹ì•„\"ë¼ëŠ” ë¹„êµ ë°ì´í„°ë§Œ ìˆìœ¼ë©´ ë¼ìš”\n",
    "- ê°•ì•„ì§€(ëª¨ë¸)ê°€ ì§ì ‘ \"ì¢‹ì€ ë‹µë³€ì€ ë” ìì£¼, ë‚˜ìœ ë‹µë³€ì€ ëœ ìì£¼\"ë¡œ ë°°ì›Œìš”\n",
    "- ìˆ˜í•™ì ìœ¼ë¡œ RLHFì™€ **ì™„ì „íˆ ê°™ì€ ê²°ê³¼**ë¥¼ ë‚´ì§€ë§Œ, í›¨ì”¬ ê°„ë‹¨í•´ìš”!\n",
    "\n",
    "#### ğŸ¯ Î²(ë² íƒ€)ëŠ” ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: Î²ëŠ” \"ì–¼ë§ˆë‚˜ ë³´ìˆ˜ì ìœ¼ë¡œ ë°°ìš¸ì§€\" ê²°ì •í•˜ëŠ” ë‹¤ì´ì–¼ì´ì—ìš”!\n",
    "\n",
    "- **Î²ê°€ í¬ë©´**: \"ì›ë˜ í•˜ë˜ ëŒ€ë¡œ ì¡°ê¸ˆë§Œ ë°”ê¿”\" â†’ ì•ˆì „í•˜ì§€ë§Œ ëŠë¦° ë³€í™”\n",
    "- **Î²ê°€ ì‘ìœ¼ë©´**: \"ê³¼ê°í•˜ê²Œ ë°”ê¿”!\" â†’ ë¹ ë¥´ì§€ë§Œ ìœ„í—˜í•  ìˆ˜ ìˆìŒ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: DPO ì•”ë¬µì  ë³´ìƒ ê³„ì‚°\n",
    "\n",
    "ì •ì±…ì˜ log-ratioê°€ $\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} = 0.8$ì´ê³  \n",
    "$\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} = -0.3$ì¼ ë•Œ, $\\beta = 0.1$ì—ì„œ DPO lossë¥¼ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\text{logit} = \\beta\\left(\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right) = 0.1 \\times (0.8 - (-0.3)) = 0.1 \\times 1.1 = 0.11$$\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\log\\sigma(0.11) = -\\log\\left(\\frac{1}{1+e^{-0.11}}\\right) = -\\log(0.5275) \\approx 0.6394$$\n",
    "\n",
    "â†’ ì„ í˜¸ ì‘ë‹µê³¼ ë¹„ì„ í˜¸ ì‘ë‹µì˜ log-ratio ì°¨ì´ê°€ ì•„ì§ ì‘ì•„ì„œ ì†ì‹¤ì´ ë¹„êµì  í½ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: Î²ì— ë”°ë¥¸ ìµœì  ì •ì±… ë³€í™”\n",
    "\n",
    "$\\pi_{ref}(y|x) = 0.3$ì´ê³  $r^*(y|x) = 2.0$, $Z(x) = 1$ì¼ ë•Œ:\n",
    "- $\\beta = 0.5$ì—ì„œ ìµœì  ì •ì±… $\\pi^*(y|x)$ëŠ”?\n",
    "- $\\beta = 2.0$ì—ì„œ ìµœì  ì •ì±… $\\pi^*(y|x)$ëŠ”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\!\\left(\\frac{r^*(y|x)}{\\beta}\\right)$$\n",
    "\n",
    "**Î² = 0.5:** $\\pi^* = 0.3 \\times \\exp(2.0/0.5) = 0.3 \\times e^4 = 0.3 \\times 54.60 = 16.38$\n",
    "(ì •ê·œí™” ì „ ê°’ì´ë¯€ë¡œ Zë¡œ ë‚˜ëˆ„ë©´ 1 ë¯¸ë§Œì´ ë©ë‹ˆë‹¤)\n",
    "\n",
    "**Î² = 2.0:** $\\pi^* = 0.3 \\times \\exp(2.0/2.0) = 0.3 \\times e^1 = 0.3 \\times 2.718 = 0.816$\n",
    "\n",
    "â†’ Î²ê°€ ì‘ì„ìˆ˜ë¡ ë³´ìƒì´ ë†’ì€ ì‘ë‹µì— **ê¸‰ê²©íˆ** í™•ë¥ ì„ ì§‘ì¤‘ì‹œí‚µë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 2. DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ <a name='2.-DPO-ì†ì‹¤-í•¨ìˆ˜'></a>\n",
    "\n",
    "DPO ì†ì‹¤ì€ ì„ í˜¸ ì‘ë‹µê³¼ ë¹„ì„ í˜¸ ì‘ë‹µì˜ log-ratio ì°¨ì´ì— ì‹œê·¸ëª¨ì´ë“œë¥¼ ì ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\log\\sigma\\!\\left(\\beta\\left[\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right]\\right)$$\n",
    "\n",
    "ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í† í° ë‹¨ìœ„ log-probabilityë¥¼ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ í‰ê· í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ DPO ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì‹¤ì œ log-probabilityë¥¼ ì‚¬ìš©í•œ DPO loss ê³„ì‚°\n",
    "\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps,\n",
    "             ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
    "    # log-ratio ê³„ì‚°\n",
    "    chosen_log_ratios = policy_chosen_logps - ref_chosen_logps\n",
    "    rejected_log_ratios = policy_rejected_logps - ref_rejected_logps\n",
    "\n",
    "    # DPO logit\n",
    "    logits = beta * (chosen_log_ratios - rejected_log_ratios)\n",
    "\n",
    "    # DPO loss = -log(sigmoid(logits))\n",
    "    losses = -tf.math.log_sigmoid(logits)\n",
    "\n",
    "    # ì•”ë¬µì  ë³´ìƒ ì¶”ì¶œ\n",
    "    chosen_rewards = beta * chosen_log_ratios\n",
    "    rejected_rewards = beta * rejected_log_ratios\n",
    "\n",
    "    return tf.reduce_mean(losses), chosen_rewards, rejected_rewards\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜: í•©ì„± log-probability ë°ì´í„° ìƒì„±\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# ê¸°ì¤€ ì •ì±…ì˜ log-prob\n",
    "ref_chosen_lp = np.random.normal(-2.0, 0.5, n_samples).astype(np.float32)\n",
    "ref_rejected_lp = np.random.normal(-2.5, 0.5, n_samples).astype(np.float32)\n",
    "\n",
    "# í•™ìŠµ ì •ì±…: ì´ˆê¸°ì—ëŠ” refì™€ ìœ ì‚¬, í•™ìŠµ í›„ ì°¨ì´ ë°œìƒ\n",
    "policy_chosen_lp = ref_chosen_lp + np.random.normal(0.3, 0.2, n_samples).astype(np.float32)\n",
    "policy_rejected_lp = ref_rejected_lp + np.random.normal(-0.1, 0.2, n_samples).astype(np.float32)\n",
    "\n",
    "# DPO loss ê³„ì‚°\n",
    "loss_val, c_rewards, r_rewards = dpo_loss(\n",
    "    tf.constant(policy_chosen_lp), tf.constant(policy_rejected_lp),\n",
    "    tf.constant(ref_chosen_lp), tf.constant(ref_rejected_lp),\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(\"DPO ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚° ê²°ê³¼\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  DPO Loss: {loss_val.numpy():.4f}\")\n",
    "print(f\"  ì„ í˜¸ ì‘ë‹µ ì•”ë¬µì  ë³´ìƒ í‰ê· : {tf.reduce_mean(c_rewards).numpy():.4f}\")\n",
    "print(f\"  ë¹„ì„ í˜¸ ì‘ë‹µ ì•”ë¬µì  ë³´ìƒ í‰ê· : {tf.reduce_mean(r_rewards).numpy():.4f}\")\n",
    "print(f\"  ë³´ìƒ ë§ˆì§„ (chosen - rejected): {(tf.reduce_mean(c_rewards) - tf.reduce_mean(r_rewards)).numpy():.4f}\")\n",
    "\n",
    "# log-ratio ë¶„í¬ í™•ì¸\n",
    "chosen_ratios = policy_chosen_lp - ref_chosen_lp\n",
    "rejected_ratios = policy_rejected_lp - ref_rejected_lp\n",
    "print(f\"\\n  log-ratio í†µê³„:\")\n",
    "print(f\"  chosen log(pi/pi_ref)  í‰ê· ={chosen_ratios.mean():.3f}, std={chosen_ratios.std():.3f}\")\n",
    "print(f\"  rejected log(pi/pi_ref) í‰ê· ={rejected_ratios.mean():.3f}, std={rejected_ratios.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™” <a name='3.-Î²-íš¨ê³¼-ì‹œê°í™”'></a>\n",
    "\n",
    "$\\beta$ëŠ” DPOì—ì„œ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ê¸°ì¤€ ì •ì±…ìœ¼ë¡œë¶€í„°ì˜ ì´íƒˆ ì •ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤:\n",
    "\n",
    "- **í° Î²**: ê¸°ì¤€ ì •ì±…ì— ê°€ê¹Œìš´ ë³´ìˆ˜ì  í•™ìŠµ â†’ ì•ˆì •ì ì´ë‚˜ ëŠë¦° ë³€í™”\n",
    "- **ì‘ì€ Î²**: ì„ í˜¸ ë°ì´í„°ì— ê°•í•˜ê²Œ ì í•© â†’ ë¹ ë¥´ì§€ë§Œ ê³¼ì í•© ìœ„í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Î²(ì˜¨ë„) íš¨ê³¼ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë‹¤ì–‘í•œ Î²ì—ì„œ DPO lossì˜ gradient ê°•ë„ ë¹„êµ\n",
    "\n",
    "log_ratio_diffs = np.linspace(-3, 3, 300)\n",
    "betas = [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# (1) DPO loss vs log-ratio ì°¨ì´\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.9, len(betas)))\n",
    "for beta, color in zip(betas, colors):\n",
    "    logits = beta * log_ratio_diffs\n",
    "    loss_curve = -np.log(1 / (1 + np.exp(-logits)) + 1e-10)\n",
    "    ax1.plot(log_ratio_diffs, loss_curve, lw=2, color=color,\n",
    "             label=f'beta={beta}')\n",
    "ax1.set_xlabel(r'$\\Delta \\log(\\pi/\\pi_{ref})$', fontsize=11)\n",
    "ax1.set_ylabel('DPO Loss', fontsize=11)\n",
    "ax1.set_title(r'$\beta$ì— ë”°ë¥¸ DPO ì†ì‹¤', fontweight='bold')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 4)\n",
    "\n",
    "# (2) ê·¸ë˜ë””ì–¸íŠ¸ ê°•ë„ ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "for beta, color in zip(betas, colors):\n",
    "    logits = beta * log_ratio_diffs\n",
    "    sigmoid_val = 1 / (1 + np.exp(-logits))\n",
    "    gradient = -beta * (1 - sigmoid_val)\n",
    "    ax2.plot(log_ratio_diffs, np.abs(gradient), lw=2, color=color,\n",
    "             label=f'beta={beta}')\n",
    "ax2.set_xlabel(r'$\\Delta \\log(\\pi/\\pi_{ref})$', fontsize=11)\n",
    "ax2.set_ylabel(r'$|\n",
    "abla_\theta \\mathcal{L}|$', fontsize=11)\n",
    "ax2.set_title(r'$\beta$ì— ë”°ë¥¸ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°', fontweight='bold')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# (3) Î² ë²”ìœ„ë³„ accuracy ì‹œë®¬ë ˆì´ì…˜\n",
    "ax3 = axes[2]\n",
    "beta_range = np.linspace(0.01, 1.0, 50)\n",
    "accuracies = []\n",
    "losses_by_beta = []\n",
    "for b in beta_range:\n",
    "    logits_sim = b * (chosen_ratios - rejected_ratios)\n",
    "    acc = np.mean(logits_sim > 0)\n",
    "    loss_sim = np.mean(-np.log(1 / (1 + np.exp(-logits_sim)) + 1e-10))\n",
    "    accuracies.append(acc)\n",
    "    losses_by_beta.append(loss_sim)\n",
    "\n",
    "ax3.plot(beta_range, accuracies, 'b-', lw=2.5, label='ì„ í˜¸ ì •í™•ë„')\n",
    "ax3_twin = ax3.twinx()\n",
    "ax3_twin.plot(beta_range, losses_by_beta, 'r--', lw=2, label='DPO Loss')\n",
    "ax3.set_xlabel(r'$\beta$', fontsize=11)\n",
    "ax3.set_ylabel('Accuracy', fontsize=11, color='blue')\n",
    "ax3_twin.set_ylabel('Loss', fontsize=11, color='red')\n",
    "ax3.set_title(r'$\beta$ ë²”ìœ„ë³„ ì •í™•ë„ì™€ ì†ì‹¤', fontweight='bold')\n",
    "ax3.legend(loc='center left', fontsize=9)\n",
    "ax3_twin.legend(loc='center right', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dpo_beta_effect.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_beta_effect.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ ë¹„êµ\n",
    "print(f\"\\nbetaë³„ DPO ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(f\"{'beta':>8} | {'Loss':>10} | {'Accuracy':>10}\")\n",
    "print(\"-\" * 35)\n",
    "for b in [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]:\n",
    "    logits_sim = b * (chosen_ratios - rejected_ratios)\n",
    "    acc = np.mean(logits_sim > 0)\n",
    "    loss_sim = np.mean(-np.log(1 / (1 + np.exp(-logits_sim)) + 1e-10))\n",
    "    print(f\"{b:>8.2f} | {loss_sim:>10.4f} | {acc:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ <a name='4.-DPO-vs-RLHF-ë¹„êµ'></a>\n",
    "\n",
    "DPOì™€ RLHFì˜ í•™ìŠµ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •:**\n",
    "- ì •ì±… ë„¤íŠ¸ì›Œí¬: ê°„ë‹¨í•œ MLP (ì…ë ¥ â†’ ì¶œë ¥ í™•ë¥  ë¶„í¬)\n",
    "- DPO: ì„ í˜¸ ìŒ ë°ì´í„°ë¡œ ì§ì ‘ ì •ì±… í•™ìŠµ\n",
    "- RLHF: Reward Model + PPO ê·¼ì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ DPO vs RLHF í•™ìŠµ ê³¡ì„  ë¹„êµ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "n_steps = 200\n",
    "\n",
    "# í•©ì„± ì„ í˜¸ ë°ì´í„°\n",
    "input_dim = 8\n",
    "n_data = 300\n",
    "X = np.random.randn(n_data, input_dim).astype(np.float32)\n",
    "true_reward = X @ np.random.randn(input_dim, 1).astype(np.float32) * 0.5\n",
    "Y_chosen_logp = -1.5 + 0.3 * true_reward.flatten() + np.random.randn(n_data).astype(np.float32) * 0.2\n",
    "Y_rejected_logp = -2.5 - 0.2 * true_reward.flatten() + np.random.randn(n_data).astype(np.float32) * 0.3\n",
    "\n",
    "# DPO í•™ìŠµ\n",
    "dpo_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "dpo_opt = tf.keras.optimizers.Adam(0.005)\n",
    "ref_logps_chosen = Y_chosen_logp.copy()\n",
    "ref_logps_rejected = Y_rejected_logp.copy()\n",
    "\n",
    "dpo_losses = []\n",
    "dpo_accs = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    idx = np.random.choice(n_data, 64, replace=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = dpo_model(X[idx], training=True)\n",
    "        pi_chosen = out[:, 0]\n",
    "        pi_rejected = out[:, 1]\n",
    "        logits = 0.1 * ((pi_chosen - ref_logps_chosen[idx]) -\n",
    "                         (pi_rejected - ref_logps_rejected[idx]))\n",
    "        loss = -tf.reduce_mean(tf.math.log_sigmoid(logits))\n",
    "    grads = tape.gradient(loss, dpo_model.trainable_variables)\n",
    "    dpo_opt.apply_gradients(zip(grads, dpo_model.trainable_variables))\n",
    "    acc = tf.reduce_mean(tf.cast(logits > 0, tf.float32)).numpy()\n",
    "    dpo_losses.append(loss.numpy())\n",
    "    dpo_accs.append(acc)\n",
    "\n",
    "# RLHF ì‹œë®¬ë ˆì´ì…˜ (Reward Model + PPO ê·¼ì‚¬)\n",
    "rm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "rm_opt = tf.keras.optimizers.Adam(0.005)\n",
    "\n",
    "# RM ì‚¬ì „í•™ìŠµ\n",
    "for _ in range(80):\n",
    "    idx = np.random.choice(n_data, 64, replace=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        r_w = rm_model(X[idx], training=True)[:, 0]\n",
    "        r_l = rm_model(X[idx] + np.random.randn(64, input_dim).astype(np.float32) * 0.5,\n",
    "                        training=True)[:, 0]\n",
    "        rm_loss = -tf.reduce_mean(tf.math.log_sigmoid(r_w - r_l))\n",
    "    grads = tape.gradient(rm_loss, rm_model.trainable_variables)\n",
    "    rm_opt.apply_gradients(zip(grads, rm_model.trainable_variables))\n",
    "\n",
    "rlhf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation='relu', input_shape=(input_dim,)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "rlhf_opt = tf.keras.optimizers.Adam(0.003)\n",
    "\n",
    "rlhf_losses = []\n",
    "rlhf_accs = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    idx = np.random.choice(n_data, 64, replace=False)\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = rlhf_model(X[idx], training=True)\n",
    "        reward_estimate = rm_model(X[idx], training=False)[:, 0]\n",
    "        # PPO ê·¼ì‚¬: reward ê¸°ë°˜ ì •ì±… ê²½ì‚¬\n",
    "        chosen_logp = out[:, 0]\n",
    "        kl_penalty = 0.1 * tf.reduce_mean(tf.square(chosen_logp - ref_logps_chosen[idx]))\n",
    "        rlhf_loss = -tf.reduce_mean(reward_estimate * chosen_logp) + kl_penalty\n",
    "        # PPOì— ë…¸ì´ì¦ˆ ì¶”ê°€ (í˜„ì‹¤ì  ë¶ˆì•ˆì •ì„± ë°˜ì˜)\n",
    "        noise = tf.random.normal([], stddev=0.05)\n",
    "        rlhf_loss = rlhf_loss + noise\n",
    "\n",
    "    grads = tape.gradient(rlhf_loss, rlhf_model.trainable_variables)\n",
    "    rlhf_opt.apply_gradients(zip(grads, rlhf_model.trainable_variables))\n",
    "\n",
    "    out_eval = rlhf_model(X[idx], training=False)\n",
    "    logits_eval = 0.1 * ((out_eval[:, 0] - ref_logps_chosen[idx]) -\n",
    "                          (out_eval[:, 1] - ref_logps_rejected[idx]))\n",
    "    acc = tf.reduce_mean(tf.cast(logits_eval > 0, tf.float32)).numpy()\n",
    "    rlhf_losses.append(rlhf_loss.numpy())\n",
    "    rlhf_accs.append(acc)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "window = 10\n",
    "dpo_smooth = np.convolve(dpo_losses, np.ones(window)/window, mode='valid')\n",
    "rlhf_smooth = np.convolve(rlhf_losses, np.ones(window)/window, mode='valid')\n",
    "ax1.plot(dpo_smooth, 'b-', lw=2.5, label='DPO Loss')\n",
    "ax1.plot(rlhf_smooth, 'r-', lw=2.5, alpha=0.7, label='RLHF Loss')\n",
    "ax1.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('DPO vs RLHF í•™ìŠµ ê³¡ì„ ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ì •í™•ë„ ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "dpo_acc_smooth = np.convolve(dpo_accs, np.ones(window)/window, mode='valid')\n",
    "rlhf_acc_smooth = np.convolve(rlhf_accs, np.ones(window)/window, mode='valid')\n",
    "ax2.plot(dpo_acc_smooth, 'b-', lw=2.5, label='DPO Accuracy')\n",
    "ax2.plot(rlhf_acc_smooth, 'r-', lw=2.5, alpha=0.7, label='RLHF Accuracy')\n",
    "ax2.axhline(y=0.5, color='gray', ls='--', lw=1.5, label='ëœë¤ ê¸°ì¤€ì„ ')\n",
    "ax2.set_xlabel('í•™ìŠµ ìŠ¤í…', fontsize=11)\n",
    "ax2.set_ylabel('ì„ í˜¸ ì •í™•ë„', fontsize=11)\n",
    "ax2.set_title('DPO vs RLHF ì„ í˜¸ ì •í™•ë„', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.3, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dpo_vs_rlhf_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_vs_rlhf_comparison.png\")\n",
    "\n",
    "print(f\"\\nìµœì¢… ë¹„êµ ê²°ê³¼:\")\n",
    "print(f\"{'ë©”íŠ¸ë¦­':<20} | {'DPO':>12} | {'RLHF':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'ìµœì¢… Loss':<20} | {np.mean(dpo_losses[-20:]):>12.4f} | {np.mean(rlhf_losses[-20:]):>12.4f}\")\n",
    "print(f\"{'ìµœì¢… Accuracy':<20} | {np.mean(dpo_accs[-20:]):>12.4f} | {np.mean(rlhf_accs[-20:]):>12.4f}\")\n",
    "print(f\"{'Loss ë¶„ì‚° (ì•ˆì •ì„±)':<20} | {np.var(dpo_losses[-50:]):>12.6f} | {np.var(rlhf_losses[-50:]):>12.6f}\")\n",
    "print(f\"\\n  â†’ DPO: ì•ˆì •ì  ìˆ˜ë ´, ë‹¨ìˆœí•œ êµ¬ì¡°\")\n",
    "print(f\"  â†’ RLHF: ë¶„ì‚°ì´ í¬ê³ , RM + PPOì˜ ë³µì¡í•œ íŒŒì´í”„ë¼ì¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²• <a name='5.-íŒŒìƒ-ê¸°ë²•'></a>\n",
    "\n",
    "DPOì˜ ì„±ê³µ ì´í›„ ë‹¤ì–‘í•œ íŒŒìƒ ê¸°ë²•ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "| ê¸°ë²• | í•µì‹¬ ì•„ì´ë””ì–´ | ìˆ˜ì‹ íŠ¹ì§• |\n",
    "|------|-------------|----------|\n",
    "| **ORPO** | SFTì™€ DPOë¥¼ í•œ ë²ˆì— | $\\mathcal{L} = \\mathcal{L}_{SFT} + \\lambda \\cdot \\mathcal{L}_{OR}$ |\n",
    "| **KTO** | ìŒ(pair) ë°ì´í„° ë¶ˆí•„ìš” | ê°œë³„ ì‘ë‹µì— \"ì¢‹ë‹¤/ë‚˜ì˜ë‹¤\" ë¼ë²¨ë§Œ í•„ìš” |\n",
    "| **SimPO** | Reference-free DPO | $\\pi_{ref}$ ì—†ì´ ê¸¸ì´ ì •ê·œí™”ëœ log-prob ì‚¬ìš© |\n",
    "| **IPO** | Ïƒ-free ìµœì í™” | ì‹œê·¸ëª¨ì´ë“œ ëŒ€ì‹  ì œê³± ì†ì‹¤ ì‚¬ìš© |\n",
    "\n",
    "### ORPO (Odds Ratio Preference Optimization)\n",
    "\n",
    "$$\\mathcal{L}_{ORPO} = \\mathcal{L}_{NLL} + \\lambda \\cdot \\mathcal{L}_{OR}$$\n",
    "\n",
    "$$\\mathcal{L}_{OR} = -\\log\\sigma\\!\\left(\\log\\frac{P(y_w|x)}{1-P(y_w|x)} - \\log\\frac{P(y_l|x)}{1-P(y_l|x)}\\right)$$\n",
    "\n",
    "### KTO (Kahneman-Tversky Optimization)\n",
    "\n",
    "$$\\mathcal{L}_{KTO} = \\begin{cases} -\\lambda_w \\sigma(\\beta r_w - z_{ref}) & \\text{if } y \\text{ is desirable} \\\\ -\\lambda_l \\sigma(z_{ref} - \\beta r_l) & \\text{if } y \\text{ is undesirable}\\end{cases}$$\n",
    "\n",
    "### SimPO (Simple Preference Optimization)\n",
    "\n",
    "$$\\mathcal{L}_{SimPO} = -\\log\\sigma\\!\\left(\\frac{\\beta}{|y_w|}\\log P(y_w|x) - \\frac{\\beta}{|y_l|}\\log P(y_l|x) - \\gamma\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ORPO/KTO/SimPO ë¹„êµ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "n_sim = 1000\n",
    "\n",
    "# í•©ì„± ì„ í˜¸ ë°ì´í„°: log-probability\n",
    "chosen_lp = np.random.normal(-1.5, 0.5, n_sim)\n",
    "rejected_lp = np.random.normal(-2.5, 0.6, n_sim)\n",
    "ref_chosen_lp_sim = np.random.normal(-1.8, 0.5, n_sim)\n",
    "ref_rejected_lp_sim = np.random.normal(-2.3, 0.5, n_sim)\n",
    "chosen_lengths = np.random.randint(10, 50, n_sim)\n",
    "rejected_lengths = np.random.randint(10, 50, n_sim)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(x, -30, 30)))\n",
    "\n",
    "# DPO Loss\n",
    "beta_sim = 0.1\n",
    "dpo_logits = beta_sim * ((chosen_lp - ref_chosen_lp_sim) - (rejected_lp - ref_rejected_lp_sim))\n",
    "dpo_loss_vals = -np.log(sigmoid(dpo_logits) + 1e-10)\n",
    "\n",
    "# ORPO Loss (Odds Ratio)\n",
    "chosen_prob = np.exp(chosen_lp)\n",
    "rejected_prob = np.exp(rejected_lp)\n",
    "odds_w = np.log(chosen_prob / (1 - chosen_prob + 1e-10) + 1e-10)\n",
    "odds_l = np.log(rejected_prob / (1 - rejected_prob + 1e-10) + 1e-10)\n",
    "orpo_logits = odds_w - odds_l\n",
    "orpo_loss_vals = -np.log(sigmoid(orpo_logits) + 1e-10)\n",
    "\n",
    "# KTO Loss (ê°œë³„ ì‘ë‹µ)\n",
    "z_ref = beta_sim * (ref_chosen_lp_sim - ref_rejected_lp_sim).mean()\n",
    "kto_chosen_loss = 1 - sigmoid(beta_sim * (chosen_lp - ref_chosen_lp_sim) - z_ref)\n",
    "kto_rejected_loss = 1 - sigmoid(z_ref - beta_sim * (rejected_lp - ref_rejected_lp_sim))\n",
    "kto_loss_vals = 0.5 * kto_chosen_loss + 0.5 * kto_rejected_loss\n",
    "\n",
    "# SimPO Loss (Reference-free, length-normalized)\n",
    "gamma_sim = 0.5\n",
    "simpo_logits = beta_sim * (chosen_lp / chosen_lengths - rejected_lp / rejected_lengths) - gamma_sim\n",
    "simpo_loss_vals = -np.log(sigmoid(simpo_logits) + 1e-10)\n",
    "\n",
    "# ë¹„êµ í‘œ\n",
    "methods = ['DPO', 'ORPO', 'KTO', 'SimPO']\n",
    "mean_losses = [dpo_loss_vals.mean(), orpo_loss_vals.mean(),\n",
    "               kto_loss_vals.mean(), simpo_loss_vals.mean()]\n",
    "std_losses = [dpo_loss_vals.std(), orpo_loss_vals.std(),\n",
    "              kto_loss_vals.std(), simpo_loss_vals.std()]\n",
    "accs = [np.mean(dpo_logits > 0), np.mean(orpo_logits > 0),\n",
    "        np.mean(kto_chosen_loss < 0.5), np.mean(simpo_logits > 0)]\n",
    "\n",
    "print(\"ORPO/KTO/SimPO íŒŒìƒ ê¸°ë²• ë¹„êµ\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'ê¸°ë²•':<8} | {'í‰ê·  Loss':>12} | {'Loss í‘œì¤€í¸ì°¨':>14} | {'ì •í™•ë„':>8} | {'Ref í•„ìš”':>10}\")\n",
    "print(\"-\" * 65)\n",
    "for m, ml, sl, a in zip(methods, mean_losses, std_losses, accs):\n",
    "    needs_ref = \"í•„ìš”\" if m in ['DPO', 'KTO'] else \"ë¶ˆí•„ìš”\"\n",
    "    print(f\"{m:<8} | {ml:>12.4f} | {sl:>14.4f} | {a:>8.4f} | {needs_ref:>10}\")\n",
    "\n",
    "print(f\"\\ní•µì‹¬ ì°¨ì´ì :\")\n",
    "print(f\"  ORPO: SFT + ì„ í˜¸í•™ìŠµ ë™ì‹œ â†’ í•™ìŠµ ë‹¨ê³„ ì ˆì•½\")\n",
    "print(f\"  KTO:  ìŒ(pair) ë°ì´í„° ì—†ì´ ê°œë³„ 'ì¢‹ë‹¤/ë‚˜ì˜ë‹¤' ë¼ë²¨ë¡œ í•™ìŠµ\")\n",
    "print(f\"  SimPO: Reference model ë¶ˆí•„ìš” â†’ ë©”ëª¨ë¦¬ ì ˆì•½, ê¸¸ì´ ì •ê·œí™”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ íŒŒìƒ ê¸°ë²• ì†ì‹¤ ë¶„í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ì†ì‹¤ ë¶„í¬ ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "all_losses = [dpo_loss_vals, orpo_loss_vals, kto_loss_vals, simpo_loss_vals]\n",
    "colors_method = ['#2196F3', '#FF9800', '#E91E63', '#4CAF50']\n",
    "positions = [1, 2, 3, 4]\n",
    "bp = ax1.boxplot(all_losses, positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], colors_method):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "ax1.set_xticklabels(methods, fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('ì„ í˜¸ í•™ìŠµ ê¸°ë²•ë³„ ì†ì‹¤ ë¶„í¬', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# (2) ê¸°ë²•ë³„ íŠ¹ì„± ë ˆì´ë” ì°¨íŠ¸ (ë°” ì°¨íŠ¸ë¡œ í‘œí˜„)\n",
    "ax2 = axes[1]\n",
    "categories = ['í•™ìŠµ ì•ˆì •ì„±', 'ë©”ëª¨ë¦¬ íš¨ìœ¨', 'ë°ì´í„° íš¨ìœ¨', 'êµ¬í˜„ ë‹¨ìˆœì„±']\n",
    "# 5ì  ë§Œì  ìƒëŒ€ ì ìˆ˜ (ë…¼ë¬¸ ê¸°ë°˜ ì •ì„±ì  í‰ê°€)\n",
    "scores = {\n",
    "    'DPO':   [4.5, 3.0, 3.5, 4.5],\n",
    "    'ORPO':  [4.0, 4.0, 3.0, 3.5],\n",
    "    'KTO':   [3.5, 3.5, 4.5, 3.0],\n",
    "    'SimPO': [4.0, 4.5, 3.5, 4.0],\n",
    "}\n",
    "\n",
    "x_pos = np.arange(len(categories))\n",
    "width = 0.18\n",
    "for i, (method, sc) in enumerate(scores.items()):\n",
    "    ax2.bar(x_pos + i * width, sc, width, label=method,\n",
    "            color=colors_method[i], alpha=0.8)\n",
    "ax2.set_xticks(x_pos + width * 1.5)\n",
    "ax2.set_xticklabels(categories, fontsize=10)\n",
    "ax2.set_ylabel('ì ìˆ˜ (5ì  ë§Œì )', fontsize=11)\n",
    "ax2.set_title('ê¸°ë²•ë³„ íŠ¹ì„± ë¹„êµ', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 5.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dpo_variants_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/dpo_variants_comparison.png\")\n",
    "\n",
    "# ì •ë¦¬ í‘œ\n",
    "print(f\"\\nê¸°ë²•ë³„ ìš”êµ¬ì‚¬í•­ ë¹„êµ:\")\n",
    "print(f\"{'ê¸°ë²•':<8} | {'Reference Model':>16} | {'Pair ë°ì´í„°':>12} | {'RM í•„ìš”':>10} | {'í•™ìŠµ ë‹¨ê³„':>10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'RLHF':<8} | {'í•„ìš”':>16} | {'í•„ìš”':>12} | {'í•„ìš”':>10} | {'3ë‹¨ê³„':>10}\")\n",
    "print(f\"{'DPO':<8} | {'í•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}\")\n",
    "print(f\"{'ORPO':<8} | {'ë¶ˆí•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}\")\n",
    "print(f\"{'KTO':<8} | {'í•„ìš”':>16} | {'ë¶ˆí•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}\")\n",
    "print(f\"{'SimPO':<8} | {'ë¶ˆí•„ìš”':>16} | {'í•„ìš”':>12} | {'ë¶ˆí•„ìš”':>10} | {'1ë‹¨ê³„':>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| DPO ë„ì¶œ | RLHF ìµœì í•´ì˜ ë‹«íŒ í˜•íƒœ â†’ BT ëª¨ë¸ ëŒ€ì… | â­â­â­ |\n",
    "| ì•”ë¬µì  ë³´ìƒ | $r^* = \\beta\\log(\\pi^*/\\pi_{ref}) + \\beta\\log Z$ â€” ì •ì±…ì´ ê³§ ë³´ìƒ | â­â­â­ |\n",
    "| DPO Loss | $-\\mathbb{E}[\\log\\sigma(\\beta\\Delta\\log\\pi)]$ â€” ë³„ë„ RM/PPO ë¶ˆí•„ìš” | â­â­â­ |\n",
    "| Î² íŒŒë¼ë¯¸í„° | í° Î² = ë³´ìˆ˜ì , ì‘ì€ Î² = ê³µê²©ì  í•™ìŠµ | â­â­â­ |\n",
    "| RLHF ë“±ê°€ì„± | DPO = RLHFì˜ ìˆ˜í•™ì ìœ¼ë¡œ ë™ë“±í•œ ë‹¨ìˆœí™” | â­â­â­ |\n",
    "| ORPO | SFT + ì„ í˜¸í•™ìŠµ ë™ì‹œ, Odds Ratio ì‚¬ìš© | â­â­ |\n",
    "| KTO | ìŒ ë°ì´í„° ë¶ˆí•„ìš”, ê°œë³„ ì„ í˜¸/ë¹„ì„ í˜¸ ë¼ë²¨ | â­â­ |\n",
    "| SimPO | Reference-free, ê¸¸ì´ ì •ê·œí™” | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\mathcal{L}_{DPO}(\\pi_\\theta) = -\\mathbb{E}\\left[\\log\\sigma\\!\\left(\\beta\\log\\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta\\log\\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right]$$\n",
    "\n",
    "$$r^*(y|x) = \\beta\\log\\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta\\log Z(x)$$\n",
    "\n",
    "$$\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_{ref}(y|x)\\exp\\!\\left(\\frac{r^*(y|x)}{\\beta}\\right)$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**05_constitutional_ai_and_rlaif.ipynb** â€” Anthropicì˜ Constitutional AI ì›ì¹™, AI-í”¼ë“œë°±(RLAIF) ìë™í™” íŒŒì´í”„ë¼ì¸, Red Teamingê³¼ Jailbreak ë°©ì–´ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}