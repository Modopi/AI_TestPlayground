{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: AI ì–¼ë¼ì¸ë¨¼íŠ¸ â€” Actor-Criticê³¼ PPO\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Advantage Functionì˜ ìˆ˜í•™ì  ì˜ë¯¸ì™€ ë¶„ì‚° ê°ì†Œ íš¨ê³¼ë¥¼ ì´í•´í•œë‹¤\n",
    "- Actor-Critic êµ¬ì¡°ì—ì„œ Criticì´ Baseline ì—­í• ì„ í•˜ëŠ” ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤\n",
    "- PPO-Clip ëª©ì í•¨ìˆ˜ì˜ 3êµ¬ê°„ ë™ì‘ì„ ìˆ˜ì‹ìœ¼ë¡œ ì™„ì „ ì „ê°œí•˜ê³  ì‹œê°í™”í•œë‹¤\n",
    "- KL í˜ë„í‹° ë°©ì‹ê³¼ Clip ë°©ì‹ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•˜ê³  êµ¬í˜„í•œë‹¤\n",
    "- A2Cì—ì„œ PPOë¡œì˜ ë°œì „ ê³¼ì •ê³¼ LLM ì •ë ¬ì—ì„œì˜ ì—­í• ì„ ì„¤ëª…í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Advantage Functionê³¼ PPO](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [Advantage Function ê³„ì‚° ë°ëª¨](#2.-Advantage-Function-ê³„ì‚°)\n",
    "3. [PPO-Clip ëª©ì í•¨ìˆ˜ 3êµ¬ê°„ ì‹œê°í™”](#3.-PPO-Clip-3êµ¬ê°„-ì‹œê°í™”)\n",
    "4. [A2C vs PPO ë¹„êµ êµ¬í˜„](#4.-A2C-vs-PPO-ë¹„êµ)\n",
    "5. [KL í˜ë„í‹° vs Clip ë¹„êµ](#5.-KL-í˜ë„í‹°-vs-Clip)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### Advantage Function\n",
    "\n",
    "REINFORCEì˜ ë†’ì€ ë¶„ì‚° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Baseline** $b(s)$ë¥¼ ë¹¼ì¤ë‹ˆë‹¤:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[(G_t - b(s_t)) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right]$$\n",
    "\n",
    "ìµœì ì˜ Baselineì€ ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ $V^\\pi(s)$ì´ë©°, ì´ë•Œ $(G_t - V^\\pi(s_t))$ê°€ **Advantage**ì…ë‹ˆë‹¤:\n",
    "\n",
    "$$A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$$\n",
    "\n",
    "- $A^\\pi(s, a) > 0$: í•´ë‹¹ í–‰ë™ì´ **í‰ê· ë³´ë‹¤ ë‚˜ìŒ** â†’ í™•ë¥  ì¦ê°€\n",
    "- $A^\\pi(s, a) < 0$: í•´ë‹¹ í–‰ë™ì´ **í‰ê· ë³´ë‹¤ ëª»í•¨** â†’ í™•ë¥  ê°ì†Œ\n",
    "- $A^\\pi(s, a) = 0$: í‰ê·  ìˆ˜ì¤€\n",
    "\n",
    "### GAE (Generalized Advantage Estimation)\n",
    "\n",
    "TD ì˜¤ì°¨ë¥¼ ê°€ì¤‘ í•©ì‚°í•˜ì—¬ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì œì–´í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "$$\\hat{A}_t^{GAE} = \\sum_{l=0}^{T-t} (\\gamma \\lambda)^l \\delta_{t+l}$$\n",
    "\n",
    "- $\\lambda = 0$: 1-step TD (ë‚®ì€ ë¶„ì‚°, ë†’ì€ í¸í–¥)\n",
    "- $\\lambda = 1$: Monte Carlo (ë†’ì€ ë¶„ì‚°, ë‚®ì€ í¸í–¥)\n",
    "\n",
    "### PPO-Clip ëª©ì í•¨ìˆ˜\n",
    "\n",
    "í™•ë¥  ë¹„ìœ¨:\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**PPO-Clip ëª©ì í•¨ìˆ˜:**\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "- $\\epsilon$: í´ë¦¬í•‘ ë²”ìœ„ (ë³´í†µ 0.1 ~ 0.2)\n",
    "\n",
    "**3êµ¬ê°„ ë¶„ì„:**\n",
    "\n",
    "| êµ¬ê°„ | ì¡°ê±´ | $\\hat{A}_t > 0$ (ì¢‹ì€ í–‰ë™) | $\\hat{A}_t < 0$ (ë‚˜ìœ í–‰ë™) |\n",
    "|------|------|-----|------|\n",
    "| ì¢Œì¸¡ | $r_t < 1 - \\epsilon$ | í´ë¦¬í•‘ë¨ (í™•ë¥  ê°ì†Œ ì œí•œ) | ì›ë˜ ê°’ ì‚¬ìš© |\n",
    "| ì¤‘ì•™ | $1-\\epsilon \\leq r_t \\leq 1+\\epsilon$ | ì›ë˜ ê°’ ì‚¬ìš© | ì›ë˜ ê°’ ì‚¬ìš© |\n",
    "| ìš°ì¸¡ | $r_t > 1 + \\epsilon$ | ì›ë˜ ê°’ ì‚¬ìš© | í´ë¦¬í•‘ë¨ (í™•ë¥  ì¦ê°€ ì œí•œ) |\n",
    "\n",
    "### KL í˜ë„í‹° ë°©ì‹ (PPO-Penalty)\n",
    "\n",
    "$$L^{KL}(\\theta) = \\mathbb{E}\\left[r_t(\\theta)\\hat{A}_t - \\beta D_{KL}[\\pi_{\\theta_{old}} \\| \\pi_\\theta]\\right]$$\n",
    "\n",
    "- $\\beta$: KL í˜ë„í‹° ê³„ìˆ˜ (ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆ ê°€ëŠ¥)\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| Advantage | $A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$ | í‰ê·  ëŒ€ë¹„ í–‰ë™ì˜ ìƒëŒ€ì  ê°€ì¹˜ |\n",
    "| PPO-Clip | $\\min(r_t \\hat{A}_t, \\text{clip}(r_t) \\hat{A}_t)$ | ì •ì±… ì—…ë°ì´íŠ¸ ë²”ìœ„ ì œí•œ |\n",
    "| í™•ë¥  ë¹„ìœ¨ | $r_t = \\pi_\\theta / \\pi_{\\theta_{old}}$ | ìƒˆ ì •ì±… / ì´ì „ ì •ì±… í™•ë¥  ë¹„ |\n",
    "| GAE | $\\hat{A}_t = \\sum_l (\\gamma\\lambda)^l \\delta_{t+l}$ | í¸í–¥-ë¶„ì‚° ê· í˜• Advantage |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ PPO ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ Advantageê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì ìˆ˜ë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "- ë°˜ í‰ê· ì´ 80ì ($V(s)$)ì¸ë° ë‚´ê°€ 95ì ($Q(s,a)$)ì„ ë°›ìœ¼ë©´ â†’ **Advantage = +15** (ì˜í–ˆë‹¤!)\n",
    "- ë°˜ í‰ê· ì´ 80ì ì¸ë° ë‚´ê°€ 60ì ì„ ë°›ìœ¼ë©´ â†’ **Advantage = -20** (ì¢€ ë” ë…¸ë ¥í•´ì•¼!)\n",
    "- ì¦‰, AdvantageëŠ” \"í‰ê·  ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ì˜í–ˆë‚˜?\"ë¥¼ ì•Œë ¤ì£¼ëŠ” ê±°ì˜ˆìš”.\n",
    "\n",
    "#### ğŸ”’ PPO-Clipì´ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìì „ê±° ë³´ì¡° ë°”í€´ë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "PPO-Clipì€ AIê°€ í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³€í•˜ì§€ ì•Šë„ë¡ **ë³´ì¡° ë°”í€´**ë¥¼ ë‹¬ì•„ì£¼ëŠ” ê±°ì˜ˆìš”:\n",
    "- ì •ì±…ì´ í¬ê²Œ ë°”ë€Œë©´($r_t$ê°€ 1ì—ì„œ ë©€ì–´ì§€ë©´) â†’ **\"ì ê¹, ë„ˆë¬´ ê¸‰í•˜ê²Œ ë°”ê¾¸ì§€ ë§ˆ!\"** ë¼ê³  ì œí•œ\n",
    "- ì •ì±…ì´ ì¡°ê¸ˆë§Œ ë°”ë€Œë©´($r_t \\approx 1$) â†’ **\"ì¢‹ì•„, ê·¸ ì •ë„ë©´ ê´œì°®ì•„!\"** ë¼ê³  í—ˆìš©\n",
    "- ì´ë ‡ê²Œ í•˜ë©´ í•™ìŠµì´ ì•ˆì •ì ìœ¼ë¡œ ì§„í–‰ë¼ìš”!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: Advantage ë¶€í˜¸ í•´ì„\n",
    "\n",
    "ì–´ë–¤ ìƒíƒœì—ì„œ $Q^\\pi(s, \\text{left}) = 8$, $Q^\\pi(s, \\text{right}) = 3$, $V^\\pi(s) = 5$ì¼ ë•Œ:\n",
    "- $A^\\pi(s, \\text{left})$ê³¼ $A^\\pi(s, \\text{right})$ë¥¼ ê°ê° êµ¬í•˜ì„¸ìš”.\n",
    "- ê° í–‰ë™ì˜ í™•ë¥ ì€ ì–´ë–»ê²Œ ë³€í•´ì•¼ í•˜ë‚˜ìš”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$A^\\pi(s, \\text{left}) = Q^\\pi(s, \\text{left}) - V^\\pi(s) = 8 - 5 = +3$$\n",
    "\n",
    "$$A^\\pi(s, \\text{right}) = Q^\\pi(s, \\text{right}) - V^\\pi(s) = 3 - 5 = -2$$\n",
    "\n",
    "- left í–‰ë™: $A > 0$ â†’ í™•ë¥ ì„ **ì¦ê°€**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤ (í‰ê· ë³´ë‹¤ ì¢‹ì€ í–‰ë™)\n",
    "- right í–‰ë™: $A < 0$ â†’ í™•ë¥ ì„ **ê°ì†Œ**ì‹œì¼œì•¼ í•©ë‹ˆë‹¤ (í‰ê· ë³´ë‹¤ ë‚˜ìœ í–‰ë™)\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: PPO-Clip ëª©ì í•¨ìˆ˜ ê°’ ê³„ì‚°\n",
    "\n",
    "$r_t = 1.3$, $\\hat{A}_t = 2.0$, $\\epsilon = 0.2$ì¼ ë•Œ $L^{CLIP}$ì˜ ê°’ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\text{ì›ë˜}: r_t \\hat{A}_t = 1.3 \\times 2.0 = 2.6$$\n",
    "\n",
    "$$\\text{í´ë¦¬í•‘}: \\text{clip}(1.3, 0.8, 1.2) \\times 2.0 = 1.2 \\times 2.0 = 2.4$$\n",
    "\n",
    "$$L^{CLIP} = \\min(2.6, 2.4) = 2.4$$\n",
    "\n",
    "â†’ $r_t > 1+\\epsilon$ì´ê³  $\\hat{A}_t > 0$ì´ë¯€ë¡œ $\\min$ì„ ì·¨í•˜ë©´ 2.4ê°€ ë˜ì–´, í™•ë¥  ì¦ê°€í­ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì œí•œë©ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advantage Function ê³„ì‚° ë°ëª¨ <a name='2.-Advantage-Function-ê³„ì‚°'></a>\n",
    "\n",
    "ê°„ë‹¨í•œ ì—í”¼ì†Œë“œë¥¼ ìƒì„±í•˜ê³  **TD ì”ì°¨, GAE Advantage**ë¥¼ ë‹¨ê³„ë³„ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t), \\quad \\hat{A}_t^{GAE} = \\sum_{l=0}^{T-t}(\\gamma\\lambda)^l \\delta_{t+l}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Advantage Function ê³„ì‚° ë°ëª¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê°„ë‹¨í•œ ì—í”¼ì†Œë“œ (5 timestep)\n",
    "rewards = np.array([1.0, 0.5, 2.0, -1.0, 3.0])\n",
    "values  = np.array([2.0, 1.5, 3.0, 0.5, 2.5])  # V(s_t) ì¶”ì •ê°’\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "T = len(rewards)\n",
    "next_value = 0.0  # ì—í”¼ì†Œë“œ ì¢…ë£Œ\n",
    "\n",
    "print(f\"ì—í”¼ì†Œë“œ ë°ì´í„°:\")\n",
    "print(f\"{'t':>4} | {'r_t':>6} | {'V(s_t)':>8} | {'V(s_t+1)':>10}\")\n",
    "print(f\"{'-'*38}\")\n",
    "for t in range(T):\n",
    "    v_next = values[t+1] if t < T-1 else next_value\n",
    "    print(f\"{t:>4} | {rewards[t]:>6.1f} | {values[t]:>8.2f} | {v_next:>10.2f}\")\n",
    "\n",
    "# TD ì”ì°¨ ê³„ì‚°\n",
    "deltas = np.zeros(T)\n",
    "for t in range(T):\n",
    "    v_next = values[t+1] if t < T-1 else next_value\n",
    "    deltas[t] = rewards[t] + gamma * v_next - values[t]\n",
    "\n",
    "print(f\"\\nTD ì”ì°¨ delta_t:\")\n",
    "for t in range(T):\n",
    "    vn = values[t+1] if t < T-1 else next_value\n",
    "    print(f\"  delta_{t} = r_{t} + gamma*V(s_{t+1}) - V(s_{t}) = \"\n",
    "          f\"{rewards[t]:.1f} + {gamma}*{vn:.2f} - {values[t]:.2f} = {deltas[t]:+.4f}\")\n",
    "\n",
    "# GAE ê³„ì‚° (ì—­ìˆœ ëˆ„ì )\n",
    "gae_advantages = np.zeros(T)\n",
    "gae = 0\n",
    "for t in reversed(range(T)):\n",
    "    gae = deltas[t] + gamma * lam * gae\n",
    "    gae_advantages[t] = gae\n",
    "\n",
    "# Monte Carlo Advantage (ë¹„êµìš©)\n",
    "mc_returns = np.zeros(T)\n",
    "G = next_value\n",
    "for t in reversed(range(T)):\n",
    "    G = rewards[t] + gamma * G\n",
    "    mc_returns[t] = G\n",
    "mc_advantages = mc_returns - values\n",
    "\n",
    "print(f\"\\n{'t':>4} | {'delta_t (TD)':>10} | {'GAE(lambda)':>12} | {'MC Advantage':>14}\")\n",
    "print(f\"{'-'*50}\")\n",
    "for t in range(T):\n",
    "    print(f\"{t:>4} | {deltas[t]:>+10.4f} | {gae_advantages[t]:>+12.4f} | {mc_advantages[t]:>+14.4f}\")\n",
    "\n",
    "print(f\"\\në¶„ì‚° ë¹„êµ:\")\n",
    "print(f\"  TD(lambda=0) ë¶„ì‚°:  {np.var(deltas):.4f}\")\n",
    "print(f\"  GAE(lambda={lam}) ë¶„ì‚°: {np.var(gae_advantages):.4f}\")\n",
    "print(f\"  MC(lambda=1) ë¶„ì‚°:  {np.var(mc_advantages):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO-Clip ëª©ì í•¨ìˆ˜ 3êµ¬ê°„ ì‹œê°í™” <a name='3.-PPO-Clip-3êµ¬ê°„-ì‹œê°í™”'></a>\n",
    "\n",
    "PPO-Clipì˜ í•µì‹¬ì€ í™•ë¥  ë¹„ìœ¨ $r_t(\\theta)$ì— ë”°ë¼ ëª©ì í•¨ìˆ˜ê°€ **3êµ¬ê°„ìœ¼ë¡œ ë¶„ë¦¬**ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "- **êµ¬ê°„ 1** ($r_t < 1-\\epsilon$): $\\hat{A}_t > 0$ì¼ ë•Œ í´ë¦¬í•‘ â†’ ì¢‹ì€ í–‰ë™ì˜ í™•ë¥  **ê°ì†Œ ì œí•œ**\n",
    "- **êµ¬ê°„ 2** ($1-\\epsilon \\leq r_t \\leq 1+\\epsilon$): í´ë¦¬í•‘ ì—†ìŒ â†’ ììœ ë¡œìš´ ì—…ë°ì´íŠ¸\n",
    "- **êµ¬ê°„ 3** ($r_t > 1+\\epsilon$): $\\hat{A}_t < 0$ì¼ ë•Œ í´ë¦¬í•‘ â†’ ë‚˜ìœ í–‰ë™ì˜ í™•ë¥  **ì¦ê°€ ì œí•œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ PPO-Clip 3êµ¬ê°„ ì‹œê°í™” (PLAIN TEXT labels) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "epsilon = 0.2\n",
    "r = np.linspace(0.3, 2.0, 500)\n",
    "\n",
    "def ppo_clip_objective(r, A, eps):\n",
    "    surr1 = r * A\n",
    "    r_clipped = np.clip(r, 1 - eps, 1 + eps)\n",
    "    surr2 = r_clipped * A\n",
    "    return np.minimum(surr1, surr2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# (1) A > 0 (ì¢‹ì€ í–‰ë™)\n",
    "ax1 = axes[0]\n",
    "A_pos = 1.0\n",
    "surr1_pos = r * A_pos\n",
    "r_clip = np.clip(r, 1 - epsilon, 1 + epsilon)\n",
    "surr2_pos = r_clip * A_pos\n",
    "L_pos = np.minimum(surr1_pos, surr2_pos)\n",
    "\n",
    "ax1.plot(r, surr1_pos, 'b--', lw=1.5, alpha=0.5, label='r_t * A_t (ì›ë˜)')\n",
    "ax1.plot(r, surr2_pos, 'r--', lw=1.5, alpha=0.5, label='clip(r_t) * A_t')\n",
    "ax1.plot(r, L_pos, 'g-', lw=3, label='L_CLIP (ìµœì¢…)')\n",
    "\n",
    "ax1.axvspan(0.3, 1-epsilon, alpha=0.1, color='red', label=f'êµ¬ê°„1: r < {1-epsilon}')\n",
    "ax1.axvspan(1-epsilon, 1+epsilon, alpha=0.1, color='green', label='êµ¬ê°„2: í´ë¦¬í•‘ ì—†ìŒ')\n",
    "ax1.axvspan(1+epsilon, 2.0, alpha=0.1, color='blue', label=f'êµ¬ê°„3: r > {1+epsilon}')\n",
    "\n",
    "ax1.axvline(x=1.0, color='gray', ls=':', lw=1)\n",
    "ax1.axvline(x=1-epsilon, color='red', ls='--', lw=1.5, alpha=0.7)\n",
    "ax1.axvline(x=1+epsilon, color='blue', ls='--', lw=1.5, alpha=0.7)\n",
    "ax1.set_xlabel('r_t (theta) - í™•ë¥  ë¹„ìœ¨', fontsize=11)\n",
    "ax1.set_ylabel('Objective', fontsize=11)\n",
    "ax1.set_title('A_t > 0 (ì¢‹ì€ í–‰ë™)', fontweight='bold', fontsize=13)\n",
    "ax1.legend(fontsize=8, loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0.3, 2.0)\n",
    "\n",
    "# (2) A < 0 (ë‚˜ìœ í–‰ë™)\n",
    "ax2 = axes[1]\n",
    "A_neg = -1.0\n",
    "surr1_neg = r * A_neg\n",
    "surr2_neg = r_clip * A_neg\n",
    "L_neg = np.minimum(surr1_neg, surr2_neg)\n",
    "\n",
    "ax2.plot(r, surr1_neg, 'b--', lw=1.5, alpha=0.5, label='r_t * A_t (ì›ë˜)')\n",
    "ax2.plot(r, surr2_neg, 'r--', lw=1.5, alpha=0.5, label='clip(r_t) * A_t')\n",
    "ax2.plot(r, L_neg, 'g-', lw=3, label='L_CLIP (ìµœì¢…)')\n",
    "\n",
    "ax2.axvspan(0.3, 1-epsilon, alpha=0.1, color='red', label=f'êµ¬ê°„1: r < {1-epsilon}')\n",
    "ax2.axvspan(1-epsilon, 1+epsilon, alpha=0.1, color='green', label='êµ¬ê°„2: í´ë¦¬í•‘ ì—†ìŒ')\n",
    "ax2.axvspan(1+epsilon, 2.0, alpha=0.1, color='blue', label=f'êµ¬ê°„3: r > {1+epsilon}')\n",
    "\n",
    "ax2.axvline(x=1.0, color='gray', ls=':', lw=1)\n",
    "ax2.axvline(x=1-epsilon, color='red', ls='--', lw=1.5, alpha=0.7)\n",
    "ax2.axvline(x=1+epsilon, color='blue', ls='--', lw=1.5, alpha=0.7)\n",
    "ax2.set_xlabel('r_t (theta) - í™•ë¥  ë¹„ìœ¨', fontsize=11)\n",
    "ax2.set_ylabel('Objective', fontsize=11)\n",
    "ax2.set_title('A_t < 0 (ë‚˜ìœ í–‰ë™)', fontweight='bold', fontsize=13)\n",
    "ax2.legend(fontsize=8, loc='lower left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0.3, 2.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppo_clip_3regions.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/ppo_clip_3regions.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ ë¶„ì„\n",
    "print(f\"\\nPPO-Clip 3êµ¬ê°„ ìˆ˜ì¹˜ ë¶„ì„ (epsilon={epsilon}):\")\n",
    "print(f\"{'':=<60}\")\n",
    "test_ratios = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "for rt in test_ratios:\n",
    "    for A_val, A_label in [(1.0, \"A>0\"), (-1.0, \"A<0\")]:\n",
    "        L_val = min(rt * A_val, np.clip(rt, 1-epsilon, 1+epsilon) * A_val)\n",
    "        clipped = \"í´ë¦¬í•‘\" if abs(L_val - rt * A_val) > 1e-6 else \"ì›ë˜ê°’\"\n",
    "        print(f\"  r_t={rt:.1f}, {A_label}: L_clip={L_val:+.2f} ({clipped})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. A2C vs PPO ë¹„êµ êµ¬í˜„ <a name='4.-A2C-vs-PPO-ë¹„êµ'></a>\n",
    "\n",
    "ê°„ë‹¨í•œ ì—°ì† Bandit í™˜ê²½ì—ì„œ **A2C(Advantage Actor-Critic)**ì™€ **PPO-Clip**ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "| ì•Œê³ ë¦¬ì¦˜ | ëª©ì í•¨ìˆ˜ | íŠ¹ì§• |\n",
    "|---------|---------|------|\n",
    "| A2C | $\\hat{A}_t \\nabla_\\theta \\log \\pi_\\theta$ | ë‹¨ìˆœí•˜ì§€ë§Œ í° ì—…ë°ì´íŠ¸ ê°€ëŠ¥ |\n",
    "| PPO-Clip | $\\min(r_t \\hat{A}_t, \\text{clip}(r_t) \\hat{A}_t)$ | ì—…ë°ì´íŠ¸ í¬ê¸° ì œí•œ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ A2C vs PPO ë¹„êµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n_actions = 5\n",
    "true_values = np.array([0.3, -0.2, 1.8, 0.5, -0.5])\n",
    "n_episodes = 300\n",
    "epsilon = 0.2\n",
    "\n",
    "def run_a2c(lr=0.15):\n",
    "    logits = tf.Variable(tf.zeros(n_actions))\n",
    "    value_est = tf.Variable(0.0)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    rewards_hist = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        with tf.GradientTape() as tape:\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            action = tf.random.categorical(tf.math.log(probs[tf.newaxis, :]), 1)[0, 0]\n",
    "            reward = true_values[action.numpy()] + np.random.randn() * 0.3\n",
    "\n",
    "            advantage = reward - value_est\n",
    "            log_prob = tf.math.log(probs[action] + 1e-8)\n",
    "            actor_loss = -advantage * log_prob\n",
    "            critic_loss = tf.square(advantage)\n",
    "            loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "        grads = tape.gradient(loss, [logits, value_est])\n",
    "        opt.apply_gradients(zip(grads, [logits, value_est]))\n",
    "        rewards_hist.append(reward)\n",
    "\n",
    "    return rewards_hist\n",
    "\n",
    "def run_ppo(lr=0.15, eps=0.2, n_updates=3):\n",
    "    logits = tf.Variable(tf.zeros(n_actions))\n",
    "    value_est = tf.Variable(0.0)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    rewards_hist = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        old_probs = tf.nn.softmax(logits).numpy().copy()\n",
    "        action_idx = np.random.choice(n_actions, p=old_probs)\n",
    "        reward = true_values[action_idx] + np.random.randn() * 0.3\n",
    "        advantage = reward - value_est.numpy()\n",
    "\n",
    "        for _ in range(n_updates):\n",
    "            with tf.GradientTape() as tape:\n",
    "                probs = tf.nn.softmax(logits)\n",
    "                ratio = probs[action_idx] / (old_probs[action_idx] + 1e-8)\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = tf.clip_by_value(ratio, 1.0 - eps, 1.0 + eps) * advantage\n",
    "                actor_loss = -tf.minimum(surr1, surr2)\n",
    "                critic_loss = tf.square(reward - value_est)\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "            grads = tape.gradient(loss, [logits, value_est])\n",
    "            opt.apply_gradients(zip(grads, [logits, value_est]))\n",
    "\n",
    "        rewards_hist.append(reward)\n",
    "\n",
    "    return rewards_hist\n",
    "\n",
    "n_runs = 5\n",
    "a2c_all = []\n",
    "ppo_all = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    np.random.seed(run * 100)\n",
    "    tf.random.set_seed(run * 100)\n",
    "    a2c_all.append(run_a2c())\n",
    "\n",
    "    np.random.seed(run * 100)\n",
    "    tf.random.set_seed(run * 100)\n",
    "    ppo_all.append(run_ppo())\n",
    "\n",
    "a2c_mean = np.mean(a2c_all, axis=0)\n",
    "ppo_mean = np.mean(ppo_all, axis=0)\n",
    "a2c_std = np.std(a2c_all, axis=0)\n",
    "ppo_std = np.std(ppo_all, axis=0)\n",
    "\n",
    "window = 20\n",
    "a2c_smooth = np.convolve(a2c_mean, np.ones(window)/window, mode='valid')\n",
    "ppo_smooth = np.convolve(ppo_mean, np.ones(window)/window, mode='valid')\n",
    "\n",
    "print(f\"A2C vs PPO ë¹„êµ ({n_runs}íšŒ í‰ê· )\")\n",
    "print(f\"{'':=<50}\")\n",
    "print(f\"  ìµœì  ë³´ìƒ: {true_values.max():.1f} (Arm {np.argmax(true_values)})\")\n",
    "print(f\"  A2C ìµœì¢… í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(a2c_mean[-50:]):.3f}\")\n",
    "print(f\"  PPO ìµœì¢… í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(ppo_mean[-50:]):.3f}\")\n",
    "print(f\"  A2C ë³´ìƒ ë¶„ì‚° (ë§ˆì§€ë§‰ 50):     {np.mean(a2c_std[-50:]):.3f}\")\n",
    "print(f\"  PPO ë³´ìƒ ë¶„ì‚° (ë§ˆì§€ë§‰ 50):     {np.mean(ppo_std[-50:]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ A2C vs PPO í•™ìŠµ ê³¡ì„  ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "x_range = range(len(a2c_smooth))\n",
    "ax1.plot(x_range, a2c_smooth, 'b-', lw=2, label='A2C', alpha=0.9)\n",
    "ax1.plot(x_range, ppo_smooth, 'r-', lw=2, label='PPO-Clip', alpha=0.9)\n",
    "ax1.axhline(y=true_values.max(), color='green', ls='--', lw=1.5,\n",
    "            label=f'ìµœì  ë³´ìƒ ({true_values.max():.1f})')\n",
    "ax1.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)\n",
    "ax1.set_ylabel('í‰ê·  ë³´ìƒ (ì´ë™ í‰ê· )', fontsize=11)\n",
    "ax1.set_title('A2C vs PPO í•™ìŠµ ê³¡ì„ ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "a2c_rolling_var = np.array([np.var(a2c_mean[max(0,i-window):i+1])\n",
    "                            for i in range(len(a2c_mean))])\n",
    "ppo_rolling_var = np.array([np.var(ppo_mean[max(0,i-window):i+1])\n",
    "                            for i in range(len(ppo_mean))])\n",
    "ax2.plot(a2c_rolling_var, 'b-', lw=2, label='A2C ë¶„ì‚°', alpha=0.9)\n",
    "ax2.plot(ppo_rolling_var, 'r-', lw=2, label='PPO ë¶„ì‚°', alpha=0.9)\n",
    "ax2.set_xlabel('ì—í”¼ì†Œë“œ', fontsize=11)\n",
    "ax2.set_ylabel('ë³´ìƒ ë¶„ì‚° (ë¡¤ë§)', fontsize=11)\n",
    "ax2.set_title('í•™ìŠµ ì•ˆì •ì„± ë¹„êµ', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('a2c_vs_ppo.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/a2c_vs_ppo.png\")\n",
    "print(\"PPOëŠ” A2C ëŒ€ë¹„ ë” ì•ˆì •ì ì´ê³  ì¼ê´€ëœ í•™ìŠµ ê³¡ì„ ì„ ë³´ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. KL í˜ë„í‹° vs Clip ë¹„êµ <a name='5.-KL-í˜ë„í‹°-vs-Clip'></a>\n",
    "\n",
    "PPOì—ëŠ” ë‘ ê°€ì§€ ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "**PPO-Clip** (ì‹¤ë¬´ì—ì„œ ì£¼ë¡œ ì‚¬ìš©):\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min(r_t \\hat{A}_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)\\right]$$\n",
    "\n",
    "**PPO-Penalty** (KL í˜ë„í‹°):\n",
    "\n",
    "$$L^{KL}(\\theta) = \\mathbb{E}\\left[r_t \\hat{A}_t\\right] - \\beta D_{KL}[\\pi_{\\theta_{old}} \\| \\pi_\\theta]$$\n",
    "\n",
    "| íŠ¹ì„± | PPO-Clip | PPO-Penalty |\n",
    "|------|----------|-------------|\n",
    "| í•˜ì´í¼íŒŒë¼ë¯¸í„° | $\\epsilon$ (ê³ ì •) | $\\beta$ (ì ì‘ ì¡°ì ˆ) |\n",
    "| êµ¬í˜„ ë‚œì´ë„ | ì‰¬ì›€ | ì¤‘ê°„ |\n",
    "| ì•ˆì •ì„± | ë†’ìŒ | $\\beta$ ì¡°ì ˆì— ë¯¼ê° |\n",
    "| ì‚¬ìš©ì²˜ | ëŒ€ë¶€ë¶„ì˜ RL | RLHF (InstructGPT) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ KL í˜ë„í‹° vs Clip ë¹„êµ ì‹œê°í™” (PLAIN TEXT labels) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "r = np.linspace(0.3, 2.0, 500)\n",
    "epsilon = 0.2\n",
    "A = 1.0\n",
    "\n",
    "surr1 = r * A\n",
    "r_clip = np.clip(r, 1 - epsilon, 1 + epsilon)\n",
    "surr2 = r_clip * A\n",
    "L_clip = np.minimum(surr1, surr2)\n",
    "\n",
    "kl_approx = (r - 1) - np.log(r + 1e-8)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(r, L_clip, 'g-', lw=3, label='L_CLIP')\n",
    "ax1.plot(r, surr1, 'b--', lw=1.5, alpha=0.4, label='r_t * A_t')\n",
    "ax1.axvline(x=1-epsilon, color='red', ls='--', lw=1, alpha=0.5)\n",
    "ax1.axvline(x=1+epsilon, color='red', ls='--', lw=1, alpha=0.5)\n",
    "ax1.axvline(x=1.0, color='gray', ls=':', lw=1)\n",
    "ax1.set_xlabel('r_t (theta) - í™•ë¥  ë¹„ìœ¨', fontsize=11)\n",
    "ax1.set_ylabel('Objective', fontsize=11)\n",
    "ax1.set_title('PPO-Clip', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "betas = [0.01, 0.05, 0.1, 0.3]\n",
    "colors = ['#2196F3', '#FF9800', '#E91E63', '#9C27B0']\n",
    "for beta, color in zip(betas, colors):\n",
    "    L_kl = r * A - beta * kl_approx\n",
    "    ax2.plot(r, L_kl, lw=2, color=color, label=f'beta={beta}')\n",
    "ax2.plot(r, surr1, 'k--', lw=1, alpha=0.3, label='r_t * A_t (ì›ë˜)')\n",
    "ax2.axvline(x=1.0, color='gray', ls=':', lw=1)\n",
    "ax2.set_xlabel('r_t (theta) - í™•ë¥  ë¹„ìœ¨', fontsize=11)\n",
    "ax2.set_ylabel('Objective', fontsize=11)\n",
    "ax2.set_title('PPO-Penalty (ë‹¤ì–‘í•œ beta)', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3 = axes[2]\n",
    "ax3.plot(r, kl_approx, 'r-', lw=2.5, label='D_KL approx (r-1) - log(r)')\n",
    "ax3.fill_between(r, 0, kl_approx, alpha=0.1, color='red')\n",
    "ax3.axvline(x=1.0, color='gray', ls=':', lw=1, label='r=1 (ì •ì±… ë™ì¼)')\n",
    "ax3.set_xlabel('r_t (theta) - í™•ë¥  ë¹„ìœ¨', fontsize=11)\n",
    "ax3.set_ylabel('D_KL', fontsize=11)\n",
    "ax3.set_title('KL Divergence ê·¼ì‚¬', fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('kl_penalty_vs_clip.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter15_alignment_rlhf/kl_penalty_vs_clip.png\")\n",
    "\n",
    "print(f\"\\nPPO-Clip vs PPO-Penalty ë¹„êµ:\")\n",
    "print(f\"{'':=<55}\")\n",
    "print(f\"{'r_t':>6} | {'L_clip':>8} | {'L_penalty(beta=0.1)':>18} | {'KL':>8}\")\n",
    "print(f\"{'-'*45}\")\n",
    "for rt in [0.5, 0.8, 1.0, 1.2, 1.5, 1.8]:\n",
    "    lc = min(rt * A, np.clip(rt, 1-epsilon, 1+epsilon) * A)\n",
    "    kl = (rt - 1) - np.log(rt)\n",
    "    lp = rt * A - 0.1 * kl\n",
    "    print(f\"{rt:>6.1f} | {lc:>+8.3f} | {lp:>+18.3f} | {kl:>8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ PPOì˜ LLM ì •ë ¬ ì ìš© ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vocab_size = 50\n",
    "seq_len = 8\n",
    "n_steps = 200\n",
    "epsilon = 0.2\n",
    "beta_kl = 0.05\n",
    "\n",
    "ref_logits = tf.constant(np.random.randn(vocab_size).astype(np.float32) * 0.1)\n",
    "ref_probs = tf.nn.softmax(ref_logits).numpy()\n",
    "\n",
    "policy_logits = tf.Variable(tf.identity(ref_logits))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "preferred_tokens = set(range(10, 16))\n",
    "\n",
    "kl_history = []\n",
    "reward_history = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    old_probs = tf.nn.softmax(policy_logits).numpy().copy()\n",
    "\n",
    "    action = np.random.choice(vocab_size, p=old_probs)\n",
    "    reward = 1.0 if action in preferred_tokens else -0.1\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        probs = tf.nn.softmax(policy_logits)\n",
    "\n",
    "        ratio = probs[action] / (old_probs[action] + 1e-8)\n",
    "        advantage = reward - tf.reduce_sum(probs * tf.constant(\n",
    "            [1.0 if i in preferred_tokens else -0.1 for i in range(vocab_size)]))\n",
    "\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = tf.clip_by_value(ratio, 1.0 - epsilon, 1.0 + epsilon) * advantage\n",
    "        clip_loss = -tf.minimum(surr1, surr2)\n",
    "\n",
    "        kl = tf.reduce_sum(probs * tf.math.log(probs / (ref_probs + 1e-8) + 1e-8))\n",
    "        total_loss = clip_loss + beta_kl * kl\n",
    "\n",
    "    grads = tape.gradient(total_loss, [policy_logits])\n",
    "    optimizer.apply_gradients(zip(grads, [policy_logits]))\n",
    "\n",
    "    kl_history.append(kl.numpy())\n",
    "    reward_history.append(reward)\n",
    "\n",
    "final_probs = tf.nn.softmax(policy_logits).numpy()\n",
    "preferred_mass = sum(final_probs[i] for i in preferred_tokens)\n",
    "print(f\"RLHF ìŠ¤íƒ€ì¼ PPO ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:\")\n",
    "print(f\"{'':=<50}\")\n",
    "print(f\"  ì„ í˜¸ í† í°(ID 10~15) ì´ í™•ë¥ :\")\n",
    "print(f\"    ì´ˆê¸°: {sum(ref_probs[i] for i in preferred_tokens):.4f}\")\n",
    "print(f\"    ìµœì¢…: {preferred_mass:.4f}\")\n",
    "print(f\"  ìµœì¢… KL divergence: {kl_history[-1]:.4f}\")\n",
    "print(f\"  í‰ê·  ë³´ìƒ (ë§ˆì§€ë§‰ 50): {np.mean(reward_history[-50:]):.3f}\")\n",
    "print(f\"\\n  KL í˜ë„í‹° ë•ë¶„ì— ê¸°ì¤€ ì •ì±…ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ì„œ\")\n",
    "print(f\"  ì„ í˜¸ í† í°ì˜ í™•ë¥ ì´ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| Advantage Function | $A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$ â€” í‰ê·  ëŒ€ë¹„ í–‰ë™ ê°€ì¹˜ | â­â­â­ |\n",
    "| GAE | $\\hat{A}_t = \\sum_l (\\gamma\\lambda)^l \\delta_{t+l}$ â€” í¸í–¥/ë¶„ì‚° ì¡°ì ˆ | â­â­ |\n",
    "| PPO-Clip | $\\min(r_t\\hat{A}_t, \\text{clip}(r_t)\\hat{A}_t)$ â€” 3êµ¬ê°„ ì œí•œ | â­â­â­ |\n",
    "| PPO-Penalty | $r_t\\hat{A}_t - \\beta D_{KL}$ â€” KL ê¸°ë°˜ ì œí•œ | â­â­ |\n",
    "| í™•ë¥  ë¹„ìœ¨ $r_t$ | $\\pi_\\theta / \\pi_{\\theta_{old}}$ â€” ì •ì±… ë³€í™” ì¸¡ì • | â­â­â­ |\n",
    "| Actor-Critic | Actor(ì •ì±…) + Critic(ê°€ì¹˜) ë™ì‹œ í•™ìŠµ | â­â­ |\n",
    "| Trust Region | ì •ì±… ì—…ë°ì´íŠ¸ ë²”ìœ„ë¥¼ ì‹ ë¢° ì˜ì—­ ë‚´ë¡œ ì œí•œ | â­â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$A^\\pi(s, a) = Q^\\pi(s, a) - V^\\pi(s)$$\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**03_rlhf_pipeline_overview.ipynb** â€” InstructGPTì˜ SFTâ†’Reward Modelâ†’PPO 3ë‹¨ê³„ RLHF íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ê³ , Bradley-Terry ì„ í˜¸ ëª¨ë¸ì˜ ìˆ˜ì‹ì„ ë„ì¶œí•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}