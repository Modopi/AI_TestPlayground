{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 06-02: 단어 임베딩 (Word Embeddings)\n",
    "\n",
    "## 학습 목표\n",
    "- One-Hot 인코딩의 한계를 이해한다.\n",
    "- 단어 임베딩의 개념과 장점을 파악한다.\n",
    "- `tf.keras.layers.Embedding` 레이어를 구성하고 활용한다.\n",
    "- 사전 학습된 임베딩(GloVe 등)을 불러오는 방법을 익힌다.\n",
    "- 코사인 유사도를 통해 단어 간 의미적 유사도를 계산한다.\n",
    "\n",
    "## 목차\n",
    "1. [기본 임포트](#1.-기본-임포트)\n",
    "2. [코사인 유사도 수식](#2.-코사인-유사도-수식)\n",
    "3. [One-Hot 인코딩의 한계](#3.-One-Hot-인코딩의-한계)\n",
    "4. [Embedding 레이어](#4.-Embedding-레이어)\n",
    "5. [사전 학습 임베딩 (GloVe 시뮬레이션)](#5.-사전-학습-임베딩)\n",
    "6. [코사인 유사도 계산](#6.-코사인-유사도-계산)\n",
    "7. [정리](#7.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-0002-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"NumPy 버전: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0003-0001-000000000003",
   "metadata": {},
   "source": [
    "## 2. 코사인 유사도 수식\n",
    "\n",
    "두 벡터 $A$와 $B$ 사이의 **코사인 유사도**는 벡터 간의 각도로 유사성을 측정한다.\n",
    "\n",
    "$$\\cos(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- $A \\cdot B$: 두 벡터의 내적 (dot product)\n",
    "- $\\|A\\|$, $\\|B\\|$: 각 벡터의 L2 노름 (크기)\n",
    "\n",
    "### 해석\n",
    "| 코사인 유사도 값 | 의미 |\n",
    "|-----------------|------|\n",
    "| $1.0$ | 완전히 동일한 방향 (매우 유사) |\n",
    "| $0.0$ | 직교 (관련 없음) |\n",
    "| $-1.0$ | 완전히 반대 방향 (상반됨) |\n",
    "\n",
    "임베딩 공간에서 의미적으로 유사한 단어들은 코사인 유사도가 높다.  \n",
    "예: $\\cos(\\vec{\\text{king}}, \\vec{\\text{queen}}) \\approx 0.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0004-0001-000000000004",
   "metadata": {},
   "source": [
    "## 3. One-Hot 인코딩의 한계\n",
    "\n",
    "One-Hot 인코딩은 각 단어를 하나의 차원만 1이고 나머지는 0인 벡터로 표현한다.\n",
    "\n",
    "### 문제점\n",
    "\n",
    "1. **고차원성**: 어휘 사전 크기가 10만 개면 벡터 차원도 10만 차원\n",
    "2. **희소성(Sparsity)**: 대부분의 값이 0 → 계산 비효율\n",
    "3. **의미 부재**: 모든 단어 쌍의 거리가 동일 → 유사한 단어 관계 표현 불가\n",
    "\n",
    "예를 들어, \"cat\"과 \"dog\"는 모두 동물이지만 One-Hot 표현에서는:\n",
    "- cat = [1, 0, 0, 0, 0, ...]\n",
    "- dog = [0, 1, 0, 0, 0, ...]\n",
    "- car = [0, 0, 1, 0, 0, ...]\n",
    "\n",
    "$\\cos(\\vec{\\text{cat}}, \\vec{\\text{dog}}) = 0$, $\\cos(\\vec{\\text{cat}}, \\vec{\\text{car}}) = 0$ → 구분 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-0005-0001-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot 인코딩의 한계 시각화\n",
    "\n",
    "# 예시 어휘 사전\n",
    "vocab = [\"cat\", \"dog\", \"car\", \"truck\", \"kitten\", \"puppy\"]\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# One-Hot 벡터 생성\n",
    "def one_hot(word, vocab_size, word_to_idx):\n",
    "    \"\"\"단어를 One-Hot 벡터로 변환\"\"\"\n",
    "    vec = np.zeros(vocab_size)\n",
    "    vec[word_to_idx[word]] = 1.0\n",
    "    return vec\n",
    "\n",
    "# 코사인 유사도 계산 함수\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"두 벡터 간의 코사인 유사도 계산\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "\n",
    "# One-Hot 벡터 계산\n",
    "cat_oh    = one_hot(\"cat\",    vocab_size, word_to_idx)\n",
    "dog_oh    = one_hot(\"dog\",    vocab_size, word_to_idx)\n",
    "kitten_oh = one_hot(\"kitten\", vocab_size, word_to_idx)\n",
    "car_oh    = one_hot(\"car\",    vocab_size, word_to_idx)\n",
    "\n",
    "print(\"=== One-Hot 벡터 ===\")\n",
    "print(f\"  cat   : {cat_oh}\")\n",
    "print(f\"  dog   : {dog_oh}\")\n",
    "print(f\"  kitten: {kitten_oh}\")\n",
    "print(f\"  car   : {car_oh}\")\n",
    "print()\n",
    "\n",
    "print(\"=== One-Hot 코사인 유사도 ===\")\n",
    "print(f\"  cat vs dog   : {cosine_similarity(cat_oh, dog_oh):.4f}  (동물끼리 → 0이어야 하지 않음!)\")\n",
    "print(f\"  cat vs kitten: {cosine_similarity(cat_oh, kitten_oh):.4f}  (cat-kitten 매우 유사한데 0!)\")\n",
    "print(f\"  cat vs car   : {cosine_similarity(cat_oh, car_oh):.4f}  (완전 다른데도 0)\")\n",
    "print()\n",
    "print(\"→ One-Hot은 모든 단어 쌍의 코사인 유사도가 0 → 의미 구분 불가\")\n",
    "\n",
    "# One-Hot 행렬 시각화\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "oh_matrix = np.array([one_hot(w, vocab_size, word_to_idx) for w in vocab])\n",
    "im = ax.imshow(oh_matrix, cmap='Blues', aspect='auto')\n",
    "ax.set_xticks(range(vocab_size))\n",
    "ax.set_xticklabels(vocab, rotation=45)\n",
    "ax.set_yticks(range(vocab_size))\n",
    "ax.set_yticklabels(vocab)\n",
    "ax.set_title(\"One-Hot 인코딩 행렬\\n(어휘 크기가 커지면 행렬도 거대해짐)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0006-0001-000000000006",
   "metadata": {},
   "source": [
    "## 4. Embedding 레이어\n",
    "\n",
    "`tf.keras.layers.Embedding`은 정수 인덱스를 밀집(dense) 벡터로 변환하는 레이어이다.  \n",
    "내부적으로 **가중치 행렬** $W \\in \\mathbb{R}^{\\text{vocab\\_size} \\times \\text{embed\\_dim}}$을 학습한다.\n",
    "\n",
    "### 주요 파라미터\n",
    "\n",
    "| 파라미터 | 설명 |\n",
    "|----------|------|\n",
    "| `input_dim` | 어휘 사전 크기 (정수 인덱스 최대값 + 1) |\n",
    "| `output_dim` | 임베딩 벡터 차원 수 |\n",
    "| `embeddings_initializer` | 가중치 초기화 방법 (기본: 균일 분포) |\n",
    "| `mask_zero` | 패딩 마스킹 활성화 여부 |\n",
    "| `trainable` | 가중치 업데이트 여부 (사전 학습 임베딩 고정 시 False) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-0007-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding 레이어 실습\n",
    "\n",
    "# 어휘 사전 크기와 임베딩 차원 설정\n",
    "VOCAB_SIZE = 1000   # 어휘 사전 크기\n",
    "EMBED_DIM  = 16     # 임베딩 벡터 차원\n",
    "\n",
    "# Embedding 레이어 생성\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=VOCAB_SIZE,              # 어휘 사전 크기\n",
    "    output_dim=EMBED_DIM,              # 임베딩 차원\n",
    "    embeddings_initializer='uniform',  # 초기화: 균일 분포\n",
    "    mask_zero=True,                    # 패딩 마스킹 활성화\n",
    "    name='word_embedding'\n",
    ")\n",
    "\n",
    "# 입력 형태: (배치 크기, 시퀀스 길이)\n",
    "# 출력 형태: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "sample_input = tf.constant([[1, 2, 3, 4, 0],   # 배치 샘플 1 (0은 패딩)\n",
    "                             [5, 6, 7, 0, 0]])  # 배치 샘플 2\n",
    "output = embedding_layer(sample_input)\n",
    "\n",
    "print(\"입력 형태:\", sample_input.shape, \"→ (배치 크기, 시퀀스 길이)\")\n",
    "print(\"출력 형태:\", output.shape, \"→ (배치 크기, 시퀀스 길이, 임베딩 차원)\")\n",
    "print()\n",
    "print(\"임베딩 가중치 행렬 크기:\", embedding_layer.embeddings.shape)\n",
    "print(f\"  → {VOCAB_SIZE}개 단어 × {EMBED_DIM}차원\")\n",
    "print()\n",
    "print(\"단어 인덱스 1의 임베딩 벡터:\", output[0][0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0008-0001-000000000008",
   "metadata": {},
   "source": [
    "## 5. 사전 학습 임베딩 (GloVe 시뮬레이션)\n",
    "\n",
    "**GloVe(Global Vectors for Word Representation)**는 대규모 텍스트 코퍼스에서  \n",
    "사전 학습된 단어 임베딩이다. 실무에서 자주 사용하는 방법은 다음과 같다:\n",
    "\n",
    "1. GloVe 파일(`.txt`)을 다운로드한다.\n",
    "2. 각 단어의 임베딩 벡터를 읽어 딕셔너리에 저장한다.\n",
    "3. 자신의 어휘 사전에 맞는 임베딩 행렬을 구성한다.\n",
    "4. Embedding 레이어 가중치를 해당 행렬로 초기화하고 `trainable=False`로 고정한다.\n",
    "\n",
    "아래 코드는 GloVe 로드 과정을 **시뮬레이션**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-0009-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습 임베딩 가중치 직접 설정 (GloVe 로드 시뮬레이션)\n",
    "\n",
    "# 시뮬레이션용 어휘 사전과 임베딩 차원\n",
    "words = [\"cat\", \"dog\", \"kitten\", \"puppy\", \"car\", \"truck\"]\n",
    "SIM_VOCAB_SIZE = len(words) + 2  # 패딩(0) + OOV(1) + 실제 단어들\n",
    "SIM_EMBED_DIM  = 8\n",
    "\n",
    "word_to_idx_sim = {word: idx + 2 for idx, word in enumerate(words)}  # 0=패딩, 1=OOV\n",
    "\n",
    "# GloVe 임베딩 시뮬레이션\n",
    "# 실제에서는 파일에서 읽어오지만, 여기서는 의미 있는 값을 수동으로 정의\n",
    "# 차원 의미 (임의 설정): [동물성, 작음, 털, 반려, 기계, 무거움, 속도, 육지]\n",
    "simulated_glove = {\n",
    "    \"cat\":    np.array([0.9,  0.6,  0.9,  0.8, -0.5, -0.3,  0.2,  0.4]),\n",
    "    \"dog\":    np.array([0.9,  0.5,  0.8,  0.9, -0.4, -0.2,  0.4,  0.5]),\n",
    "    \"kitten\": np.array([0.8,  0.9,  0.9,  0.7, -0.5, -0.4,  0.1,  0.3]),\n",
    "    \"puppy\":  np.array([0.8,  0.8,  0.7,  0.9, -0.4, -0.3,  0.2,  0.4]),\n",
    "    \"car\":    np.array([-0.2, 0.3, -0.3, -0.1,  0.9,  0.5,  0.8,  0.6]),\n",
    "    \"truck\":  np.array([-0.1, 0.1, -0.2, -0.2,  0.9,  0.9,  0.6,  0.8]),\n",
    "}\n",
    "\n",
    "# 임베딩 행렬 초기화 (패딩과 OOV 토큰은 0 벡터)\n",
    "embedding_matrix = np.zeros((SIM_VOCAB_SIZE, SIM_EMBED_DIM))\n",
    "for word, idx in word_to_idx_sim.items():\n",
    "    if word in simulated_glove:\n",
    "        embedding_matrix[idx] = simulated_glove[word]\n",
    "\n",
    "print(\"임베딩 행렬 크기:\", embedding_matrix.shape)\n",
    "print()\n",
    "\n",
    "# Embedding 레이어에 사전 학습 가중치 적용\n",
    "pretrained_embedding = tf.keras.layers.Embedding(\n",
    "    input_dim=SIM_VOCAB_SIZE,\n",
    "    output_dim=SIM_EMBED_DIM,\n",
    "    # 사전 학습 가중치를 초기값으로 사용\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # 사전 학습 가중치 고정 (미세 조정 시 True로 변경)\n",
    "    name='pretrained_glove'\n",
    ")\n",
    "\n",
    "# 단어 인덱스로 임베딩 벡터 조회\n",
    "cat_idx = word_to_idx_sim[\"cat\"]\n",
    "cat_embed = pretrained_embedding(tf.constant([[cat_idx]]))\n",
    "print(f\"'cat' (인덱스 {cat_idx}) 임베딩 벡터:\")\n",
    "print(f\"  {cat_embed.numpy()[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0010-0001-000000000010",
   "metadata": {},
   "source": [
    "## 6. 코사인 유사도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-0011-0001-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 벡터 코사인 유사도 계산 예시\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"두 벡터의 코사인 유사도 계산\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "\n",
    "# 각 단어의 임베딩 벡터 추출\n",
    "word_vectors = {word: simulated_glove[word] for word in words}\n",
    "\n",
    "# 유사도 행렬 계산\n",
    "n = len(words)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words):\n",
    "        sim_matrix[i, j] = cosine_similarity(word_vectors[w1], word_vectors[w2])\n",
    "\n",
    "# 유사도 행렬 시각화\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(n))\n",
    "ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "ax.set_yticks(range(n))\n",
    "ax.set_yticklabels(words)\n",
    "ax.set_title(\"단어 임베딩 코사인 유사도 행렬\\n(초록=높은 유사도, 빨강=낮은 유사도)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 유사도 값 표시\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax.text(j, i, f\"{sim_matrix[i,j]:.2f}\",\n",
    "                ha='center', va='center', fontsize=9,\n",
    "                color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n주목할 유사도:\")\n",
    "pairs = [\n",
    "    (\"cat\",    \"kitten\", \"비슷한 동물\"),\n",
    "    (\"dog\",    \"puppy\",  \"비슷한 동물\"),\n",
    "    (\"car\",    \"truck\",  \"비슷한 탈것\"),\n",
    "    (\"cat\",    \"car\",    \"완전히 다른 범주\"),\n",
    "]\n",
    "for w1, w2, desc in pairs:\n",
    "    sim = cosine_similarity(word_vectors[w1], word_vectors[w2])\n",
    "    print(f\"  {w1:8s} vs {w2:8s} ({desc:15s}): {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0002-0012-0001-000000000012",
   "metadata": {},
   "source": [
    "## 7. 정리\n",
    "\n",
    "### 핵심 개념 요약\n",
    "\n",
    "| 방법 | 차원 | 의미 표현 | 특징 |\n",
    "|------|------|-----------|------|\n",
    "| **One-Hot** | 어휘 크기 | 불가 | 희소, 고차원, 계산 비효율 |\n",
    "| **Word Embedding** | 수십~수백 | 가능 | 밀집, 저차원, 의미적 유사도 포착 |\n",
    "| **사전 학습 임베딩** | 수백 | 매우 우수 | 대규모 학습, 전이 학습 |\n",
    "\n",
    "### 임베딩 공간의 특성\n",
    "잘 학습된 임베딩은 다음과 같은 벡터 연산이 가능하다:\n",
    "\n",
    "$$\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} \\approx \\vec{\\text{queen}}$$\n",
    "\n",
    "### 다음 챕터 예고\n",
    "- **Chapter 06-03**: RNN, LSTM, GRU  \n",
    "  시퀀스 데이터를 처리하는 순환 신경망 구조를 학습한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
