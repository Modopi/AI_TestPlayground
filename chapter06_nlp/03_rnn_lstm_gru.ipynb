{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 06-03: RNN, LSTM, GRU\n",
    "\n",
    "## 학습 목표\n",
    "- 순환 신경망(RNN)의 작동 원리와 기울기 소실 문제를 이해한다.\n",
    "- LSTM의 게이트 메커니즘을 수식으로 파악한다.\n",
    "- GRU의 간소화된 구조를 이해한다.\n",
    "- `return_sequences`, Bidirectional, Stacked 구조를 실습한다.\n",
    "- RNN/LSTM/GRU의 파라미터 수와 성능을 비교한다.\n",
    "\n",
    "## 목차\n",
    "1. [기본 임포트](#1.-기본-임포트)\n",
    "2. [수식 정리](#2.-수식-정리)\n",
    "3. [기울기 소실 문제](#3.-기울기-소실-문제)\n",
    "4. [SimpleRNN vs LSTM vs GRU 비교](#4.-비교)\n",
    "5. [return_sequences](#5.-return_sequences)\n",
    "6. [Bidirectional LSTM](#6.-Bidirectional-LSTM)\n",
    "7. [Stacked LSTM](#7.-Stacked-LSTM)\n",
    "8. [정리](#8.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0003-0002-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0003-0001-000000000003",
   "metadata": {},
   "source": [
    "## 2. 수식 정리\n",
    "\n",
    "### 2.1 SimpleRNN\n",
    "\n",
    "시간 스텝 $t$에서의 은닉 상태 $h_t$:\n",
    "\n",
    "$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n",
    "\n",
    "- $x_t$: 현재 시간 스텝의 입력\n",
    "- $h_{t-1}$: 이전 시간 스텝의 은닉 상태\n",
    "- $W_h$, $W_x$: 학습 가중치 행렬\n",
    "- $b$: 편향(bias)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM은 **망각 게이트($f_t$)**, **입력 게이트($i_t$)**, **출력 게이트($o_t$)**와  \n",
    "**셀 상태($C_t$)**를 추가하여 장기 의존성을 학습한다.\n",
    "\n",
    "**망각 게이트** - 이전 셀 상태에서 무엇을 잊을지 결정:\n",
    "$$f_t = \\sigma(W_f[h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "**입력 게이트** - 새 정보에서 무엇을 저장할지 결정:\n",
    "$$i_t = \\sigma(W_i[h_{t-1}, x_t] + b_i)$$\n",
    "\n",
    "**셀 상태 업데이트** - 이전 셀 상태를 망각/갱신:\n",
    "$$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "**출력 게이트** - 은닉 상태로 무엇을 출력할지 결정:\n",
    "$$o_t = \\sigma(W_o[h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**은닉 상태 업데이트**:\n",
    "$$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "여기서 $\\odot$는 요소별 곱(Hadamard product), $\\sigma$는 시그모이드 함수이다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 GRU (Gated Recurrent Unit)\n",
    "\n",
    "GRU는 LSTM을 간소화하여 **업데이트 게이트($z_t$)**와 **리셋 게이트($r_t$)**만 사용한다.\n",
    "\n",
    "**업데이트 게이트** - 이전 상태와 새 상태를 얼마나 섞을지:\n",
    "$$z_t = \\sigma(W_z[h_{t-1}, x_t])$$\n",
    "\n",
    "**은닉 상태 업데이트**:\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "$z_t = 0$이면 이전 상태를 그대로 유지, $z_t = 1$이면 새 상태로 완전 교체."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0004-0001-000000000004",
   "metadata": {},
   "source": [
    "## 3. 기울기 소실 문제와 장기 의존성\n",
    "\n",
    "### SimpleRNN의 한계\n",
    "\n",
    "역전파(BPTT: Backpropagation Through Time) 과정에서  \n",
    "긴 시퀀스를 다룰 때 기울기가 지수적으로 작아지는 **기울기 소실(Vanishing Gradient)** 문제가 발생한다.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial h_0} = \\prod_{t=1}^{T} \\frac{\\partial h_t}{\\partial h_{t-1}}$$\n",
    "\n",
    "각 항이 1보다 작으면 $T$가 클수록 기울기가 $\\approx 0$이 된다.\n",
    "\n",
    "### 장기 의존성(Long-Term Dependency)\n",
    "\n",
    "예시 문장:\n",
    "> \"I grew up in France... (중간에 50개 단어) ...so I speak fluent **French**.\"\n",
    "\n",
    "\"French\"를 예측하려면 멀리 앞의 \"France\"를 기억해야 하지만,  \n",
    "SimpleRNN은 이 정보를 유지하지 못한다.\n",
    "\n",
    "### LSTM과 GRU의 해결책\n",
    "- **LSTM**: 셀 상태($C_t$)라는 별도의 경로로 기울기가 흐를 수 있어 소실 완화\n",
    "- **GRU**: LSTM보다 단순하지만 유사한 효과, 파라미터 수 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0003-0005-0001-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleRNN, LSTM, GRU 레이어 비교\n",
    "\n",
    "# 공통 설정\n",
    "VOCAB_SIZE   = 10000\n",
    "EMBED_DIM    = 64\n",
    "HIDDEN_UNITS = 64\n",
    "SEQ_LEN      = 100\n",
    "\n",
    "def build_model(rnn_type, name):\n",
    "    \"\"\"RNN 유형에 따른 모델 생성\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(SEQ_LEN,), name='input')\n",
    "    x = tf.keras.layers.Embedding(VOCAB_SIZE, EMBED_DIM)(inputs)\n",
    "    \n",
    "    if rnn_type == 'SimpleRNN':\n",
    "        # SimpleRNN: 가장 단순한 순환 구조\n",
    "        x = tf.keras.layers.SimpleRNN(HIDDEN_UNITS)(x)\n",
    "    elif rnn_type == 'LSTM':\n",
    "        # LSTM: 3개 게이트, 셀 상태 추가\n",
    "        x = tf.keras.layers.LSTM(HIDDEN_UNITS)(x)\n",
    "    elif rnn_type == 'GRU':\n",
    "        # GRU: 2개 게이트, LSTM보다 경량\n",
    "        x = tf.keras.layers.GRU(HIDDEN_UNITS)(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs, outputs, name=name)\n",
    "\n",
    "# 세 가지 모델 생성\n",
    "models = {\n",
    "    'SimpleRNN': build_model('SimpleRNN', 'SimpleRNN_Model'),\n",
    "    'LSTM':      build_model('LSTM',      'LSTM_Model'),\n",
    "    'GRU':       build_model('GRU',       'GRU_Model'),\n",
    "}\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'모델':12s} {'총 파라미터':>15s} {'RNN 레이어 파라미터':>20s}\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for name, model in models.items():\n",
    "    total_params = model.count_params()\n",
    "    # RNN 레이어만의 파라미터 계산\n",
    "    rnn_layer = [l for l in model.layers if name in type(l).__name__][0]\n",
    "    rnn_params = rnn_layer.count_params()\n",
    "    print(f\"{name:12s} {total_params:>15,d} {rnn_params:>20,d}\")\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print()\n",
    "print(\"파라미터 수 비교 (은닉 유닛 h=64, 입력 x=64):\")\n",
    "h, x = HIDDEN_UNITS, EMBED_DIM\n",
    "print(f\"  SimpleRNN: (h+x+1)×h = ({h}+{x}+1)×{h} = {(h+x+1)*h:,d}\")\n",
    "print(f\"  LSTM:      4×(h+x+1)×h = 4×{(h+x+1)*h:,d} = {4*(h+x+1)*h:,d}  (게이트 4개)\")\n",
    "print(f\"  GRU:       3×(h+x+1)×h = 3×{(h+x+1)*h:,d} = {3*(h+x+1)*h:,d}  (게이트 3개)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0006-0001-000000000006",
   "metadata": {},
   "source": [
    "## 5. return_sequences=True vs False\n",
    "\n",
    "RNN 계열 레이어에서 `return_sequences` 파라미터는 출력 형태를 결정한다.\n",
    "\n",
    "| `return_sequences` | 출력 형태 | 용도 |\n",
    "|--------------------|-----------|------|\n",
    "| `False` (기본값) | `(batch, hidden_units)` | 마지막 타임스텝만 출력, 분류 등 |\n",
    "| `True` | `(batch, timesteps, hidden_units)` | 모든 타임스텝 출력, 스택 RNN, Seq2Seq 등 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0003-0007-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return_sequences 차이 실습\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "SEQ_LEN    = 5\n",
    "INPUT_DIM  = 8   # 각 타임스텝의 입력 차원\n",
    "UNITS      = 4   # LSTM 은닉 유닛 수\n",
    "\n",
    "# 샘플 입력 데이터: (배치, 시퀀스 길이, 입력 차원)\n",
    "sample_input = tf.random.normal((BATCH_SIZE, SEQ_LEN, INPUT_DIM))\n",
    "print(f\"입력 형태: {sample_input.shape} → (배치, 시퀀스 길이, 입력 차원)\")\n",
    "print()\n",
    "\n",
    "# return_sequences=False (기본값): 마지막 타임스텝만 반환\n",
    "lstm_false = tf.keras.layers.LSTM(UNITS, return_sequences=False)\n",
    "output_false = lstm_false(sample_input)\n",
    "print(f\"return_sequences=False: {output_false.shape}\")\n",
    "print(f\"  → (배치={BATCH_SIZE}, 은닉={UNITS}) - 마지막 타임스텝만\")\n",
    "print()\n",
    "\n",
    "# return_sequences=True: 모든 타임스텝 반환\n",
    "lstm_true = tf.keras.layers.LSTM(UNITS, return_sequences=True)\n",
    "output_true = lstm_true(sample_input)\n",
    "print(f\"return_sequences=True: {output_true.shape}\")\n",
    "print(f\"  → (배치={BATCH_SIZE}, 시퀀스={SEQ_LEN}, 은닉={UNITS}) - 모든 타임스텝\")\n",
    "print()\n",
    "\n",
    "# 시각화: 각 타임스텝의 LSTM 출력값\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# return_sequences=False: 마지막 출력만\n",
    "axes[0].bar(range(UNITS), output_false[0].numpy())\n",
    "axes[0].set_title(f\"return_sequences=False\\n출력 형태: {output_false.shape}\")\n",
    "axes[0].set_xlabel(\"은닉 유닛 인덱스\")\n",
    "axes[0].set_ylabel(\"활성화 값\")\n",
    "\n",
    "# return_sequences=True: 모든 타임스텝 출력\n",
    "im = axes[1].imshow(output_true[0].numpy(), cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1].set_title(f\"return_sequences=True\\n출력 형태: {output_true.shape}\")\n",
    "axes[1].set_xlabel(\"은닉 유닛 인덱스\")\n",
    "axes[1].set_ylabel(\"타임스텝\")\n",
    "axes[1].set_yticks(range(SEQ_LEN))\n",
    "axes[1].set_yticklabels([f\"t={i}\" for i in range(SEQ_LEN)])\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.suptitle(\"LSTM return_sequences 비교\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0008-0001-000000000008",
   "metadata": {},
   "source": [
    "## 6. Bidirectional LSTM\n",
    "\n",
    "단방향 LSTM은 왼쪽→오른쪽(순방향) 문맥만 학습한다.  \n",
    "**양방향 LSTM(Bidirectional LSTM)**은 순방향과 역방향 두 LSTM을 병렬로 실행한 뒤  \n",
    "출력을 연결(concat)하여 **양쪽 문맥**을 모두 반영한다.\n",
    "\n",
    "$$\\vec{h_t} = \\text{LSTM}_{\\text{forward}}(x_1, \\ldots, x_t)$$\n",
    "$$\\overleftarrow{h_t} = \\text{LSTM}_{\\text{backward}}(x_T, \\ldots, x_t)$$\n",
    "$$h_t = [\\vec{h_t}; \\overleftarrow{h_t}]$$\n",
    "\n",
    "출력 차원은 단방향의 2배가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0003-0009-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM 실습\n",
    "\n",
    "# 샘플 입력\n",
    "sample_input = tf.random.normal((2, 10, 16))  # (배치, 시퀀스, 특성)\n",
    "\n",
    "# 단방향 LSTM\n",
    "unidirectional = tf.keras.layers.LSTM(32, return_sequences=True)\n",
    "uni_output = unidirectional(sample_input)\n",
    "\n",
    "# Bidirectional LSTM\n",
    "# merge_mode: 'concat'(기본), 'sum', 'mul', 'ave'\n",
    "bidirectional = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    merge_mode='concat'  # 순방향+역방향 결합 방식\n",
    ")\n",
    "bi_output = bidirectional(sample_input)\n",
    "\n",
    "print(f\"입력 형태:              {sample_input.shape}\")\n",
    "print(f\"단방향 LSTM 출력:      {uni_output.shape}  → (배치, 시퀀스, 32)\")\n",
    "print(f\"양방향 LSTM 출력:      {bi_output.shape}  → (배치, 시퀀스, 32×2=64)\")\n",
    "print()\n",
    "\n",
    "# Bidirectional LSTM을 사용한 텍스트 분류 모델\n",
    "bi_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 64, mask_zero=True),\n",
    "    # Bidirectional로 감싸면 양방향 학습\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='BiLSTM_Classifier')\n",
    "\n",
    "bi_model.build(input_shape=(None, 100))\n",
    "bi_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0010-0001-000000000010",
   "metadata": {},
   "source": [
    "## 7. Stacked LSTM (다층 LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0003-0011-0001-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM: 여러 층의 LSTM을 쌓는 방법\n",
    "# 첫 번째 LSTM은 return_sequences=True로 모든 타임스텝을 다음 레이어에 전달\n",
    "\n",
    "stacked_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 64, mask_zero=True),\n",
    "    \n",
    "    # 첫 번째 LSTM: return_sequences=True → 다음 LSTM에 전체 시퀀스 전달\n",
    "    tf.keras.layers.LSTM(128, return_sequences=True, name='lstm_1'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # 두 번째 LSTM: return_sequences=False → 마지막 타임스텝만 출력\n",
    "    tf.keras.layers.LSTM(64, return_sequences=False, name='lstm_2'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "], name='Stacked_LSTM')\n",
    "\n",
    "stacked_model.build(input_shape=(None, 100))\n",
    "stacked_model.summary()\n",
    "print()\n",
    "print(\"[핵심] 첫 번째 LSTM에서 return_sequences=True를 설정해야\")\n",
    "print(\"       두 번째 LSTM이 (배치, 시퀀스, 특성) 형태의 입력을 받을 수 있다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-0012-0001-000000000012",
   "metadata": {},
   "source": [
    "## 8. 정리 - RNN/LSTM/GRU 비교 표\n",
    "\n",
    "| 항목 | SimpleRNN | LSTM | GRU |\n",
    "|------|-----------|------|-----|\n",
    "| **게이트 수** | 없음 | 3개 (f, i, o) | 2개 (z, r) |\n",
    "| **셀 상태** | 없음 | 있음 ($C_t$) | 없음 |\n",
    "| **파라미터 수** | 적음 | 많음 (4배) | 중간 (3배) |\n",
    "| **장기 의존성** | 약함 | 강함 | 강함 |\n",
    "| **학습 속도** | 빠름 | 느림 | 중간 |\n",
    "| **권장 사용처** | 짧은 시퀀스 | 긴 시퀀스 | 긴 시퀀스 (경량) |\n",
    "\n",
    "### 선택 가이드\n",
    "- 짧은 시퀀스이고 속도가 중요: **SimpleRNN**\n",
    "- 긴 시퀀스이고 정확도가 중요: **LSTM**\n",
    "- 긴 시퀀스이고 속도도 중요: **GRU**\n",
    "- 문맥이 양방향으로 필요: **Bidirectional LSTM/GRU**\n",
    "\n",
    "### 다음 챕터 예고\n",
    "- **Chapter 06-04**: 텍스트 분류 (Text Classification)  \n",
    "  IMDB 데이터셋으로 감성 분석 모델을 구현하고 비교한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
