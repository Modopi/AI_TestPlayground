{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 06-01: 텍스트 전처리\n",
    "\n",
    "## 학습 목표\n",
    "- 텍스트 데이터를 신경망에 입력하기 위한 전처리 과정을 이해한다.\n",
    "- 토큰화(Tokenization)의 다양한 수준을 비교한다.\n",
    "- TensorFlow의 `TextVectorization` 레이어를 활용한다.\n",
    "- 패딩(Padding)과 마스킹(Masking)의 필요성을 이해한다.\n",
    "- 한국어 텍스트의 특수성을 파악한다.\n",
    "\n",
    "## 목차\n",
    "1. [기본 임포트](#1.-기본-임포트)\n",
    "2. [토큰화 수준 비교](#2.-토큰화-수준-비교)\n",
    "3. [TextVectorization 레이어](#3.-TextVectorization-레이어)\n",
    "4. [패딩과 마스킹](#4.-패딩과-마스킹)\n",
    "5. [한국어 특수성](#5.-한국어-특수성)\n",
    "6. [정리](#6.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0002-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"NumPy 버전: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0003-0001-000000000003",
   "metadata": {},
   "source": [
    "## 2. 토큰화 수준 비교\n",
    "\n",
    "텍스트를 숫자로 변환하기 위해 먼저 **토큰(Token)**으로 분할해야 한다.  \n",
    "토큰화 수준에 따라 아래와 같이 세 가지로 분류할 수 있다.\n",
    "\n",
    "| 수준 | 예시 입력 | 토큰 결과 | 특징 |\n",
    "|------|-----------|-----------|------|\n",
    "| **단어 수준** | \"I love NLP\" | [\"I\", \"love\", \"NLP\"] | 직관적, 어휘 사전이 커짐, OOV 문제 |\n",
    "| **서브워드 수준** | \"I love NLP\" | [\"I\", \"love\", \"NL\", \"##P\"] | OOV 완화, BERT 등에서 사용 (WordPiece, BPE) |\n",
    "| **문자 수준** | \"I love NLP\" | [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", ...] | 어휘 사전 최소화, 시퀀스 매우 길어짐 |\n",
    "\n",
    "### OOV(Out-Of-Vocabulary) 문제\n",
    "학습 데이터에 없는 단어가 테스트 시 등장하는 문제이다.  \n",
    "서브워드 방식은 단어를 더 작은 단위로 쪼개어 이 문제를 완화한다.  \n",
    "예: \"unhappiness\" → [\"un\", \"##happi\", \"##ness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0004-0001-000000000004",
   "metadata": {},
   "source": [
    "## 3. TextVectorization 레이어\n",
    "\n",
    "`tf.keras.layers.TextVectorization`은 원시 문자열 텍스트를  \n",
    "정수 인덱스 시퀀스로 변환하는 전처리 레이어이다.\n",
    "\n",
    "주요 단계:\n",
    "1. **adapt()**: 훈련 데이터로부터 어휘 사전(vocabulary)을 구축한다.\n",
    "2. **call()**: 텍스트를 정수 시퀀스로 변환한다.\n",
    "3. **get_vocabulary()**: 구축된 어휘 사전을 반환한다.\n",
    "\n",
    "특수 토큰:\n",
    "- 인덱스 `0`: 패딩(padding) 토큰 `''`\n",
    "- 인덱스 `1`: 미등록 단어(OOV) 토큰 `'[UNK]'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0005-0001-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization 레이어 실습\n",
    "\n",
    "# 샘플 텍스트 데이터\n",
    "sample_texts = [\n",
    "    \"I love deep learning\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"I love natural language processing\",\n",
    "    \"natural language processing is fun\",\n",
    "]\n",
    "\n",
    "# TextVectorization 레이어 생성\n",
    "# max_tokens: 어휘 사전의 최대 크기 (None이면 무제한)\n",
    "# output_sequence_length: 출력 시퀀스의 고정 길이 (None이면 가변)\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=50,\n",
    "    output_sequence_length=6,  # 모든 시퀀스를 길이 6으로 고정\n",
    "    output_mode='int',         # 정수 인덱스 출력\n",
    ")\n",
    "\n",
    "# adapt(): 훈련 데이터로 어휘 사전 구축\n",
    "vectorizer.adapt(sample_texts)\n",
    "\n",
    "# 어휘 사전 확인\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(\"어휘 사전 크기:\", len(vocab))\n",
    "print(\"어휘 사전 내용:\", vocab)\n",
    "print()\n",
    "\n",
    "# 텍스트를 정수 시퀀스로 변환\n",
    "test_input = [\"I love learning\", \"unknown word test\"]\n",
    "encoded = vectorizer(test_input)\n",
    "print(\"입력 텍스트:\", test_input)\n",
    "print(\"인코딩 결과:\\n\", encoded.numpy())\n",
    "print()\n",
    "print(\"인덱스 0: 패딩 토큰 ''\") \n",
    "print(\"인덱스 1: OOV 토큰 '[UNK]' - 사전에 없는 단어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0006-0001-000000000006",
   "metadata": {},
   "source": [
    "## 4. 패딩(Padding)과 마스킹(Masking)\n",
    "\n",
    "신경망은 고정 크기의 배치 입력을 요구한다.  \n",
    "하지만 자연어 문장의 길이는 제각각이므로, **패딩**을 통해 길이를 맞춘다.\n",
    "\n",
    "### 패딩 방식\n",
    "- **post padding**: 시퀀스 뒤에 0을 채운다. `[1, 2, 3, 0, 0]` (기본값)\n",
    "- **pre padding**: 시퀀스 앞에 0을 채운다. `[0, 0, 1, 2, 3]`\n",
    "\n",
    "### 마스킹(Masking)\n",
    "패딩된 0은 실제 의미 있는 데이터가 아니므로,  \n",
    "모델이 이를 **무시**하도록 마스크를 사용한다.  \n",
    "Embedding 레이어에서 `mask_zero=True`로 설정하면 자동으로 처리된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0007-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩과 마스킹 실습\n",
    "\n",
    "# 가변 길이 시퀀스 예시\n",
    "sequences = [\n",
    "    [1, 2, 3, 4, 5],      # 길이 5\n",
    "    [1, 2, 3],             # 길이 3\n",
    "    [1, 2, 3, 4, 5, 6, 7], # 길이 7\n",
    "    [1],                   # 길이 1\n",
    "]\n",
    "\n",
    "# post 패딩 (기본값): 뒤에 0을 채움\n",
    "padded_post = tf.keras.utils.pad_sequences(\n",
    "    sequences,\n",
    "    padding='post',   # 뒤에 0 추가\n",
    "    truncating='post' # 최대 길이 초과 시 뒤를 자름\n",
    ")\n",
    "print(\"Post 패딩 결과:\")\n",
    "print(padded_post)\n",
    "print()\n",
    "\n",
    "# pre 패딩: 앞에 0을 채움\n",
    "padded_pre = tf.keras.utils.pad_sequences(\n",
    "    sequences,\n",
    "    padding='pre',    # 앞에 0 추가\n",
    "    truncating='pre'  # 최대 길이 초과 시 앞을 자름\n",
    ")\n",
    "print(\"Pre 패딩 결과:\")\n",
    "print(padded_pre)\n",
    "print()\n",
    "\n",
    "# mask_zero=True를 사용한 Embedding 레이어\n",
    "# mask_zero=True: 패딩 토큰(0)을 무시하는 마스크를 자동 생성\n",
    "embedding_with_mask = tf.keras.layers.Embedding(\n",
    "    input_dim=10,\n",
    "    output_dim=4,\n",
    "    mask_zero=True  # 0 인덱스에 대한 마스크 생성\n",
    ")\n",
    "\n",
    "# 패딩된 시퀀스로 임베딩 수행\n",
    "sample_input = tf.constant([[1, 2, 3, 0, 0]])  # 0은 패딩\n",
    "output = embedding_with_mask(sample_input)\n",
    "\n",
    "# 마스크 확인\n",
    "mask = embedding_with_mask.compute_mask(sample_input)\n",
    "print(\"입력 시퀀스:\", sample_input.numpy())\n",
    "print(\"마스크 (True=유효, False=패딩):\", mask.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0008-0001-000000000008",
   "metadata": {},
   "source": [
    "## 5. 한국어 특수성\n",
    "\n",
    "### 교착어(Agglutinative Language)로서의 한국어\n",
    "\n",
    "한국어는 **교착어**로, 어근에 다양한 접사가 붙어 의미와 문법 기능이 결정된다.  \n",
    "따라서 단순히 공백으로 분리하면 같은 단어의 다양한 형태를 다른 단어로 인식한다.\n",
    "\n",
    "예시: \"먹다\"의 다양한 형태\n",
    "- 먹었습니다 = 먹(어간) + 었(과거 시제) + 습니다(공손 종결)\n",
    "- 먹고 = 먹(어간) + 고(연결 어미)\n",
    "- 먹어서 = 먹(어간) + 어서(이유 연결 어미)\n",
    "\n",
    "공백 분리 시 \"먹었습니다\", \"먹고\", \"먹어서\"는 서로 다른 단어로 처리된다.  \n",
    "형태소 분석을 사용하면 모두 어간 \"먹\"을 공유함을 파악할 수 있다.\n",
    "\n",
    "### KoNLPy\n",
    "한국어 형태소 분석을 위한 Python 라이브러리이다.  \n",
    "Okt, Mecab, Komoran, Hannanum, Kkma 등 다양한 분석기를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0009-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 텍스트 처리 실습\n",
    "\n",
    "# 교착어 예시: \"먹다\"의 다양한 활용형\n",
    "korean_examples = [\n",
    "    \"나는 밥을 먹었습니다\",\n",
    "    \"밥을 먹고 싶어요\",\n",
    "    \"점심을 먹어서 배불러요\",\n",
    "    \"저녁을 먹을 예정입니다\",\n",
    "]\n",
    "\n",
    "print(\"=== 공백 기준 단순 분리 ===\")\n",
    "for text in korean_examples:\n",
    "    tokens = text.split()\n",
    "    # \"먹\"이 포함된 토큰 찾기\n",
    "    eat_tokens = [t for t in tokens if '먹' in t]\n",
    "    print(f\"  입력: '{text}'\")\n",
    "    print(f"  토큰: {tokens}\")\n",
    "    print(f"  '먹' 포함 토큰: {eat_tokens}\")\n",
    "    print()\n",
    "\n",
    "print(\"→ 공백 분리 시 '먹었습니다', '먹고', '먹어서', '먹을'이 모두 다른 토큰으로 처리됨\")\n",
    "print(\"→ 형태소 분석 시 모두 어간 '먹'을 공유함을 파악 가능\")\n",
    "\n",
    "# KoNLPy 설치 여부에 따른 처리\n",
    "print(\"\\n=== KoNLPy 형태소 분석 ===\")\n",
    "try:\n",
    "    from konlpy.tag import Okt\n",
    "    okt = Okt()\n",
    "    \n",
    "    for text in korean_examples:\n",
    "        # morphs(): 형태소 단위로 분리\n",
    "        morphs = okt.morphs(text)\n",
    "        print(f\"  입력: '{text}'\")\n",
    "        print(f\"  형태소: {morphs}\")\n",
    "        print()\n",
    "        \nexcept ImportError:\n",
    "    print(\"KoNLPy가 설치되지 않았습니다.\")\n",
    "    print(\"설치 방법: pip install konlpy\")\n",
    "    print()\n",
    "    print(\"[시뮬레이션] KoNLPy Okt 형태소 분석 예상 결과:\")\n",
    "    \n",
    "    # 형태소 분석 결과 시뮬레이션\n",
    "    simulated_results = [\n",
    "        (\"나는 밥을 먹었습니다\", [\"나\", \"는\", \"밥\", \"을\", \"먹었\", \"습니다\"]),\n",
    "        (\"밥을 먹고 싶어요\",    [\"밥\", \"을\", \"먹고\", \"싶어요\"]),\n",
    "    ]\n",
    "    for text, morphs in simulated_results:\n",
    "        print(f\"  입력: '{text}'\")\n",
    "        print(f\"  형태소: {morphs}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0010-0001-000000000010",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### 핵심 개념 요약\n",
    "\n",
    "| 개념 | 설명 | TensorFlow API |\n",
    "|------|------|----------------|\n",
    "| **토큰화** | 텍스트를 의미 단위로 분할 | `TextVectorization` |\n",
    "| **어휘 사전** | 토큰과 인덱스의 매핑 | `adapt()`, `get_vocabulary()` |\n",
    "| **OOV 처리** | 미등록 단어 → `[UNK]` (인덱스 1) | 자동 처리 |\n",
    "| **패딩** | 시퀀스 길이를 맞추기 위해 0 추가 | `pad_sequences()` |\n",
    "| **마스킹** | 패딩 토큰을 모델이 무시하도록 설정 | `mask_zero=True` |\n",
    "| **형태소 분석** | 한국어 교착어 처리 | `KoNLPy` |\n",
    "\n",
    "### 다음 챕터 예고\n",
    "- **Chapter 06-02**: 단어 임베딩(Word Embeddings)  \n",
    "  정수 인덱스를 의미 있는 연속 벡터로 변환하는 방법을 학습한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
