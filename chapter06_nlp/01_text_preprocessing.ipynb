{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ“ ì™œ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•´ìš”?\n",
    "\n",
    "ì»´í“¨í„°ëŠ” ìˆ«ìë§Œ ì´í•´í•´ìš”! 'ì•ˆë…•í•˜ì„¸ìš”'ë¥¼ ê·¸ëŒ€ë¡œ ì…ë ¥í•  ìˆ˜ ì—†ì–´ìš”.\n",
    "í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê³¼ì •ì´ **ì „ì²˜ë¦¬(Preprocessing)**ì˜ˆìš”!\n",
    "\n",
    "#### ğŸ”„ í…ìŠ¤íŠ¸ â†’ ìˆ«ì ë³€í™˜ ê³¼ì •\n",
    "\n",
    "```\n",
    "ì›ë¬¸: \"ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì— ê°”ë‹¤\"\n",
    "    â†“ í† í°í™”(Tokenization)\n",
    "[\"ë‚˜ëŠ”\", \"ì˜¤ëŠ˜\", \"í•™êµì—\", \"ê°”ë‹¤\"]\n",
    "    â†“ ì–´íœ˜ì‚¬ì „ êµ¬ì¶• (vocab)\n",
    "{\"ë‚˜ëŠ”\":1, \"ì˜¤ëŠ˜\":2, \"í•™êµì—\":3, \"ê°”ë‹¤\":4}\n",
    "    â†“ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "[1, 2, 3, 4]\n",
    "    â†“ íŒ¨ë”© (ê¸¸ì´ ë§ì¶”ê¸°)\n",
    "[1, 2, 3, 4, 0, 0]  â† ê¸¸ì´ 6ìœ¼ë¡œ í†µì¼ (0=íŒ¨ë”©)\n",
    "```\n",
    "\n",
    "#### ğŸŒ í† í°í™” ë°©ì‹ ë¹„êµ\n",
    "\n",
    "| ë°©ì‹ | ì˜ˆì‹œ | íŠ¹ì§• |\n",
    "|------|------|------|\n",
    "| **ì–´ì ˆ ë‹¨ìœ„** | \"ë‚˜ëŠ”\" \"í•™êµì—\" | í•œêµ­ì–´ì— ìì—°ìŠ¤ëŸ¬ì›€ |\n",
    "| **ìì†Œ ë‹¨ìœ„** | \"ã„´\" \"ã…\" \"ã„´\" \"ã…¡\" \"ã„´\" | ë§¤ìš° ì„¸ë°€í•¨ |\n",
    "| **ì„œë¸Œì›Œë“œ** | \"ë‚˜\" + \"ëŠ”\" | ë¯¸ë“±ë¡ ë‹¨ì–´ ì²˜ë¦¬ ê°€ëŠ¥ |\n",
    "\n",
    "> ğŸ’¡ **íŒ¨ë”©(Padding)ì´ ì™œ í•„ìš”í•´ìš”?**\n",
    "> ë¬¸ì¥ë§ˆë‹¤ ê¸¸ì´ê°€ ë‹¬ë¼ìš”. ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´ ëª¨ë‘ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•´ìš”!\n",
    "> ì§§ì€ ë¬¸ì¥ ë’¤ì— 0(íŒ¨ë”© í† í°)ì„ ì±„ì›Œ ê¸¸ì´ë¥¼ ë§ì¶°ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 06-01: í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ì— ì…ë ¥í•˜ê¸° ìœ„í•œ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì´í•´í•œë‹¤.\n",
    "- í† í°í™”(Tokenization)ì˜ ë‹¤ì–‘í•œ ìˆ˜ì¤€ì„ ë¹„êµí•œë‹¤.\n",
    "- TensorFlowì˜ `TextVectorization` ë ˆì´ì–´ë¥¼ í™œìš©í•œë‹¤.\n",
    "- íŒ¨ë”©(Padding)ê³¼ ë§ˆìŠ¤í‚¹(Masking)ì˜ í•„ìš”ì„±ì„ ì´í•´í•œë‹¤.\n",
    "- í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ íŠ¹ìˆ˜ì„±ì„ íŒŒì•…í•œë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ê¸°ë³¸ ì„í¬íŠ¸](#1.-ê¸°ë³¸-ì„í¬íŠ¸)\n",
    "2. [í† í°í™” ìˆ˜ì¤€ ë¹„êµ](#2.-í† í°í™”-ìˆ˜ì¤€-ë¹„êµ)\n",
    "3. [TextVectorization ë ˆì´ì–´](#3.-TextVectorization-ë ˆì´ì–´)\n",
    "4. [íŒ¨ë”©ê³¼ ë§ˆìŠ¤í‚¹](#4.-íŒ¨ë”©ê³¼-ë§ˆìŠ¤í‚¹)\n",
    "5. [í•œêµ­ì–´ íŠ¹ìˆ˜ì„±](#5.-í•œêµ­ì–´-íŠ¹ìˆ˜ì„±)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0002-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0003-0001-000000000003",
   "metadata": {},
   "source": [
    "## 2. í† í°í™” ìˆ˜ì¤€ ë¹„êµ\n",
    "\n",
    "í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ë¨¼ì € **í† í°(Token)**ìœ¼ë¡œ ë¶„í• í•´ì•¼ í•œë‹¤.  \n",
    "í† í°í™” ìˆ˜ì¤€ì— ë”°ë¼ ì•„ë˜ì™€ ê°™ì´ ì„¸ ê°€ì§€ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "| ìˆ˜ì¤€ | ì˜ˆì‹œ ì…ë ¥ | í† í° ê²°ê³¼ | íŠ¹ì§• |\n",
    "|------|-----------|-----------|------|\n",
    "| **ë‹¨ì–´ ìˆ˜ì¤€** | \"I love NLP\" | [\"I\", \"love\", \"NLP\"] | ì§ê´€ì , ì–´íœ˜ ì‚¬ì „ì´ ì»¤ì§, OOV ë¬¸ì œ |\n",
    "| **ì„œë¸Œì›Œë“œ ìˆ˜ì¤€** | \"I love NLP\" | [\"I\", \"love\", \"NL\", \"##P\"] | OOV ì™„í™”, BERT ë“±ì—ì„œ ì‚¬ìš© (WordPiece, BPE) |\n",
    "| **ë¬¸ì ìˆ˜ì¤€** | \"I love NLP\" | [\"I\", \" \", \"l\", \"o\", \"v\", \"e\", ...] | ì–´íœ˜ ì‚¬ì „ ìµœì†Œí™”, ì‹œí€€ìŠ¤ ë§¤ìš° ê¸¸ì–´ì§ |\n",
    "\n",
    "### OOV(Out-Of-Vocabulary) ë¬¸ì œ\n",
    "í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ë‹¨ì–´ê°€ í…ŒìŠ¤íŠ¸ ì‹œ ë“±ì¥í•˜ëŠ” ë¬¸ì œì´ë‹¤.  \n",
    "ì„œë¸Œì›Œë“œ ë°©ì‹ì€ ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œì–´ ì´ ë¬¸ì œë¥¼ ì™„í™”í•œë‹¤.  \n",
    "ì˜ˆ: \"unhappiness\" â†’ [\"un\", \"##happi\", \"##ness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0004-0001-000000000004",
   "metadata": {},
   "source": [
    "## 3. TextVectorization ë ˆì´ì–´\n",
    "\n",
    "`tf.keras.layers.TextVectorization`ì€ ì›ì‹œ ë¬¸ìì—´ í…ìŠ¤íŠ¸ë¥¼  \n",
    "ì •ìˆ˜ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ ë ˆì´ì–´ì´ë‹¤.\n",
    "\n",
    "ì£¼ìš” ë‹¨ê³„:\n",
    "1. **adapt()**: í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ì–´íœ˜ ì‚¬ì „(vocabulary)ì„ êµ¬ì¶•í•œë‹¤.\n",
    "2. **call()**: í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.\n",
    "3. **get_vocabulary()**: êµ¬ì¶•ëœ ì–´íœ˜ ì‚¬ì „ì„ ë°˜í™˜í•œë‹¤.\n",
    "\n",
    "íŠ¹ìˆ˜ í† í°:\n",
    "- ì¸ë±ìŠ¤ `0`: íŒ¨ë”©(padding) í† í° `''`\n",
    "- ì¸ë±ìŠ¤ `1`: ë¯¸ë“±ë¡ ë‹¨ì–´(OOV) í† í° `'[UNK]'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0005-0001-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization ë ˆì´ì–´ ì‹¤ìŠµ\n",
    "\n",
    "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ ë°ì´í„°\n",
    "sample_texts = [\n",
    "    \"I love deep learning\",\n",
    "    \"deep learning is amazing\",\n",
    "    \"I love natural language processing\",\n",
    "    \"natural language processing is fun\",\n",
    "]\n",
    "\n",
    "# TextVectorization ë ˆì´ì–´ ìƒì„±\n",
    "# max_tokens: ì–´íœ˜ ì‚¬ì „ì˜ ìµœëŒ€ í¬ê¸° (Noneì´ë©´ ë¬´ì œí•œ)\n",
    "# output_sequence_length: ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ê³ ì • ê¸¸ì´ (Noneì´ë©´ ê°€ë³€)\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=50,\n",
    "    output_sequence_length=6,  # ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ê¸¸ì´ 6ìœ¼ë¡œ ê³ ì •\n",
    "    output_mode='int',         # ì •ìˆ˜ ì¸ë±ìŠ¤ ì¶œë ¥\n",
    ")\n",
    "\n",
    "# adapt(): í›ˆë ¨ ë°ì´í„°ë¡œ ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•\n",
    "vectorizer.adapt(sample_texts)\n",
    "\n",
    "# ì–´íœ˜ ì‚¬ì „ í™•ì¸\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "print(\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°:\", len(vocab))\n",
    "print(\"ì–´íœ˜ ì‚¬ì „ ë‚´ìš©:\", vocab)\n",
    "print()\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "test_input = [\"I love learning\", \"unknown word test\"]\n",
    "encoded = vectorizer(test_input)\n",
    "print(\"ì…ë ¥ í…ìŠ¤íŠ¸:\", test_input)\n",
    "print(\"ì¸ì½”ë”© ê²°ê³¼:\\n\", encoded.numpy())\n",
    "print()\n",
    "print(\"ì¸ë±ìŠ¤ 0: íŒ¨ë”© í† í° ''\") \n",
    "print(\"ì¸ë±ìŠ¤ 1: OOV í† í° '[UNK]' - ì‚¬ì „ì— ì—†ëŠ” ë‹¨ì–´\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0006-0001-000000000006",
   "metadata": {},
   "source": [
    "## 4. íŒ¨ë”©(Padding)ê³¼ ë§ˆìŠ¤í‚¹(Masking)\n",
    "\n",
    "ì‹ ê²½ë§ì€ ê³ ì • í¬ê¸°ì˜ ë°°ì¹˜ ì…ë ¥ì„ ìš”êµ¬í•œë‹¤.  \n",
    "í•˜ì§€ë§Œ ìì—°ì–´ ë¬¸ì¥ì˜ ê¸¸ì´ëŠ” ì œê°ê°ì´ë¯€ë¡œ, **íŒ¨ë”©**ì„ í†µí•´ ê¸¸ì´ë¥¼ ë§ì¶˜ë‹¤.\n",
    "\n",
    "### íŒ¨ë”© ë°©ì‹\n",
    "- **post padding**: ì‹œí€€ìŠ¤ ë’¤ì— 0ì„ ì±„ìš´ë‹¤. `[1, 2, 3, 0, 0]` (ê¸°ë³¸ê°’)\n",
    "- **pre padding**: ì‹œí€€ìŠ¤ ì•ì— 0ì„ ì±„ìš´ë‹¤. `[0, 0, 1, 2, 3]`\n",
    "\n",
    "### ë§ˆìŠ¤í‚¹(Masking)\n",
    "íŒ¨ë”©ëœ 0ì€ ì‹¤ì œ ì˜ë¯¸ ìˆëŠ” ë°ì´í„°ê°€ ì•„ë‹ˆë¯€ë¡œ,  \n",
    "ëª¨ë¸ì´ ì´ë¥¼ **ë¬´ì‹œ**í•˜ë„ë¡ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œë‹¤.  \n",
    "Embedding ë ˆì´ì–´ì—ì„œ `mask_zero=True`ë¡œ ì„¤ì •í•˜ë©´ ìë™ìœ¼ë¡œ ì²˜ë¦¬ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0007-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒ¨ë”©ê³¼ ë§ˆìŠ¤í‚¹ ì‹¤ìŠµ\n",
    "\n",
    "# ê°€ë³€ ê¸¸ì´ ì‹œí€€ìŠ¤ ì˜ˆì‹œ\n",
    "sequences = [\n",
    "    [1, 2, 3, 4, 5],      # ê¸¸ì´ 5\n",
    "    [1, 2, 3],             # ê¸¸ì´ 3\n",
    "    [1, 2, 3, 4, 5, 6, 7], # ê¸¸ì´ 7\n",
    "    [1],                   # ê¸¸ì´ 1\n",
    "]\n",
    "\n",
    "# post íŒ¨ë”© (ê¸°ë³¸ê°’): ë’¤ì— 0ì„ ì±„ì›€\n",
    "padded_post = tf.keras.utils.pad_sequences(\n",
    "    sequences,\n",
    "    padding='post',   # ë’¤ì— 0 ì¶”ê°€\n",
    "    truncating='post' # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ë’¤ë¥¼ ìë¦„\n",
    ")\n",
    "print(\"Post íŒ¨ë”© ê²°ê³¼:\")\n",
    "print(padded_post)\n",
    "print()\n",
    "\n",
    "# pre íŒ¨ë”©: ì•ì— 0ì„ ì±„ì›€\n",
    "padded_pre = tf.keras.utils.pad_sequences(\n",
    "    sequences,\n",
    "    padding='pre',    # ì•ì— 0 ì¶”ê°€\n",
    "    truncating='pre'  # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ì•ì„ ìë¦„\n",
    ")\n",
    "print(\"Pre íŒ¨ë”© ê²°ê³¼:\")\n",
    "print(padded_pre)\n",
    "print()\n",
    "\n",
    "# mask_zero=Trueë¥¼ ì‚¬ìš©í•œ Embedding ë ˆì´ì–´\n",
    "# mask_zero=True: íŒ¨ë”© í† í°(0)ì„ ë¬´ì‹œí•˜ëŠ” ë§ˆìŠ¤í¬ë¥¼ ìë™ ìƒì„±\n",
    "embedding_with_mask = tf.keras.layers.Embedding(\n",
    "    input_dim=10,\n",
    "    output_dim=4,\n",
    "    mask_zero=True  # 0 ì¸ë±ìŠ¤ì— ëŒ€í•œ ë§ˆìŠ¤í¬ ìƒì„±\n",
    ")\n",
    "\n",
    "# íŒ¨ë”©ëœ ì‹œí€€ìŠ¤ë¡œ ì„ë² ë”© ìˆ˜í–‰\n",
    "sample_input = tf.constant([[1, 2, 3, 0, 0]])  # 0ì€ íŒ¨ë”©\n",
    "output = embedding_with_mask(sample_input)\n",
    "\n",
    "# ë§ˆìŠ¤í¬ í™•ì¸\n",
    "mask = embedding_with_mask.compute_mask(sample_input)\n",
    "print(\"ì…ë ¥ ì‹œí€€ìŠ¤:\", sample_input.numpy())\n",
    "print(\"ë§ˆìŠ¤í¬ (True=ìœ íš¨, False=íŒ¨ë”©):\", mask.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0008-0001-000000000008",
   "metadata": {},
   "source": [
    "## 5. í•œêµ­ì–´ íŠ¹ìˆ˜ì„±\n",
    "\n",
    "### êµì°©ì–´(Agglutinative Language)ë¡œì„œì˜ í•œêµ­ì–´\n",
    "\n",
    "í•œêµ­ì–´ëŠ” **êµì°©ì–´**ë¡œ, ì–´ê·¼ì— ë‹¤ì–‘í•œ ì ‘ì‚¬ê°€ ë¶™ì–´ ì˜ë¯¸ì™€ ë¬¸ë²• ê¸°ëŠ¥ì´ ê²°ì •ëœë‹¤.  \n",
    "ë”°ë¼ì„œ ë‹¨ìˆœíˆ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ë©´ ê°™ì€ ë‹¨ì–´ì˜ ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì¸ì‹í•œë‹¤.\n",
    "\n",
    "ì˜ˆì‹œ: \"ë¨¹ë‹¤\"ì˜ ë‹¤ì–‘í•œ í˜•íƒœ\n",
    "- ë¨¹ì—ˆìŠµë‹ˆë‹¤ = ë¨¹(ì–´ê°„) + ì—ˆ(ê³¼ê±° ì‹œì œ) + ìŠµë‹ˆë‹¤(ê³µì† ì¢…ê²°)\n",
    "- ë¨¹ê³  = ë¨¹(ì–´ê°„) + ê³ (ì—°ê²° ì–´ë¯¸)\n",
    "- ë¨¹ì–´ì„œ = ë¨¹(ì–´ê°„) + ì–´ì„œ(ì´ìœ  ì—°ê²° ì–´ë¯¸)\n",
    "\n",
    "ê³µë°± ë¶„ë¦¬ ì‹œ \"ë¨¹ì—ˆìŠµë‹ˆë‹¤\", \"ë¨¹ê³ \", \"ë¨¹ì–´ì„œ\"ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‹¨ì–´ë¡œ ì²˜ë¦¬ëœë‹¤.  \n",
    "í˜•íƒœì†Œ ë¶„ì„ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë‘ ì–´ê°„ \"ë¨¹\"ì„ ê³µìœ í•¨ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "### KoNLPy\n",
    "í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.  \n",
    "Okt, Mecab, Komoran, Hannanum, Kkma ë“± ë‹¤ì–‘í•œ ë¶„ì„ê¸°ë¥¼ ì§€ì›í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0009-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤ìŠµ\n",
    "\n",
    "# êµì°©ì–´ ì˜ˆì‹œ: \"ë¨¹ë‹¤\"ì˜ ë‹¤ì–‘í•œ í™œìš©í˜•\n",
    "korean_examples = [\n",
    "    \"ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤\",\n",
    "    \"ë°¥ì„ ë¨¹ê³  ì‹¶ì–´ìš”\",\n",
    "    \"ì ì‹¬ì„ ë¨¹ì–´ì„œ ë°°ë¶ˆëŸ¬ìš”\",\n",
    "    \"ì €ë…ì„ ë¨¹ì„ ì˜ˆì •ì…ë‹ˆë‹¤\",\n",
    "]\n",
    "\n",
    "print(\"=== ê³µë°± ê¸°ì¤€ ë‹¨ìˆœ ë¶„ë¦¬ ===\")\n",
    "for text in korean_examples:\n",
    "    tokens = text.split()\n",
    "    # \"ë¨¹\"ì´ í¬í•¨ëœ í† í° ì°¾ê¸°\n",
    "    eat_tokens = [t for t in tokens if 'ë¨¹' in t]\n",
    "    print(f\"  ì…ë ¥: '{text}'\")\n",
    "    print(f\"  í† í°: {tokens}\")\n",
    "    print(f\"  'ë¨¹' í¬í•¨ í† í°: {eat_tokens}\")\n",
    "    print()\n",
    "\n",
    "print(\"â†’ ê³µë°± ë¶„ë¦¬ ì‹œ 'ë¨¹ì—ˆìŠµë‹ˆë‹¤', 'ë¨¹ê³ ', 'ë¨¹ì–´ì„œ', 'ë¨¹ì„'ì´ ëª¨ë‘ ë‹¤ë¥¸ í† í°ìœ¼ë¡œ ì²˜ë¦¬ë¨\")\n",
    "print(\"â†’ í˜•íƒœì†Œ ë¶„ì„ ì‹œ ëª¨ë‘ ì–´ê°„ 'ë¨¹'ì„ ê³µìœ í•¨ì„ íŒŒì•… ê°€ëŠ¥\")\n",
    "\n",
    "# KoNLPy ì„¤ì¹˜ ì—¬ë¶€ì— ë”°ë¥¸ ì²˜ë¦¬\n",
    "print(\"\\n=== KoNLPy í˜•íƒœì†Œ ë¶„ì„ ===\")\n",
    "try:\n",
    "    from konlpy.tag import Okt\n",
    "    okt = Okt()\n",
    "    \n",
    "    for text in korean_examples:\n",
    "        # morphs(): í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "        morphs = okt.morphs(text)\n",
    "        print(f\"  ì…ë ¥: '{text}'\")\n",
    "        print(f\"  í˜•íƒœì†Œ: {morphs}\")\n",
    "        print()\n",
    "        \nexcept ImportError:\n",
    "    print(\"KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ì„¤ì¹˜ ë°©ë²•: pip install konlpy\")\n",
    "    print()\n",
    "    print(\"[ì‹œë®¬ë ˆì´ì…˜] KoNLPy Okt í˜•íƒœì†Œ ë¶„ì„ ì˜ˆìƒ ê²°ê³¼:\")\n",
    "    \n",
    "    # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜\n",
    "    simulated_results = [\n",
    "        (\"ë‚˜ëŠ” ë°¥ì„ ë¨¹ì—ˆìŠµë‹ˆë‹¤\", [\"ë‚˜\", \"ëŠ”\", \"ë°¥\", \"ì„\", \"ë¨¹ì—ˆ\", \"ìŠµë‹ˆë‹¤\"]),\n",
    "        (\"ë°¥ì„ ë¨¹ê³  ì‹¶ì–´ìš”\",    [\"ë°¥\", \"ì„\", \"ë¨¹ê³ \", \"ì‹¶ì–´ìš”\"]),\n",
    "    ]\n",
    "    for text, morphs in simulated_results:\n",
    "        print(f\"  ì…ë ¥: '{text}'\")\n",
    "        print(f\"  í˜•íƒœì†Œ: {morphs}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0010-0001-000000000010",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | TensorFlow API |\n",
    "|------|------|----------------|\n",
    "| **í† í°í™”** | í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  | `TextVectorization` |\n",
    "| **ì–´íœ˜ ì‚¬ì „** | í† í°ê³¼ ì¸ë±ìŠ¤ì˜ ë§¤í•‘ | `adapt()`, `get_vocabulary()` |\n",
    "| **OOV ì²˜ë¦¬** | ë¯¸ë“±ë¡ ë‹¨ì–´ â†’ `[UNK]` (ì¸ë±ìŠ¤ 1) | ìë™ ì²˜ë¦¬ |\n",
    "| **íŒ¨ë”©** | ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ 0 ì¶”ê°€ | `pad_sequences()` |\n",
    "| **ë§ˆìŠ¤í‚¹** | íŒ¨ë”© í† í°ì„ ëª¨ë¸ì´ ë¬´ì‹œí•˜ë„ë¡ ì„¤ì • | `mask_zero=True` |\n",
    "| **í˜•íƒœì†Œ ë¶„ì„** | í•œêµ­ì–´ êµì°©ì–´ ì²˜ë¦¬ | `KoNLPy` |\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "- **Chapter 06-02**: ë‹¨ì–´ ì„ë² ë”©(Word Embeddings)  \n",
    "  ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì˜ë¯¸ ìˆëŠ” ì—°ì† ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}