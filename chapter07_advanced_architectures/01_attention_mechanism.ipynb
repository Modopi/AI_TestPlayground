{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-01: Attention 메카니즘\n",
    "\n",
    "## 학습 목표\n",
    "- Attention 메카니즘의 핵심 수식과 직관적 의미를 이해한다\n",
    "- NumPy로 Scaled Dot-Product Attention을 직접 구현한다\n",
    "- TensorFlow/Keras의 Attention 및 MultiHeadAttention 레이어를 사용한다\n",
    "- Self-Attention과 Cross-Attention의 차이를 구별한다\n",
    "\n",
    "## 목차\n",
    "1. Attention 수식 이해\n",
    "2. NumPy 수동 구현\n",
    "3. Attention Weight 시각화\n",
    "4. `tf.keras.layers.Attention` 사용법\n",
    "5. `tf.keras.layers.MultiHeadAttention` 사용법\n",
    "6. 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow 버전: {tf.__version__}')\n",
    "print(f'NumPy 버전: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. Attention 수식 이해\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### 각 행렬의 역할\n",
    "\n",
    "| 기호 | 이름 | 역할 |\n",
    "|------|------|------|\n",
    "| $Q$ | Query  | 현재 위치가 다른 위치들에게 \"무엇을 원하는가\" 질문 |\n",
    "| $K$ | Key    | 각 위치가 가진 \"내용 설명\" — Query와 비교되는 레이블 |\n",
    "| $V$ | Value  | 실제로 가져올 정보 — Attention Weight에 따라 가중합 |\n",
    "\n",
    "### $\\sqrt{d_k}$로 나누는 이유\n",
    "\n",
    "$d_k$ (Key 차원)가 커질수록 내적 $QK^T$의 분산이 $d_k$배 증가한다.  \n",
    "값이 극단적으로 커지면 softmax 출력이 0 또는 1에 집중되어 **기울기 소실**이 발생한다.  \n",
    "$\\sqrt{d_k}$로 나누어 분산을 1로 안정화한다.\n",
    "\n",
    "### 직관적 비유\n",
    "\n",
    "> 도서관(V)에서 원하는 책을 찾는 과정:  \n",
    "> 내 검색어(Q)와 각 책의 색인 키워드(K)를 비교해 유사도를 계산하고,  \n",
    "> 유사도(Attention Weight)에 따라 책의 내용(V)을 가중합하여 가져온다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. NumPy로 Attention 수동 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention 수동 구현\n",
    "    \n",
    "    Args:\n",
    "        Q: Query 행렬  shape = (..., seq_q, d_k)\n",
    "        K: Key 행렬    shape = (..., seq_k, d_k)\n",
    "        V: Value 행렬  shape = (..., seq_k, d_v)\n",
    "        mask: 선택적 마스크 (패딩 또는 미래 위치 차단)\n",
    "    Returns:\n",
    "        output: 가중합 결과  shape = (..., seq_q, d_v)\n",
    "        weights: Attention 가중치  shape = (..., seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]  # Key 차원\n",
    "    \n",
    "    # Step 1: Q와 K의 내적 계산 → 유사도 점수\n",
    "    scores = np.matmul(Q, K.transpose(-2, -1))  # (..., seq_q, seq_k)\n",
    "    \n",
    "    # Step 2: 스케일링 — sqrt(d_k)로 나누어 기울기 안정화\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: 마스크 적용 (선택)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)  # 마스킹 위치를 매우 작은 값으로\n",
    "    \n",
    "    # Step 4: Softmax → 확률 분포(Attention Weight)\n",
    "    # 수치 안정성을 위해 max를 빼고 softmax 계산\n",
    "    scores_max = np.max(scores, axis=-1, keepdims=True)\n",
    "    exp_scores = np.exp(scores - scores_max)\n",
    "    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 5: Value와 가중합\n",
    "    output = np.matmul(weights, V)  # (..., seq_q, d_v)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# 예시: 시퀀스 길이=5, d_k=d_v=8\n",
    "np.random.seed(42)\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_k)  # (5, 8)\n",
    "K = np.random.randn(seq_len, d_k)  # (5, 8)\n",
    "V = np.random.randn(seq_len, d_v)  # (5, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print('Q shape:', Q.shape)\n",
    "print('K shape:', K.shape)\n",
    "print('V shape:', V.shape)\n",
    "print('출력 shape:', output.shape)\n",
    "print('Attention Weight shape:', weights.shape)\n",
    "print()\n",
    "print('Attention Weights (각 행의 합 = 1):')\n",
    "print(np.round(weights, 3))\n",
    "print('각 행의 합:', np.round(weights.sum(axis=-1), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. Attention Weight 히트맵 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 예시 토큰 (시각화용 레이블)\n",
    "tokens = ['나는', '오늘', '학교에', '갔다', '.']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- 왼쪽: 현재 Attention Weight ---\n",
    "ax = axes[0]\n",
    "im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(tokens, fontsize=11)\n",
    "ax.set_yticklabels(tokens, fontsize=11)\n",
    "ax.set_xlabel('Key (어떤 위치를 참조하는가)', fontsize=10)\n",
    "ax.set_ylabel('Query (어느 위치의 Attention인가)', fontsize=10)\n",
    "ax.set_title('Scaled Dot-Product Attention Weight', fontsize=12)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 각 셀에 값 표시\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax.text(j, i, f'{weights[i, j]:.2f}',\n",
    "                ha='center', va='center', fontsize=8,\n",
    "                color='white' if weights[i, j] > 0.5 else 'black')\n",
    "\n",
    "# --- 오른쪽: 임의 마스크 적용 (미래 위치 차단 = Causal Mask) ---\n",
    "causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1)  # 상삼각 행렬\n",
    "_, masked_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(masked_weights, cmap='Oranges', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(seq_len))\n",
    "ax2.set_yticks(range(seq_len))\n",
    "ax2.set_xticklabels(tokens, fontsize=11)\n",
    "ax2.set_yticklabels(tokens, fontsize=11)\n",
    "ax2.set_xlabel('Key', fontsize=10)\n",
    "ax2.set_ylabel('Query', fontsize=10)\n",
    "ax2.set_title('Causal (마스크 적용) Attention Weight', fontsize=12)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax2.text(j, i, f'{masked_weights[i, j]:.2f}',\n",
    "                 ha='center', va='center', fontsize=8,\n",
    "                 color='white' if masked_weights[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.suptitle('Attention Weight 히트맵 비교', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Causal Mask (상삼각 = 미래 위치):')\n",
    "print(causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. `tf.keras.layers.Attention` 레이어 사용법\n",
    "\n",
    "`tf.keras.layers.Attention`은 Bahdanau-style(additive) 또는 dot-product Attention을 제공한다.  \n",
    "주로 Seq2Seq 디코더의 Cross-Attention에 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Attention 기본 사용법\n",
    "batch_size = 2\n",
    "seq_q = 4   # Query 시퀀스 길이 (디코더)\n",
    "seq_k = 6   # Key/Value 시퀀스 길이 (인코더)\n",
    "dim = 16    # 임베딩 차원\n",
    "\n",
    "# 랜덤 입력 텐서 생성\n",
    "query_input = tf.random.normal((batch_size, seq_q, dim))\n",
    "key_input   = tf.random.normal((batch_size, seq_k, dim))\n",
    "value_input = tf.random.normal((batch_size, seq_k, dim))\n",
    "\n",
    "# Attention 레이어 생성 (use_scale=True → Scaled Dot-Product)\n",
    "attention_layer = tf.keras.layers.Attention(use_scale=True)\n",
    "\n",
    "# 호출: [query, value] 또는 [query, value, key] 순서로 전달\n",
    "attention_output = attention_layer([query_input, value_input, key_input])\n",
    "\n",
    "print('입력 Query shape:', query_input.shape)   # (2, 4, 16)\n",
    "print('입력 Key shape:  ', key_input.shape)     # (2, 6, 16)\n",
    "print('입력 Value shape:', value_input.shape)   # (2, 6, 16)\n",
    "print('Attention 출력 shape:', attention_output.shape)  # (2, 4, 16)\n",
    "\n",
    "# Attention Score(Weight)도 반환받기\n",
    "attention_output2, attention_scores = attention_layer(\n",
    "    [query_input, value_input, key_input],\n",
    "    return_attention_scores=True\n",
    ")\n",
    "print('Attention Scores shape:', attention_scores.shape)  # (2, 4, 6)\n",
    "\n",
    "# 모델에서 사용하는 예시\n",
    "print('\\n--- Functional API 예시 ---')\n",
    "query_in = tf.keras.Input(shape=(seq_q, dim), name='query')\n",
    "key_in   = tf.keras.Input(shape=(seq_k, dim), name='key')\n",
    "val_in   = tf.keras.Input(shape=(seq_k, dim), name='value')\n",
    "attn_out = tf.keras.layers.Attention(use_scale=True)([query_in, val_in, key_in])\n",
    "model = tf.keras.Model(inputs=[query_in, key_in, val_in], outputs=attn_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. `tf.keras.layers.MultiHeadAttention` 레이어 사용법\n",
    "\n",
    "Multi-Head Attention은 여러 개의 Attention을 병렬로 수행하여  \n",
    "서로 다른 표현 부분 공간(representation subspace)에서 정보를 동시에 학습한다.\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q,\\; KW_i^K,\\; VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention 기본 사용법\n",
    "num_heads = 4   # Attention 헤드 수\n",
    "key_dim   = 8   # 각 헤드의 Key 차원 (d_k)\n",
    "seq_len   = 10  # 시퀀스 길이\n",
    "embed_dim = 32  # 임베딩 차원 (= num_heads × key_dim)\n",
    "\n",
    "# 레이어 생성\n",
    "mha_layer = tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads=num_heads,\n",
    "    key_dim=key_dim,\n",
    "    value_dim=key_dim,   # Value 차원 (생략하면 key_dim과 동일)\n",
    "    dropout=0.0,\n",
    "    name='multi_head_attention'\n",
    ")\n",
    "\n",
    "# 입력 텐서\n",
    "x = tf.random.normal((batch_size, seq_len, embed_dim))\n",
    "\n",
    "# Self-Attention: Query = Key = Value = x\n",
    "self_attn_output, self_attn_weights = mha_layer(\n",
    "    query=x,\n",
    "    key=x,\n",
    "    value=x,\n",
    "    return_attention_scores=True\n",
    ")\n",
    "print('Self-Attention 출력 shape:', self_attn_output.shape)  # (2, 10, 32)\n",
    "print('Self-Attention Weight shape:', self_attn_weights.shape)  # (2, 4, 10, 10)\n",
    "print('  → (batch, num_heads, seq_q, seq_k)')\n",
    "\n",
    "# Cross-Attention: Query는 디코더, Key/Value는 인코더\n",
    "encoder_output = tf.random.normal((batch_size, 6, embed_dim))  # 인코더 출력\n",
    "decoder_query  = tf.random.normal((batch_size, 4, embed_dim))  # 디코더 상태\n",
    "\n",
    "cross_attn_output = mha_layer(\n",
    "    query=decoder_query,\n",
    "    key=encoder_output,\n",
    "    value=encoder_output\n",
    ")\n",
    "print('\\nCross-Attention 출력 shape:', cross_attn_output.shape)  # (2, 4, 32)\n",
    "\n",
    "# 각 헤드의 Attention Weight 시각화 (첫 번째 샘플)\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 3))\n",
    "for i in range(num_heads):\n",
    "    ax = axes[i]\n",
    "    w = self_attn_weights[0, i].numpy()  # (seq_len, seq_len)\n",
    "    im = ax.imshow(w, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}', fontsize=10)\n",
    "    ax.set_xlabel('Key', fontsize=8)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Query', fontsize=8)\n",
    "\n",
    "fig.suptitle('Multi-Head Self-Attention Weights (헤드별 비교)', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### Self-Attention vs Cross-Attention 비교\n",
    "\n",
    "| 구분 | Self-Attention | Cross-Attention |\n",
    "|------|---------------|----------------|\n",
    "| Q, K, V 출처 | 동일한 시퀀스 | Q: 디코더, K/V: 인코더 |\n",
    "| 사용 위치 | Transformer Encoder, GPT | Transformer Decoder, Seq2Seq |\n",
    "| 목적 | 시퀀스 내 각 토큰이 서로를 참조 | 디코더가 인코더 정보를 참조 |\n",
    "| 예시 | 문장 내 단어 간 관계 파악 | 번역 시 소스-타겟 정렬 |\n",
    "\n",
    "### 핵심 요약\n",
    "\n",
    "- **Attention**은 시퀀스 내 임의 위치 간 직접 연결을 가능하게 한다 (RNN의 장거리 의존성 문제 해결)\n",
    "- **Scaling** ($\\div\\sqrt{d_k}$)은 내적 값의 분산 폭발을 방지한다\n",
    "- **Multi-Head**는 여러 관점에서 동시에 Attention을 계산하여 표현력을 높인다\n",
    "- **Causal Mask**는 Decoder에서 미래 정보 누출을 방지한다\n",
    "\n",
    "### 다음 챕터 예고\n",
    "\n",
    "**Chapter 07-02: Transformer Basics** — Positional Encoding, Encoder Block, Transformer를 직접 구현하고 텍스트 분류에 적용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
