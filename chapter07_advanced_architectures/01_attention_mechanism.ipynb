{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-01: Attention ë©”ì¹´ë‹ˆì¦˜\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Attention ë©”ì¹´ë‹ˆì¦˜ì˜ í•µì‹¬ ìˆ˜ì‹ê³¼ ì§ê´€ì  ì˜ë¯¸ë¥¼ ì´í•´í•œë‹¤\n",
    "- NumPyë¡œ Scaled Dot-Product Attentionì„ ì§ì ‘ êµ¬í˜„í•œë‹¤\n",
    "- TensorFlow/Kerasì˜ Attention ë° MultiHeadAttention ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•œë‹¤\n",
    "- Self-Attentionê³¼ Cross-Attentionì˜ ì°¨ì´ë¥¼ êµ¬ë³„í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. Attention ìˆ˜ì‹ ì´í•´\n",
    "2. NumPy ìˆ˜ë™ êµ¬í˜„\n",
    "3. Attention Weight ì‹œê°í™”\n",
    "4. `tf.keras.layers.Attention` ì‚¬ìš©ë²•\n",
    "5. `tf.keras.layers.MultiHeadAttention` ì‚¬ìš©ë²•\n",
    "6. ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow ë²„ì „: {tf.__version__}')\n",
    "print(f'NumPy ë²„ì „: {np.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. Attention ìˆ˜ì‹ ì´í•´\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "### ê° í–‰ë ¬ì˜ ì—­í• \n",
    "\n",
    "| ê¸°í˜¸ | ì´ë¦„ | ì—­í•  |\n",
    "|------|------|------|\n",
    "| $Q$ | Query  | í˜„ì¬ ìœ„ì¹˜ê°€ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì—ê²Œ \"ë¬´ì—‡ì„ ì›í•˜ëŠ”ê°€\" ì§ˆë¬¸ |\n",
    "| $K$ | Key    | ê° ìœ„ì¹˜ê°€ ê°€ì§„ \"ë‚´ìš© ì„¤ëª…\" â€” Queryì™€ ë¹„êµë˜ëŠ” ë ˆì´ë¸” |\n",
    "| $V$ | Value  | ì‹¤ì œë¡œ ê°€ì ¸ì˜¬ ì •ë³´ â€” Attention Weightì— ë”°ë¼ ê°€ì¤‘í•© |\n",
    "\n",
    "### $\\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ \n",
    "\n",
    "$d_k$ (Key ì°¨ì›)ê°€ ì»¤ì§ˆìˆ˜ë¡ ë‚´ì  $QK^T$ì˜ ë¶„ì‚°ì´ $d_k$ë°° ì¦ê°€í•œë‹¤.  \n",
    "ê°’ì´ ê·¹ë‹¨ì ìœ¼ë¡œ ì»¤ì§€ë©´ softmax ì¶œë ¥ì´ 0 ë˜ëŠ” 1ì— ì§‘ì¤‘ë˜ì–´ **ê¸°ìš¸ê¸° ì†Œì‹¤**ì´ ë°œìƒí•œë‹¤.  \n",
    "$\\sqrt{d_k}$ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì‚°ì„ 1ë¡œ ì•ˆì •í™”í•œë‹¤.\n",
    "\n",
    "### ì§ê´€ì  ë¹„ìœ \n",
    "\n",
    "> ë„ì„œê´€(V)ì—ì„œ ì›í•˜ëŠ” ì±…ì„ ì°¾ëŠ” ê³¼ì •:  \n",
    "> ë‚´ ê²€ìƒ‰ì–´(Q)ì™€ ê° ì±…ì˜ ìƒ‰ì¸ í‚¤ì›Œë“œ(K)ë¥¼ ë¹„êµí•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ ,  \n",
    "> ìœ ì‚¬ë„(Attention Weight)ì— ë”°ë¼ ì±…ì˜ ë‚´ìš©(V)ì„ ê°€ì¤‘í•©í•˜ì—¬ ê°€ì ¸ì˜¨ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Attention ë©”ì¹´ë‹ˆì¦˜ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### â“ Attentionì´ ì™œ ë“±ì¥í–ˆì–´ìš”?\n",
    "\n",
    "RNN(ìˆœí™˜ ì‹ ê²½ë§)ì€ ë¬¸ì¥ì„ **ìˆœì„œëŒ€ë¡œ í•œ ê¸€ìì”©** ì²˜ë¦¬í•´ìš”.\n",
    "í•˜ì§€ë§Œ ê¸´ ë¬¸ì¥ì—ì„œ ì•ë¶€ë¶„ì„ ê¸°ì–µí•˜ê¸°ê°€ ì–´ë ¤ì›Œìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì±… 100í˜ì´ì§€ë¥¼ ì½ê³ , 1í˜ì´ì§€ ë‚´ìš©ì„ ê¸°ì–µí•˜ë ¤ëŠ” ê²ƒì²˜ëŸ¼ ì–´ë ¤ì›Œìš” ğŸ˜µ\n",
    "\n",
    "Attentionì€ ì´ ë¬¸ì œë¥¼ í•´ê²°í•´ìš”: **ì–´ë–¤ ë¶€ë¶„ì´ ì¤‘ìš”í•œì§€ ì§ì ‘ ì„ íƒí•  ìˆ˜ ìˆì–´ìš”!**\n",
    "\n",
    "#### ğŸ¯ Q, K, Vê°€ ë­ì˜ˆìš”?\n",
    "\n",
    "**ë„ì„œê´€ ê²€ìƒ‰ ì‹œìŠ¤í…œ** ë¹„ìœ ë¡œ ì„¤ëª…í•´ë³¼ê²Œìš”!\n",
    "\n",
    "| ê¸°í˜¸ | ì´ë¦„ | ì—­í•  | ë¹„ìœ  |\n",
    "|------|------|------|------|\n",
    "| **Q** | Query (ì§ˆë¬¸) | ë‚´ê°€ ì°¾ëŠ” ê²ƒ | ê²€ìƒ‰ì°½ì— ì…ë ¥í•˜ëŠ” í‚¤ì›Œë“œ ğŸ” |\n",
    "| **K** | Key (í‚¤) | ê° ì •ë³´ì˜ íƒœê·¸ | ì±…ë§ˆë‹¤ ë¶™ì€ ìƒ‰ì¸ íƒœê·¸ ğŸ·ï¸ |\n",
    "| **V** | Value (ê°’) | ì‹¤ì œ ë‚´ìš© | ì±…ì˜ ë³¸ë¬¸ ë‚´ìš© ğŸ“– |\n",
    "\n",
    "```\n",
    "1. Qì™€ Kë¥¼ ë¹„êµ â†’ 'ì´ ì •ë³´ê°€ ë‚´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆì§€?' (Attention Score)\n",
    "2. Scoreë¥¼ Softmax â†’ ê° ì •ë³´ì— ëŒ€í•œ ê´€ì‹¬ë„ ë¹„ìœ¨ (Attention Weight)\n",
    "3. Weight Ã— V â†’ ê´€ì‹¬ë„ì— ë”°ë¼ ì •ë³´ë¥¼ ê°€ì¤‘í•© (ì¶œë ¥)\n",
    "```\n",
    "\n",
    "#### ğŸ”¢ ìˆ˜ì‹ í•´ì„¤\n",
    "\n",
    "$$\\text{Attention}(Q,K,V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "1. $QK^T$: Qì™€ Kì˜ ìœ ì‚¬ë„ ì¸¡ì • (ë‚´ì )\n",
    "2. $\\div \\sqrt{d_k}$: ê°’ì´ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒ ë°©ì§€ (ì•ˆì •í™”)\n",
    "3. $\\text{softmax}$: í•©ì´ 1ì¸ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜ â†’ Attention Weight\n",
    "4. $\\times V$: ì¤‘ìš”ë„ì— ë”°ë¼ ì‹¤ì œ ê°’ì„ ì„ì–´ì„œ ì¶œë ¥\n",
    "\n",
    "#### ğŸ—ºï¸ ë²ˆì—­ ì˜ˆì‹œë¡œ ì´í•´í•´ë³´ê¸°\n",
    "\n",
    "ì˜ì–´ 'The cat sat on the mat' â†’ í•œêµ­ì–´ë¡œ ë²ˆì—­í•  ë•Œ:\n",
    "\n",
    "```\n",
    "'ì•‰ì•˜ë‹¤' ìƒì„± ì‹œ Attention:\n",
    "the  â†’  ë‚®ì€ ê´€ì‹¬ (0.05)\n",
    "cat  â†’  ë‚®ì€ ê´€ì‹¬ (0.10)\n",
    "sat  â†’  ë†’ì€ ê´€ì‹¬ (0.70)  â† 'ì•‰ë‹¤'ì™€ ì§ì ‘ ê´€ë ¨!\n",
    "on   â†’  ë‚®ì€ ê´€ì‹¬ (0.10)\n",
    "mat  â†’  ë‚®ì€ ê´€ì‹¬ (0.05)\n",
    "```\n",
    "\n",
    "> ğŸ’¡ ë‹¨ì–´ í•˜ë‚˜ë¥¼ ì¶œë ¥í•  ë•Œë§ˆë‹¤, ì…ë ¥ ì „ì²´ë¥¼ 'í•œ ë²ˆì—' ì°¸ê³ í•  ìˆ˜ ìˆì–´ìš”!\n",
    "> RNNì²˜ëŸ¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•  í•„ìš”ê°€ ì—†ì–´ìš”.\n",
    "\n",
    "#### ğŸ­ Multi-Head Attentionì´ ë­ì˜ˆìš”?\n",
    "\n",
    "Attentionì„ ì—¬ëŸ¬ ê°œ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ê±°ì˜ˆìš”!\n",
    "ê° **'í—¤ë“œ'**ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ìœ¼ë¡œ ë¬¸ì¥ì„ ë¶„ì„í•´ìš”:\n",
    "\n",
    "```\n",
    "í—¤ë“œ 1: ë¬¸ë²• ê´€ê³„ íŒŒì•… (ì£¼ì–´-ì„œìˆ ì–´)\n",
    "í—¤ë“œ 2: ì˜ë¯¸ ê´€ê³„ íŒŒì•… (ìœ ì‚¬ ì˜ë¯¸)\n",
    "í—¤ë“œ 3: ìœ„ì¹˜ ê´€ê³„ íŒŒì•… (ì•ë’¤ ë‹¨ì–´)\n",
    "í—¤ë“œ 4: ë¬¸ë§¥ ê´€ê³„ íŒŒì•… (ëŒ€ëª…ì‚¬ ì°¸ì¡°)\n",
    "    â†“\n",
    "ëª¨ë“  í—¤ë“œ ê²°í•© â†’ ë” í’ë¶€í•œ í‘œí˜„!\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì—¬ëŸ¬ ì „ë¬¸ê°€ê°€ ê°ìì˜ ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ ,\n",
    "> ê·¸ ì˜ê²¬ì„ ëª¨ë‘ í•©ì³ì„œ ìµœì¢… íŒë‹¨ì„ ë‚´ë¦¬ëŠ” ê²ƒê³¼ ê°™ì•„ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. NumPyë¡œ Attention ìˆ˜ë™ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention ìˆ˜ë™ êµ¬í˜„\n",
    "    \n",
    "    Args:\n",
    "        Q: Query í–‰ë ¬  shape = (..., seq_q, d_k)\n",
    "        K: Key í–‰ë ¬    shape = (..., seq_k, d_k)\n",
    "        V: Value í–‰ë ¬  shape = (..., seq_k, d_v)\n",
    "        mask: ì„ íƒì  ë§ˆìŠ¤í¬ (íŒ¨ë”© ë˜ëŠ” ë¯¸ë˜ ìœ„ì¹˜ ì°¨ë‹¨)\n",
    "    Returns:\n",
    "        output: ê°€ì¤‘í•© ê²°ê³¼  shape = (..., seq_q, d_v)\n",
    "        weights: Attention ê°€ì¤‘ì¹˜  shape = (..., seq_q, seq_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]  # Key ì°¨ì›\n",
    "    \n",
    "    # Step 1: Qì™€ Kì˜ ë‚´ì  ê³„ì‚° â†’ ìœ ì‚¬ë„ ì ìˆ˜\n",
    "    scores = np.matmul(Q, K.transpose(-2, -1))  # (..., seq_q, seq_k)\n",
    "    \n",
    "    # Step 2: ìŠ¤ì¼€ì¼ë§ â€” sqrt(d_k)ë¡œ ë‚˜ëˆ„ì–´ ê¸°ìš¸ê¸° ì•ˆì •í™”\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: ë§ˆìŠ¤í¬ ì ìš© (ì„ íƒ)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)  # ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ\n",
    "    \n",
    "    # Step 4: Softmax â†’ í™•ë¥  ë¶„í¬(Attention Weight)\n",
    "    # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ maxë¥¼ ë¹¼ê³  softmax ê³„ì‚°\n",
    "    scores_max = np.max(scores, axis=-1, keepdims=True)\n",
    "    exp_scores = np.exp(scores - scores_max)\n",
    "    weights = exp_scores / np.sum(exp_scores, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Step 5: Valueì™€ ê°€ì¤‘í•©\n",
    "    output = np.matmul(weights, V)  # (..., seq_q, d_v)\n",
    "    \n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# ì˜ˆì‹œ: ì‹œí€€ìŠ¤ ê¸¸ì´=5, d_k=d_v=8\n",
    "np.random.seed(42)\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_k)  # (5, 8)\n",
    "K = np.random.randn(seq_len, d_k)  # (5, 8)\n",
    "V = np.random.randn(seq_len, d_v)  # (5, 8)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print('Q shape:', Q.shape)\n",
    "print('K shape:', K.shape)\n",
    "print('V shape:', V.shape)\n",
    "print('ì¶œë ¥ shape:', output.shape)\n",
    "print('Attention Weight shape:', weights.shape)\n",
    "print()\n",
    "print('Attention Weights (ê° í–‰ì˜ í•© = 1):')\n",
    "print(np.round(weights, 3))\n",
    "print('ê° í–‰ì˜ í•©:', np.round(weights.sum(axis=-1), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. Attention Weight íˆíŠ¸ë§µ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ ì˜ˆì‹œ í† í° (ì‹œê°í™”ìš© ë ˆì´ë¸”)\n",
    "tokens = ['ë‚˜ëŠ”', 'ì˜¤ëŠ˜', 'í•™êµì—', 'ê°”ë‹¤', '.']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- ì™¼ìª½: í˜„ì¬ Attention Weight ---\n",
    "ax = axes[0]\n",
    "im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(tokens, fontsize=11)\n",
    "ax.set_yticklabels(tokens, fontsize=11)\n",
    "ax.set_xlabel('Key (ì–´ë–¤ ìœ„ì¹˜ë¥¼ ì°¸ì¡°í•˜ëŠ”ê°€)', fontsize=10)\n",
    "ax.set_ylabel('Query (ì–´ëŠ ìœ„ì¹˜ì˜ Attentionì¸ê°€)', fontsize=10)\n",
    "ax.set_title('Scaled Dot-Product Attention Weight', fontsize=12)\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# ê° ì…€ì— ê°’ í‘œì‹œ\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax.text(j, i, f'{weights[i, j]:.2f}',\n",
    "                ha='center', va='center', fontsize=8,\n",
    "                color='white' if weights[i, j] > 0.5 else 'black')\n",
    "\n",
    "# --- ì˜¤ë¥¸ìª½: ì„ì˜ ë§ˆìŠ¤í¬ ì ìš© (ë¯¸ë˜ ìœ„ì¹˜ ì°¨ë‹¨ = Causal Mask) ---\n",
    "causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1)  # ìƒì‚¼ê° í–‰ë ¬\n",
    "_, masked_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(masked_weights, cmap='Oranges', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(seq_len))\n",
    "ax2.set_yticks(range(seq_len))\n",
    "ax2.set_xticklabels(tokens, fontsize=11)\n",
    "ax2.set_yticklabels(tokens, fontsize=11)\n",
    "ax2.set_xlabel('Key', fontsize=10)\n",
    "ax2.set_ylabel('Query', fontsize=10)\n",
    "ax2.set_title('Causal (ë§ˆìŠ¤í¬ ì ìš©) Attention Weight', fontsize=12)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        ax2.text(j, i, f'{masked_weights[i, j]:.2f}',\n",
    "                 ha='center', va='center', fontsize=8,\n",
    "                 color='white' if masked_weights[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.suptitle('Attention Weight íˆíŠ¸ë§µ ë¹„êµ', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Causal Mask (ìƒì‚¼ê° = ë¯¸ë˜ ìœ„ì¹˜):')\n",
    "print(causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. `tf.keras.layers.Attention` ë ˆì´ì–´ ì‚¬ìš©ë²•\n",
    "\n",
    "`tf.keras.layers.Attention`ì€ Bahdanau-style(additive) ë˜ëŠ” dot-product Attentionì„ ì œê³µí•œë‹¤.  \n",
    "ì£¼ë¡œ Seq2Seq ë””ì½”ë”ì˜ Cross-Attentionì— ì‚¬ìš©ëœë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Attention ê¸°ë³¸ ì‚¬ìš©ë²•\n",
    "batch_size = 2\n",
    "seq_q = 4   # Query ì‹œí€€ìŠ¤ ê¸¸ì´ (ë””ì½”ë”)\n",
    "seq_k = 6   # Key/Value ì‹œí€€ìŠ¤ ê¸¸ì´ (ì¸ì½”ë”)\n",
    "dim = 16    # ì„ë² ë”© ì°¨ì›\n",
    "\n",
    "# ëœë¤ ì…ë ¥ í…ì„œ ìƒì„±\n",
    "query_input = tf.random.normal((batch_size, seq_q, dim))\n",
    "key_input   = tf.random.normal((batch_size, seq_k, dim))\n",
    "value_input = tf.random.normal((batch_size, seq_k, dim))\n",
    "\n",
    "# Attention ë ˆì´ì–´ ìƒì„± (use_scale=True â†’ Scaled Dot-Product)\n",
    "attention_layer = tf.keras.layers.Attention(use_scale=True)\n",
    "\n",
    "# í˜¸ì¶œ: [query, value] ë˜ëŠ” [query, value, key] ìˆœì„œë¡œ ì „ë‹¬\n",
    "attention_output = attention_layer([query_input, value_input, key_input])\n",
    "\n",
    "print('ì…ë ¥ Query shape:', query_input.shape)   # (2, 4, 16)\n",
    "print('ì…ë ¥ Key shape:  ', key_input.shape)     # (2, 6, 16)\n",
    "print('ì…ë ¥ Value shape:', value_input.shape)   # (2, 6, 16)\n",
    "print('Attention ì¶œë ¥ shape:', attention_output.shape)  # (2, 4, 16)\n",
    "\n",
    "# Attention Score(Weight)ë„ ë°˜í™˜ë°›ê¸°\n",
    "attention_output2, attention_scores = attention_layer(\n",
    "    [query_input, value_input, key_input],\n",
    "    return_attention_scores=True\n",
    ")\n",
    "print('Attention Scores shape:', attention_scores.shape)  # (2, 4, 6)\n",
    "\n",
    "# ëª¨ë¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜ˆì‹œ\n",
    "print('\\n--- Functional API ì˜ˆì‹œ ---')\n",
    "query_in = tf.keras.Input(shape=(seq_q, dim), name='query')\n",
    "key_in   = tf.keras.Input(shape=(seq_k, dim), name='key')\n",
    "val_in   = tf.keras.Input(shape=(seq_k, dim), name='value')\n",
    "attn_out = tf.keras.layers.Attention(use_scale=True)([query_in, val_in, key_in])\n",
    "model = tf.keras.Model(inputs=[query_in, key_in, val_in], outputs=attn_out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. `tf.keras.layers.MultiHeadAttention` ë ˆì´ì–´ ì‚¬ìš©ë²•\n",
    "\n",
    "Multi-Head Attentionì€ ì—¬ëŸ¬ ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬  \n",
    "ì„œë¡œ ë‹¤ë¥¸ í‘œí˜„ ë¶€ë¶„ ê³µê°„(representation subspace)ì—ì„œ ì •ë³´ë¥¼ ë™ì‹œì— í•™ìŠµí•œë‹¤.\n",
    "\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q,\\; KW_i^K,\\; VW_i^V)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention ê¸°ë³¸ ì‚¬ìš©ë²•\n",
    "num_heads = 4   # Attention í—¤ë“œ ìˆ˜\n",
    "key_dim   = 8   # ê° í—¤ë“œì˜ Key ì°¨ì› (d_k)\n",
    "seq_len   = 10  # ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "embed_dim = 32  # ì„ë² ë”© ì°¨ì› (= num_heads Ã— key_dim)\n",
    "\n",
    "# ë ˆì´ì–´ ìƒì„±\n",
    "mha_layer = tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads=num_heads,\n",
    "    key_dim=key_dim,\n",
    "    value_dim=key_dim,   # Value ì°¨ì› (ìƒëµí•˜ë©´ key_dimê³¼ ë™ì¼)\n",
    "    dropout=0.0,\n",
    "    name='multi_head_attention'\n",
    ")\n",
    "\n",
    "# ì…ë ¥ í…ì„œ\n",
    "x = tf.random.normal((batch_size, seq_len, embed_dim))\n",
    "\n",
    "# Self-Attention: Query = Key = Value = x\n",
    "self_attn_output, self_attn_weights = mha_layer(\n",
    "    query=x,\n",
    "    key=x,\n",
    "    value=x,\n",
    "    return_attention_scores=True\n",
    ")\n",
    "print('Self-Attention ì¶œë ¥ shape:', self_attn_output.shape)  # (2, 10, 32)\n",
    "print('Self-Attention Weight shape:', self_attn_weights.shape)  # (2, 4, 10, 10)\n",
    "print('  â†’ (batch, num_heads, seq_q, seq_k)')\n",
    "\n",
    "# Cross-Attention: QueryëŠ” ë””ì½”ë”, Key/ValueëŠ” ì¸ì½”ë”\n",
    "encoder_output = tf.random.normal((batch_size, 6, embed_dim))  # ì¸ì½”ë” ì¶œë ¥\n",
    "decoder_query  = tf.random.normal((batch_size, 4, embed_dim))  # ë””ì½”ë” ìƒíƒœ\n",
    "\n",
    "cross_attn_output = mha_layer(\n",
    "    query=decoder_query,\n",
    "    key=encoder_output,\n",
    "    value=encoder_output\n",
    ")\n",
    "print('\\nCross-Attention ì¶œë ¥ shape:', cross_attn_output.shape)  # (2, 4, 32)\n",
    "\n",
    "# ê° í—¤ë“œì˜ Attention Weight ì‹œê°í™” (ì²« ë²ˆì§¸ ìƒ˜í”Œ)\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 3))\n",
    "for i in range(num_heads):\n",
    "    ax = axes[i]\n",
    "    w = self_attn_weights[0, i].numpy()  # (seq_len, seq_len)\n",
    "    im = ax.imshow(w, cmap='viridis', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}', fontsize=10)\n",
    "    ax.set_xlabel('Key', fontsize=8)\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Query', fontsize=8)\n",
    "\n",
    "fig.suptitle('Multi-Head Self-Attention Weights (í—¤ë“œë³„ ë¹„êµ)', fontsize=12)\n",
    "plt.colorbar(im, ax=axes[-1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬\n",
    "\n",
    "### Self-Attention vs Cross-Attention ë¹„êµ\n",
    "\n",
    "| êµ¬ë¶„ | Self-Attention | Cross-Attention |\n",
    "|------|---------------|----------------|\n",
    "| Q, K, V ì¶œì²˜ | ë™ì¼í•œ ì‹œí€€ìŠ¤ | Q: ë””ì½”ë”, K/V: ì¸ì½”ë” |\n",
    "| ì‚¬ìš© ìœ„ì¹˜ | Transformer Encoder, GPT | Transformer Decoder, Seq2Seq |\n",
    "| ëª©ì  | ì‹œí€€ìŠ¤ ë‚´ ê° í† í°ì´ ì„œë¡œë¥¼ ì°¸ì¡° | ë””ì½”ë”ê°€ ì¸ì½”ë” ì •ë³´ë¥¼ ì°¸ì¡° |\n",
    "| ì˜ˆì‹œ | ë¬¸ì¥ ë‚´ ë‹¨ì–´ ê°„ ê´€ê³„ íŒŒì•… | ë²ˆì—­ ì‹œ ì†ŒìŠ¤-íƒ€ê²Ÿ ì •ë ¬ |\n",
    "\n",
    "### í•µì‹¬ ìš”ì•½\n",
    "\n",
    "- **Attention**ì€ ì‹œí€€ìŠ¤ ë‚´ ì„ì˜ ìœ„ì¹˜ ê°„ ì§ì ‘ ì—°ê²°ì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤ (RNNì˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë¬¸ì œ í•´ê²°)\n",
    "- **Scaling** ($\\div\\sqrt{d_k}$)ì€ ë‚´ì  ê°’ì˜ ë¶„ì‚° í­ë°œì„ ë°©ì§€í•œë‹¤\n",
    "- **Multi-Head**ëŠ” ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— Attentionì„ ê³„ì‚°í•˜ì—¬ í‘œí˜„ë ¥ì„ ë†’ì¸ë‹¤\n",
    "- **Causal Mask**ëŠ” Decoderì—ì„œ ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œì„ ë°©ì§€í•œë‹¤\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "\n",
    "**Chapter 07-02: Transformer Basics** â€” Positional Encoding, Encoder Block, Transformerë¥¼ ì§ì ‘ êµ¬í˜„í•˜ê³  í…ìŠ¤íŠ¸ ë¶„ë¥˜ì— ì ìš©í•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}