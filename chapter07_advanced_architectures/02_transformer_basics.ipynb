{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-02: Transformer 기초\n",
    "\n",
    "## 학습 목표\n",
    "- Positional Encoding의 수식과 역할을 이해하고 직접 구현한다\n",
    "- Transformer Encoder Block을 구성하는 요소를 구현한다\n",
    "- 여러 Encoder Block을 쌓아 완전한 Transformer Encoder를 만든다\n",
    "- IMDB 감성 분류에 Transformer를 적용하여 성능을 측정한다\n",
    "\n",
    "## 목차\n",
    "1. 핵심 수식\n",
    "2. Positional Encoding 구현 및 시각화\n",
    "3. Encoder Block 구현\n",
    "4. Transformer Encoder 스택\n",
    "5. IMDB 텍스트 분류 실험\n",
    "6. 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow 버전: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. 핵심 수식\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Transformer는 순서 정보가 없으므로, 각 토큰의 위치(position)를 sin/cos 함수로 인코딩하여 임베딩에 더한다.\n",
    "\n",
    "$$PE_{(pos,\\,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos,\\,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- $pos$: 시퀀스 내 토큰의 위치 인덱스 (0, 1, 2, ...)\n",
    "- $i$: 차원 인덱스 (0, 1, ..., $d_{model}/2 - 1$)\n",
    "- 짝수 차원에는 $\\sin$, 홀수 차원에는 $\\cos$ 적용\n",
    "- 주기가 차원마다 다르므로 각 위치는 고유한 패턴을 가짐\n",
    "\n",
    "### Feed-Forward Network (FFN)\n",
    "\n",
    "각 Encoder Block 내부의 위치별 완전연결층:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0,\\; xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "- 첫 번째 Linear → ReLU → 두 번째 Linear 구조\n",
    "- 각 위치에 독립적으로 적용 (position-wise)\n",
    "- 내부 차원($d_{ff}$)은 보통 $d_{model}$의 4배"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding 구현 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Positional Encoding 행렬 계산\n",
    "    \n",
    "    Args:\n",
    "        max_seq_len: 최대 시퀀스 길이\n",
    "        d_model: 모델 임베딩 차원\n",
    "    Returns:\n",
    "        pe: shape = (1, max_seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # 위치 인덱스 벡터 (max_seq_len, 1)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis]  # (max_seq_len, 1)\n",
    "    \n",
    "    # 차원 인덱스 벡터 (1, d_model/2)\n",
    "    dims = np.arange(0, d_model, 2)[np.newaxis, :]     # (1, d_model/2)\n",
    "    \n",
    "    # 분모 계산: 10000^(2i/d_model)\n",
    "    div_term = np.power(10000.0, dims / d_model)       # (1, d_model/2)\n",
    "    \n",
    "    # Positional Encoding 행렬 초기화\n",
    "    pe = np.zeros((max_seq_len, d_model))\n",
    "    \n",
    "    # 짝수 차원: sin\n",
    "    pe[:, 0::2] = np.sin(positions / div_term)\n",
    "    # 홀수 차원: cos\n",
    "    pe[:, 1::2] = np.cos(positions / div_term)\n",
    "    \n",
    "    # 배치 차원 추가 → (1, max_seq_len, d_model)\n",
    "    return pe[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "# 시각화\n",
    "max_seq_len = 50\n",
    "d_model = 64\n",
    "pe = get_positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# 전체 PE 행렬 히트맵\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(pe[0], aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('차원 인덱스 (d_model)', fontsize=11)\n",
    "ax1.set_ylabel('위치 (position)', fontsize=11)\n",
    "ax1.set_title(f'Positional Encoding 행렬\\n(max_seq_len={max_seq_len}, d_model={d_model})', fontsize=11)\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# 특정 차원에서의 sin/cos 파형\n",
    "ax2 = axes[1]\n",
    "positions_range = np.arange(max_seq_len)\n",
    "for dim_idx in [0, 2, 4, 8, 16]:\n",
    "    ax2.plot(positions_range, pe[0, :, dim_idx],\n",
    "             label=f'dim={dim_idx} ({\"sin\" if dim_idx % 2 == 0 else \"cos\"})')\n",
    "ax2.set_xlabel('위치 (position)', fontsize=11)\n",
    "ax2.set_ylabel('PE 값', fontsize=11)\n",
    "ax2.set_title('차원별 Positional Encoding 파형', fontsize=11)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Positional Encoding 시각화', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'PE 행렬 shape: {pe.shape}')  # (1, 50, 64)\n",
    "print(f'위치 0의 첫 8차원: {pe[0, 0, :8].round(4)}')\n",
    "print(f'위치 1의 첫 8차원: {pe[0, 1, :8].round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. Encoder Block 구현\n",
    "\n",
    "Encoder Block 구조:\n",
    "```\n",
    "입력\n",
    " ├─ MultiHeadAttention(Q=K=V=입력)\n",
    " ├─ Add & Norm (잔차 연결 + Layer Normalization)\n",
    " ├─ Feed-Forward Network\n",
    " └─ Add & Norm\n",
    "출력\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    Multi-Head Self-Attention + Add&Norm + FFN + Add&Norm 구조\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Multi-Head Self-Attention\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,  # 헤드당 차원\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Feed-Forward Network: d_model → d_ff → d_model\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='relu'),  # W1, b1 (ReLU)\n",
    "            tf.keras.layers.Dense(d_model)                   # W2, b2\n",
    "        ])\n",
    "        \n",
    "        # Layer Normalization (두 개)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        # --- Sub-Layer 1: Multi-Head Self-Attention ---\n",
    "        # Q = K = V = x (Self-Attention)\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # 잔차 연결 + Layer Norm\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # --- Sub-Layer 2: Feed-Forward Network ---\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # 잔차 연결 + Layer Norm\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2  # (batch, seq_len, d_model)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_ff': self.d_ff\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# 동작 확인\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 128  # d_model의 4배\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "dummy_input = tf.random.normal((batch_size, seq_len, d_model))\n",
    "block_output = encoder_block(dummy_input, training=False)\n",
    "\n",
    "print('Encoder Block 입력 shape:', dummy_input.shape)   # (2, 10, 32)\n",
    "print('Encoder Block 출력 shape:', block_output.shape)  # (2, 10, 32)\n",
    "print('입력과 출력 차원 동일:', dummy_input.shape == block_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder 스택 (여러 Encoder Block 쌓기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    토큰 임베딩 + Positional Encoding을 합산하는 레이어\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # 토큰 임베딩\n",
    "        self.token_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=d_model\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding (trainable=False 고정값)\n",
    "        pe_matrix = get_positional_encoding(max_seq_len, d_model)\n",
    "        self.pos_encoding = tf.cast(pe_matrix, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # 토큰 임베딩\n",
    "        embed = self.token_embedding(x)  # (batch, seq_len, d_model)\n",
    "        # sqrt(d_model)로 스케일링 (임베딩 값을 PE와 비교 가능한 크기로)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional Encoding 더하기\n",
    "        embed += self.pos_encoding[:, :seq_len, :]\n",
    "        return embed\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    N개의 Encoder Block을 쌓은 Transformer Encoder\n",
    "    분류 작업용 헤드 포함\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff,\n",
    "                 num_layers, num_classes, max_seq_len=512,\n",
    "                 dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # 임베딩 + Positional Encoding\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, max_seq_len)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # N개의 Encoder Block 스택\n",
    "        self.encoder_blocks = [\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout_rate,\n",
    "                                    name=f'encoder_block_{i}')\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # 분류 헤드\n",
    "        self.global_avg_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.fc_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes,\n",
    "                                                 activation='sigmoid' if num_classes == 1 else 'softmax')\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # 임베딩 + Positional Encoding\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # N개 Encoder Block 통과\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, training=training)\n",
    "        \n",
    "        # 시퀀스 차원 평균 풀링\n",
    "        x = self.global_avg_pool(x)       # (batch, d_model)\n",
    "        x = self.fc1(x)                   # (batch, 64)\n",
    "        x = self.fc_dropout(x, training=training)\n",
    "        return self.classifier(x)          # (batch, num_classes)\n",
    "\n",
    "\n",
    "# 모델 구조 확인\n",
    "vocab_size = 10000\n",
    "d_model_demo = 64\n",
    "num_heads_demo = 4\n",
    "num_layers_demo = 2\n",
    "\n",
    "demo_model = TransformerEncoder(\n",
    "    vocab_size=vocab_size, d_model=d_model_demo, num_heads=num_heads_demo,\n",
    "    d_ff=d_model_demo*4, num_layers=num_layers_demo, num_classes=1\n",
    ")\n",
    "\n",
    "# Build 하여 파라미터 수 확인\n",
    "dummy_tokens = tf.ones((1, 20), dtype=tf.int32)\n",
    "_ = demo_model(dummy_tokens)\n",
    "demo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. IMDB 텍스트 분류 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "D_FF = 256          # D_MODEL의 4배\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# IMDB 데이터 로드\n",
    "print('IMDB 데이터 로딩...')\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "# 패딩 (최대 길이로 통일)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "print(f'훈련 데이터: {x_train.shape}, 레이블: {y_train.shape}')\n",
    "print(f'테스트 데이터: {x_test.shape}, 레이블: {y_test.shape}')\n",
    "print(f'클래스 분포 (훈련): 긍정={y_train.sum()}, 부정={len(y_train)-y_train.sum()}')\n",
    "\n",
    "# Transformer 모델 생성\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=VOCAB_SIZE + 1,  # +1: 패딩 토큰\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=1,\n",
    "    max_seq_len=MAX_LEN,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Build\n",
    "_ = model(x_train[:1])\n",
    "model.summary()\n",
    "\n",
    "# 학습\n",
    "print('\\n모델 학습 시작...')\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 평가\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\n테스트 정확도: {test_acc:.4f}')\n",
    "print(f'테스트 손실:   {test_loss:.4f}')\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(history.history['accuracy'], label='훈련 정확도', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='검증 정확도', marker='s')\n",
    "axes[0].set_title('정확도', fontsize=12)\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='훈련 손실', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='검증 손실', marker='s')\n",
    "axes[1].set_title('손실', fontsize=12)\n",
    "axes[1].set_xlabel('에포크')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('IMDB Transformer 학습 곡선', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### Transformer Encoder 구성 요소 요약\n",
    "\n",
    "| 구성 요소 | 역할 | 수식/설명 |\n",
    "|-----------|------|-----------|\n",
    "| Token Embedding | 정수 인덱스 → 벡터 | 학습 가능한 임베딩 행렬 |\n",
    "| Positional Encoding | 위치 정보 부여 | sin/cos 고정값 |\n",
    "| Multi-Head Attention | 시퀀스 내 관계 모델링 | $h$개 헤드 병렬 Attention |\n",
    "| Add & Norm | 안정적 학습 | 잔차 연결 + Layer Normalization |\n",
    "| Feed-Forward Network | 비선형 변환 | ReLU 활성화 |\n",
    "| Global Avg Pooling | 시퀀스 → 벡터 | 분류 헤드 입력용 |\n",
    "\n",
    "### 주요 하이퍼파라미터\n",
    "\n",
    "- **d_model**: 임베딩 차원 (보통 64~512)\n",
    "- **num_heads**: 헤드 수 (d_model의 약수)\n",
    "- **d_ff**: FFN 내부 차원 (보통 d_model × 4)\n",
    "- **num_layers**: Encoder Block 스택 수 (BERT: 12, GPT-2 small: 12)\n",
    "\n",
    "### 다음 단계\n",
    "\n",
    "**Chapter 07-03: BERT Fine-Tuning** — 사전학습된 대규모 Transformer(BERT)를 다운로드하여 특정 태스크에 파인튜닝한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
