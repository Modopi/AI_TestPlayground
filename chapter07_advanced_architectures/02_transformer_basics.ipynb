{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-02: Transformer ê¸°ì´ˆ\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Positional Encodingì˜ ìˆ˜ì‹ê³¼ ì—­í• ì„ ì´í•´í•˜ê³  ì§ì ‘ êµ¬í˜„í•œë‹¤\n",
    "- Transformer Encoder Blockì„ êµ¬ì„±í•˜ëŠ” ìš”ì†Œë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- ì—¬ëŸ¬ Encoder Blockì„ ìŒ“ì•„ ì™„ì „í•œ Transformer Encoderë¥¼ ë§Œë“ ë‹¤\n",
    "- IMDB ê°ì„± ë¶„ë¥˜ì— Transformerë¥¼ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. í•µì‹¬ ìˆ˜ì‹\n",
    "2. Positional Encoding êµ¬í˜„ ë° ì‹œê°í™”\n",
    "3. Encoder Block êµ¬í˜„\n",
    "4. Transformer Encoder ìŠ¤íƒ\n",
    "5. IMDB í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹¤í—˜\n",
    "6. ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Transformer ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ¤” RNN ëŒ€ì‹  ì™œ Transformer?\n",
    "\n",
    "| ë¬¸ì œì  | RNN/LSTM | Transformer |\n",
    "|--------|----------|-------------|\n",
    "| ê¸´ ì˜ì¡´ì„± | ì•ë¶€ë¶„ ê¸°ì–µ ëª»í•¨ | Attentionìœ¼ë¡œ ì§ì ‘ ì—°ê²°! |\n",
    "| ë³‘ë ¬ ì²˜ë¦¬ | ìˆœì„œëŒ€ë¡œë§Œ ì²˜ë¦¬ | ëª¨ë“  ìœ„ì¹˜ ë™ì‹œ ì²˜ë¦¬! |\n",
    "| í•™ìŠµ ì†ë„ | ëŠë¦¼ | í›¨ì”¬ ë¹ ë¦„ |\n",
    "\n",
    "#### ğŸ—ï¸ Transformer í•µì‹¬ êµ¬ì¡°\n",
    "\n",
    "```\n",
    "ì…ë ¥ ë¬¸ì¥\n",
    "    â†“ Embedding (ë‹¨ì–´ â†’ ë²¡í„°)\n",
    "    â†“ Positional Encoding (ìœ„ì¹˜ ì •ë³´ ì¶”ê°€)\n",
    "    â†“ [Encoder ë¸”ë¡ Ã— N]\n",
    "       - Multi-Head Self-Attention\n",
    "       - Add & Norm (ì”ì°¨ ì—°ê²°)\n",
    "       - Feed Forward\n",
    "       - Add & Norm\n",
    "    â†“ ìµœì¢… í‘œí˜„ (ê° ë‹¨ì–´ì˜ ë¬¸ë§¥ ì •ë³´ í¬í•¨!)\n",
    "```\n",
    "\n",
    "#### ğŸ“ Positional Encodingì´ ì™œ í•„ìš”í•´ìš”?\n",
    "\n",
    "Attentionì€ ìœ„ì¹˜ ì •ë³´ê°€ ì—†ì–´ìš”! 'ë‚˜ëŠ” í•™êµì— ê°”ë‹¤'ì™€\n",
    "'ê°”ë‹¤ í•™êµì— ë‚˜ëŠ”'ì´ Attentionì—ê²ŒëŠ” **ê°™ì•„ ë³´ì—¬ìš”**!\n",
    "\n",
    "> ğŸ’¡ Positional Encoding = ê° ìœ„ì¹˜ì— **ê³ ìœ í•œ ì£¼íŒŒìˆ˜ ì‹ í˜¸**ë¥¼ ë”í•´ì¤˜ìš”.\n",
    "> ë§ˆì¹˜ ìš°í¸ë²ˆí˜¸ì²˜ëŸ¼ ê° ë‹¨ì–´ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶”ê°€!\n",
    "\n",
    "#### ğŸ­ Add & Norm (ì”ì°¨ ì—°ê²° + ì •ê·œí™”)\n",
    "\n",
    "```\n",
    "ì¶œë ¥ = LayerNorm(x + Sublayer(x))\n",
    "```\n",
    "- **Residual(ì”ì°¨)**: ì…ë ¥ì„ ì§ì ‘ ë”í•´ ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€\n",
    "- **LayerNorm**: ê°’ ë¶„í¬ë¥¼ ì•ˆì •í™”í•´ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow ë²„ì „: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "TransformerëŠ” ìˆœì„œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ, ê° í† í°ì˜ ìœ„ì¹˜(position)ë¥¼ sin/cos í•¨ìˆ˜ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì„ë² ë”©ì— ë”í•œë‹¤.\n",
    "\n",
    "$$PE_{(pos,\\,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos,\\,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "- $pos$: ì‹œí€€ìŠ¤ ë‚´ í† í°ì˜ ìœ„ì¹˜ ì¸ë±ìŠ¤ (0, 1, 2, ...)\n",
    "- $i$: ì°¨ì› ì¸ë±ìŠ¤ (0, 1, ..., $d_{model}/2 - 1$)\n",
    "- ì§ìˆ˜ ì°¨ì›ì—ëŠ” $\\sin$, í™€ìˆ˜ ì°¨ì›ì—ëŠ” $\\cos$ ì ìš©\n",
    "- ì£¼ê¸°ê°€ ì°¨ì›ë§ˆë‹¤ ë‹¤ë¥´ë¯€ë¡œ ê° ìœ„ì¹˜ëŠ” ê³ ìœ í•œ íŒ¨í„´ì„ ê°€ì§\n",
    "\n",
    "### Feed-Forward Network (FFN)\n",
    "\n",
    "ê° Encoder Block ë‚´ë¶€ì˜ ìœ„ì¹˜ë³„ ì™„ì „ì—°ê²°ì¸µ:\n",
    "\n",
    "$$\\text{FFN}(x) = \\max(0,\\; xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "- ì²« ë²ˆì§¸ Linear â†’ ReLU â†’ ë‘ ë²ˆì§¸ Linear êµ¬ì¡°\n",
    "- ê° ìœ„ì¹˜ì— ë…ë¦½ì ìœ¼ë¡œ ì ìš© (position-wise)\n",
    "- ë‚´ë¶€ ì°¨ì›($d_{ff}$)ì€ ë³´í†µ $d_{model}$ì˜ 4ë°°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding êµ¬í˜„ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Positional Encoding í–‰ë ¬ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "        d_model: ëª¨ë¸ ì„ë² ë”© ì°¨ì›\n",
    "    Returns:\n",
    "        pe: shape = (1, max_seq_len, d_model)\n",
    "    \"\"\"\n",
    "    # ìœ„ì¹˜ ì¸ë±ìŠ¤ ë²¡í„° (max_seq_len, 1)\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis]  # (max_seq_len, 1)\n",
    "    \n",
    "    # ì°¨ì› ì¸ë±ìŠ¤ ë²¡í„° (1, d_model/2)\n",
    "    dims = np.arange(0, d_model, 2)[np.newaxis, :]     # (1, d_model/2)\n",
    "    \n",
    "    # ë¶„ëª¨ ê³„ì‚°: 10000^(2i/d_model)\n",
    "    div_term = np.power(10000.0, dims / d_model)       # (1, d_model/2)\n",
    "    \n",
    "    # Positional Encoding í–‰ë ¬ ì´ˆê¸°í™”\n",
    "    pe = np.zeros((max_seq_len, d_model))\n",
    "    \n",
    "    # ì§ìˆ˜ ì°¨ì›: sin\n",
    "    pe[:, 0::2] = np.sin(positions / div_term)\n",
    "    # í™€ìˆ˜ ì°¨ì›: cos\n",
    "    pe[:, 1::2] = np.cos(positions / div_term)\n",
    "    \n",
    "    # ë°°ì¹˜ ì°¨ì› ì¶”ê°€ â†’ (1, max_seq_len, d_model)\n",
    "    return pe[np.newaxis, :, :]\n",
    "\n",
    "\n",
    "# ì‹œê°í™”\n",
    "max_seq_len = 50\n",
    "d_model = 64\n",
    "pe = get_positional_encoding(max_seq_len, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ì „ì²´ PE í–‰ë ¬ íˆíŠ¸ë§µ\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(pe[0], aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "ax1.set_xlabel('ì°¨ì› ì¸ë±ìŠ¤ (d_model)', fontsize=11)\n",
    "ax1.set_ylabel('ìœ„ì¹˜ (position)', fontsize=11)\n",
    "ax1.set_title(f'Positional Encoding í–‰ë ¬\\n(max_seq_len={max_seq_len}, d_model={d_model})', fontsize=11)\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# íŠ¹ì • ì°¨ì›ì—ì„œì˜ sin/cos íŒŒí˜•\n",
    "ax2 = axes[1]\n",
    "positions_range = np.arange(max_seq_len)\n",
    "for dim_idx in [0, 2, 4, 8, 16]:\n",
    "    ax2.plot(positions_range, pe[0, :, dim_idx],\n",
    "             label=f'dim={dim_idx} ({\"sin\" if dim_idx % 2 == 0 else \"cos\"})')\n",
    "ax2.set_xlabel('ìœ„ì¹˜ (position)', fontsize=11)\n",
    "ax2.set_ylabel('PE ê°’', fontsize=11)\n",
    "ax2.set_title('ì°¨ì›ë³„ Positional Encoding íŒŒí˜•', fontsize=11)\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Positional Encoding ì‹œê°í™”', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'PE í–‰ë ¬ shape: {pe.shape}')  # (1, 50, 64)\n",
    "print(f'ìœ„ì¹˜ 0ì˜ ì²« 8ì°¨ì›: {pe[0, 0, :8].round(4)}')\n",
    "print(f'ìœ„ì¹˜ 1ì˜ ì²« 8ì°¨ì›: {pe[0, 1, :8].round(4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. Encoder Block êµ¬í˜„\n",
    "\n",
    "Encoder Block êµ¬ì¡°:\n",
    "```\n",
    "ì…ë ¥\n",
    " â”œâ”€ MultiHeadAttention(Q=K=V=ì…ë ¥)\n",
    " â”œâ”€ Add & Norm (ì”ì°¨ ì—°ê²° + Layer Normalization)\n",
    " â”œâ”€ Feed-Forward Network\n",
    " â””â”€ Add & Norm\n",
    "ì¶œë ¥\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    Multi-Head Self-Attention + Add&Norm + FFN + Add&Norm êµ¬ì¡°\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Multi-Head Self-Attention\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,  # í—¤ë“œë‹¹ ì°¨ì›\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Feed-Forward Network: d_model â†’ d_ff â†’ d_model\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_ff, activation='relu'),  # W1, b1 (ReLU)\n",
    "            tf.keras.layers.Dense(d_model)                   # W2, b2\n",
    "        ])\n",
    "        \n",
    "        # Layer Normalization (ë‘ ê°œ)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training=False, mask=None):\n",
    "        # --- Sub-Layer 1: Multi-Head Self-Attention ---\n",
    "        # Q = K = V = x (Self-Attention)\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # ì”ì°¨ ì—°ê²° + Layer Norm\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # --- Sub-Layer 2: Feed-Forward Network ---\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # ì”ì°¨ ì—°ê²° + Layer Norm\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2  # (batch, seq_len, d_model)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'd_model': self.d_model,\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_ff': self.d_ff\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# ë™ì‘ í™•ì¸\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 128  # d_modelì˜ 4ë°°\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "encoder_block = TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "dummy_input = tf.random.normal((batch_size, seq_len, d_model))\n",
    "block_output = encoder_block(dummy_input, training=False)\n",
    "\n",
    "print('Encoder Block ì…ë ¥ shape:', dummy_input.shape)   # (2, 10, 32)\n",
    "print('Encoder Block ì¶œë ¥ shape:', block_output.shape)  # (2, 10, 32)\n",
    "print('ì…ë ¥ê³¼ ì¶œë ¥ ì°¨ì› ë™ì¼:', dummy_input.shape == block_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. Transformer Encoder ìŠ¤íƒ (ì—¬ëŸ¬ Encoder Block ìŒ“ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    í† í° ì„ë² ë”© + Positional Encodingì„ í•©ì‚°í•˜ëŠ” ë ˆì´ì–´\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # í† í° ì„ë² ë”©\n",
    "        self.token_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=d_model\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding (trainable=False ê³ ì •ê°’)\n",
    "        pe_matrix = get_positional_encoding(max_seq_len, d_model)\n",
    "        self.pos_encoding = tf.cast(pe_matrix, dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        # í† í° ì„ë² ë”©\n",
    "        embed = self.token_embedding(x)  # (batch, seq_len, d_model)\n",
    "        # sqrt(d_model)ë¡œ ìŠ¤ì¼€ì¼ë§ (ì„ë² ë”© ê°’ì„ PEì™€ ë¹„êµ ê°€ëŠ¥í•œ í¬ê¸°ë¡œ)\n",
    "        embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # Positional Encoding ë”í•˜ê¸°\n",
    "        embed += self.pos_encoding[:, :seq_len, :]\n",
    "        return embed\n",
    "\n",
    "\n",
    "class TransformerEncoder(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Nê°œì˜ Encoder Blockì„ ìŒ“ì€ Transformer Encoder\n",
    "    ë¶„ë¥˜ ì‘ì—…ìš© í—¤ë“œ í¬í•¨\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff,\n",
    "                 num_layers, num_classes, max_seq_len=512,\n",
    "                 dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # ì„ë² ë”© + Positional Encoding\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model, max_seq_len)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "        # Nê°œì˜ Encoder Block ìŠ¤íƒ\n",
    "        self.encoder_blocks = [\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout_rate,\n",
    "                                    name=f'encoder_block_{i}')\n",
    "            for i in range(num_layers)\n",
    "        ]\n",
    "        \n",
    "        # ë¶„ë¥˜ í—¤ë“œ\n",
    "        self.global_avg_pool = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.fc_dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes,\n",
    "                                                 activation='sigmoid' if num_classes == 1 else 'softmax')\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # ì„ë² ë”© + Positional Encoding\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Nê°œ Encoder Block í†µê³¼\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, training=training)\n",
    "        \n",
    "        # ì‹œí€€ìŠ¤ ì°¨ì› í‰ê·  í’€ë§\n",
    "        x = self.global_avg_pool(x)       # (batch, d_model)\n",
    "        x = self.fc1(x)                   # (batch, 64)\n",
    "        x = self.fc_dropout(x, training=training)\n",
    "        return self.classifier(x)          # (batch, num_classes)\n",
    "\n",
    "\n",
    "# ëª¨ë¸ êµ¬ì¡° í™•ì¸\n",
    "vocab_size = 10000\n",
    "d_model_demo = 64\n",
    "num_heads_demo = 4\n",
    "num_layers_demo = 2\n",
    "\n",
    "demo_model = TransformerEncoder(\n",
    "    vocab_size=vocab_size, d_model=d_model_demo, num_heads=num_heads_demo,\n",
    "    d_ff=d_model_demo*4, num_layers=num_layers_demo, num_classes=1\n",
    ")\n",
    "\n",
    "# Build í•˜ì—¬ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\n",
    "dummy_tokens = tf.ones((1, 20), dtype=tf.int32)\n",
    "_ = demo_model(dummy_tokens)\n",
    "demo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. IMDB í…ìŠ¤íŠ¸ ë¶„ë¥˜ Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 200\n",
    "D_MODEL = 64\n",
    "NUM_HEADS = 4\n",
    "D_FF = 256          # D_MODELì˜ 4ë°°\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT_RATE = 0.1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "# IMDB ë°ì´í„° ë¡œë“œ\n",
    "print('IMDB ë°ì´í„° ë¡œë”©...')\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "# íŒ¨ë”© (ìµœëŒ€ ê¸¸ì´ë¡œ í†µì¼)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "print(f'í›ˆë ¨ ë°ì´í„°: {x_train.shape}, ë ˆì´ë¸”: {y_train.shape}')\n",
    "print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°: {x_test.shape}, ë ˆì´ë¸”: {y_test.shape}')\n",
    "print(f'í´ë˜ìŠ¤ ë¶„í¬ (í›ˆë ¨): ê¸ì •={y_train.sum()}, ë¶€ì •={len(y_train)-y_train.sum()}')\n",
    "\n",
    "# Transformer ëª¨ë¸ ìƒì„±\n",
    "model = TransformerEncoder(\n",
    "    vocab_size=VOCAB_SIZE + 1,  # +1: íŒ¨ë”© í† í°\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=1,\n",
    "    max_seq_len=MAX_LEN,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Build\n",
    "_ = model(x_train[:1])\n",
    "model.summary()\n",
    "\n",
    "# í•™ìŠµ\n",
    "print('\\nëª¨ë¸ í•™ìŠµ ì‹œì‘...')\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# í‰ê°€\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'\\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc:.4f}')\n",
    "print(f'í…ŒìŠ¤íŠ¸ ì†ì‹¤:   {test_loss:.4f}')\n",
    "\n",
    "# í•™ìŠµ ê³¡ì„  ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(history.history['accuracy'], label='í›ˆë ¨ ì •í™•ë„', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='ê²€ì¦ ì •í™•ë„', marker='s')\n",
    "axes[0].set_title('ì •í™•ë„', fontsize=12)\n",
    "axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='í›ˆë ¨ ì†ì‹¤', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='ê²€ì¦ ì†ì‹¤', marker='s')\n",
    "axes[1].set_title('ì†ì‹¤', fontsize=12)\n",
    "axes[1].set_xlabel('ì—í¬í¬')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('IMDB Transformer í•™ìŠµ ê³¡ì„ ', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬\n",
    "\n",
    "### Transformer Encoder êµ¬ì„± ìš”ì†Œ ìš”ì•½\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ | ì—­í•  | ìˆ˜ì‹/ì„¤ëª… |\n",
    "|-----------|------|-----------|\n",
    "| Token Embedding | ì •ìˆ˜ ì¸ë±ìŠ¤ â†’ ë²¡í„° | í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© í–‰ë ¬ |\n",
    "| Positional Encoding | ìœ„ì¹˜ ì •ë³´ ë¶€ì—¬ | sin/cos ê³ ì •ê°’ |\n",
    "| Multi-Head Attention | ì‹œí€€ìŠ¤ ë‚´ ê´€ê³„ ëª¨ë¸ë§ | $h$ê°œ í—¤ë“œ ë³‘ë ¬ Attention |\n",
    "| Add & Norm | ì•ˆì •ì  í•™ìŠµ | ì”ì°¨ ì—°ê²° + Layer Normalization |\n",
    "| Feed-Forward Network | ë¹„ì„ í˜• ë³€í™˜ | ReLU í™œì„±í™” |\n",
    "| Global Avg Pooling | ì‹œí€€ìŠ¤ â†’ ë²¡í„° | ë¶„ë¥˜ í—¤ë“œ ì…ë ¥ìš© |\n",
    "\n",
    "### ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "\n",
    "- **d_model**: ì„ë² ë”© ì°¨ì› (ë³´í†µ 64~512)\n",
    "- **num_heads**: í—¤ë“œ ìˆ˜ (d_modelì˜ ì•½ìˆ˜)\n",
    "- **d_ff**: FFN ë‚´ë¶€ ì°¨ì› (ë³´í†µ d_model Ã— 4)\n",
    "- **num_layers**: Encoder Block ìŠ¤íƒ ìˆ˜ (BERT: 12, GPT-2 small: 12)\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**Chapter 07-03: BERT Fine-Tuning** â€” ì‚¬ì „í•™ìŠµëœ ëŒ€ê·œëª¨ Transformer(BERT)ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ íŠ¹ì • íƒœìŠ¤í¬ì— íŒŒì¸íŠœë‹í•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}