{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-03: BERT Fine-Tuning\n",
    "\n",
    "## 학습 목표\n",
    "- BERT의 두 가지 사전학습 목표(MLM, NSP)를 이해한다\n",
    "- HuggingFace `transformers` 라이브러리와 TensorFlow 백엔드를 함께 사용한다\n",
    "- AutoTokenizer로 텍스트를 토크나이징하는 방법을 익힌다\n",
    "- `TFAutoModelForSequenceClassification`으로 감성 분류 Fine-Tuning을 수행한다\n",
    "- 한국어 BERT 모델의 종류와 특징을 파악한다\n",
    "\n",
    "## 목차\n",
    "1. 라이브러리 임포트\n",
    "2. BERT 사전학습 목표 이해\n",
    "3. HuggingFace + TF 백엔드 사용 방법\n",
    "4. 모델 및 토크나이저 로드\n",
    "5. 토크나이저 사용법\n",
    "6. Fine-Tuning 코드\n",
    "7. 한국어 BERT 모델 목록\n",
    "8. 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# HuggingFace transformers 임포트 (없으면 안내 메시지)\n",
    "try:\n",
    "    import transformers\n",
    "    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "    print(f'transformers 버전: {transformers.__version__}')\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('transformers 라이브러리가 설치되지 않았습니다.')\n",
    "    print('설치 방법: pip install transformers')\n",
    "    print('이 노트북의 코드 셀은 transformers 설치 후 실행 가능합니다.')\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "print(f'TensorFlow 버전: {tf.__version__}')\n",
    "\n",
    "# TF 백엔드 사용 설정\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # TF 경고 메시지 억제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 2. BERT 사전학습 목표\n",
    "\n",
    "BERT(Bidirectional Encoder Representations from Transformers)는 두 가지 자기지도학습(self-supervised) 목표로 사전학습된다.\n",
    "\n",
    "### 2-1. MLM (Masked Language Model)\n",
    "\n",
    "입력 토큰의 **15%를 무작위로 마스킹**하고, 마스킹된 토큰을 예측하는 태스크.\n",
    "\n",
    "```\n",
    "입력: \"나는 [MASK] 밥을 먹었다\"\n",
    "목표: [MASK] → \"오늘\"\n",
    "```\n",
    "\n",
    "| 처리 방식 | 비율 | 설명 |\n",
    "|-----------|------|------|\n",
    "| `[MASK]` 교체 | 80% | 실제 마스킹 |\n",
    "| 무작위 토큰 교체 | 10% | 노이즈 추가 |\n",
    "| 원래 토큰 유지 | 10% | 복원 능력 학습 |\n",
    "\n",
    "**GPT(단방향)**와 달리 BERT는 **양방향(Bidirectional)**으로 문맥을 봄 → 더 풍부한 표현 학습\n",
    "\n",
    "### 2-2. NSP (Next Sentence Prediction)\n",
    "\n",
    "두 문장 A, B가 **실제로 연속된 문장인지 예측**하는 이진 분류 태스크.\n",
    "\n",
    "```\n",
    "[CLS] 문장A [SEP] 문장B [SEP]\n",
    "→ IsNext (50%) 또는 NotNext (50%)\n",
    "```\n",
    "\n",
    "- `[CLS]` 토큰의 최종 표현이 문장 쌍 분류에 사용됨\n",
    "- 문서 이해, QA, NLI 등 문장 간 관계 파악 태스크에 유용\n",
    "\n",
    "### 2-3. BERT 입력 형식\n",
    "\n",
    "```\n",
    "토큰:    [CLS]  나는  오늘  [SEP]  학교에  갔다  [SEP]\n",
    "세그먼트: 0      0    0      0      1       1     1\n",
    "위치:     0      1    2      3      4       5     6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 3. HuggingFace + TF 백엔드 사용 방법\n",
    "\n",
    "### 설치\n",
    "\n",
    "```bash\n",
    "pip install transformers\n",
    "pip install tensorflow  # 이미 설치된 경우 생략\n",
    "```\n",
    "\n",
    "### TF 백엔드 강제 설정\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "```\n",
    "\n",
    "### HuggingFace 모델 네이밍 규칙\n",
    "\n",
    "| 접두사 | 백엔드 | 예시 |\n",
    "|--------|--------|------|\n",
    "| `TF` + 모델명 | TensorFlow | `TFAutoModelForSequenceClassification` |\n",
    "| 모델명 (접두사 없음) | PyTorch | `AutoModelForSequenceClassification` |\n",
    "\n",
    "### Fine-Tuning 전략\n",
    "\n",
    "1. **전체 파인튜닝 (Full Fine-Tuning)**: 모든 레이어를 업데이트 → 높은 성능, 느린 학습\n",
    "2. **헤드만 학습 (Feature Extraction)**: BERT 가중치 동결, 분류 헤드만 학습 → 빠름, 낮은 성능\n",
    "3. **점진적 파인튜닝 (Gradual Unfreezing)**: 위쪽 레이어부터 순차적으로 해동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORMERS_AVAILABLE:\n",
    "    # 사용할 사전학습 모델 지정\n",
    "    # 영어: 'bert-base-uncased'\n",
    "    # 한국어: 'klue/bert-base' (KLUE 벤치마크 기반)\n",
    "    MODEL_NAME = 'bert-base-uncased'  # 영어 모델 (다운로드 크기 ~400MB)\n",
    "    \n",
    "    print(f'모델 로딩: {MODEL_NAME}')\n",
    "    print('(처음 실행 시 HuggingFace Hub에서 모델을 다운로드합니다...)')\n",
    "    \n",
    "    # AutoTokenizer: 모델에 맞는 토크나이저 자동 선택\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # TFAutoModelForSequenceClassification: 이진/다중 분류용 모델 (TF 백엔드)\n",
    "    # num_labels=2 → 긍정/부정 이진 분류\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    print(f'\\n토크나이저 타입: {type(tokenizer).__name__}')\n",
    "    print(f'모델 타입:       {type(bert_model).__name__}')\n",
    "    print(f'어휘 크기:       {tokenizer.vocab_size}')\n",
    "    print(f'최대 시퀀스 길이: {tokenizer.model_max_length}')\n",
    "else:\n",
    "    print('transformers가 설치되지 않아 이 셀을 건너뜁니다.')\n",
    "    print('pip install transformers 후 재실행하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0006-4006-8006-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORMERS_AVAILABLE:\n",
    "    # 토크나이저 사용 예시\n",
    "    sample_texts = [\n",
    "        \"This movie was absolutely fantastic! I loved every minute.\",\n",
    "        \"Terrible film. Complete waste of time.\",\n",
    "        \"An average movie with some good moments.\"\n",
    "    ]\n",
    "    \n",
    "    print('=== 단일 문장 토크나이징 ===')\n",
    "    single_enc = tokenizer(\n",
    "        sample_texts[0],\n",
    "        max_length=32,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'  # TensorFlow 텐서 반환\n",
    "    )\n",
    "    print(f'input_ids:      {single_enc[\"input_ids\"][0].numpy()[:15]}...')\n",
    "    print(f'attention_mask: {single_enc[\"attention_mask\"][0].numpy()[:15]}...')\n",
    "    print(f'shape: {single_enc[\"input_ids\"].shape}')\n",
    "    \n",
    "    # 특수 토큰 확인\n",
    "    print(f'\\n[CLS] 토큰 ID: {tokenizer.cls_token_id}')\n",
    "    print(f'[SEP] 토큰 ID: {tokenizer.sep_token_id}')\n",
    "    print(f'[PAD] 토큰 ID: {tokenizer.pad_token_id}')\n",
    "    print(f'[MASK] 토큰 ID: {tokenizer.mask_token_id}')\n",
    "    \n",
    "    print('\\n=== 토큰 → 원문 복원 ===')\n",
    "    token_ids = single_enc['input_ids'][0].numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids[:12])\n",
    "    print('토큰:', tokens)\n",
    "    \n",
    "    print('\\n=== 배치 토크나이징 (패딩 자동 처리) ===')\n",
    "    batch_enc = tokenizer(\n",
    "        sample_texts,\n",
    "        max_length=64,\n",
    "        padding=True,         # 가장 긴 문장 길이에 맞춤\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    print(f'input_ids shape:      {batch_enc[\"input_ids\"].shape}')\n",
    "    print(f'attention_mask shape: {batch_enc[\"attention_mask\"].shape}')\n",
    "    print('attention_mask (1=실제 토큰, 0=패딩):')\n",
    "    print(batch_enc['attention_mask'].numpy())\n",
    "else:\n",
    "    print('transformers가 설치되지 않아 이 셀을 건너뜁니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRANSFORMERS_AVAILABLE:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "    matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    # 간단한 감성 분류 데이터셋 (데모용)\n",
    "    train_texts = [\n",
    "        \"I loved this movie, it was amazing!\",\n",
    "        \"Absolutely wonderful film, highly recommend!\",\n",
    "        \"Great acting and fantastic story.\",\n",
    "        \"One of the best movies I have ever seen.\",\n",
    "        \"Outstanding performance by all actors.\",\n",
    "        \"Terrible movie, I hated every second.\",\n",
    "        \"Worst film ever made, complete disaster.\",\n",
    "        \"Boring and predictable, do not watch.\",\n",
    "        \"Awful acting and terrible script.\",\n",
    "        \"A complete waste of my time and money.\"\n",
    "    ]\n",
    "    train_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]  # 1=긍정, 0=부정\n",
    "\n",
    "    val_texts = [\n",
    "        \"Pretty good movie overall.\",\n",
    "        \"Not impressed, expected better.\"\n",
    "    ]\n",
    "    val_labels = [1, 0]\n",
    "\n",
    "    # 토크나이징\n",
    "    MAX_LEN = 64\n",
    "\n",
    "    def encode_texts(texts, max_len=MAX_LEN):\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "\n",
    "    train_encodings = encode_texts(train_texts)\n",
    "    val_encodings   = encode_texts(val_texts)\n",
    "\n",
    "    # TF Dataset 생성\n",
    "    def make_dataset(encodings, labels):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(encodings),\n",
    "            labels\n",
    "        ))\n",
    "        return dataset.batch(4)\n",
    "\n",
    "    train_dataset = make_dataset(train_encodings, train_labels)\n",
    "    val_dataset   = make_dataset(val_encodings, val_labels)\n",
    "\n",
    "    # Fine-Tuning 설정\n",
    "    LEARNING_RATE = 2e-5  # BERT Fine-Tuning 권장 학습률\n",
    "    EPOCHS = 3\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "    bert_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    print('BERT Fine-Tuning 시작...')\n",
    "    print(f'학습 데이터: {len(train_texts)}개 | 검증 데이터: {len(val_texts)}개')\n",
    "    print('(실제 사용 시 더 많은 데이터와 에포크가 필요합니다)')\n",
    "\n",
    "    history = bert_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # 추론 예시\n",
    "    print('\\n=== 추론 결과 ===')\n",
    "    test_sentences = [\n",
    "        \"This was the best movie of the year!\",\n",
    "        \"I fell asleep halfway through.\"\n",
    "    ]\n",
    "\n",
    "    test_enc = encode_texts(test_sentences)\n",
    "    logits = bert_model.predict(dict(test_enc), verbose=0).logits\n",
    "    probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "\n",
    "    for i, (sent, prob) in enumerate(zip(test_sentences, probs)):\n",
    "        pred_label = '긍정' if prob[1] > prob[0] else '부정'\n",
    "        print(f'문장: \"{sent}\"')\n",
    "        print(f'  부정 확률: {prob[0]:.3f} | 긍정 확률: {prob[1]:.3f} → 예측: {pred_label}\\n')\n",
    "else:\n",
    "    print('transformers가 설치되지 않아 이 셀을 건너뜁니다.')\n",
    "    print('pip install transformers 후 재실행하세요.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 7. 한국어 BERT 모델 목록\n",
    "\n",
    "### 주요 한국어 사전학습 모델 (HuggingFace Hub)\n",
    "\n",
    "| 모델명 | 기관 | 특징 | 파라미터 수 |\n",
    "|--------|------|------|------------|\n",
    "| `klue/bert-base` | KLUE 팀 | KLUE 벤치마크 기반, 범용 | 110M |\n",
    "| `klue/roberta-base` | KLUE 팀 | RoBERTa 아키텍처, 높은 성능 | 110M |\n",
    "| `snunlp/KR-ELECTRA-discriminator` | SNU NLP | ELECTRA 기반, 빠른 학습 | 110M |\n",
    "| `monologg/koelectra-base-v3-discriminator` | monologg | KoELECTRA v3 | 110M |\n",
    "| `beomi/kcbert-base` | beomi | KcBERT (커뮤니티 댓글 학습) | 110M |\n",
    "| `kakaobank/kf-deberta-base` | 카카오뱅크 | DeBERTa 기반 | 185M |\n",
    "| `skt/kobert-base-v1` | SKT | KoBERT | 92M |\n",
    "\n",
    "### 로드 방법\n",
    "\n",
    "```python\n",
    "# 한국어 BERT (KLUE) 로드 예시\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    'klue/bert-base',\n",
    "    num_labels=2\n",
    ")\n",
    "```\n",
    "\n",
    "### KLUE 벤치마크 태스크\n",
    "\n",
    "| 태스크 | 설명 | 모델 클래스 |\n",
    "|--------|------|------------|\n",
    "| TC (감성 분류) | 긍/부정 분류 | `TFAutoModelForSequenceClassification` |\n",
    "| STS (문장 유사도) | 두 문장 유사도 점수 | `TFAutoModelForSequenceClassification` |\n",
    "| NLI (자연어 추론) | 함의/중립/모순 분류 | `TFAutoModelForSequenceClassification` |\n",
    "| NER (개체명 인식) | 인물/장소/날짜 태깅 | `TFAutoModelForTokenClassification` |\n",
    "| QA (기계독해) | 지문에서 답변 추출 | `TFAutoModelForQuestionAnswering` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-0009-4009-8009-000000000009",
   "metadata": {},
   "source": [
    "## 8. 정리\n",
    "\n",
    "### BERT Fine-Tuning 핵심 요약\n",
    "\n",
    "1. **사전학습(Pre-training)**: 대규모 코퍼스에서 MLM + NSP로 언어 표현 학습\n",
    "2. **파인튜닝(Fine-Tuning)**: 소량의 레이블 데이터로 특정 태스크에 적응\n",
    "3. **토크나이저**: WordPiece/BPE로 서브워드 분리 → OOV 문제 해결\n",
    "4. **특수 토큰**: `[CLS]`(문장 분류), `[SEP]`(문장 구분), `[PAD]`(패딩), `[MASK]`(마스킹)\n",
    "\n",
    "### Fine-Tuning 권장 하이퍼파라미터\n",
    "\n",
    "| 파라미터 | 권장 범위 |\n",
    "|----------|---------|\n",
    "| 학습률 | 2e-5 ~ 5e-5 |\n",
    "| 에포크 수 | 3 ~ 5 |\n",
    "| 배치 크기 | 16, 32 |\n",
    "| 최대 시퀀스 길이 | 128, 256, 512 |\n",
    "| Warm-up 비율 | 전체 스텝의 10% |\n",
    "\n",
    "### 주의 사항\n",
    "\n",
    "- 영어 BERT는 영어 데이터, 한국어 BERT는 한국어 데이터에 적합\n",
    "- 도메인이 다를수록(예: 의료, 법률) 도메인 특화 모델 또는 추가 도메인 사전학습 권장\n",
    "- GPU 메모리 부족 시: 배치 크기 감소, 시퀀스 길이 감소, 그래디언트 누적(gradient accumulation) 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
