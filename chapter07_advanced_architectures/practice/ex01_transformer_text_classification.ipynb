{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07 실습 1: Transformer 텍스트 분류\n",
    "\n",
    "## 실습 목표\n",
    "- Positional Encoding 클래스를 직접 구현한다\n",
    "- TransformerBlock 클래스(MultiHeadAttention + FFN)를 구현한다\n",
    "- TextVectorization으로 텍스트 전처리 파이프라인을 구성한다\n",
    "- IMDB 데이터에 Transformer 분류 모델을 적용하고 평가한다\n",
    "- BiLSTM 모델과 성능을 비교한다 (도전 과제)\n",
    "\n",
    "## 실습 개요\n",
    "\n",
    "```\n",
    "입력 텍스트\n",
    "    ↓ TextVectorization\n",
    "정수 시퀀스\n",
    "    ↓ Embedding + Positional Encoding\n",
    "임베딩 + 위치 정보\n",
    "    ↓ TransformerBlock × 2\n",
    "컨텍스트 벡터\n",
    "    ↓ GlobalAveragePooling1D\n",
    "고정 크기 벡터\n",
    "    ↓ Dense(64, relu) → Dense(1, sigmoid)\n",
    "감성 예측 (긍정/부정)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow 버전: {tf.__version__}')\n",
    "\n",
    "# 재현성 시드\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "VOCAB_SIZE    = 20000   # 어휘 크기\n",
    "MAX_LEN       = 200     # 최대 시퀀스 길이\n",
    "EMBED_DIM     = 64      # 임베딩 차원 (= d_model)\n",
    "NUM_HEADS     = 4       # Multi-Head Attention 헤드 수\n",
    "FF_DIM        = 256     # Feed-Forward 내부 차원 (= d_ff)\n",
    "NUM_BLOCKS    = 2       # Transformer Block 수\n",
    "DROPOUT_RATE  = 0.1     # Dropout 비율\n",
    "BATCH_SIZE    = 64      # 배치 크기\n",
    "EPOCHS        = 10      # 학습 에포크 수\n",
    "LEARNING_RATE = 1e-4    # 학습률\n",
    "\n",
    "print('하이퍼파라미터 설정 완료')\n",
    "print(f'  VOCAB_SIZE:   {VOCAB_SIZE}')\n",
    "print(f'  MAX_LEN:      {MAX_LEN}')\n",
    "print(f'  EMBED_DIM:    {EMBED_DIM}')\n",
    "print(f'  NUM_HEADS:    {NUM_HEADS}')\n",
    "print(f'  FF_DIM:       {FF_DIM}')\n",
    "print(f'  NUM_BLOCKS:   {NUM_BLOCKS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0003-4003-8003-000000000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Encoding 레이어\n",
    "    토큰 임베딩에 위치 정보를 더함\n",
    "    \n",
    "    PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len, d_model, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model     = d_model\n",
    "        \n",
    "        # 토큰 임베딩 레이어\n",
    "        self.embedding = keras.layers.Embedding(\n",
    "            input_dim=VOCAB_SIZE + 1,  # +1: 패딩 인덱스 0 포함\n",
    "            output_dim=d_model,\n",
    "            mask_zero=True             # 패딩 토큰 마스킹 활성화\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding 행렬 사전 계산\n",
    "        self.pos_encoding = self._create_positional_encoding(max_seq_len, d_model)\n",
    "    \n",
    "    def _create_positional_encoding(self, max_seq_len, d_model):\n",
    "        \"\"\"sin/cos 기반 Positional Encoding 계산\"\"\"\n",
    "        # 위치 벡터: shape = (max_seq_len, 1)\n",
    "        positions = np.arange(max_seq_len, dtype=np.float32)[:, np.newaxis]\n",
    "        \n",
    "        # 차원 인덱스 (짝수만): shape = (1, d_model // 2)\n",
    "        dims = np.arange(0, d_model, 2, dtype=np.float32)[np.newaxis, :]\n",
    "        \n",
    "        # 주파수 분모: 10000^(2i/d_model)\n",
    "        div_term = np.power(10000.0, dims / d_model)\n",
    "        \n",
    "        # PE 행렬 초기화\n",
    "        pe = np.zeros((max_seq_len, d_model), dtype=np.float32)\n",
    "        pe[:, 0::2] = np.sin(positions / div_term)  # 짝수 차원: sin\n",
    "        pe[:, 1::2] = np.cos(positions / div_term)  # 홀수 차원: cos\n",
    "        \n",
    "        # 배치 차원 추가: (1, max_seq_len, d_model)\n",
    "        return tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        \n",
    "        # 토큰 임베딩: (batch, seq_len) → (batch, seq_len, d_model)\n",
    "        x_embed = self.embedding(x)\n",
    "        \n",
    "        # 임베딩 스케일링: sqrt(d_model)을 곱해 PE와 크기를 맞춤\n",
    "        x_embed *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # Positional Encoding 더하기 (브로드캐스팅)\n",
    "        x_embed += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        return x_embed\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'max_seq_len': self.max_seq_len, 'd_model': self.d_model})\n",
    "        return config\n",
    "\n",
    "\n",
    "# 동작 확인\n",
    "pe_layer = PositionalEncoding(MAX_LEN, EMBED_DIM)\n",
    "dummy_tokens = tf.ones((2, 10), dtype=tf.int32)  # (batch=2, seq=10)\n",
    "pe_output = pe_layer(dummy_tokens)\n",
    "print('PositionalEncoding 입력 shape:', dummy_tokens.shape)\n",
    "print('PositionalEncoding 출력 shape:', pe_output.shape)  # (2, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0004-4004-8004-000000000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    Multi-Head Self-Attention + Add&Norm + FFN + Add&Norm\n",
    "    \n",
    "    구조:\n",
    "        x → MHA(Q=K=V=x) → Add → LayerNorm → FFN → Add → LayerNorm → 출력\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.num_heads   = num_heads\n",
    "        self.ff_dim      = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # --- Multi-Head Self-Attention ---\n",
    "        self.mha = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim // num_heads,  # 헤드당 차원\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # --- Feed-Forward Network ---\n",
    "        # FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "        self.ffn = keras.Sequential([\n",
    "            keras.layers.Dense(ff_dim, activation='relu'),  # 확장: embed_dim → ff_dim\n",
    "            keras.layers.Dense(embed_dim)                   # 축소: ff_dim → embed_dim\n",
    "        ], name='ffn')\n",
    "        \n",
    "        # --- Layer Normalization (Pre-LN 방식) ---\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # --- Dropout ---\n",
    "        self.dropout1 = keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        # --- Sub-Layer 1: Multi-Head Self-Attention ---\n",
    "        # Self-Attention: Q = K = V = x\n",
    "        attn_output = self.mha(\n",
    "            query=x, key=x, value=x,\n",
    "            training=training\n",
    "        )\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # 잔차 연결(Residual) + 레이어 정규화\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        \n",
    "        # --- Sub-Layer 2: Position-wise Feed-Forward Network ---\n",
    "        ffn_output = self.ffn(out1, training=training)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # 잔차 연결 + 레이어 정규화\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2  # (batch, seq_len, embed_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim':    self.embed_dim,\n",
    "            'num_heads':    self.num_heads,\n",
    "            'ff_dim':       self.ff_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# 동작 확인\n",
    "tf_block = TransformerBlock(EMBED_DIM, NUM_HEADS, FF_DIM, DROPOUT_RATE)\n",
    "dummy_embed = tf.random.normal((2, 10, EMBED_DIM))  # (batch, seq, embed)\n",
    "block_out = tf_block(dummy_embed, training=False)\n",
    "print('TransformerBlock 입력 shape:', dummy_embed.shape)\n",
    "print('TransformerBlock 출력 shape:', block_out.shape)  # (2, 10, 64)\n",
    "print('입출력 차원 동일:', dummy_embed.shape == block_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB 데이터 로드\n",
    "print('IMDB 데이터 로딩...')\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=VOCAB_SIZE\n",
    ")\n",
    "\n",
    "# 패딩 처리\n",
    "x_train_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_train_raw, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "x_test_padded = keras.preprocessing.sequence.pad_sequences(\n",
    "    x_test_raw, maxlen=MAX_LEN, padding='post', truncating='post'\n",
    ")\n",
    "\n",
    "print(f'훈련 데이터: {x_train_padded.shape}')\n",
    "print(f'테스트 데이터: {x_test_padded.shape}')\n",
    "print(f'레이블 분포 (훈련): 긍정={y_train.sum()}, 부정={len(y_train)-y_train.sum()}')\n",
    "\n",
    "# TextVectorization을 사용한 추가 전처리 예시\n",
    "# (이미 정수 인코딩된 IMDB 데이터와 병행하여 사용 가능)\n",
    "print('\\n--- TextVectorization 사용 예시 ---')\n",
    "# 실제 텍스트 데이터가 있을 경우 사용 방법:\n",
    "sample_texts = [\n",
    "    \"This movie was absolutely fantastic!\",\n",
    "    \"I really enjoyed the plot and characters.\",\n",
    "    \"Terrible film, complete waste of time.\"\n",
    "]\n",
    "\n",
    "# TextVectorization 레이어 설정\n",
    "vectorizer = keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,           # 최대 어휘 크기\n",
    "    output_mode='int',               # 정수 인코딩\n",
    "    output_sequence_length=MAX_LEN   # 고정 길이 출력\n",
    ")\n",
    "\n",
    "# 텍스트 어휘 학습 (adapt)\n",
    "vectorizer.adapt(sample_texts)\n",
    "\n",
    "# 변환 예시\n",
    "vectorized = vectorizer(sample_texts[:1])\n",
    "print(f'입력 텍스트: \"{sample_texts[0]}\"')\n",
    "print(f'벡터화 결과: {vectorized[0].numpy()[:10]}...')\n",
    "print(f'어휘 크기: {len(vectorizer.get_vocabulary())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0006-4006-8006-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer_classifier():\n",
    "    \"\"\"\n",
    "    전체 Transformer 텍스트 분류 모델 구성:\n",
    "    Input → Embedding + PE → TransformerBlock × 2 → GlobalAvgPool → Dense → 출력\n",
    "    \"\"\"\n",
    "    # 입력 레이어\n",
    "    inputs = keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='token_ids')\n",
    "    \n",
    "    # --- Stage 1: Positional Embedding ---\n",
    "    x = PositionalEncoding(\n",
    "        max_seq_len=MAX_LEN,\n",
    "        d_model=EMBED_DIM,\n",
    "        name='positional_encoding'\n",
    "    )(inputs)\n",
    "    x = keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "    \n",
    "    # --- Stage 2: Transformer Blocks × NUM_BLOCKS ---\n",
    "    for i in range(NUM_BLOCKS):\n",
    "        x = TransformerBlock(\n",
    "            embed_dim=EMBED_DIM,\n",
    "            num_heads=NUM_HEADS,\n",
    "            ff_dim=FF_DIM,\n",
    "            dropout_rate=DROPOUT_RATE,\n",
    "            name=f'transformer_block_{i+1}'\n",
    "        )(x)\n",
    "    \n",
    "    # --- Stage 3: 시퀀스 → 벡터 변환 ---\n",
    "    # GlobalAveragePooling1D: 시퀀스 차원에 대해 평균\n",
    "    x = keras.layers.GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    \n",
    "    # --- Stage 4: 분류 헤드 ---\n",
    "    x = keras.layers.Dense(64, activation='relu', name='dense_hidden')(x)\n",
    "    x = keras.layers.Dropout(DROPOUT_RATE)(x)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='TransformerClassifier')\n",
    "    return model\n",
    "\n",
    "\n",
    "# 모델 생성\n",
    "model = build_transformer_classifier()\n",
    "\n",
    "# 컴파일\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 파라미터 수 계산\n",
    "total_params = model.count_params()\n",
    "print(f'\\n총 파라미터 수: {total_params:,}')\n",
    "print(f'  = 약 {total_params/1e6:.2f}M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "print('Transformer 분류 모델 학습 시작...')\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_padded, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 테스트 평가\n",
    "test_loss, test_acc = model.evaluate(x_test_padded, y_test, verbose=0)\n",
    "print(f'\\n테스트 정확도: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'테스트 손실:   {test_loss:.4f}')\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 정확도\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history.history['accuracy'], 'b-o', label='훈련 정확도', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], 'r-s', label='검증 정확도', linewidth=2)\n",
    "ax1.set_title(f'정확도\\n(최종 테스트: {test_acc:.4f})', fontsize=12)\n",
    "ax1.set_xlabel('에포크')\n",
    "ax1.set_ylabel('정확도')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim([0.5, 1.0])\n",
    "\n",
    "# 손실\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history.history['loss'], 'b-o', label='훈련 손실', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], 'r-s', label='검증 손실', linewidth=2)\n",
    "ax2.set_title(f'손실\\n(최종 테스트: {test_loss:.4f})', fontsize=12)\n",
    "ax2.set_xlabel('에포크')\n",
    "ax2.set_ylabel('손실')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Transformer 텍스트 분류 학습 결과', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 예측 예시\n",
    "print('\\n=== 예측 예시 ===')\n",
    "n_examples = 5\n",
    "preds = model.predict(x_test_padded[:n_examples], verbose=0)\n",
    "for i, (pred, true_label) in enumerate(zip(preds, y_test[:n_examples])):\n",
    "    pred_label = '긍정' if pred[0] > 0.5 else '부정'\n",
    "    true_str   = '긍정' if true_label == 1 else '부정'\n",
    "    correct    = '✓' if pred_label == true_str else '✗'\n",
    "    print(f'샘플 {i+1}: 예측={pred_label} ({pred[0]:.3f}) | 실제={true_str} {correct}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 도전 과제: BiLSTM과 성능 비교\n",
    "\n",
    "아래 BiLSTM 모델을 구현하고 Transformer 모델과 성능을 비교해 보자.\n",
    "\n",
    "### BiLSTM 모델 구조 (참고)\n",
    "\n",
    "```python\n",
    "def build_bilstm_classifier():\n",
    "    inputs = keras.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    x = keras.layers.Embedding(VOCAB_SIZE + 1, EMBED_DIM, mask_zero=True)(inputs)\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(32))(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.3)(x)\n",
    "    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(inputs, outputs, name='BiLSTM_Classifier')\n",
    "```\n",
    "\n",
    "### 비교 항목\n",
    "\n",
    "| 항목 | Transformer | BiLSTM |\n",
    "|------|------------|--------|\n",
    "| 테스트 정확도 | ? | ? |\n",
    "| 학습 시간/에포크 | ? | ? |\n",
    "| 파라미터 수 | ? | ? |\n",
    "| 수렴 속도 | ? | ? |\n",
    "\n",
    "### 추가 탐구 아이디어\n",
    "\n",
    "1. `NUM_BLOCKS`를 1, 2, 4로 변경하여 성능 변화 관찰\n",
    "2. `NUM_HEADS`를 2, 4, 8로 변경하여 성능 변화 관찰\n",
    "3. `GlobalAveragePooling1D` 대신 `[CLS]` 토큰 방식 구현\n",
    "4. 학습률 스케줄러(Warm-up + Decay) 적용\n",
    "5. 데이터 증강: 동의어 대체(Synonym Replacement) 적용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
