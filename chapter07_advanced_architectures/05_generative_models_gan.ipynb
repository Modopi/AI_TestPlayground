{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-05: ìƒì„± ëª¨ë¸ â€” GAN (Generative Adversarial Network)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- GANì˜ ëª©ì  í•¨ìˆ˜ì™€ ìƒì„±ì/íŒë³„ìì˜ ì—­í• ì„ ì´í•´í•œë‹¤\n",
    "- DCGAN(Deep Convolutional GAN)ì˜ Generatorì™€ Discriminatorë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- ì»¤ìŠ¤í…€ í•™ìŠµ ë£¨í”„ë¡œ ìƒì„±ìì™€ íŒë³„ìë¥¼ êµëŒ€ ì—…ë°ì´íŠ¸í•œë‹¤\n",
    "- ì—í¬í¬ë³„ ìƒì„± ì´ë¯¸ì§€ë¥¼ ì‹œê°í™”í•˜ì—¬ í•™ìŠµ ì§„í–‰ì„ ëª¨ë‹ˆí„°ë§í•œë‹¤\n",
    "- GAN í•™ìŠµ ë¶ˆì•ˆì •ì„±ì˜ ì›ì¸ê³¼ í•´ê²°ì±…ì„ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. GAN ëª©ì  í•¨ìˆ˜\n",
    "2. DCGAN Generator êµ¬í˜„\n",
    "3. DCGAN Discriminator êµ¬í˜„\n",
    "4. ì»¤ìŠ¤í…€ GAN í•™ìŠµ ë£¨í”„\n",
    "5. ì—í¬í¬ë³„ ìƒì„± ì´ë¯¸ì§€ ì‹œê°í™”\n",
    "6. í•™ìŠµ ë¶ˆì•ˆì •ì„±ê³¼ í•´ê²°ì±…\n",
    "7. ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ GAN ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ­ GAN(ìƒì„±ì  ì ëŒ€ ì‹ ê²½ë§)ì´ ë­ì˜ˆìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìœ„ì¡°ì§€íë²” vs ê°ë³„ì‚¬!\n",
    "> - **ìƒì„±ì(Generator)**: ê°€ì§œ ì§€íë¥¼ ë§Œë“œëŠ” ìœ„ì¡°ë²”\n",
    "> - **íŒë³„ì(Discriminator)**: ì§„ì§œ/ê°€ì§œë¥¼ êµ¬ë³„í•˜ëŠ” ê°ë³„ì‚¬\n",
    "> - ë‘˜ì´ ì„œë¡œ ê²½ìŸí•˜ë©° ë°œì „! ê²°êµ­ ì™„ë²½í•œ ìœ„ì¡° ì§€í(=ì§„ì§œ ê°™ì€ ì´ë¯¸ì§€) ìƒì„±!\n",
    "\n",
    "#### âš”ï¸ í•™ìŠµ ê³¼ì • â€” ë‘ ë„¤íŠ¸ì›Œí¬ì˜ ëŒ€ê²°\n",
    "\n",
    "```\n",
    "[ìƒì„±ì í•™ìŠµ]\n",
    "ëœë¤ ë…¸ì´ì¦ˆ z â†’ ìƒì„±ì â†’ ê°€ì§œ ì´ë¯¸ì§€\n",
    "                              â†“\n",
    "                         íŒë³„ì â†’ \"ì§„ì§œ\" ì˜ˆì¸¡í•˜ë„ë¡ ì†ì´ê¸°!\n",
    "\n",
    "[íŒë³„ì í•™ìŠµ]\n",
    "ì§„ì§œ ì´ë¯¸ì§€ â†’ íŒë³„ì â†’ \"ì§„ì§œ\" ì¶œë ¥\n",
    "ê°€ì§œ ì´ë¯¸ì§€ â†’ íŒë³„ì â†’ \"ê°€ì§œ\" ì¶œë ¥  â† ì˜ êµ¬ë³„í•˜ë„ë¡!\n",
    "```\n",
    "\n",
    "#### âš ï¸ GAN í•™ìŠµì˜ ì–´ë ¤ì›€\n",
    "\n",
    "| ë¬¸ì œ | ì„¤ëª… | ì¦ìƒ |\n",
    "|------|------|------|\n",
    "| **ëª¨ë“œ ë¶•ê´´** | ìƒì„±ìê°€ í•œ ì¢…ë¥˜ë§Œ ê³„ì† ìƒì„± | í•­ìƒ ê°™ì€ ì´ë¯¸ì§€ ì¶œë ¥ |\n",
    "| **í•™ìŠµ ë¶ˆì•ˆì •** | ë‘ ë„¤íŠ¸ì›Œí¬ ê· í˜• ë§ì¶”ê¸° ì–´ë ¤ì›€ | ì†ì‹¤ì´ ì§„ë™ |\n",
    "| **í‰ê°€ ì–´ë ¤ì›€** | ì´ë¯¸ì§€ í’ˆì§ˆ ê°ê´€ì  ì¸¡ì • ì–´ë ¤ì›€ | FID ì ìˆ˜ ì‚¬ìš© |\n",
    "\n",
    "> ğŸ’¡ **ëª©ì  í•¨ìˆ˜**: ìƒì„±ìì™€ íŒë³„ìê°€ ì„œë¡œ ë°˜ëŒ€ ëª©í‘œë¥¼ ê°€ì§!\n",
    "> ìƒì„±ì: íŒë³„ìë¥¼ ì†ì´ë ¤ í•¨ (ì†ì‹¤ ìµœì†Œí™”)\n",
    "> íŒë³„ì: ì†ì§€ ì•Šìœ¼ë ¤ í•¨ (êµ¬ë³„ ì •í™•ë„ ìµœëŒ€í™”)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow ë²„ì „: {tf.__version__}')\n",
    "\n",
    "# ì¬í˜„ì„± ì‹œë“œ ê³ ì •\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. GAN ëª©ì  í•¨ìˆ˜\n",
    "\n",
    "GANì€ **ìƒì„±ì(Generator, G)**ì™€ **íŒë³„ì(Discriminator, D)**ê°€ ì„œë¡œ ê²½ìŸí•˜ëŠ” ë¯¸ë‹ˆë§¥ìŠ¤ ê²Œì„:\n",
    "\n",
    "$$\\min_G \\max_D\\; \\mathbb{E}[\\log D(x)] + \\mathbb{E}[\\log(1-D(G(z)))]$$\n",
    "\n",
    "| í•­ | í•´ì„ | ìµœì í™” ë°©í–¥ |\n",
    "|----|------|------------|\n",
    "| $\\mathbb{E}[\\log D(x)]$ | ì‹¤ì œ ì´ë¯¸ì§€ $x$ë¥¼ ì‹¤ì œë¡œ íŒë³„í•  í™•ë¥  | DëŠ” **ìµœëŒ€í™”** (1ì— ê°€ê¹ê²Œ) |\n",
    "| $\\mathbb{E}[\\log(1-D(G(z)))]$ | ê°€ì§œ ì´ë¯¸ì§€ $G(z)$ë¥¼ ê°€ì§œë¡œ íŒë³„í•  í™•ë¥  | DëŠ” **ìµœëŒ€í™”**, GëŠ” **ìµœì†Œí™”** |\n",
    "\n",
    "### ì§ê´€ì  ì´í•´\n",
    "\n",
    "```\n",
    "Generator G: ë…¸ì´ì¦ˆ z â†’ ê°€ì§œ ì´ë¯¸ì§€ G(z) â†’ Dë¥¼ ì†ì´ë ¤ í•œë‹¤\n",
    "Discriminator D: ì‹¤ì œ/ê°€ì§œ ì´ë¯¸ì§€ â†’ ì§„ìœ„ í™•ë¥  D(x) â†’ Gë¥¼ ê²€ì¶œí•˜ë ¤ í•œë‹¤\n",
    "```\n",
    "\n",
    "- **ê· í˜•ì (Nash Equilibrium)**: D(x) = 0.5 â†’ íŒë³„ìê°€ ë” ì´ìƒ êµ¬ë¶„ ë¶ˆê°€\n",
    "- **ì‹¤ì œ í•™ìŠµ**: G ì†ì‹¤ = $-\\log(D(G(z)))$ (Non-saturating ë³€í˜•ìœ¼ë¡œ ê¸°ìš¸ê¸° ì•ˆì •í™”)\n",
    "\n",
    "### ì†ì‹¤ í•¨ìˆ˜ (ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)\n",
    "\n",
    "- **D ì†ì‹¤**: $-\\mathbb{E}[\\log D(x)] - \\mathbb{E}[\\log(1-D(G(z)))]$\n",
    "  - ì‹¤ì œ ì´ë¯¸ì§€ ë ˆì´ë¸” = 1, ê°€ì§œ ì´ë¯¸ì§€ ë ˆì´ë¸” = 0\n",
    "- **G ì†ì‹¤**: $-\\mathbb{E}[\\log D(G(z))]$\n",
    "  - ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì‹¤ì œ(ë ˆì´ë¸”=1)ë¡œ ì†ì´ë ¤ í•¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. DCGAN Generator êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100  # ì ì¬ ë²¡í„°(ë…¸ì´ì¦ˆ) ì°¨ì›\n",
    "\n",
    "def build_generator(latent_dim=LATENT_DIM):\n",
    "    \"\"\"\n",
    "    DCGAN Generator\n",
    "    ë…¸ì´ì¦ˆ z (latent_dim,) â†’ ì´ë¯¸ì§€ (28, 28, 1)\n",
    "    \n",
    "    êµ¬ì¡°: Dense â†’ Reshape â†’ Conv2DTranspose ìŠ¤íƒ â†’ tanh ì¶œë ¥\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name='Generator')\n",
    "    \n",
    "    # --- Dense + Reshape ---\n",
    "    # ì ì¬ ë²¡í„°ë¥¼ ì‘ì€ ê³µê°„ ë§µìœ¼ë¡œ ë³€í™˜ (7Ã—7Ã—256 íŠ¹ì§• ë§µ)\n",
    "    model.add(keras.layers.Dense(7 * 7 * 256, use_bias=False,\n",
    "                                  input_shape=(latent_dim,)))\n",
    "    model.add(keras.layers.BatchNormalization())   # ë°°ì¹˜ ì •ê·œí™”ë¡œ í•™ìŠµ ì•ˆì •í™”\n",
    "    model.add(keras.layers.LeakyReLU(0.2))        # LeakyReLU (ìŒìˆ˜ ê¸°ìš¸ê¸° í—ˆìš©)\n",
    "    \n",
    "    model.add(keras.layers.Reshape((7, 7, 256)))   # (7, 7, 256)\n",
    "    \n",
    "    # --- Conv2DTranspose (ì „ì¹˜ í•©ì„±ê³±) ìŠ¤íƒ ---\n",
    "    # ê³µê°„ í•´ìƒë„ë¥¼ ì ì§„ì ìœ¼ë¡œ 2ë°°ì”© ì—…ìƒ˜í”Œë§\n",
    "    \n",
    "    # (7, 7, 256) â†’ (7, 7, 128)\n",
    "    model.add(keras.layers.Conv2DTranspose(\n",
    "        128, kernel_size=5, strides=1, padding='same', use_bias=False\n",
    "    ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    \n",
    "    # (7, 7, 128) â†’ (14, 14, 64)  â† strides=2ë¡œ í•´ìƒë„ 2ë°°\n",
    "    model.add(keras.layers.Conv2DTranspose(\n",
    "        64, kernel_size=5, strides=2, padding='same', use_bias=False\n",
    "    ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    \n",
    "    # (14, 14, 64) â†’ (28, 28, 1)  â† ìµœì¢… ì¶œë ¥ì¸µ\n",
    "    model.add(keras.layers.Conv2DTranspose(\n",
    "        1, kernel_size=5, strides=2, padding='same',\n",
    "        use_bias=False, activation='tanh'  # tanh: ì¶œë ¥ ë²”ìœ„ [-1, 1]\n",
    "    ))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()\n",
    "\n",
    "# ë™ì‘ í™•ì¸\n",
    "test_noise = tf.random.normal([1, LATENT_DIM])\n",
    "test_gen_output = generator(test_noise, training=False)\n",
    "print(f'\\në…¸ì´ì¦ˆ shape: {test_noise.shape}')\n",
    "print(f'ìƒì„± ì´ë¯¸ì§€ shape: {test_gen_output.shape}')  # (1, 28, 28, 1)\n",
    "print(f'í”½ì…€ ë²”ìœ„: [{test_gen_output.numpy().min():.3f}, {test_gen_output.numpy().max():.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. DCGAN Discriminator êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    \"\"\"\n",
    "    DCGAN Discriminator\n",
    "    ì´ë¯¸ì§€ (28, 28, 1) â†’ í™•ë¥  ìŠ¤ì¹¼ë¼ (0~1)\n",
    "    \n",
    "    êµ¬ì¡°: Conv2D ìŠ¤íƒ â†’ Flatten â†’ Dense â†’ Sigmoid ì¶œë ¥\n",
    "    ë°°ì¹˜ ì •ê·œí™” ëŒ€ì‹  Dropout ì‚¬ìš© (GAN íŒë³„ì ê¶Œì¥)\n",
    "    \"\"\"\n",
    "    model = keras.Sequential(name='Discriminator')\n",
    "    \n",
    "    # --- Conv2D ìŠ¤íƒ (í•´ìƒë„ ì ì§„ì  ì¶•ì†Œ) ---\n",
    "    \n",
    "    # (28, 28, 1) â†’ (14, 14, 64)\n",
    "    model.add(keras.layers.Conv2D(\n",
    "        64, kernel_size=5, strides=2, padding='same',\n",
    "        input_shape=(28, 28, 1)\n",
    "    ))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))  # ìŒìˆ˜ ê¸°ìš¸ê¸° í—ˆìš© (0.2 slope)\n",
    "    model.add(keras.layers.Dropout(0.3))    # ê³¼ì í•© ë°©ì§€ (BN ëŒ€ì‹  Dropout ì‚¬ìš©)\n",
    "    \n",
    "    # (14, 14, 64) â†’ (7, 7, 128)\n",
    "    model.add(keras.layers.Conv2D(\n",
    "        128, kernel_size=5, strides=2, padding='same'\n",
    "    ))\n",
    "    model.add(keras.layers.LeakyReLU(0.2))\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    \n",
    "    # --- ë¶„ë¥˜ í—¤ë“œ ---\n",
    "    model.add(keras.layers.Flatten())       # (7*7*128,) = (6272,)\n",
    "    model.add(keras.layers.Dense(\n",
    "        1, activation='sigmoid'             # 0(ê°€ì§œ) ~ 1(ì‹¤ì œ) í™•ë¥ \n",
    "    ))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()\n",
    "\n",
    "# ë™ì‘ í™•ì¸\n",
    "test_image = tf.random.normal([1, 28, 28, 1])\n",
    "test_d_output = discriminator(test_image, training=False)\n",
    "print(f'\\nì…ë ¥ ì´ë¯¸ì§€ shape: {test_image.shape}')\n",
    "print(f'íŒë³„ ê²°ê³¼ shape: {test_d_output.shape}')      # (1, 1)\n",
    "print(f'íŒë³„ í™•ë¥  (ì´ˆê¸°): {test_d_output.numpy()[0][0]:.4f}')  # ì´ˆê¸°ì—ëŠ” ë¬´ì‘ìœ„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. ì»¤ìŠ¤í…€ GAN í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST ë°ì´í„° ì¤€ë¹„\n",
    "(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# ì •ê·œí™”: [0, 255] â†’ [-1, 1] (tanh ì¶œë ¥ê³¼ ì¼ì¹˜)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5        # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”\n",
    "x_train = x_train[..., np.newaxis]          # (60000, 28, 28) â†’ (60000, 28, 28, 1)\n",
    "\n",
    "print(f'í›ˆë ¨ ë°ì´í„° shape: {x_train.shape}')\n",
    "print(f'í”½ì…€ ë²”ìœ„: [{x_train.min():.1f}, {x_train.max():.1f}]')\n",
    "\n",
    "# TF Dataset íŒŒì´í”„ë¼ì¸\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE  = 256\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(x_train)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)  # ë¹„ë™ê¸° ë°ì´í„° ë¡œë”©\n",
    ")\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜: ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ (from_logits=False, ì´ë¯¸ sigmoid ì ìš©ë¨)\n",
    "bce_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    íŒë³„ì ì†ì‹¤:\n",
    "    - ì‹¤ì œ ì´ë¯¸ì§€ â†’ 1ë¡œ ë¶„ë¥˜ (real_loss)\n",
    "    - ê°€ì§œ ì´ë¯¸ì§€ â†’ 0ìœ¼ë¡œ ë¶„ë¥˜ (fake_loss)\n",
    "    \"\"\"\n",
    "    real_loss = bce_loss(tf.ones_like(real_output), real_output)   # ì‹¤ì œ=1\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)  # ê°€ì§œ=0\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    \"\"\"\n",
    "    ìƒì„±ì ì†ì‹¤ (Non-saturating):\n",
    "    ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì‹¤ì œ(1)ë¡œ ì†ì´ë„ë¡ í•™ìŠµ\n",
    "    \"\"\"\n",
    "    return bce_loss(tf.ones_like(fake_output), fake_output)        # ê°€ì§œë¥¼ 1ë¡œ\n",
    "\n",
    "# ë³„ë„ Optimizer (Dì™€ Gê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ)\n",
    "gen_optimizer  = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)  # GAN ê¶Œì¥\n",
    "disc_optimizer = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n",
    "\n",
    "print('ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ')\n",
    "\n",
    "@tf.function  # ê·¸ë˜í”„ ëª¨ë“œë¡œ ì»´íŒŒì¼ â†’ ì†ë„ í–¥ìƒ\n",
    "def train_step(real_images):\n",
    "    \"\"\"\n",
    "    í•œ ë°°ì¹˜ì˜ GAN í•™ìŠµ ìŠ¤í…:\n",
    "    1. G: ë…¸ì´ì¦ˆ â†’ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n",
    "    2. D: ì‹¤ì œ/ê°€ì§œ ì´ë¯¸ì§€ íŒë³„ í›„ ì†ì‹¤ ê³„ì‚° â†’ D ì—…ë°ì´íŠ¸\n",
    "    3. G: íŒë³„ ê²°ê³¼ë¡œ ì†ì‹¤ ê³„ì‚° â†’ G ì—…ë°ì´íŠ¸\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    \n",
    "    # ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ (z ~ N(0, I))\n",
    "    noise = tf.random.normal([batch_size, LATENT_DIM])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # G ìˆœì „íŒŒ: ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n",
    "        fake_images = generator(noise, training=True)\n",
    "        \n",
    "        # D ìˆœì „íŒŒ: ì‹¤ì œ/ê°€ì§œ ì´ë¯¸ì§€ íŒë³„\n",
    "        real_output = discriminator(real_images, training=True)\n",
    "        fake_output = discriminator(fake_images, training=True)\n",
    "        \n",
    "        # ì†ì‹¤ ê³„ì‚°\n",
    "        gen_loss  = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    # D ì—­ì „íŒŒ (Dì˜ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸)\n",
    "    disc_grads = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_optimizer.apply_gradients(zip(disc_grads, discriminator.trainable_variables))\n",
    "    \n",
    "    # G ì—­ì „íŒŒ (Gì˜ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸)\n",
    "    gen_grads = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_optimizer.apply_gradients(zip(gen_grads, generator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "EPOCHS = 20\n",
    "\n",
    "# ì‹œê°í™”ìš© ê³ ì • ë…¸ì´ì¦ˆ (í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§)\n",
    "seed_noise = tf.random.normal([16, LATENT_DIM])\n",
    "\n",
    "gen_losses  = []\n",
    "disc_losses = []\n",
    "saved_images = []  # ì—í¬í¬ë³„ ìƒì„± ì´ë¯¸ì§€ ì €ì¥\n",
    "\n",
    "print(f'GAN í•™ìŠµ ì‹œì‘ (ì´ {EPOCHS} ì—í¬í¬)...')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_gen_loss  = []\n",
    "    epoch_disc_loss = []\n",
    "    \n",
    "    for batch in train_dataset:\n",
    "        g_loss, d_loss = train_step(batch)\n",
    "        epoch_gen_loss.append(g_loss.numpy())\n",
    "        epoch_disc_loss.append(d_loss.numpy())\n",
    "    \n",
    "    avg_g_loss = np.mean(epoch_gen_loss)\n",
    "    avg_d_loss = np.mean(epoch_disc_loss)\n",
    "    gen_losses.append(avg_g_loss)\n",
    "    disc_losses.append(avg_d_loss)\n",
    "    \n",
    "    # ì—í¬í¬ë³„ ìƒì„± ì´ë¯¸ì§€ ì €ì¥\n",
    "    generated = generator(seed_noise, training=False).numpy()\n",
    "    saved_images.append(generated)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'ì—í¬í¬ {epoch+1:3d}/{EPOCHS} | '\n",
    "          f'G ì†ì‹¤: {avg_g_loss:.4f} | '\n",
    "          f'D ì†ì‹¤: {avg_d_loss:.4f} | '\n",
    "          f'ê²½ê³¼: {elapsed:.1f}ì´ˆ')\n",
    "\n",
    "print('\\ní•™ìŠµ ì™„ë£Œ!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. ì—í¬í¬ë³„ ìƒì„± ì´ë¯¸ì§€ ê²©ì ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_images(images, epoch, n_rows=4, n_cols=4):\n",
    "    \"\"\"\n",
    "    ìƒì„± ì´ë¯¸ì§€ë¥¼ n_rows Ã— n_cols ê²©ìë¡œ ì‹œê°í™”\n",
    "    images: shape = (n, 28, 28, 1), í”½ì…€ ë²”ìœ„ [-1, 1]\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(images):\n",
    "            # [-1, 1] â†’ [0, 1]ë¡œ ë³€í™˜\n",
    "            img = (images[i, :, :, 0] + 1) / 2.0\n",
    "            ax.imshow(img, cmap='gray')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'ì—í¬í¬ {epoch}: GAN ìƒì„± ì´ë¯¸ì§€', fontsize=13, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ì´ˆê¸°/ì¤‘ê°„/ìµœì¢… ì—í¬í¬ ì´ë¯¸ì§€ ë¹„êµ\n",
    "milestone_epochs = [1, EPOCHS//4, EPOCHS//2, EPOCHS]\n",
    "\n",
    "for ep in milestone_epochs:\n",
    "    idx = min(ep - 1, len(saved_images) - 1)\n",
    "    plot_generated_images(saved_images[idx], epoch=ep)\n",
    "\n",
    "# ì†ì‹¤ ê³¡ì„ \n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(range(1, EPOCHS+1), gen_losses,  label='Generator ì†ì‹¤',     linewidth=2, color='blue')\n",
    "ax.plot(range(1, EPOCHS+1), disc_losses, label='Discriminator ì†ì‹¤', linewidth=2, color='red')\n",
    "ax.axhline(y=np.log(2), color='gray', linestyle='--', alpha=0.7, label='ì´ë¡ ì  ê· í˜•ì  (ln2 â‰ˆ 0.693)')\n",
    "ax.set_xlabel('ì—í¬í¬', fontsize=11)\n",
    "ax.set_ylabel('ì†ì‹¤', fontsize=11)\n",
    "ax.set_title('GAN í•™ìŠµ ì†ì‹¤ ê³¡ì„ ', fontsize=13)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'ìµœì¢… Generator ì†ì‹¤:     {gen_losses[-1]:.4f}')\n",
    "print(f'ìµœì¢… Discriminator ì†ì‹¤: {disc_losses[-1]:.4f}')\n",
    "print(f'ì´ë¡ ì  ê· í˜•ì :            {np.log(2):.4f} (ln 2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. í•™ìŠµ ë¶ˆì•ˆì •ì„± ì›ì¸ê³¼ í•´ê²°ì±…\n",
    "\n",
    "### ì£¼ìš” ë¬¸ì œ 1: Mode Collapse (ëª¨ë“œ ë¶•ê´´)\n",
    "\n",
    "**í˜„ìƒ**: Generatorê°€ í•­ìƒ ë™ì¼í•˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ì´ë¯¸ì§€ë§Œ ìƒì„±  \n",
    "**ì›ì¸**: Dë¥¼ ì‰½ê²Œ ì†ì´ëŠ” íŠ¹ì • ëª¨ë“œì—ë§Œ ì§‘ì¤‘í•˜ëŠ” ì§€ë¦„ê¸¸ í•™ìŠµ\n",
    "\n",
    "```\n",
    "ì˜ˆ: MNISTì—ì„œ í•­ìƒ ìˆ«ì '1'ë§Œ ìƒì„±í•˜ì—¬ Dë¥¼ ì†ì´ëŠ” ê²½ìš°\n",
    "```\n",
    "\n",
    "**í•´ê²°ì±…**:\n",
    "- **Minibatch Discrimination**: ë°°ì¹˜ ë‚´ ë‹¤ì–‘ì„±ì„ D ì…ë ¥ì— ì¶”ê°€\n",
    "- **Feature Matching**: Gê°€ Dì˜ ì¤‘ê°„ ë ˆì´ì–´ íŠ¹ì§• ë¶„í¬ë¥¼ ë§¤ì¹­\n",
    "- **Wasserstein GAN (WGAN)**: Earth Mover Distanceë¡œ ì†ì‹¤ ëŒ€ì²´\n",
    "- **Mode Seeking Regularization**: ë‹¤ì–‘í•œ z â†’ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì¥ë ¤\n",
    "\n",
    "### ì£¼ìš” ë¬¸ì œ 2: Vanishing Gradient (ê¸°ìš¸ê¸° ì†Œì‹¤)\n",
    "\n",
    "**í˜„ìƒ**: Dê°€ ë„ˆë¬´ ì˜ í•™ìŠµë˜ë©´ Gì˜ ê¸°ìš¸ê¸°ê°€ 0ì— ê°€ê¹Œì›Œì ¸ G í•™ìŠµ ë¶ˆê°€  \n",
    "**ì›ì¸**: ì™„ë²½í•œ D â†’ $D(G(z)) \\approx 0$ â†’ $\\log(1-D(G(z))) \\approx 0$ì˜ ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤\n",
    "\n",
    "**í•´ê²°ì±…**:\n",
    "- **Non-Saturating Loss**: $\\log(D(G(z)))$ ìµœëŒ€í™”ë¡œ ë³€ê²½ (ê¸°ìš¸ê¸° í¬í™” ë°©ì§€)\n",
    "- **Label Smoothing**: ì‹¤ì œ ì´ë¯¸ì§€ ë ˆì´ë¸”ì„ 1 ëŒ€ì‹  0.9ë¡œ ì„¤ì •\n",
    "- **WGAN-GP**: Gradient Penaltyë¡œ Dì˜ Lipschitz ì œì•½ ê°•í™”\n",
    "\n",
    "### ì£¼ìš” ë¬¸ì œ 3: í•™ìŠµ ë¶ˆì•ˆì • (Oscillation)\n",
    "\n",
    "**í˜„ìƒ**: Gì™€ D ì†ì‹¤ì´ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ì§„ë™  \n",
    "**í•´ê²°ì±…**:\n",
    "- **Two Time-Scale Update Rule (TTUR)**: Gì™€ Dì— ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš©\n",
    "- **Spectral Normalization**: Dì˜ ê°€ì¤‘ì¹˜ ì •ê·œí™”ë¡œ ì•ˆì •í™”\n",
    "- **D ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¡°ì ˆ**: G 1íšŒ ì—…ë°ì´íŠ¸ ë‹¹ Dë¥¼ níšŒ ì—…ë°ì´íŠ¸\n",
    "\n",
    "### GAN ë³€í˜• ëª¨ë¸ ë¹„êµ\n",
    "\n",
    "| ëª¨ë¸ | í•µì‹¬ ì•„ì´ë””ì–´ | ì¥ì  |\n",
    "|------|-------------|------|\n",
    "| **DCGAN** | Conv2DTranspose ê¸°ë°˜ | ì´ë¯¸ì§€ ìƒì„± ê¸°ì¤€ ëª¨ë¸ |\n",
    "| **WGAN** | Wasserstein Distance | Mode Collapse ê°ì†Œ |\n",
    "| **WGAN-GP** | Gradient Penalty | í•™ìŠµ ì•ˆì •í™” |\n",
    "| **StyleGAN** | ìŠ¤íƒ€ì¼ ì œì–´ | ê³ í™”ì§ˆ ì–¼êµ´ ìƒì„± |\n",
    "| **CycleGAN** | ë¹„ìŒ ì´ë¯¸ì§€ ë³€í™˜ | ë„ë©”ì¸ ë³€í™˜ |\n",
    "| **Pix2Pix** | ìŒ ì´ë¯¸ì§€ ë³€í™˜ | ì¡°ê±´ë¶€ ì´ë¯¸ì§€ ìƒì„± |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8-0013-4013-8013-000000000013",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬\n",
    "\n",
    "### DCGAN êµ¬í˜„ í•µì‹¬ ìš”ì•½\n",
    "\n",
    "| ìš”ì†Œ | Generator | Discriminator |\n",
    "|------|-----------|---------------|\n",
    "| ì…ë ¥ | ë…¸ì´ì¦ˆ $z \\sim \\mathcal{N}(0,I)$ | ì´ë¯¸ì§€ (28Ã—28Ã—1) |\n",
    "| ì¶œë ¥ | ì´ë¯¸ì§€ (28Ã—28Ã—1) | í™•ë¥  ìŠ¤ì¹¼ë¼ [0,1] |\n",
    "| í•µì‹¬ ë ˆì´ì–´ | Conv2DTranspose | Conv2D |\n",
    "| ì •ê·œí™” | BatchNormalization | Dropout |\n",
    "| í™œì„±í™” | LeakyReLU + tanh(ì¶œë ¥) | LeakyReLU + sigmoid(ì¶œë ¥) |\n",
    "| ì†ì‹¤ ëª©í‘œ | D(G(z)) â†’ 1 | D(x) â†’ 1, D(G(z)) â†’ 0 |\n",
    "\n",
    "### í•™ìŠµ íŒ\n",
    "\n",
    "- **í•™ìŠµë¥ **: Gì™€ D ëª¨ë‘ `2e-4`, `beta_1=0.5` (Adam)\n",
    "- **í”½ì…€ ì •ê·œí™”**: ì…ë ¥ ì´ë¯¸ì§€ë¥¼ `[-1, 1]`ë¡œ (tanh ì¶œë ¥ê³¼ ì¼ì¹˜)\n",
    "- **ë°°ì¹˜ í¬ê¸°**: 64 ~ 256 (í¬ë©´ í•™ìŠµ ì•ˆì •ì )\n",
    "- **D ë¨¼ì € ì—…ë°ì´íŠ¸**: ì¼ë°˜ì ìœ¼ë¡œ D â†’ G ìˆœì„œë¡œ ì—…ë°ì´íŠ¸\n",
    "- **ê· í˜• ëª¨ë‹ˆí„°ë§**: G ì†ì‹¤ â‰ˆ D ì†ì‹¤ â‰ˆ `ln(2) â‰ˆ 0.693` ì´ ì´ìƒì "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}