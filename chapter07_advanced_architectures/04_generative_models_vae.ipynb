{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0001-4001-8001-000000000001",
   "metadata": {},
   "source": [
    "# Chapter 07-04: ìƒì„± ëª¨ë¸ â€” VAE (Variational Autoencoder)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Autoencoderì™€ VAEì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤\n",
    "- VAEì˜ ELBO ì†ì‹¤ í•¨ìˆ˜ì™€ ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­ì„ ì´í•´í•œë‹¤\n",
    "- MNIST ë°ì´í„°ë¡œ VAEë¥¼ êµ¬í˜„í•˜ê³  í•™ìŠµí•œë‹¤\n",
    "- ì ì¬ ê³µê°„(Latent Space)ì„ ì‹œê°í™”í•˜ê³  ë³´ê°„(Interpolation)ìœ¼ë¡œ ìƒˆ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. ìˆ˜ì‹ ì´í•´\n",
    "2. ê¸°ë³¸ Autoencoder êµ¬í˜„\n",
    "3. VAE í™•ì¥ (ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­)\n",
    "4. ì»¤ìŠ¤í…€ VAE ì†ì‹¤ í•¨ìˆ˜\n",
    "5. ì ì¬ ê³µê°„ ì‹œê°í™”\n",
    "6. ì ì¬ ê³µê°„ ë³´ê°„ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±\n",
    "7. ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ VAE ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ¨ VAE(ë³€ë¶„ ì˜¤í† ì¸ì½”ë”)ê°€ ë­ì˜ˆìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ê·¸ë¦¼ ê·¸ë¦¬ëŠ” ë²•ì„ ë°°ìš°ëŠ” ê²ƒì²˜ëŸ¼!\n",
    "> - **ì¸ì½”ë”**: ê·¸ë¦¼ì„ ë³´ê³  'ì´ ê·¸ë¦¼ì˜ íŠ¹ì§•'ì„ ê°„ëµíˆ ë©”ëª¨ (ì••ì¶•)\n",
    "> - **ë””ì½”ë”**: ë©”ëª¨ë§Œ ë³´ê³  ë‹¤ì‹œ ê·¸ë¦¼ì„ ì¬í˜„ (ë³µì›)\n",
    "> - **VAEì˜ íŠ¹ë³„í•¨**: ë©”ëª¨ë¥¼ 'í™•ë¥  ë¶„í¬'ë¡œ í‘œí˜„í•´ì„œ ìƒˆ ê·¸ë¦¼ ìƒì„± ê°€ëŠ¥!\n",
    "\n",
    "#### ğŸ”¢ í•µì‹¬ ì°¨ì´ â€” ì˜¤í† ì¸ì½”ë” vs VAE\n",
    "\n",
    "| | ì˜¤í† ì¸ì½”ë” | VAE |\n",
    "|--|-----------|-----|\n",
    "| ì ì¬ í‘œí˜„ | í•˜ë‚˜ì˜ ì  | í™•ë¥  ë¶„í¬ (í‰ê·  Î¼, ë¶„ì‚° Ïƒ) |\n",
    "| ìƒì„± ëŠ¥ë ¥ | ë³µì›ë§Œ ê°€ëŠ¥ | ìƒˆë¡œìš´ ë°ì´í„° ìƒì„± ê°€ëŠ¥! |\n",
    "| ì ì¬ ê³µê°„ | ë¶ˆê·œì¹™ | ë§¤ë„ëŸ½ê³  ì—°ì†ì  |\n",
    "\n",
    "```\n",
    "ì´ë¯¸ì§€ â†’ ì¸ì½”ë” â†’ [Î¼, Ïƒ] â†’ ìƒ˜í”Œë§ z ~ N(Î¼,ÏƒÂ²) â†’ ë””ì½”ë” â†’ ì¬êµ¬ì„±\n",
    "                                 â†‘\n",
    "                        ì´ ë¶€ë¶„ì´ VAEì˜ í•µì‹¬!\n",
    "                        zë¥¼ ì‚´ì§ ë°”ê¾¸ë©´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ ìƒì„±!\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ì†ì‹¤ í•¨ìˆ˜** = ì¬êµ¬ì„± ì†ì‹¤ + KL ë°œì‚°\n",
    "> - **ì¬êµ¬ì„± ì†ì‹¤**: ì›ë³¸ê³¼ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€\n",
    "> - **KL ë°œì‚°**: ì ì¬ ê³µê°„ì´ í‘œì¤€ ì •ê·œë¶„í¬ì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ (ê·œì¹™ì„± ë³´ì¥)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0002-4002-8002-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f'TensorFlow ë²„ì „: {tf.__version__}')\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0003-4003-8003-000000000003",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜ì‹ ì´í•´\n",
    "\n",
    "### VAE ELBO (Evidence Lower BOund)\n",
    "\n",
    "VAEëŠ” ë°ì´í„°ì˜ ë¡œê·¸ ìš°ë„(log-likelihood) $\\log p(x)$ë¥¼ ì§ì ‘ ìµœì í™”í•˜ê¸° ì–´ë ¤ìš°ë¯€ë¡œ,  \n",
    "**í•˜í•œ(Lower Bound)**ì¸ ELBOë¥¼ ìµœëŒ€í™”í•˜ì—¬ ê°„ì ‘ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤:\n",
    "\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))$$\n",
    "\n",
    "| í•­ | ì´ë¦„ | ì—­í•  |\n",
    "|----|------|------|\n",
    "| $\\mathbb{E}_{q(z|x)}[\\log p(x|z)]$ | ì¬êµ¬ì„± ì†ì‹¤ | ë””ì½”ë”ê°€ ì›ë³¸ $x$ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë³µì›í•˜ëŠ”ê°€ |\n",
    "| $D_{KL}(q(z|x) \\| p(z))$ | KL ë°œì‚° | ì¸ì½”ë”ì˜ ì ì¬ ë¶„í¬ë¥¼ í‘œì¤€ ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ |\n",
    "\n",
    "### KL ë°œì‚° (ê°€ìš°ì‹œì•ˆ ê°€ì •)\n",
    "\n",
    "$q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$, $p(z) = \\mathcal{N}(0, I)$ ê°€ì • ì‹œ ë‹«íŒ í˜•íƒœ:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum\\left(1 + \\log\\sigma^2 - \\mu^2 - \\sigma^2\\right)$$\n",
    "\n",
    "### ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­ (Reparameterization Trick)\n",
    "\n",
    "ìƒ˜í”Œë§ ì—°ì‚°ì€ ì—­ì „íŒŒê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ, ìƒ˜í”Œë§ì„ ê²°ì •ë¡ ì  ì—°ì‚°ìœ¼ë¡œ ë¶„ë¦¬í•œë‹¤:\n",
    "\n",
    "$$z = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I)$$\n",
    "\n",
    "- $\\epsilon$ì„ **í‘œì¤€ ì •ê·œë¶„í¬**ì—ì„œ ìƒ˜í”Œë§ (ê¸°ìš¸ê¸° ë¶ˆí•„ìš”)\n",
    "- $\\mu$ì™€ $\\sigma$ëŠ” ì¸ì½”ë”ì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì¶œë ¥ â†’ ì—­ì „íŒŒ ê°€ëŠ¥\n",
    "- $\\odot$: ì›ì†Œë³„ ê³±ì…ˆ(element-wise multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0004-4004-8004-000000000004",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ Autoencoder êµ¬í˜„ (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0005-4005-8005-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# ì •ê·œí™” [0, 255] â†’ [0, 1]\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test  = x_test.astype('float32')  / 255.0\n",
    "\n",
    "# í‰íƒ„í™”: (28, 28) â†’ (784,)\n",
    "x_train_flat = x_train.reshape(-1, 784)\n",
    "x_test_flat  = x_test.reshape(-1, 784)\n",
    "\n",
    "print(f'í›ˆë ¨ ë°ì´í„°: {x_train_flat.shape}')\n",
    "print(f'í…ŒìŠ¤íŠ¸ ë°ì´í„°: {x_test_flat.shape}')\n",
    "\n",
    "# ---- ê¸°ë³¸ Autoencoder ----\n",
    "LATENT_DIM_AE = 32  # ì ì¬ ë²¡í„° ì°¨ì›\n",
    "\n",
    "# Encoder: 784 â†’ 256 â†’ 128 â†’ latent_dim\n",
    "ae_encoder_input = keras.Input(shape=(784,), name='ae_encoder_input')\n",
    "ae_x = keras.layers.Dense(256, activation='relu')(ae_encoder_input)\n",
    "ae_x = keras.layers.Dense(128, activation='relu')(ae_x)\n",
    "ae_latent = keras.layers.Dense(LATENT_DIM_AE, activation='relu', name='latent')(ae_x)\n",
    "ae_encoder = keras.Model(ae_encoder_input, ae_latent, name='AE_Encoder')\n",
    "\n",
    "# Decoder: latent_dim â†’ 128 â†’ 256 â†’ 784\n",
    "ae_decoder_input = keras.Input(shape=(LATENT_DIM_AE,), name='ae_decoder_input')\n",
    "ae_d = keras.layers.Dense(128, activation='relu')(ae_decoder_input)\n",
    "ae_d = keras.layers.Dense(256, activation='relu')(ae_d)\n",
    "ae_output = keras.layers.Dense(784, activation='sigmoid', name='ae_output')(ae_d)  # sigmoid â†’ [0,1]\n",
    "ae_decoder = keras.Model(ae_decoder_input, ae_output, name='AE_Decoder')\n",
    "\n",
    "# Autoencoder ì¡°í•©\n",
    "ae_input = keras.Input(shape=(784,))\n",
    "ae_encoded = ae_encoder(ae_input)\n",
    "ae_reconstructed = ae_decoder(ae_encoded)\n",
    "autoencoder = keras.Model(ae_input, ae_reconstructed, name='Autoencoder')\n",
    "\n",
    "autoencoder.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy'  # í”½ì…€ë³„ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼\n",
    ")\n",
    "\n",
    "print('\\nê¸°ë³¸ Autoencoder êµ¬ì¡°:')\n",
    "ae_encoder.summary()\n",
    "\n",
    "# í•™ìŠµ\n",
    "print('\\nAutoencoder í•™ìŠµ ì‹œì‘...')\n",
    "ae_history = autoencoder.fit(\n",
    "    x_train_flat, x_train_flat,  # ì…ë ¥ê³¼ íƒ€ê²Ÿì´ ë™ì¼\n",
    "    batch_size=256,\n",
    "    epochs=10,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ì¬êµ¬ì„± ê²°ê³¼ ì‹œê°í™”\n",
    "n_samples = 8\n",
    "test_samples = x_test_flat[:n_samples]\n",
    "reconstructed = autoencoder.predict(test_samples, verbose=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(16, 4))\n",
    "for i in range(n_samples):\n",
    "    axes[0, i].imshow(test_samples[i].reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('ì›ë³¸', fontsize=10, pad=3)\n",
    "    axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('ì¬êµ¬ì„±', fontsize=10, pad=3)\n",
    "\n",
    "plt.suptitle('Autoencoder ì¬êµ¬ì„± ê²°ê³¼', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0006-4006-8006-000000000006",
   "metadata": {},
   "source": [
    "## 3. VAE í™•ì¥ â€” Sampling ë ˆì´ì–´(ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0007-4007-8007-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 2  # 2D ì‹œê°í™”ë¥¼ ìœ„í•´ ì ì¬ ì°¨ì› = 2\n",
    "\n",
    "# ---- Sampling ë ˆì´ì–´ (ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­ êµ¬í˜„) ----\n",
    "class SamplingLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­: z = mu + sigma * epsilon\n",
    "    epsilon ~ N(0, I)\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        mu, log_var = inputs  # ì¸ì½”ë”ë¡œë¶€í„° í‰ê· ê³¼ ë¡œê·¸ ë¶„ì‚° ìˆ˜ì‹ \n",
    "        \n",
    "        # ë°°ì¹˜ í¬ê¸°ì™€ ì ì¬ ì°¨ì› ì¶”ì¶œ\n",
    "        batch = tf.shape(mu)[0]\n",
    "        dim   = tf.shape(mu)[1]\n",
    "        \n",
    "        # í‘œì¤€ ì •ê·œ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§ (ì—­ì „íŒŒ ë¶ˆí•„ìš”)\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        \n",
    "        # z = mu + exp(0.5 * log_var) * epsilon\n",
    "        # sigma = exp(0.5 * log_var) = sqrt(exp(log_var)) = sqrt(var)\n",
    "        return mu + tf.exp(0.5 * log_var) * epsilon\n",
    "\n",
    "\n",
    "# ---- VAE Encoder ----\n",
    "encoder_input = keras.Input(shape=(784,), name='encoder_input')\n",
    "enc_h1 = keras.layers.Dense(256, activation='relu')(encoder_input)\n",
    "enc_h2 = keras.layers.Dense(128, activation='relu')(enc_h1)\n",
    "\n",
    "# í‰ê· (mu)ê³¼ ë¡œê·¸ ë¶„ì‚°(log_var) ë™ì‹œ ì¶œë ¥\n",
    "z_mu      = keras.layers.Dense(LATENT_DIM, name='z_mu')(enc_h2)       # í‰ê· \n",
    "z_log_var = keras.layers.Dense(LATENT_DIM, name='z_log_var')(enc_h2)  # ë¡œê·¸ ë¶„ì‚°\n",
    "\n",
    "# ì¬íŒŒë¼ë¯¸í„°í™” ìƒ˜í”Œë§\n",
    "z = SamplingLayer(name='z_sampling')([z_mu, z_log_var])\n",
    "\n",
    "# ì¸ì½”ë” ëª¨ë¸ (mu, log_var, z ëª¨ë‘ ì¶œë ¥)\n",
    "vae_encoder = keras.Model(encoder_input, [z_mu, z_log_var, z], name='VAE_Encoder')\n",
    "\n",
    "# ---- VAE Decoder ----\n",
    "decoder_input = keras.Input(shape=(LATENT_DIM,), name='decoder_input')\n",
    "dec_h1 = keras.layers.Dense(128, activation='relu')(decoder_input)\n",
    "dec_h2 = keras.layers.Dense(256, activation='relu')(dec_h1)\n",
    "decoder_output = keras.layers.Dense(784, activation='sigmoid', name='decoder_output')(dec_h2)\n",
    "\n",
    "vae_decoder = keras.Model(decoder_input, decoder_output, name='VAE_Decoder')\n",
    "\n",
    "print('VAE Encoder êµ¬ì¡°:')\n",
    "vae_encoder.summary()\n",
    "print('\\nVAE Decoder êµ¬ì¡°:')\n",
    "vae_decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0008-4008-8008-000000000008",
   "metadata": {},
   "source": [
    "## 4. ì»¤ìŠ¤í…€ VAE ì†ì‹¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0009-4009-8009-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder\n",
    "    ì»¤ìŠ¤í…€ train_stepìœ¼ë¡œ ELBO ì†ì‹¤(ì¬êµ¬ì„± + KL) ê³„ì‚°\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        # ì†ì‹¤ ì¶”ì ê¸°\n",
    "        self.total_loss_tracker    = keras.metrics.Mean(name='total_loss')\n",
    "        self.recon_loss_tracker    = keras.metrics.Mean(name='recon_loss')\n",
    "        self.kl_loss_tracker       = keras.metrics.Mean(name='kl_loss')\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker, self.recon_loss_tracker, self.kl_loss_tracker]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x = data  # VAEëŠ” ì…ë ¥ = íƒ€ê²Ÿ\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # ì¸ì½”ë” ì‹¤í–‰: mu, log_var, z ì¶”ì¶œ\n",
    "            z_mu, z_log_var, z = self.encoder(x, training=True)\n",
    "            \n",
    "            # ë””ì½”ë” ì‹¤í–‰: zì—ì„œ ì´ë¯¸ì§€ ì¬êµ¬ì„±\n",
    "            x_reconstructed = self.decoder(z, training=True)\n",
    "            \n",
    "            # ì¬êµ¬ì„± ì†ì‹¤: í”½ì…€ë³„ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ì˜ í•©\n",
    "            recon_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(x, x_reconstructed),\n",
    "                    axis=-1  # 784ê°œ í”½ì…€ì— ëŒ€í•´ í•©ì‚°\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # KL ë°œì‚° ì†ì‹¤: -0.5 * sum(1 + log_var - mu^2 - exp(log_var))\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    1 + z_log_var - tf.square(z_mu) - tf.exp(z_log_var),\n",
    "                    axis=-1  # ì ì¬ ì°¨ì›ì— ëŒ€í•´ í•©ì‚°\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # ì „ì²´ ì†ì‹¤ = ì¬êµ¬ì„± ì†ì‹¤ + KL ì†ì‹¤\n",
    "            total_loss = recon_loss + kl_loss\n",
    "        \n",
    "        # ì—­ì „íŒŒ\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        # ì†ì‹¤ ì—…ë°ì´íŠ¸\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.recon_loss_tracker.update_state(recon_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            'total_loss': self.total_loss_tracker.result(),\n",
    "            'recon_loss': self.recon_loss_tracker.result(),\n",
    "            'kl_loss':    self.kl_loss_tracker.result()\n",
    "        }\n",
    "\n",
    "\n",
    "# VAE ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í•™ìŠµ\n",
    "vae = VAE(vae_encoder, vae_decoder, name='VAE')\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "print('VAE í•™ìŠµ ì‹œì‘...')\n",
    "vae_history = vae.fit(\n",
    "    x_train_flat,\n",
    "    batch_size=256,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ì†ì‹¤ ê³¡ì„  ì‹œê°í™”\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(vae_history.history['total_loss'], label='ì „ì²´ ì†ì‹¤', linewidth=2)\n",
    "ax.plot(vae_history.history['recon_loss'], label='ì¬êµ¬ì„± ì†ì‹¤', linewidth=2, linestyle='--')\n",
    "ax.plot(vae_history.history['kl_loss'],   label='KL ë°œì‚° ì†ì‹¤', linewidth=2, linestyle=':')\n",
    "ax.set_xlabel('ì—í¬í¬', fontsize=11)\n",
    "ax.set_ylabel('ì†ì‹¤', fontsize=11)\n",
    "ax.set_title('VAE í•™ìŠµ ì†ì‹¤ (ì¬êµ¬ì„± + KL ë°œì‚°)', fontsize=12)\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0010-4010-8010-000000000010",
   "metadata": {},
   "source": [
    "## 5. ì ì¬ ê³µê°„ ì‹œê°í™” (2D ì ì¬ ê³µê°„)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0011-4011-8011-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©\n",
    "z_mu_test, z_log_var_test, z_test = vae_encoder.predict(x_test_flat, verbose=0)\n",
    "\n",
    "print(f'í…ŒìŠ¤íŠ¸ ì ì¬ ë²¡í„° shape: {z_test.shape}')  # (10000, 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# --- ì ì¬ ê³µê°„ ì‚°ì ë„ (í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ) ---\n",
    "ax = axes[0]\n",
    "scatter = ax.scatter(\n",
    "    z_test[:, 0], z_test[:, 1],\n",
    "    c=y_test, cmap='tab10',\n",
    "    alpha=0.5, s=5\n",
    ")\n",
    "plt.colorbar(scatter, ax=ax, label='ìˆ«ì í´ë˜ìŠ¤')\n",
    "ax.set_xlabel('ì ì¬ ì°¨ì› 1 ($z_1$)', fontsize=11)\n",
    "ax.set_ylabel('ì ì¬ ì°¨ì› 2 ($z_2$)', fontsize=11)\n",
    "ax.set_title('VAE 2D ì ì¬ ê³µê°„ (í…ŒìŠ¤íŠ¸ ë°ì´í„°)', fontsize=12)\n",
    "\n",
    "# ê° í´ë˜ìŠ¤ ì¤‘ì‹¬ì ì— ë ˆì´ë¸” í‘œì‹œ\n",
    "for digit in range(10):\n",
    "    mask = y_test == digit\n",
    "    center_x = z_test[mask, 0].mean()\n",
    "    center_y = z_test[mask, 1].mean()\n",
    "    ax.annotate(str(digit), (center_x, center_y),\n",
    "                fontsize=14, fontweight='bold', ha='center', va='center',\n",
    "                color='white',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='black', alpha=0.6))\n",
    "\n",
    "# --- ì ì¬ ê³µê°„ ê²©ì â†’ ë””ì½”ë”©ëœ ì´ë¯¸ì§€ ---\n",
    "ax2 = axes[1]\n",
    "\n",
    "# ì ì¬ ê³µê°„ì˜ ë²”ìœ„ ì„¤ì • ([-3, 3] Ã— [-3, 3])\n",
    "n_grid = 12\n",
    "grid_range = np.linspace(-3, 3, n_grid)\n",
    "\n",
    "# ê²©ì í¬ì¸íŠ¸ ìƒì„± â†’ ë””ì½”ë”© â†’ ì´ë¯¸ì§€ ë°°ì¹˜\n",
    "canvas = np.zeros((28 * n_grid, 28 * n_grid))\n",
    "\n",
    "for i, z2 in enumerate(grid_range[::-1]):\n",
    "    for j, z1 in enumerate(grid_range):\n",
    "        z_point = np.array([[z1, z2]])  # (1, 2)\n",
    "        decoded = vae_decoder.predict(z_point, verbose=0)  # (1, 784)\n",
    "        img = decoded[0].reshape(28, 28)\n",
    "        canvas[i*28:(i+1)*28, j*28:(j+1)*28] = img\n",
    "\n",
    "ax2.imshow(canvas, cmap='gray')\n",
    "ax2.set_title(f'ì ì¬ ê³µê°„ ê²©ì ë””ì½”ë”© ({n_grid}Ã—{n_grid})', fontsize=12)\n",
    "ax2.set_xlabel('$z_1$', fontsize=11)\n",
    "ax2.set_ylabel('$z_2$', fontsize=11)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.suptitle('VAE ì ì¬ ê³µê°„ ë¶„ì„', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0012-4012-8012-000000000012",
   "metadata": {},
   "source": [
    "## 6. ì ì¬ ê³µê°„ ë³´ê°„(Interpolation)ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7-0013-4013-8013-000000000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_latent(z_start, z_end, n_steps=10):\n",
    "    \"\"\"\n",
    "    ë‘ ì ì¬ ë²¡í„° ì‚¬ì´ë¥¼ ì„ í˜• ë³´ê°„\n",
    "    z = (1 - alpha) * z_start + alpha * z_end, alpha in [0, 1]\n",
    "    \"\"\"\n",
    "    alphas = np.linspace(0, 1, n_steps)\n",
    "    interpolated = np.array([\n",
    "        (1 - alpha) * z_start + alpha * z_end\n",
    "        for alpha in alphas\n",
    "    ])  # (n_steps, latent_dim)\n",
    "    return interpolated\n",
    "\n",
    "\n",
    "# ê° ìˆ«ì ìŒì˜ ì ì¬ ë²¡í„° ì¶”ì¶œ (í´ë˜ìŠ¤ í‰ê· ê°’ ì‚¬ìš©)\n",
    "digit_pairs = [(0, 1), (2, 7), (3, 8), (4, 9)]\n",
    "n_steps = 12\n",
    "\n",
    "fig, axes = plt.subplots(len(digit_pairs), n_steps, figsize=(20, 7))\n",
    "\n",
    "for row, (digit_a, digit_b) in enumerate(digit_pairs):\n",
    "    # ê° í´ë˜ìŠ¤ì˜ ì ì¬ ë²¡í„° í‰ê·  ê³„ì‚°\n",
    "    z_a = z_mu_test[y_test == digit_a].mean(axis=0)  # (2,)\n",
    "    z_b = z_mu_test[y_test == digit_b].mean(axis=0)  # (2,)\n",
    "    \n",
    "    # ì„ í˜• ë³´ê°„\n",
    "    z_interp = interpolate_latent(z_a, z_b, n_steps)  # (n_steps, 2)\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    decoded_images = vae_decoder.predict(z_interp, verbose=0)  # (n_steps, 784)\n",
    "    \n",
    "    for col in range(n_steps):\n",
    "        ax = axes[row, col]\n",
    "        ax.imshow(decoded_images[col].reshape(28, 28), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # ì‹œì‘/ë ë ˆì´ë¸”\n",
    "        if col == 0:\n",
    "            ax.set_title(f'{digit_a}', fontsize=11, fontweight='bold', color='blue')\n",
    "        elif col == n_steps - 1:\n",
    "            ax.set_title(f'{digit_b}', fontsize=11, fontweight='bold', color='red')\n",
    "\n",
    "plt.suptitle('ì ì¬ ê³µê°„ ì„ í˜• ë³´ê°„: ë‘ ìˆ«ì ì‚¬ì´ì˜ ì—°ì†ì  ë³€í™˜', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('ë³´ê°„ ê²°ê³¼: ì ì¬ ê³µê°„ì´ ì—°ì†ì ì´ê³  ì˜ë¯¸ìˆëŠ” êµ¬ì¡°ë¥¼ ê°€ì§ì„ í™•ì¸')\n",
    "\n",
    "# ìˆœìˆ˜ ìƒì„±: í‘œì¤€ ì •ê·œ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§\n",
    "print('\\nëœë¤ ìƒì„± ì´ë¯¸ì§€ (z ~ N(0,I)):')\n",
    "n_generated = 16\n",
    "z_random = np.random.randn(n_generated, LATENT_DIM)  # í‘œì¤€ ì •ê·œ ìƒ˜í”Œë§\n",
    "generated = vae_decoder.predict(z_random, verbose=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated[i].reshape(28, 28), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('VAE ëœë¤ ìƒì„± ì´ë¯¸ì§€ ($z \\\\sim \\\\mathcal{N}(0,I)$)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6a7-0014-4014-8014-000000000014",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬\n",
    "\n",
    "### Autoencoder vs VAE ë¹„êµ\n",
    "\n",
    "| êµ¬ë¶„ | Autoencoder | VAE |\n",
    "|------|------------|-----|\n",
    "| ì ì¬ ê³µê°„ | ë¹„êµ¬ì¡°ì  (ì ) | í™•ë¥  ë¶„í¬ $\\mathcal{N}(\\mu, \\sigma^2)$ |\n",
    "| ì†ì‹¤ í•¨ìˆ˜ | ì¬êµ¬ì„± ì†ì‹¤ë§Œ | ì¬êµ¬ì„± ì†ì‹¤ + KL ë°œì‚° |\n",
    "| ìƒì„± ê°€ëŠ¥ì„± | ë‚®ìŒ (ìƒˆ ì  ìƒ˜í”Œë§ ì–´ë ¤ì›€) | ë†’ìŒ ($z \\sim \\mathcal{N}(0,I)$ ìƒ˜í”Œë§) |\n",
    "| ì ì¬ ê³µê°„ ì—°ì†ì„± | ë³´ì¥ ì—†ìŒ | ì—°ì†ì , ë§¤ë„ëŸ¬ìš´ ë³´ê°„ ê°€ëŠ¥ |\n",
    "| ì—­ì „íŒŒ ê°€ëŠ¥ì„± | ê°€ëŠ¥ | ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­ìœ¼ë¡œ ê°€ëŠ¥ |\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "- **ELBO**: ì§ì ‘ ìµœì í™” ë¶ˆê°€ëŠ¥í•œ ë¡œê·¸ ìš°ë„ì˜ í•˜í•œ â†’ ìµœëŒ€í™”\n",
    "- **ì¬íŒŒë¼ë¯¸í„°í™” íŠ¸ë¦­**: ìƒ˜í”Œë§ì„ ê²°ì •ë¡ ì  ì—°ì‚°ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì—­ì „íŒŒ ê°€ëŠ¥\n",
    "- **KL ë°œì‚°**: ì ì¬ ë¶„í¬ë¥¼ í‘œì¤€ ì •ê·œë¶„í¬ë¡œ ì •ê·œí™” â†’ ì ì¬ ê³µê°„ì˜ ì—°ì†ì„± ë³´ì¥\n",
    "- **ë³´ê°„**: ì ì¬ ê³µê°„ì´ ë§¤ë„ëŸ¬ìš°ë¯€ë¡œ ë‘ ì  ì‚¬ì´ì˜ ì„ í˜• ë³´ê°„ì´ ì˜ë¯¸ìˆëŠ” ì „í™˜ ìƒì„±\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "**Chapter 07-05: GAN (Generative Adversarial Networks)** â€” ìƒì„±ìì™€ íŒë³„ìì˜ ì ëŒ€ì  í•™ìŠµìœ¼ë¡œ ë”ìš± ì‚¬ì‹¤ì ì¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}