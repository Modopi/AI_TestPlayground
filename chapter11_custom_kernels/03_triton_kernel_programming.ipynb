{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11-03: Triton ì»¤ë„ í”„ë¡œê·¸ë˜ë°\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- OpenAI Tritonì˜ **Block-level í”„ë¡œê·¸ë˜ë° ëª¨ë¸**ê³¼ CUDA ìŠ¤ë ˆë“œ ëª¨ë¸ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤\n",
    "- Tritonì˜ **Block Pointer**ì™€ **íƒ€ì¼ ë§¤í•‘** íŒ¨ëŸ¬ë‹¤ì„ìœ¼ë¡œ Fused Softmax ì»¤ë„ì„ ì„¤ê³„í•œë‹¤\n",
    "- **Online Softmax** ì•Œê³ ë¦¬ì¦˜ì„ ìˆ˜ì‹ìœ¼ë¡œ ë„ì¶œí•˜ê³ , ë‹¨ì¼ íŒ¨ìŠ¤ì—ì„œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ Softmax ê³„ì‚° ì›ë¦¬ë¥¼ ì¦ëª…í•œë‹¤\n",
    "- Triton ìŠ¤íƒ€ì¼ì˜ Fused Softmaxë¥¼ NumPyë¡œ ì‹œë®¬ë ˆì´ì…˜í•˜ê³ , í‘œì¤€ ë°©ì‹ ëŒ€ë¹„ ë©”ëª¨ë¦¬ ì ‘ê·¼ íšŸìˆ˜ë¥¼ ë¹„êµí•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Online Softmaxì™€ Kernel Fusion](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [CUDA ëª¨ë¸ vs Triton ëª¨ë¸ ë¹„êµ](#2.-CUDA-vs-Triton)\n",
    "3. [í‘œì¤€ Softmaxì˜ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ ë¶„ì„](#3.-í‘œì¤€-Softmax-ë¶„ì„)\n",
    "4. [Online Softmax ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„](#4.-Online-Softmax-êµ¬í˜„)\n",
    "5. [Fused Softmax ì‹œë®¬ë ˆì´ì…˜](#5.-Fused-Softmax-ì‹œë®¬ë ˆì´ì…˜)\n",
    "6. [Triton ìŠ¤íƒ€ì¼ FlashAttention í•µì‹¬ ì›ë¦¬](#6.-FlashAttention-ì›ë¦¬)\n",
    "7. [ì •ë¦¬ ë° ì—°ìŠµ ë¬¸ì œ](#7.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### í‘œì¤€ Softmaxì˜ 3-pass ë©”ëª¨ë¦¬ ì ‘ê·¼\n",
    "\n",
    "ì…ë ¥ ë²¡í„° $x \\in \\mathbb{R}^S$ì— ëŒ€í•œ í‘œì¤€ Softmax ê³„ì‚°:\n",
    "\n",
    "1. **Pass 1 (Read)**: $m = \\max_i x_i$ â€” ìˆ˜ì¹˜ ì•ˆì •í™”ìš© ìµœëŒ“ê°’\n",
    "2. **Pass 2 (Read + Write)**: $e_i = \\exp(x_i - m)$, $d = \\sum_i e_i$\n",
    "3. **Pass 3 (Read + Write)**: $y_i = e_i / d$\n",
    "\n",
    "**ì´ Global Memory ì ‘ê·¼**: $5S$ bytes (3íšŒ ì½ê¸° + 2íšŒ ì“°ê¸°)\n",
    "\n",
    "### Online Softmax: ë‹¨ì¼ íŒ¨ìŠ¤ë¡œ í†µí•©!\n",
    "\n",
    "ìƒˆ ì›ì†Œ $x_j$ë¥¼ ì²˜ë¦¬í•  ë•Œ ê¸°ì¡´ í†µê³„ëŸ‰ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$m_j = \\max(m_{j-1}, x_j)$$\n",
    "\n",
    "$$d_j = d_{j-1} \\cdot \\exp(m_{j-1} - m_j) + \\exp(x_j - m_j)$$\n",
    "\n",
    "ì´ë ‡ê²Œ í•˜ë©´ **Pass 1ê³¼ 2ê°€ í•©ì³ì ¸** 2-pass (ë˜ëŠ” ë¸”ë¡ ë‹¨ìœ„ 1-pass)ë¡œ êµ¬í˜„ ê°€ëŠ¥!\n",
    "\n",
    "**ìˆ˜í•™ì  ë“±ê°€ì„± ì¦ëª…**:\n",
    "\n",
    "$$d_S = \\sum_{i=1}^S \\exp(x_i - m_S)$$\n",
    "\n",
    "ì™œëƒí•˜ë©´ $d_j$ì˜ ì¬ê·€ì‹ì—ì„œ $m_j$ê°€ ë‹¨ì¡° ì¦ê°€í•˜ë¯€ë¡œ, ì´ì „ í•­ë“¤ì„ $\\exp(m_{old} - m_{new})$ë¡œ ë³´ì •í•˜ë©´ í•­ìƒ ì˜¬ë°”ë¥¸ ë¶„ëª¨ê°€ ìœ ì§€ë©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”„ Kernel Fusionì´ ì™œ ì¤‘ìš”í•œê°€ìš”?\n",
    "\n",
    "> ì—¬ëŸ¬ ìš”ë¦¬(Add, Softmax, Dropout)ë¥¼ ê°ê° ë”°ë¡œ ë§Œë“¤ë©´  \n",
    "> ë§¤ ìš”ë¦¬ë§ˆë‹¤ ì¬ë£Œ(ë°ì´í„°)ë¥¼ ì°½ê³ (ë©”ëª¨ë¦¬)ì—ì„œ êº¼ë‚´ì•¼ í•´ìš”.\n",
    ">\n",
    "> **Fusion**: ì„¸ ìš”ë¦¬ë¥¼ í•œ ë²ˆì— í•´ê²°! ì¬ë£Œë¥¼ í•œ ë²ˆë§Œ ê°€ì ¸ì˜¤ê³ , í•œ ë²ˆì— ì„¸ ê°€ì§€ë¥¼ ë‹¤ ì²˜ë¦¬.\n",
    "> â†’ ì°½ê³ (ë©”ëª¨ë¦¬) ì™•ë³µ íšŸìˆ˜ê°€ 1/3ë¡œ ì¤„ì–´ë“œëŠ” ì…ˆ!\n",
    "\n",
    "#### ğŸ“Š Online Softmaxê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ì¼ë°˜ SoftmaxëŠ” ë¨¼ì € ì „ì²´ ì ìˆ˜ë¥¼ ë‹¤ ë³´ê³ (Pass 1), ê·¸ ë‹¤ìŒ expë¥¼ ê³„ì‚°(Pass 2)í•´ìš”.  \n",
    "> Online SoftmaxëŠ” í•œ ë²ˆ í›‘ìœ¼ë©´ì„œ **ë™ì‹œì—** ìµœëŒ“ê°’ê³¼ í•©ì‚°ì„ ì—…ë°ì´íŠ¸í•´ìš”!\n",
    "> ë§ˆì¹˜ ë¬¼ê±´ ê°€ê²©ì„ ë³´ë©´ì„œ ë™ì‹œì— ìµœê³ ê°€ì™€ í•©ê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì²˜ëŸ¼ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: ë©”ëª¨ë¦¬ ì ‘ê·¼ ë¹„êµ\n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ $S=4096$, FP16ì—ì„œ í‘œì¤€ Softmax(3-pass)ì™€ Fused Softmax(2-pass+Write 1íšŒ)ì˜  \n",
    "HBM ì½ê¸° ë°”ì´íŠ¸ ìˆ˜ë¥¼ ê°ê° ê³„ì‚°í•˜ë¼.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "FP16 = 2 bytes/element, $S=4096$\n",
    "\n",
    "- **í‘œì¤€ (3-pass)**: ì½ê¸° 3íšŒ + ì“°ê¸° 2íšŒ = $5 \\times 4096 \\times 2 = 40.96$ KB\n",
    "- **Fused (1-pass)**: ì½ê¸° 1íšŒ + ì“°ê¸° 1íšŒ = $2 \\times 4096 \\times 2 = 16.38$ KB\n",
    "- ì ˆì•½: **2.5ë°°** HBM ëŒ€ì—­í­ ì ˆê°\n",
    "\n",
    "(FlashAttentionì—ì„œëŠ” QK^T í–‰ë ¬ ìì²´ë„ HBM ì•ˆ ì”€ â†’ í›¨ì”¬ í° ì ˆê° íš¨ê³¼)\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: Online Softmax ì •í™•ì„± ê²€ì¦\n",
    "\n",
    "$x = [3.0, 1.0, 4.0, 1.5, 2.0]$ì— ëŒ€í•´ Online Softmaxë¥¼ ì†ìœ¼ë¡œ ë‹¨ê³„ë³„ ê³„ì‚°í•˜ê³   \n",
    "í‘œì¤€ Softmaxì™€ ê²°ê³¼ë¥¼ ë¹„êµí•˜ë¼. (ê³„ì‚° ì§€ì› í—ˆìš©)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "í‘œì¤€: $m=4.0$, $d = e^{-1}+e^{-3}+e^{0}+e^{-2.5}+e^{-2} \\approx 0.368+0.050+1.0+0.082+0.135 = 1.635$  \n",
    "â†’ $y \\approx [0.225, 0.031, 0.612, 0.050, 0.083]$\n",
    "\n",
    "Online: $m_1=3.0$, $d_1=1.0$; $m_2=3.0$, $d_2=1+e^{-2}=1.135$;  \n",
    "$m_3=4.0$, $d_3=1.135\\times e^{-1}+1=1.418$; ... (ìµœì¢… ê²°ê³¼ ë™ì¼)\n",
    "\n",
    "**ìˆ˜ì¹˜ì ìœ¼ë¡œ ë™ì¼**í•œ ê²°ê³¼ â†’ Online Softmax ì˜¬ë°”ë¦„ âœ…\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CUDA vs Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# CUDA ëª¨ë¸ vs Triton Block ëª¨ë¸ ë¹„êµ ì„¤ëª…\n",
    "# ---------------------------------------------------\n",
    "\n",
    "comparison = '''\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚       CUDA ìŠ¤ë ˆë“œ ëª¨ë¸          â”‚      Triton Block ëª¨ë¸              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ í”„ë¡œê·¸ë˜ë° ë‹¨ìœ„: ê°œë³„ ìŠ¤ë ˆë“œ    â”‚ í”„ë¡œê·¸ë˜ë° ë‹¨ìœ„: ìŠ¤ë ˆë“œ ë¸”ë¡       â”‚\n",
    "â”‚                                 â”‚                                    â”‚\n",
    "â”‚ threadIdx.x / blockIdx.x ì§ì ‘  â”‚ tl.program_id(axis=0)              â”‚\n",
    "â”‚ ë‹¤ë¤„ì•¼ í•¨                       â”‚ (ë¸”ë¡ IDë§Œ ì‹ ê²½ ì”€)                â”‚\n",
    "â”‚                                 â”‚                                    â”‚\n",
    "â”‚ Shared Memoryë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬ â”‚ Shared Memory ìë™ ê´€ë¦¬!           â”‚\n",
    "â”‚ â†’ __shared__, bank conflict     â”‚ â†’ tl.load/store, mask ê¸°ë°˜         â”‚\n",
    "â”‚                                 â”‚                                    â”‚\n",
    "â”‚ íƒ€ì´ì§„: ê°œë°œìê°€ ì§ì ‘ ì„¤ê³„      â”‚ íƒ€ì´ë§: ë¸”ë¡ í¬ê¸°ë¡œ ìë™ ê²°ì •       â”‚\n",
    "â”‚                                 â”‚                                    â”‚\n",
    "â”‚ ì–¸ì–´: CUDA C++                  â”‚ ì–¸ì–´: Python                       â”‚\n",
    "â”‚ ì»´íŒŒì¼: nvcc                    â”‚ ì»´íŒŒì¼: @triton.jit ìë™            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ ë‚œì´ë„: â˜…â˜…â˜…â˜…â˜…                 â”‚ ë‚œì´ë„: â˜…â˜…â˜…                        â”‚\n",
    "â”‚ ìœ ì—°ì„±: â˜…â˜…â˜…â˜…â˜…                 â”‚ ìœ ì—°ì„±: â˜…â˜…â˜…â˜…                       â”‚\n",
    "â”‚ ìµœì í™”: â˜…â˜…â˜…â˜…â˜…                 â”‚ ìµœì í™”: â˜…â˜…â˜…â˜… (~90% of cuBLAS)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "'''\n",
    "print(comparison)\n",
    "\n",
    "triton_softmax_pseudocode = '''\n",
    "@triton.jit\n",
    "def softmax_kernel(X_ptr, Y_ptr, stride, n_cols, BLOCK: tl.constexpr):\n",
    "    # â‘  í”„ë¡œê·¸ë¨(ë¸”ë¡) IDë¡œ ì²˜ë¦¬í•  í–‰ ê²°ì •\n",
    "    row = tl.program_id(0)\n",
    "    row_start = row * stride\n",
    "\n",
    "    # â‘¡ ì—´ ì˜¤í”„ì…‹ ê³„ì‚° (ë²¡í„°í™”)\n",
    "    cols = tl.arange(0, BLOCK)\n",
    "    mask = cols < n_cols\n",
    "\n",
    "    # â‘¢ ë°ì´í„° ë¡œë“œ (Shared Memory ìë™ ê´€ë¦¬)\n",
    "    x = tl.load(X_ptr + row_start + cols, mask=mask, other=-float(\\'inf\\'))\n",
    "\n",
    "    # â‘£ ìˆ˜ì¹˜ ì•ˆì •í™” Softmax (1-pass!)\n",
    "    x_max = tl.max(x, axis=0)          # Pass 1: ìµœëŒ“ê°’\n",
    "    x = x - x_max                      # ë¹¼ê¸°ë¡œ ì•ˆì •í™”\n",
    "    exp_x = tl.exp(x)                  # Pass 2: exp\n",
    "    sum_exp = tl.sum(exp_x, axis=0)    # í•©ì‚° (Pass 2ì™€ í•©ì¹¨!)\n",
    "    y = exp_x / sum_exp                # ì •ê·œí™”\n",
    "\n",
    "    # â‘¤ ê²°ê³¼ ì €ì¥ (1íšŒ ì“°ê¸°)\n",
    "    tl.store(Y_ptr + row_start + cols, y, mask=mask)\n",
    "'''\n",
    "print(\"Triton Fused Softmax ì»¤ë„ ì˜ì‚¬ì½”ë“œ:\")\n",
    "print(triton_softmax_pseudocode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Online Softmax êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Online Softmax ì™„ì „ êµ¬í˜„ ë° ê²€ì¦\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def softmax_standard(x):\n",
    "    \"\"\"í‘œì¤€ Softmax (ìˆ˜ì¹˜ ì•ˆì •í™” í¬í•¨, 3-pass)\"\"\"\n",
    "    # Pass 1: ìµœëŒ“ê°’ ê³„ì‚°\n",
    "    m = np.max(x, axis=-1, keepdims=True)\n",
    "    # Pass 2: exp ê³„ì‚° + í•©ì‚°\n",
    "    e = np.exp(x - m)\n",
    "    d = np.sum(e, axis=-1, keepdims=True)\n",
    "    # Pass 3: ì •ê·œí™”\n",
    "    return e / d\n",
    "\n",
    "\n",
    "def online_softmax_elementwise(x):\n",
    "    \"\"\"\n",
    "    Online Softmax: ì›ì†Œ í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ë©° m, d ë™ì‹œ ì—…ë°ì´íŠ¸.\n",
    "    í”Œë˜ì‹œì–´í…ì…˜(FlashAttention)ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜.\n",
    "    \n",
    "    ì¬ê·€ ê´€ê³„:\n",
    "      m_j = max(m_{j-1}, x_j)\n",
    "      d_j = d_{j-1} * exp(m_{j-1} - m_j) + exp(x_j - m_j)\n",
    "    \"\"\"\n",
    "    m = -np.inf\n",
    "    d = 0.0\n",
    "    exp_x = np.zeros_like(x)\n",
    "\n",
    "    # 1-pass: mê³¼ dë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸\n",
    "    for i, xi in enumerate(x):\n",
    "        m_new = max(m, xi)\n",
    "        d = d * np.exp(m - m_new) + np.exp(xi - m_new)\n",
    "        m = m_new\n",
    "        exp_x[i] = np.exp(xi - m)  # í˜„ ì‹œì  m ê¸°ì¤€ exp\n",
    "\n",
    "    # ìµœì¢… ì •ê·œí™” (exp ê°’ë“¤ë„ í˜„ì¬ m ê¸°ì¤€ìœ¼ë¡œ ë³´ì •ë¨)\n",
    "    # ì£¼ì˜: ìœ„ì—ì„œ ì €ì¥ëœ exp_xëŠ” ì¤‘ê°„ m ê¸°ì¤€ì´ë¯€ë¡œ ìµœì¢… mìœ¼ë¡œ ì¬ì¡°ì • í•„ìš”\n",
    "    exp_x_final = np.exp(x - m)  # ìµœì¢… mìœ¼ë¡œ í†µì¼\n",
    "    return exp_x_final / d\n",
    "\n",
    "\n",
    "def online_softmax_blocked(x, block_size=8):\n",
    "    \"\"\"\n",
    "    Block ë‹¨ìœ„ Online Softmax (Triton/FlashAttention ë°©ì‹).\n",
    "    ë¸”ë¡ì„ ì²˜ë¦¬í•˜ë©° m, dë¥¼ ëˆ„ì .\n",
    "    \"\"\"\n",
    "    S = len(x)\n",
    "    m = -np.inf\n",
    "    d = 0.0\n",
    "\n",
    "    # ë¸”ë¡ ë‹¨ìœ„ 1-pass\n",
    "    for start in range(0, S, block_size):\n",
    "        block = x[start:start + block_size]\n",
    "        m_block = np.max(block)\n",
    "        m_new = max(m, m_block)\n",
    "        # ê¸°ì¡´ dë¥¼ ìƒˆ mìœ¼ë¡œ ë³´ì • + ìƒˆ ë¸”ë¡ ê¸°ì—¬\n",
    "        d = d * np.exp(m - m_new) + np.sum(np.exp(block - m_new))\n",
    "        m = m_new\n",
    "\n",
    "    # ë‹¨ì¼ Normalization pass\n",
    "    return np.exp(x - m) / d\n",
    "\n",
    "\n",
    "# ê²€ì¦\n",
    "x_test = np.array([3.0, 1.0, 4.0, 1.5, 2.0, 5.0, 0.5, 3.5], dtype=np.float64)\n",
    "\n",
    "y_std    = softmax_standard(x_test)\n",
    "y_online = online_softmax_elementwise(x_test)\n",
    "y_block  = online_softmax_blocked(x_test, block_size=4)\n",
    "\n",
    "print(\"[Online Softmax ê²€ì¦]\")\n",
    "print(f\"ì…ë ¥: {x_test}\")\n",
    "print(f\"\\ní‘œì¤€:    {y_std.round(6)}\")\n",
    "print(f\"Online:  {y_online.round(6)}\")\n",
    "print(f\"Block:   {y_block.round(6)}\")\n",
    "print()\n",
    "print(f\"í‘œì¤€ vs Online ìµœëŒ€ ì˜¤ì°¨: {np.abs(y_std - y_online).max():.2e} {'âœ…' if np.allclose(y_std, y_online, atol=1e-10) else 'âŒ'}\")\n",
    "print(f\"í‘œì¤€ vs Block  ìµœëŒ€ ì˜¤ì°¨: {np.abs(y_std - y_block).max():.2e} {'âœ…' if np.allclose(y_std, y_block, atol=1e-10) else 'âŒ'}\")\n",
    "print(f\"\\ní•©ì‚° ê²€ì¦: sum(softmax) = {y_std.sum():.10f} â‰ˆ 1\")\n",
    "print(\"\\nê²°ë¡ : Online SoftmaxëŠ” í‘œì¤€ Softmaxì™€ ìˆ˜í•™ì ìœ¼ë¡œ ì™„ì „íˆ ë™ì¼!\")\n",
    "print(\"     ë‹¨ì¼ ë˜ëŠ” ë¸”ë¡ ë‹¨ìœ„ íŒ¨ìŠ¤ë¡œ ë©”ëª¨ë¦¬ ì ‘ê·¼ ìµœì†Œí™” ë‹¬ì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fused Softmax ì‹œë®¬ë ˆì´ì…˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Fused Softmax ì„±ëŠ¥ ì´ë“ ì •ëŸ‰í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "import time\n",
    "\n",
    "def count_hbm_accesses(S, method='standard'):\n",
    "    \"\"\"HBM ì ‘ê·¼ ì›ì†Œ ìˆ˜ ê³„ì‚° (FP16 = 2bytes/ì›ì†Œ)\"\"\"\n",
    "    if method == 'standard':\n",
    "        return 5 * S  # 3ì½ê¸° + 2ì“°ê¸°\n",
    "    elif method == 'fused':\n",
    "        return 2 * S  # 1ì½ê¸° + 1ì“°ê¸°\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
    "\n",
    "print(f\"{'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'í‘œì¤€ HBM (KB)':>14} | {'Fused HBM (KB)':>15} | {'ì ˆì•½':>8}\")\n",
    "print(\"-\" * 58)\n",
    "for S in seq_lengths:\n",
    "    std_bytes  = count_hbm_accesses(S, 'standard') * 2 / 1024\n",
    "    fused_bytes = count_hbm_accesses(S, 'fused') * 2 / 1024\n",
    "    savings = (1 - fused_bytes / std_bytes) * 100\n",
    "    print(f\"{S:>12} | {std_bytes:>14.1f} | {fused_bytes:>15.1f} | {savings:>7.0f}%\")\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸\n",
    "S = 8192\n",
    "batch = 64\n",
    "X = np.random.randn(batch, S).astype(np.float32)\n",
    "\n",
    "# í‘œì¤€ (Pythonì—ì„œ 3-pass ì‹œë®¬ë ˆì´ì…˜)\n",
    "n_runs = 100\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(n_runs):\n",
    "    Y_std = softmax_standard(X)\n",
    "t_std = (time.perf_counter() - t0) / n_runs * 1000\n",
    "\n",
    "# Fused (2-pass ì‹œë®¬ë ˆì´ì…˜)\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(n_runs):\n",
    "    m = np.max(X, axis=-1, keepdims=True)   # Pass 1\n",
    "    e = np.exp(X - m)                        # Pass 2 part 1\n",
    "    d = np.sum(e, axis=-1, keepdims=True)    # Pass 2 part 2 (í•©ì‚°)\n",
    "    Y_fused = e / d                           # ì •ê·œí™” (Pass 2 ë‚´)\n",
    "t_fused = (time.perf_counter() - t0) / n_runs * 1000\n",
    "\n",
    "print(f\"\\nì‹¤í–‰ ì‹œê°„ ë¹„êµ (batch={batch}, S={S}, NumPy):\")\n",
    "print(f\"  í‘œì¤€ 3-pass:  {t_std:.2f} ms\")\n",
    "print(f\"  Fused 2-pass: {t_fused:.2f} ms\")\n",
    "print(f\"  ë“±ê°€ ê²€ì¦: {'âœ…' if np.allclose(Y_std, Y_fused, atol=1e-5) else 'âŒ'}\")\n",
    "print(f\"\\n(ì‹¤ì œ GPUì—ì„œëŠ” HBM ì ‘ê·¼ ê°ì†Œë¡œ 2~3ë°° ë¹ ë¦„, ì—¬ê¸°ì„œëŠ” NumPy CPU ì¸¡ì •)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FlashAttention ì›ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# FlashAttentionì˜ í•µì‹¬: Fused Online Attention\n",
    "# Online Softmax + Attention í•©ì³ì„œ O(S) HBM!\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def flash_attention_naive_reference(Q, K, V, scale=None):\n",
    "    \"\"\"\n",
    "    í‘œì¤€ Attention (ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ ë²„ì „).\n",
    "    SÃ—Sì˜ Attention Score í–‰ë ¬ì„ HBMì— ì“°ê³  ì½ìŒ.\n",
    "    ë©”ëª¨ë¦¬: O(SÂ²)\n",
    "    \"\"\"\n",
    "    S, d = Q.shape[-2], Q.shape[-1]\n",
    "    if scale is None:\n",
    "        scale = 1.0 / np.sqrt(d)\n",
    "\n",
    "    scores = Q @ K.transpose(0, 2, 1) * scale  # (B, S, S) â€” HBMì— ì €ì¥!\n",
    "    attn   = softmax_standard(scores)            # (B, S, S)\n",
    "    output = attn @ V                            # (B, S, d)\n",
    "    return output, attn\n",
    "\n",
    "\n",
    "def flash_attention_block(Q, K, V, block_size=4, scale=None):\n",
    "    \"\"\"\n",
    "    FlashAttention í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ (ë¸”ë¡ ë‹¨ìœ„ Online Softmax).\n",
    "    SÃ—S í–‰ë ¬ì„ HBMì— ì €ì¥í•˜ì§€ ì•ŠìŒ â†’ ë©”ëª¨ë¦¬ O(S)\n",
    "    \"\"\"\n",
    "    B, S, d = Q.shape\n",
    "    if scale is None:\n",
    "        scale = 1.0 / np.sqrt(d)\n",
    "\n",
    "    O = np.zeros_like(Q)  # ì¶œë ¥ ëˆ„ì \n",
    "    m = np.full((B, S, 1), -np.inf)  # í–‰ë³„ ìµœëŒ“ê°’\n",
    "    l = np.zeros((B, S, 1))          # í–‰ë³„ ë¶„ëª¨ í•©\n",
    "\n",
    "    # KV ë¸”ë¡ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (HBMì—ì„œ ë¸”ë¡ì”© ë¡œë“œ)\n",
    "    for j in range(0, S, block_size):\n",
    "        j_end = min(j + block_size, S)\n",
    "        Kj = K[:, j:j_end, :]   # KV ë¸”ë¡\n",
    "        Vj = V[:, j:j_end, :]\n",
    "\n",
    "        # Q Ã— Kj^T â†’ (B, S, block_size) (HBMì— ì €ì¥í•˜ì§€ ì•ŠìŒ)\n",
    "        Sij = Q @ Kj.transpose(0, 2, 1) * scale\n",
    "\n",
    "        # Online Softmax ì—…ë°ì´íŠ¸\n",
    "        mij  = np.max(Sij, axis=-1, keepdims=True)\n",
    "        m_new = np.maximum(m, mij)\n",
    "\n",
    "        # ê¸°ì¡´ O, lì„ ìƒˆ mìœ¼ë¡œ ë³´ì • + ìƒˆ ë¸”ë¡ ê¸°ì—¬ ì¶”ê°€\n",
    "        exp_new = np.exp(Sij - m_new)\n",
    "        alpha = np.exp(m - m_new)  # ê¸°ì¡´ í•­ëª© ë³´ì • ê³„ìˆ˜\n",
    "\n",
    "        O = alpha * O + exp_new @ Vj\n",
    "        l = alpha * l + np.sum(exp_new, axis=-1, keepdims=True)\n",
    "        m = m_new\n",
    "\n",
    "    return O / l  # ìµœì¢… ì •ê·œí™” (1íšŒë§Œ!)\n",
    "\n",
    "\n",
    "# ê²€ì¦\n",
    "B, S, d = 2, 16, 8\n",
    "Q = np.random.randn(B, S, d).astype(np.float64)\n",
    "K = np.random.randn(B, S, d).astype(np.float64)\n",
    "V = np.random.randn(B, S, d).astype(np.float64)\n",
    "\n",
    "O_std, _   = flash_attention_naive_reference(Q, K, V)\n",
    "O_flash    = flash_attention_block(Q, K, V, block_size=4)\n",
    "\n",
    "err_fa = np.abs(O_std - O_flash).max()\n",
    "print(\"[FlashAttention Block ê²€ì¦]\")\n",
    "print(f\"  ì…ë ¥: B={B}, S={S}, d={d}, block_size=4\")\n",
    "print(f\"  ìµœëŒ€ ì˜¤ì°¨: {err_fa:.2e}\")\n",
    "print(f\"  ìˆ˜í•™ì  ë“±ê°€: {'âœ… ì™„ì „ ì¼ì¹˜' if err_fa < 1e-10 else 'âŒ ë¶ˆì¼ì¹˜'}\")\n",
    "print()\n",
    "print(\"  í‘œì¤€ Attention HBM: O(SÂ²) = SÃ—S Attention Score í–‰ë ¬\")\n",
    "print(f\"  Flash Attention HBM: O(S) = ë¸”ë¡ í¬ê¸°ë§Œí¼ë§Œ ë¡œë“œ (Q,K,V ì›ë³¸ ì œì™¸)\")\n",
    "print(f\"  S={S}ì—ì„œ HBM ì ˆì•½: {S}Â² = {S**2} ì›ì†Œ vs {S}Ã—{d} = {S*d} ì›ì†Œ ({S//d}ë°°â†“)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Triton / FlashAttention HBM ì ˆì•½ ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "seq_range = np.array([512, 1024, 2048, 4096, 8192])\n",
    "d_model   = 64  # head dimension\n",
    "\n",
    "# í‘œì¤€ Attention HBM (SÃ—S + Q,K,V)\n",
    "standard_hbm = (seq_range**2 + 3 * seq_range * d_model) * 2  # FP16\n",
    "\n",
    "# FlashAttention HBM (ë¸”ë¡ ë‹¨ìœ„, SÃ—dë§Œ)\n",
    "flash_hbm = 5 * seq_range * d_model * 2  # Q,K,V ì½ê¸° + O ì“°ê¸° + ì¤‘ê°„\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.semilogy(seq_range, standard_hbm / 1e6, 'r-o', lw=2.5, ms=8, label='í‘œì¤€ Attention: O(SÂ²)')\n",
    "ax1.semilogy(seq_range, flash_hbm / 1e6, 'b-s', lw=2.5, ms=8, label='FlashAttention: O(S)')\n",
    "ax1.fill_between(seq_range, flash_hbm/1e6, standard_hbm/1e6, alpha=0.1, color='red')\n",
    "for s, st, fl in zip(seq_range, standard_hbm, flash_hbm):\n",
    "    ax1.annotate(f'{int(st/fl)}Ã—â†“', (s, (st*fl)**0.5/1e6),\n",
    "                fontsize=9, ha='center', color='darkred')\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (S)', fontsize=11)\n",
    "ax1.set_ylabel('HBM ì ‘ê·¼ëŸ‰ (MB, log scale)', fontsize=11)\n",
    "ax1.set_title('í‘œì¤€ Attention vs FlashAttention\\nHBM ì ‘ê·¼ëŸ‰ ë¹„êµ', fontweight='bold')\n",
    "ax1.legend(fontsize=10); ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "methods = ['Softmax\\ní‘œì¤€\\n(3-pass)', 'Softmax\\nFused\\n(2-pass)', 'Attention\\ní‘œì¤€\\n(O(SÂ²))', 'FlashAttn\\n(O(S))']\n",
    "hbm_norm = [100, 40, 100, 100*d_model/4096]  # S=4096 ê¸°ì¤€ ì •ê·œí™”\n",
    "colors_bar = ['#E53935', '#43A047', '#E53935', '#1E88E5']\n",
    "bars = ax2.bar(methods, hbm_norm, color=colors_bar, alpha=0.85, edgecolor='white', lw=2)\n",
    "for bar, val in zip(bars, hbm_norm):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 1,\n",
    "             f'{val:.0f}%', ha='center', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('HBM ì ‘ê·¼ëŸ‰ (í‘œì¤€ ëŒ€ë¹„ %)')\n",
    "ax2.set_title('Kernel Fusion íš¨ê³¼\\n(S=4096, d=64 ê¸°ì¤€)', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… |\n",
    "|------|------|\n",
    "| **Kernel Fusion** | ì—¬ëŸ¬ ì—°ì‚°ì„ 1ê°œ ì»¤ë„ë¡œ í•©ì³ HBM ì ‘ê·¼ ìµœì†Œí™” |\n",
    "| **Online Softmax** | ì›ì†Œë¥¼ ìˆœì°¨ ì²˜ë¦¬í•˜ë©° $m$, $d$ ë™ì‹œ ì—…ë°ì´íŠ¸ â†’ 1-pass |\n",
    "| **Triton Block** | CUDA ìŠ¤ë ˆë“œ ëŒ€ì‹  ë¸”ë¡ ë‹¨ìœ„ë¡œ í”„ë¡œê·¸ë˜ë° (Python) |\n",
    "| **FlashAttention** | Online Softmaxë¥¼ Attentionì— ì ìš© â†’ $O(S^2) \\to O(S)$ HBM |\n",
    "\n",
    "### Online Softmax í•µì‹¬ ì¬ê·€\n",
    "\n",
    "$$m_j = \\max(m_{j-1}, x_j), \\quad d_j = d_{j-1} \\cdot e^{m_{j-1}-m_j} + e^{x_j - m_j}$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 11-04: ì»¤ë„ í”„ë¡œíŒŒì¼ë§ê³¼ Kernel Fusion ì „ëµ** â€” GPU ì»¤ë„ ì„±ëŠ¥ ì¸¡ì • ë„êµ¬(Nsight Systems, nvprof), ë³‘ëª© ë¶„ì„ ë°©ë²•ë¡ , ì—¬ëŸ¬ ì—°ì‚°ì„ í•©ì¹˜ëŠ” Fusion ì„¤ê³„ ì „ëµì„ í•™ìŠµí•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python (tf_study)", "language": "python", "name": "tf_study"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}