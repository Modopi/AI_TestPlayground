{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: adaLN-Zero DiT 블록과 Flow Matching\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: adaLN-Zero 단일 블록](#q1)\n",
    "- [Q2: Flow Matching Loss 계산](#q2)\n",
    "- [Q3: Euler ODE 샘플링 스텝](#q3)\n",
    "- [Q4: DiT 블록 조립 (adaLN + Attention + FFN)](#q4)\n",
    "- [종합 도전: 소형 DiT 학습](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 라이브러리 임포트 ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: adaLN-Zero 단일 블록 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "adaLN-Zero의 핵심 수식을 구현하세요:\n",
    "\n",
    "$$h = x + \\alpha \\cdot f\\!\\left((1 + \\gamma) \\odot \\text{LN}(x) + \\beta\\right)$$\n",
    "\n",
    "여기서 $(\\gamma, \\beta, \\alpha)$는 조건 벡터 $c$로부터 MLP로 생성되며, **$\\alpha$의 초기값은 0**입니다.\n",
    "\n",
    "`d_model=32`, 입력 `x`의 shape이 `(1, 4, 32)`일 때:\n",
    "1. MLP가 출력해야 하는 파라미터 수는? (γ, β, α 각각 d_model차원)\n",
    "2. 초기 상태에서 출력 `h`와 입력 `x`의 차이는?\n",
    "\n",
    "**여러분의 예측:**\n",
    "- MLP 출력 차원 = `?`\n",
    "- 초기 `h - x`의 norm = `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: adaLN-Zero 단일 블록\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_model = 32\n",
    "\n",
    "class AdaLNZeroBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.linear_f = tf.keras.layers.Dense(d_model, activation='silu')\n",
    "        self.adaLN_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model * 4, activation='silu'),\n",
    "            tf.keras.layers.Dense(d_model * 3)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        last_layer = self.adaLN_mlp.layers[-1]\n",
    "        last_layer.kernel.assign(tf.zeros_like(last_layer.kernel))\n",
    "        last_layer.bias.assign(tf.zeros_like(last_layer.bias))\n",
    "\n",
    "    def call(self, x, c):\n",
    "        params = self.adaLN_mlp(c)\n",
    "        gamma = params[..., :d_model]\n",
    "        beta = params[..., d_model:2*d_model]\n",
    "        alpha = params[..., 2*d_model:]\n",
    "\n",
    "        if len(gamma.shape) == 2:\n",
    "            gamma = gamma[:, tf.newaxis, :]\n",
    "            beta = beta[:, tf.newaxis, :]\n",
    "            alpha = alpha[:, tf.newaxis, :]\n",
    "\n",
    "        normed = self.ln(x)\n",
    "        modulated = (1.0 + gamma) * normed + beta\n",
    "        h = x + alpha * self.linear_f(modulated)\n",
    "        return h\n",
    "\n",
    "block = AdaLNZeroBlock(d_model)\n",
    "x = tf.random.normal([1, 4, d_model])\n",
    "c = tf.random.normal([1, d_model])\n",
    "\n",
    "h = block(x, c)\n",
    "diff_norm = tf.reduce_mean(tf.abs(h - x)).numpy()\n",
    "\n",
    "print(f\"\\nd_model = {d_model}\")\n",
    "print(f\"MLP 출력 차원 = {d_model} x 3 (γ, β, α) = {d_model * 3}\")\n",
    "print(f\"\\n입력 x shape: {x.shape}\")\n",
    "print(f\"조건 c shape: {c.shape}\")\n",
    "print(f\"출력 h shape: {h.shape}\")\n",
    "print(f\"\\n초기 상태 |h - x| 평균: {diff_norm:.8f}\")\n",
    "print(f\"(MLP 마지막 층을 0으로 초기화 → α=0 → h=x)\")\n",
    "\n",
    "n_params = sum(p.numpy().size for p in block.trainable_variables)\n",
    "print(f\"블록 파라미터 수: {n_params:,}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  α=0이면 h = x + 0 * f(...) = x (항등 함수)\")\n",
    "print(\"  이것이 adaLN-Zero의 핵심: 초기에 블록이 아무 변화를 주지 않음\")\n",
    "print(\"  학습이 진행되며 서서히 α가 커지면서 변환이 활성화됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Flow Matching Loss 계산 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Flow Matching Loss를 직접 계산하세요:\n",
    "\n",
    "$$\\mathcal{L}_{FM} = \\mathbb{E}_{t,x_0,x_1}\\left[\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\right]$$\n",
    "\n",
    "$x_0 = [1.0, 0.0]$, $x_1 = [-1.0, 2.0]$, $t = 0.4$일 때:\n",
    "1. $x_t = (1-t)x_0 + tx_1$ = ?\n",
    "2. $v^{GT} = x_1 - x_0$ = ?\n",
    "3. 모델이 $v_\\theta = [-1.5, 1.8]$을 예측했다면 loss는?\n",
    "\n",
    "**여러분의 예측:** loss = `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: Flow Matching Loss 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "x0 = np.array([1.0, 0.0])\n",
    "x1 = np.array([-1.0, 2.0])\n",
    "t = 0.4\n",
    "\n",
    "x_t = (1 - t) * x0 + t * x1\n",
    "v_gt = x1 - x0\n",
    "v_pred = np.array([-1.5, 1.8])\n",
    "\n",
    "loss = np.mean((v_pred - v_gt) ** 2)\n",
    "\n",
    "print(f\"\\nx_0 = {x0}\")\n",
    "print(f\"x_1 = {x1}\")\n",
    "print(f\"t = {t}\")\n",
    "print(f\"\\nx_t = (1-{t}){x0} + {t}{x1}\")\n",
    "print(f\"    = {1-t}{x0} + {t}{x1}\")\n",
    "print(f\"    = {(1-t)*x0} + {t*x1}\")\n",
    "print(f\"    = {x_t}\")\n",
    "print(f\"\\nv_GT = x_1 - x_0 = {x1} - {x0} = {v_gt}\")\n",
    "print(f\"v_pred = {v_pred}\")\n",
    "print(f\"\\nLoss = mean(|v_pred - v_GT|^2)\")\n",
    "print(f\"     = mean(|{v_pred} - {v_gt}|^2)\")\n",
    "print(f\"     = mean(|{v_pred - v_gt}|^2)\")\n",
    "print(f\"     = mean({(v_pred - v_gt)**2})\")\n",
    "print(f\"     = {loss:.4f}\")\n",
    "\n",
    "# TensorFlow로 검증\n",
    "x0_tf = tf.constant([[1.0, 0.0]])\n",
    "x1_tf = tf.constant([[-1.0, 2.0]])\n",
    "t_tf = tf.constant([[0.4]])\n",
    "xt_tf = (1 - t_tf) * x0_tf + t_tf * x1_tf\n",
    "vgt_tf = x1_tf - x0_tf\n",
    "vpred_tf = tf.constant([[-1.5, 1.8]])\n",
    "loss_tf = tf.reduce_mean(tf.square(vpred_tf - vgt_tf))\n",
    "print(f\"\\nTF 검증 Loss: {loss_tf.numpy():.4f}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  FM Loss는 단순한 MSE입니다.\")\n",
    "print(\"  v_pred가 v_GT에 가까울수록 loss가 작아집니다.\")\n",
    "print(f\"  이 경우 오차는 {v_pred - v_gt}로, 작은 예측 오차입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Euler ODE 샘플링 스텝 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Euler 방법으로 ODE를 적분하여 샘플링합니다:\n",
    "\n",
    "$$x_{t+\\Delta t} = x_t + \\Delta t \\cdot v_\\theta(x_t, t)$$\n",
    "\n",
    "$x_0 = [2.0, -1.0]$에서 시작, 학습된 속도 모델이 항상 $v_\\theta = [-1.0, 1.5]$을 출력한다고 가정합니다.\n",
    "\n",
    "$\\Delta t = 0.25$ (4스텝)로 $t=0 \\to t=1$ 샘플링 경로를 구하세요.\n",
    "\n",
    "**여러분의 예측:** $x_1 = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: Euler ODE 샘플링 스텝\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "x = np.array([2.0, -1.0])\n",
    "v_const = np.array([-1.0, 1.5])\n",
    "dt = 0.25\n",
    "n_steps = 4\n",
    "\n",
    "print(f\"\\nx_0 = {x}\")\n",
    "print(f\"v_θ = {v_const} (상수)\")\n",
    "print(f\"Δt = {dt}, 스텝 수 = {n_steps}\\n\")\n",
    "\n",
    "trajectory = [x.copy()]\n",
    "for step in range(n_steps):\n",
    "    t_val = step * dt\n",
    "    x_new = x + dt * v_const\n",
    "    print(f\"스텝 {step+1}: t={t_val:.2f} → t={t_val+dt:.2f}\")\n",
    "    print(f\"  x_{t_val+dt:.2f} = {x} + {dt} * {v_const} = {x_new}\")\n",
    "    x = x_new\n",
    "    trajectory.append(x.copy())\n",
    "\n",
    "trajectory = np.array(trajectory)\n",
    "print(f\"\\n최종 x_1 = {x}\")\n",
    "expected = np.array([2.0, -1.0]) + 1.0 * v_const\n",
    "print(f\"해석적 해: x_0 + 1.0 * v = {expected}\")\n",
    "print(f\"오차: {np.linalg.norm(x - expected):.10f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(trajectory[:, 0], trajectory[:, 1], 'b-o', lw=2.5, ms=10, label='Euler 경로')\n",
    "ax.plot(trajectory[0, 0], trajectory[0, 1], 'ro', ms=14, zorder=5, label='$x_0$ (시작)')\n",
    "ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'g*', ms=18, zorder=5, label='$x_1$ (도착)')\n",
    "\n",
    "for i, (px, py) in enumerate(trajectory):\n",
    "    ax.annotate(f't={i*dt:.2f}', (px, py), textcoords=\"offset points\",\n",
    "                xytext=(10, 10), fontsize=9)\n",
    "\n",
    "ax.quiver(trajectory[:-1, 0], trajectory[:-1, 1],\n",
    "          np.full(n_steps, v_const[0]*dt), np.full(n_steps, v_const[1]*dt),\n",
    "          angles='xy', scale_units='xy', scale=1, color='orange', alpha=0.5, width=0.01)\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=11)\n",
    "ax.set_ylabel('Dimension 2', fontsize=11)\n",
    "ax.set_title('Euler ODE 샘플링 경로 (4 스텝)', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/euler_steps_q3.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n그래프 저장됨: chapter17_diffusion_transformers/practice/euler_steps_q3.png\")\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  상수 속도에서 Euler 적분은 정확합니다 (오차 = 0).\")\n",
    "print(\"  이것이 Rectified Flow의 장점입니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: DiT 블록 조립 (adaLN + Attention + FFN) <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "완전한 DiT 블록을 조립하세요:\n",
    "\n",
    "$$h = x + \\alpha_1 \\cdot \\text{Attn}\\!\\left((1 + \\gamma_1) \\odot \\text{LN}(x) + \\beta_1\\right)$$\n",
    "$$\\text{out} = h + \\alpha_2 \\cdot \\text{FFN}\\!\\left((1 + \\gamma_2) \\odot \\text{LN}(h) + \\beta_2\\right)$$\n",
    "\n",
    "`d_model=64`, `n_heads=4`, `ffn_dim=256`으로 구현하세요.\n",
    "\n",
    "**여러분의 예측:** 블록 파라미터 수 = `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: DiT 블록 조립\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class DiTBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(ffn_dim, activation='gelu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.adaLN_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model * 4, activation='silu'),\n",
    "            tf.keras.layers.Dense(d_model * 6)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        last_layer = self.adaLN_mlp.layers[-1]\n",
    "        last_layer.kernel.assign(tf.zeros_like(last_layer.kernel))\n",
    "        last_layer.bias.assign(tf.zeros_like(last_layer.bias))\n",
    "\n",
    "    def call(self, x, c):\n",
    "        params = self.adaLN_mlp(c)\n",
    "        if len(params.shape) == 2:\n",
    "            params = params[:, tf.newaxis, :]\n",
    "\n",
    "        gamma1 = params[..., 0*self.d_model:1*self.d_model]\n",
    "        beta1 = params[..., 1*self.d_model:2*self.d_model]\n",
    "        alpha1 = params[..., 2*self.d_model:3*self.d_model]\n",
    "        gamma2 = params[..., 3*self.d_model:4*self.d_model]\n",
    "        beta2 = params[..., 4*self.d_model:5*self.d_model]\n",
    "        alpha2 = params[..., 5*self.d_model:6*self.d_model]\n",
    "\n",
    "        normed1 = (1.0 + gamma1) * self.ln1(x) + beta1\n",
    "        h = x + alpha1 * self.attn(normed1, normed1)\n",
    "\n",
    "        normed2 = (1.0 + gamma2) * self.ln2(h) + beta2\n",
    "        out = h + alpha2 * self.ffn(normed2)\n",
    "        return out\n",
    "\n",
    "d_model, n_heads, ffn_dim = 64, 4, 256\n",
    "block = DiTBlock(d_model, n_heads, ffn_dim)\n",
    "\n",
    "x = tf.random.normal([2, 16, d_model])\n",
    "c = tf.random.normal([2, d_model])\n",
    "\n",
    "out = block(x, c)\n",
    "\n",
    "init_diff = tf.reduce_mean(tf.abs(out - x)).numpy()\n",
    "\n",
    "print(f\"\\nd_model={d_model}, n_heads={n_heads}, ffn_dim={ffn_dim}\")\n",
    "print(f\"\\n입력 x shape: {x.shape}\")\n",
    "print(f\"조건 c shape: {c.shape}\")\n",
    "print(f\"출력 shape: {out.shape}\")\n",
    "print(f\"\\n초기 상태 |out - x| 평균: {init_diff:.8f}\")\n",
    "print(f\"  → Zero-init 확인: {'통과' if init_diff < 1e-4 else '실패'}\")\n",
    "\n",
    "n_params = sum(p.numpy().size for p in block.trainable_variables)\n",
    "print(f\"\\n블록 파라미터 수: {n_params:,}\")\n",
    "print(f\"  - adaLN MLP: ~{d_model * d_model * 4 + d_model * 4 * 6:,}\")\n",
    "print(f\"  - Multi-Head Attention: ~{4 * d_model * d_model:,}\")\n",
    "print(f\"  - FFN: ~{d_model * ffn_dim + ffn_dim * d_model:,}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  DiT 블록 = adaLN-Zero (조건 주입) + Self-Attention + FFN\")\n",
    "print(\"  adaLN MLP가 6개 파라미터 벡터를 생성합니다: γ1, β1, α1, γ2, β2, α2\")\n",
    "print(\"  모두 0으로 초기화되어 학습 시작 시 항등 함수로 동작합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 도전: 소형 DiT 학습 <a name='bonus'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "여러 개의 DiT 블록을 쌓고, Flow Matching Loss로 합성 데이터(Moving MNIST 스타일)에 대해 학습하세요:\n",
    "\n",
    "1. 간단한 2D 데이터 생성 (원형 패턴)\n",
    "2. DiT 블록 3개 스택\n",
    "3. Flow Matching 학습 루프\n",
    "4. Euler 샘플링으로 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이: 소형 DiT 학습 ──────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"종합 도전 풀이: 소형 DiT 학습\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 합성 데이터: 8x8 \"이미지\" 위에 원형 패턴\n",
    "def make_circle_data(n=1000, img_size=8):\n",
    "    images = np.zeros((n, img_size, img_size), dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        cx = np.random.randint(2, img_size - 2)\n",
    "        cy = np.random.randint(2, img_size - 2)\n",
    "        r = np.random.uniform(1.0, 2.5)\n",
    "        for x in range(img_size):\n",
    "            for y in range(img_size):\n",
    "                dist = np.sqrt((x - cx)**2 + (y - cy)**2)\n",
    "                images[i, x, y] = max(0, 1.0 - dist / r)\n",
    "    return images\n",
    "\n",
    "data_img = make_circle_data(500, 8)\n",
    "data_flat = data_img.reshape(500, -1)\n",
    "print(f\"학습 데이터: {data_img.shape} → 평탄화: {data_flat.shape}\")\n",
    "print(f\"값 범위: [{data_flat.min():.3f}, {data_flat.max():.3f}]\")\n",
    "\n",
    "# Mini DiT\n",
    "class MiniDiT(tf.keras.Model):\n",
    "    def __init__(self, data_dim=64, d_model=64, n_blocks=3, n_heads=4, ffn_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_proj = tf.keras.layers.Dense(d_model)\n",
    "        self.time_embed = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(d_model, activation='silu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "        self.blocks = [DiTBlock(d_model, n_heads, ffn_dim) for _ in range(n_blocks)]\n",
    "        self.output_proj = tf.keras.layers.Dense(data_dim)\n",
    "\n",
    "    def call(self, x_flat, t):\n",
    "        t_emb = self.time_embed(tf.concat([tf.sin(t * np.pi * 4), tf.cos(t * np.pi * 4)], axis=-1))\n",
    "        h = self.input_proj(x_flat)\n",
    "        h = h[:, tf.newaxis, :]\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, t_emb)\n",
    "        h = h[:, 0, :]\n",
    "        return self.output_proj(h)\n",
    "\n",
    "dit = MiniDiT(data_dim=64, d_model=64, n_blocks=3, n_heads=4, ffn_dim=128)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "\n",
    "@tf.function\n",
    "def dit_train_step(x1_batch):\n",
    "    bs = tf.shape(x1_batch)[0]\n",
    "    t = tf.random.uniform([bs, 1], 0.0, 1.0)\n",
    "    x0 = tf.random.normal(tf.shape(x1_batch))\n",
    "    x_t = (1.0 - t) * x0 + t * x1_batch\n",
    "    v_target = x1_batch - x0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        v_pred = dit(x_t, t)\n",
    "        loss = tf.reduce_mean(tf.square(v_pred - v_target))\n",
    "\n",
    "    grads = tape.gradient(loss, dit.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, dit.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_flat).shuffle(500).batch(64).repeat()\n",
    "it = iter(dataset)\n",
    "\n",
    "losses = []\n",
    "for step in range(300):\n",
    "    batch = next(it)\n",
    "    loss = dit_train_step(batch)\n",
    "    losses.append(float(loss))\n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"스텝 {step+1:4d} | FM Loss: {loss:.6f}\")\n",
    "\n",
    "# Euler 샘플링\n",
    "def dit_euler_sample(model, n_samples=16, n_steps=30, data_dim=64):\n",
    "    x = tf.random.normal([n_samples, data_dim])\n",
    "    dt = 1.0 / n_steps\n",
    "    for i in range(n_steps):\n",
    "        t_val = i * dt\n",
    "        t = tf.fill([n_samples, 1], t_val)\n",
    "        v = model(x, t)\n",
    "        x = x + dt * v\n",
    "    return x.numpy()\n",
    "\n",
    "samples = dit_euler_sample(dit, n_samples=8)\n",
    "samples_img = samples.reshape(-1, 8, 8)\n",
    "samples_img = np.clip(samples_img, 0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(data_img[i], cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[0, i].set_title('원본', fontweight='bold', fontsize=10)\n",
    "\n",
    "for i in range(8):\n",
    "    axes[1, i].imshow(samples_img[i], cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1, i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[1, i].set_title('생성', fontweight='bold', fontsize=10)\n",
    "\n",
    "plt.suptitle('Mini DiT: Flow Matching 학습 결과', fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice/mini_dit_results.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "n_total_params = sum(p.numpy().size for p in dit.trainable_variables)\n",
    "print(f\"\\nMini DiT 총 파라미터: {n_total_params:,}\")\n",
    "print(f\"최종 Loss: {losses[-1]:.6f}\")\n",
    "print(f\"생성 이미지 값 범위: [{samples_img.min():.3f}, {samples_img.max():.3f}]\")\n",
    "print(f\"\\n그래프 저장됨: chapter17_diffusion_transformers/practice/mini_dit_results.png\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  3개의 DiT 블록 + Flow Matching으로 간단한 패턴을 학습했습니다.\")\n",
    "print(\"  adaLN-Zero의 Zero-init 덕분에 학습이 안정적으로 시작됩니다.\")\n",
    "print(\"  실제 DiT는 수백 블록, 수십억 파라미터로 고해상도 이미지/비디오를 생성합니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}