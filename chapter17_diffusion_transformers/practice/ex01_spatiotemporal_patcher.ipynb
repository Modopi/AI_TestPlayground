{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: 시공간 3D DiT 패치 시퀀스 변환기\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: 3D 패치 추출](#q1)\n",
    "- [Q2: 패치 시퀀스 길이 계산](#q2)\n",
    "- [Q3: 3D RoPE 주파수 계산](#q3)\n",
    "- [Q4: 시공간 위치 임베딩](#q4)\n",
    "- [종합 도전: 완전한 시공간 패처 모듈](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 라이브러리 임포트 ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: 3D 패치 추출 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "비디오 텐서 $x \\in \\mathbb{R}^{B \\times T \\times H \\times W \\times C}$에서 3D 패치를 추출합니다.\n",
    "패치 크기가 $(p_t, p_h, p_w) = (2, 4, 4)$이고, 입력이 $B=1, T=8, H=16, W=16, C=3$일 때:\n",
    "\n",
    "1. 추출되는 패치 수는?\n",
    "2. 각 패치의 벡터 크기(flatten 후)는?\n",
    "3. `tf.extract_volume_patches` 또는 reshape로 패치를 추출하세요.\n",
    "\n",
    "**여러분의 예측:**\n",
    "- 패치 수 = `?`\n",
    "- 패치 벡터 크기 = `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: 3D 패치 추출\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "B, T, H, W, C = 1, 8, 16, 16, 3\n",
    "pt, ph, pw = 2, 4, 4\n",
    "\n",
    "n_t = T // pt\n",
    "n_h = H // ph\n",
    "n_w = W // pw\n",
    "n_patches = n_t * n_h * n_w\n",
    "patch_dim = pt * ph * pw * C\n",
    "\n",
    "print(f\"\\n입력 shape: ({B}, {T}, {H}, {W}, {C})\")\n",
    "print(f\"패치 크기: ({pt}, {ph}, {pw})\")\n",
    "print(f\"\\n시간 패치 수: {T}/{pt} = {n_t}\")\n",
    "print(f\"높이 패치 수: {H}/{ph} = {n_h}\")\n",
    "print(f\"너비 패치 수: {W}/{pw} = {n_w}\")\n",
    "print(f\"총 패치 수: {n_t} x {n_h} x {n_w} = {n_patches}\")\n",
    "print(f\"패치 벡터 크기: {pt} x {ph} x {pw} x {C} = {patch_dim}\")\n",
    "\n",
    "video = tf.random.normal([B, T, H, W, C])\n",
    "\n",
    "patches = tf.reshape(video, [B, n_t, pt, n_h, ph, n_w, pw, C])\n",
    "patches = tf.transpose(patches, [0, 1, 3, 5, 2, 4, 6, 7])\n",
    "patches = tf.reshape(patches, [B, n_patches, patch_dim])\n",
    "\n",
    "print(f\"\\n추출된 패치 텐서 shape: {patches.shape}\")\n",
    "print(f\"  → (배치, 시퀀스길이, 패치차원) = ({B}, {n_patches}, {patch_dim})\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  3D 패치 추출은 reshape + transpose로 구현합니다.\")\n",
    "print(f\"  결과적으로 {n_patches}개 토큰의 시퀀스로 변환됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: 패치 시퀀스 길이 계산 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다양한 비디오 해상도에 대해 DiT 시퀀스 길이를 계산하세요.\n",
    "\n",
    "패치 크기 $(p_t, p_h, p_w) = (2, 4, 4)$, VAE 압축 $M_t=4, M_h=8, M_w=8$일 때:\n",
    "\n",
    "| 원본 비디오 | Latent 크기 | 시퀀스 길이 |\n",
    "|------------|-------------|------------|\n",
    "| 32×256×256 | ? | ? |\n",
    "| 64×512×512 | ? | ? |\n",
    "| 128×1024×1024 | ? | ? |\n",
    "\n",
    "**여러분의 예측:** 시퀀스 길이 = `?`, `?`, `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: 패치 시퀀스 길이 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "Mt, Mh, Mw = 4, 8, 8\n",
    "pt, ph, pw = 2, 4, 4\n",
    "\n",
    "videos = [\n",
    "    (32, 256, 256),\n",
    "    (64, 512, 512),\n",
    "    (128, 1024, 1024),\n",
    "]\n",
    "\n",
    "print(f\"\\nVAE 압축: Mt={Mt}, Mh={Mh}, Mw={Mw}\")\n",
    "print(f\"패치 크기: pt={pt}, ph={ph}, pw={pw}\\n\")\n",
    "\n",
    "print(f\"{'원본 비디오':>18} | {'Latent':>18} | {'시퀀스 길이':>12} | {'Self-Attn FLOPs':>16}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for T_orig, H_orig, W_orig in videos:\n",
    "    lat_T = T_orig // Mt\n",
    "    lat_H = H_orig // Mh\n",
    "    lat_W = W_orig // Mw\n",
    "\n",
    "    seq_len = (lat_T // pt) * (lat_H // ph) * (lat_W // pw)\n",
    "    flops_attn = 2 * seq_len ** 2 * 128\n",
    "\n",
    "    print(f\"{T_orig:>4}x{H_orig:>4}x{W_orig:<4} | \"\n",
    "          f\"{lat_T:>3}x{lat_H:>3}x{lat_W:<4}   | \"\n",
    "          f\"{seq_len:>12,} | \"\n",
    "          f\"{flops_attn/1e9:>13.2f} G\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  시퀀스 길이 = (T/Mt/pt) x (H/Mh/ph) x (W/Mw/pw)\")\n",
    "print(\"  해상도가 2배 → 시퀀스 길이 ~8배 → Self-Attention 비용 ~64배!\")\n",
    "print(\"  이것이 Flash Attention, Sparse Attention이 필수인 이유입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: 3D RoPE 주파수 계산 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "3D RoPE(Rotary Position Embedding)는 시간(t), 높이(h), 너비(w) 3개 축에 대해 독립적인 주파수를 계산합니다:\n",
    "\n",
    "$$\\theta_k^{(axis)} = \\frac{1}{10000^{2k/d_{axis}}}, \\quad k = 0, 1, \\ldots, d_{axis}/2 - 1$$\n",
    "\n",
    "$d_{model} = 12$ (3축 균등 배분: $d_t = d_h = d_w = 4$)일 때:\n",
    "\n",
    "1. 각 축의 주파수 벡터를 계산하세요 ($k=0, 1$)\n",
    "2. 위치 $(t=3, h=5, w=7)$에서의 회전 각도를 구하세요\n",
    "\n",
    "**여러분의 예측:** $\\theta_0^{(t)} = ?$, $\\theta_1^{(t)} = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: 3D RoPE 주파수 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_model = 12\n",
    "d_per_axis = d_model // 3\n",
    "base = 10000.0\n",
    "\n",
    "print(f\"\\nd_model = {d_model}, 축당 차원 = {d_per_axis}\")\n",
    "print(f\"base = {base}\\n\")\n",
    "\n",
    "for axis_name, d_axis in [('시간(t)', d_per_axis), ('높이(h)', d_per_axis), ('너비(w)', d_per_axis)]:\n",
    "    freqs = []\n",
    "    for k in range(d_axis // 2):\n",
    "        theta = 1.0 / (base ** (2 * k / d_axis))\n",
    "        freqs.append(theta)\n",
    "        print(f\"  {axis_name} θ_{k} = 1 / 10000^({2*k}/{d_axis}) = {theta:.6f}\")\n",
    "    print()\n",
    "\n",
    "pos_t, pos_h, pos_w = 3, 5, 7\n",
    "\n",
    "print(\"위치 (t=3, h=5, w=7)에서의 회전 각도:\\n\")\n",
    "for axis_name, pos, d_axis in [('시간', pos_t, d_per_axis), ('높이', pos_h, d_per_axis), ('너비', pos_w, d_per_axis)]:\n",
    "    angles = []\n",
    "    for k in range(d_axis // 2):\n",
    "        theta = 1.0 / (base ** (2 * k / d_axis))\n",
    "        angle = pos * theta\n",
    "        angles.append(angle)\n",
    "    print(f\"  {axis_name} 축 (pos={pos}): 각도 = {angles}\")\n",
    "\n",
    "def compute_3d_rope(positions, d_per_axis, base=10000.0):\n",
    "    B = positions.shape[0]\n",
    "    freqs_list = []\n",
    "    for axis in range(3):\n",
    "        pos = positions[:, axis:axis+1]\n",
    "        k = tf.range(d_per_axis // 2, dtype=tf.float32)\n",
    "        theta = 1.0 / tf.pow(base, 2.0 * k / float(d_per_axis))\n",
    "        angles = tf.cast(pos, tf.float32) * theta[tf.newaxis, :]\n",
    "        cos_vals = tf.cos(angles)\n",
    "        sin_vals = tf.sin(angles)\n",
    "        freqs_list.append(tf.stack([cos_vals, sin_vals], axis=-1))\n",
    "    rope = tf.concat(freqs_list, axis=1)\n",
    "    return rope\n",
    "\n",
    "test_positions = tf.constant([[3, 5, 7], [0, 0, 0], [1, 1, 1]])\n",
    "rope_result = compute_3d_rope(test_positions, d_per_axis)\n",
    "print(f\"\\n3D RoPE 텐서 shape: {rope_result.shape}\")\n",
    "print(f\"위치 (3,5,7)의 cos/sin 값:\\n{rope_result[0].numpy()}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  3D RoPE는 각 공간 축에 독립적인 주파수를 할당합니다.\")\n",
    "print(\"  차원을 3등분하여 시간/높이/너비 축에 각각 적용합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4: 시공간 위치 임베딩 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "3D 패치 시퀀스에 위치 정보를 추가하는 두 가지 방법을 비교하세요:\n",
    "\n",
    "1. **Learnable Position Embedding**: $z_i = z_i + \\text{PE}[i]$ (학습 가능)\n",
    "2. **3D RoPE**: 쿼리/키에 회전 적용 (상대 위치)\n",
    "\n",
    "$T_{\\text{lat}}=4, H_{\\text{lat}}=8, W_{\\text{lat}}=8$, 패치 크기 $(1,2,2)$, $d_{model}=64$일 때:\n",
    "- 총 시퀀스 길이는?\n",
    "- Learnable PE의 파라미터 수는?\n",
    "\n",
    "**여러분의 예측:** 시퀀스 길이 = `?`, PE 파라미터 수 = `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: 시공간 위치 임베딩\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "T_lat, H_lat, W_lat = 4, 8, 8\n",
    "pt, ph, pw = 1, 2, 2\n",
    "d_model = 64\n",
    "\n",
    "seq_len = (T_lat // pt) * (H_lat // ph) * (W_lat // pw)\n",
    "learnable_params = seq_len * d_model\n",
    "\n",
    "print(f\"\\nLatent 크기: T={T_lat}, H={H_lat}, W={W_lat}\")\n",
    "print(f\"패치 크기: ({pt}, {ph}, {pw})\")\n",
    "print(f\"시퀀스 길이: ({T_lat}/{pt}) x ({H_lat}/{ph}) x ({W_lat}/{pw}) = {seq_len}\")\n",
    "print(f\"\\n1. Learnable PE 파라미터 수: {seq_len} x {d_model} = {learnable_params:,}\")\n",
    "\n",
    "# Learnable PE 구현\n",
    "pos_embed = tf.Variable(tf.random.normal([1, seq_len, d_model]) * 0.02)\n",
    "x = tf.random.normal([2, seq_len, d_model])\n",
    "x_with_pe = x + pos_embed\n",
    "print(f\"   입력 shape: {x.shape} → 출력 shape: {x_with_pe.shape}\")\n",
    "\n",
    "# 3D RoPE 구현 (Q, K에 적용)\n",
    "print(f\"\\n2. 3D RoPE (파라미터 0개 — 학습 불필요)\")\n",
    "\n",
    "positions = []\n",
    "for t in range(T_lat // pt):\n",
    "    for h in range(H_lat // ph):\n",
    "        for w in range(W_lat // pw):\n",
    "            positions.append([t, h, w])\n",
    "positions = tf.constant(positions)\n",
    "\n",
    "d_per_axis = d_model // 3\n",
    "rope_freqs = compute_3d_rope(positions, d_per_axis + (1 if d_model % 3 else 0))\n",
    "print(f\"   위치 좌표 shape: {positions.shape}\")\n",
    "print(f\"   RoPE 주파수 shape: {rope_freqs.shape}\")\n",
    "\n",
    "print(f\"\\n비교 요약:\")\n",
    "print(f\"{'방법':<20} | {'파라미터 수':>12} | {'해상도 일반화':>15}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Learnable PE':<20} | {learnable_params:>12,} | {'제한적 (고정 길이)':>15}\")\n",
    "print(f\"{'3D RoPE':<20} | {'0':>12} | {'유연 (상대 위치)':>15}\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  3D RoPE는 파라미터 없이 상대 위치를 인코딩합니다.\")\n",
    "print(\"  학습 시와 다른 해상도에서도 자연스럽게 확장 가능합니다.\")\n",
    "print(\"  HunyuanVideo, SD3 등 최신 모델은 모두 RoPE를 사용합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 도전: 완전한 시공간 패처 모듈 <a name='bonus'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 기능을 모두 포함하는 `SpatiotemporalPatcher` 클래스를 구현하세요:\n",
    "\n",
    "1. 3D 비디오 텐서 → 패치 추출 (reshape + transpose)\n",
    "2. 패치 → 선형 임베딩 ($d_{model}$ 차원)\n",
    "3. 3D 위치 좌표 생성 (t, h, w)\n",
    "4. 3D RoPE 주파수 계산\n",
    "\n",
    "입력: `(B, T, H, W, C)` → 출력: `(B, N, d_model)` + RoPE 주파수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이 ──────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"종합 도전 풀이: 시공간 패처 모듈\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class SpatiotemporalPatcher(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size, d_model, rope_base=10000.0):\n",
    "        super().__init__()\n",
    "        self.pt, self.ph, self.pw = patch_size\n",
    "        self.d_model = d_model\n",
    "        self.rope_base = rope_base\n",
    "        self.patch_dim = self.pt * self.ph * self.pw\n",
    "        self.projection = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        C = input_shape[-1]\n",
    "        full_patch_dim = self.patch_dim * C\n",
    "        self.projection = tf.keras.layers.Dense(self.d_model)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def extract_patches(self, x):\n",
    "        B = tf.shape(x)[0]\n",
    "        T, H, W, C = x.shape[1], x.shape[2], x.shape[3], x.shape[4]\n",
    "        n_t = T // self.pt\n",
    "        n_h = H // self.ph\n",
    "        n_w = W // self.pw\n",
    "        x = tf.reshape(x, [B, n_t, self.pt, n_h, self.ph, n_w, self.pw, C])\n",
    "        x = tf.transpose(x, [0, 1, 3, 5, 2, 4, 6, 7])\n",
    "        N = n_t * n_h * n_w\n",
    "        x = tf.reshape(x, [B, N, self.pt * self.ph * self.pw * C])\n",
    "        return x, (n_t, n_h, n_w)\n",
    "\n",
    "    def make_positions(self, n_t, n_h, n_w):\n",
    "        positions = []\n",
    "        for t in range(n_t):\n",
    "            for h in range(n_h):\n",
    "                for w in range(n_w):\n",
    "                    positions.append([t, h, w])\n",
    "        return tf.constant(positions, dtype=tf.float32)\n",
    "\n",
    "    def compute_rope(self, positions, d_per_axis):\n",
    "        freqs_list = []\n",
    "        for axis in range(3):\n",
    "            pos = positions[:, axis:axis+1]\n",
    "            k = tf.range(d_per_axis // 2, dtype=tf.float32)\n",
    "            theta = 1.0 / tf.pow(self.rope_base, 2.0 * k / tf.cast(d_per_axis, tf.float32))\n",
    "            angles = pos * theta[tf.newaxis, :]\n",
    "            cos_vals = tf.cos(angles)\n",
    "            sin_vals = tf.sin(angles)\n",
    "            freqs_list.append(tf.stack([cos_vals, sin_vals], axis=-1))\n",
    "        return tf.concat(freqs_list, axis=1)\n",
    "\n",
    "    def call(self, x):\n",
    "        patches, (n_t, n_h, n_w) = self.extract_patches(x)\n",
    "        embedded = self.projection(patches)\n",
    "        positions = self.make_positions(n_t, n_h, n_w)\n",
    "        d_per_axis = self.d_model // 3\n",
    "        rope_freqs = self.compute_rope(positions, d_per_axis)\n",
    "        return embedded, rope_freqs, positions\n",
    "\n",
    "# 테스트\n",
    "B, T, H, W, C = 2, 8, 32, 32, 3\n",
    "patch_size = (2, 4, 4)\n",
    "d_model = 96\n",
    "\n",
    "patcher = SpatiotemporalPatcher(patch_size, d_model)\n",
    "video = tf.random.normal([B, T, H, W, C])\n",
    "\n",
    "embedded, rope_freqs, positions = patcher(video)\n",
    "\n",
    "print(f\"\\n입력 비디오 shape: {video.shape}\")\n",
    "print(f\"패치 크기: {patch_size}\")\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"\\n출력:\")\n",
    "print(f\"  임베딩 shape: {embedded.shape}\")\n",
    "print(f\"  RoPE 주파수 shape: {rope_freqs.shape}\")\n",
    "print(f\"  위치 좌표 shape: {positions.shape}\")\n",
    "\n",
    "expected_n = (T // patch_size[0]) * (H // patch_size[1]) * (W // patch_size[2])\n",
    "print(f\"\\n검증:\")\n",
    "print(f\"  예상 시퀀스 길이: {expected_n}\")\n",
    "print(f\"  실제 시퀀스 길이: {embedded.shape[1]}\")\n",
    "print(f\"  일치: {expected_n == embedded.shape[1]}\")\n",
    "\n",
    "n_params = sum(p.numpy().size for p in patcher.trainable_variables)\n",
    "print(f\"  총 파라미터 수: {n_params:,}\")\n",
    "\n",
    "print(f\"\\n위치 좌표 (처음 5개):\")\n",
    "for i in range(min(5, positions.shape[0])):\n",
    "    t, h, w = positions[i].numpy()\n",
    "    print(f\"  토큰 {i}: (t={t:.0f}, h={h:.0f}, w={w:.0f})\")\n",
    "\n",
    "print(\"\\n[해설]\")\n",
    "print(\"  SpatiotemporalPatcher는 비디오를 DiT가 처리할 수 있는\")\n",
    "print(\"  토큰 시퀀스로 변환하는 핵심 전처리 모듈입니다.\")\n",
    "print(\"  3D RoPE를 통해 시공간 위치 정보를 인코딩합니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}