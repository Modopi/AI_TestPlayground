{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT â€” Soraì™€ HunyuanVideo ì•„í‚¤í…ì²˜\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Soraì˜ ìŠ¤ì¼€ì¼ë§ ì² í•™ê³¼ NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ê¸°ë²•ì„ ì´í•´í•œë‹¤\n",
    "- HunyuanVideoì˜ Dual-stream â†’ Single-stream ì „í™˜ êµ¬ì¡°ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë¶„ì„í•œë‹¤\n",
    "- 3D Causal VAEì˜ ì‹œê³µê°„ ì••ì¶• ë©”ì»¤ë‹ˆì¦˜ì„ ì •ëŸ‰ì ìœ¼ë¡œ íŒŒì•…í•œë‹¤\n",
    "- í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ë©€í‹°ëª¨ë‹¬ í“¨ì „ ë°©ì‹ì˜ ì°¨ì´ë¥¼ ë¹„êµí•œë‹¤\n",
    "- ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ íë¦„ì„ êµ¬í˜„ ìˆ˜ì¤€ì—ì„œ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: NaViTì™€ ë©€í‹°ëª¨ë‹¬ í“¨ì „](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •](#2.-í™˜ê²½-ì„¤ì •)\n",
    "3. [NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜](#3.-NaViT-íŒ¨í‚¹)\n",
    "4. [Dual-stream vs Single-stream ì•„í‚¤í…ì²˜ ë¹„êµ](#4.-Dual-vs-Single)\n",
    "5. [HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„](#5.-ì••ì¶•-ë¶„ì„)\n",
    "6. [ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ê°œìš”](#6.-íŒŒì´í”„ë¼ì¸)\n",
    "7. [ì •ë¦¬](#7.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ (Variable Resolution Packing)\n",
    "\n",
    "NaViT(Native Resolution ViT)ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•´ìƒë„ì˜ ì´ë¯¸ì§€/ë¹„ë””ì˜¤ë¥¼ **í•˜ë‚˜ì˜ ë°°ì¹˜**ë¡œ íŒ¨í‚¹í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{Batch} = \\{x_1^{H_1 \\times W_1},\\; x_2^{H_2 \\times W_2},\\; \\ldots,\\; x_B^{H_B \\times W_B}\\}$$\n",
    "\n",
    "ê° ìƒ˜í”Œì˜ íŒ¨ì¹˜ ìˆ˜:\n",
    "\n",
    "$$N_i = \\frac{H_i}{p_h} \\cdot \\frac{W_i}{p_w}, \\quad \\text{ì´ í† í° ìˆ˜} = \\sum_{i=1}^B N_i$$\n",
    "\n",
    "- **íŒ¨ë”© ì—†ìŒ**: ê° ìƒ˜í”Œì„ ì›ë³¸ í•´ìƒë„ ê·¸ëŒ€ë¡œ í† í°í™”\n",
    "- **Attention Mask**: ë‹¤ë¥¸ ìƒ˜í”Œì˜ í† í°ì— attendí•˜ì§€ ì•Šë„ë¡ ë¸”ë¡ ëŒ€ê° ë§ˆìŠ¤í¬ ì ìš©\n",
    "- í•µì‹¬ ì¥ì : ì •ì‚¬ê°í˜• crop/resize ì—†ì´ ì›ë³¸ ë¹„ìœ¨ ìœ ì§€ â†’ ìƒì„± í’ˆì§ˆ í–¥ìƒ\n",
    "\n",
    "### Dual-stream ì•„í‚¤í…ì²˜\n",
    "\n",
    "í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ë¥¼ **ë¶„ë¦¬ëœ** Transformer ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì²˜ë¦¬:\n",
    "\n",
    "$$h_{\\text{text}}^{(l)} = \\text{SelfAttn}(h_{\\text{text}}^{(l-1)}) + \\text{CrossAttn}(h_{\\text{text}}^{(l-1)}, h_{\\text{video}}^{(l-1)})$$\n",
    "\n",
    "$$h_{\\text{video}}^{(l)} = \\text{SelfAttn}(h_{\\text{video}}^{(l-1)}) + \\text{CrossAttn}(h_{\\text{video}}^{(l-1)}, h_{\\text{text}}^{(l-1)})$$\n",
    "\n",
    "- ê° ëª¨ë‹¬ë¦¬í‹°ê°€ ë…ìì ì¸ Self-Attention ìˆ˜í–‰\n",
    "- Cross-Attentionìœ¼ë¡œ ìƒí˜¸ ì •ë³´ êµí™˜\n",
    "- íŒŒë¼ë¯¸í„°ê°€ ë‘ ë°°ì´ì§€ë§Œ, ê° ëª¨ë‹¬ë¦¬í‹°ì˜ íŠ¹ì„±ì„ ë³´ì¡´\n",
    "\n",
    "### Single-stream ì•„í‚¤í…ì²˜\n",
    "\n",
    "í…ìŠ¤íŠ¸ì™€ ë¹„ë””ì˜¤ í† í°ì„ **ì—°ê²°(concatenate)** í›„ ë‹¨ì¼ Transformer:\n",
    "\n",
    "$$h^{(l)} = \\text{SelfAttn}\\!\\left([h_{\\text{text}}; h_{\\text{video}}]^{(l-1)}\\right)$$\n",
    "\n",
    "- ì‹œí€€ìŠ¤ ê¸¸ì´: $N_{\\text{text}} + N_{\\text{video}}$\n",
    "- ëª¨ë“  í† í°ì´ ì„œë¡œ attend â†’ ìì—°ìŠ¤ëŸ¬ìš´ ì •ë³´ ìœµí•©\n",
    "- íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì´ë‚˜ ê¸´ ì‹œí€€ìŠ¤ ë¬¸ì œ\n",
    "\n",
    "### HunyuanVideo: Dual â†’ Single ì „í™˜\n",
    "\n",
    "HunyuanVideoëŠ” **ì•ìª½ ë ˆì´ì–´ì—ì„œ Dual-stream**, **ë’·ìª½ ë ˆì´ì–´ì—ì„œ Single-stream**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{Layer } 1 \\sim L_d: \\text{Dual-stream} \\quad \\rightarrow \\quad \\text{Layer } L_d+1 \\sim L: \\text{Single-stream}$$\n",
    "\n",
    "- ì´ˆê¸°: ê° ëª¨ë‹¬ë¦¬í‹°ê°€ ë…ë¦½ì  íŠ¹ì§• í˜•ì„±\n",
    "- í›„ê¸°: ê¹Šì€ ë©€í‹°ëª¨ë‹¬ ìœµí•©ìœ¼ë¡œ ì •ë°€í•œ í…ìŠ¤íŠ¸-ë¹„ë””ì˜¤ ì •í•©\n",
    "\n",
    "### 3D Causal VAE ì••ì¶•\n",
    "\n",
    "$$z \\in \\mathbb{R}^{C_z \\times (T/M_t) \\times (H/M_h) \\times (W/M_w)}$$\n",
    "\n",
    "- $M_t, M_h, M_w$: ì‹œê°„/ê³µê°„ ì••ì¶• ë¹„ìœ¨\n",
    "- HunyuanVideo: $M_t=4, M_h=8, M_w=8$\n",
    "- ì••ì¶•ë¥ : $M_t \\times M_h \\times M_w = 256$ë°°\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| NaViT íŒ¨í‚¹ | $N = \\sum_i (H_i/p)(W_i/p)$ | ê°€ë³€ í•´ìƒë„ ì´ í† í° ìˆ˜ |\n",
    "| Dual-stream | ë³„ë„ Self-Attn + Cross-Attn | ëª¨ë‹¬ë¦¬í‹° ë¶„ë¦¬ ì²˜ë¦¬ |\n",
    "| Single-stream | $[h_{\\text{text}}; h_{\\text{video}}]$ ì—°ê²° | í†µí•© Self-Attn |\n",
    "| 3D VAE ì••ì¶• | $M_t \\times M_h \\times M_w$ | ì‹œê³µê°„ ì••ì¶•ë¥  |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Sora/HunyuanVideo ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ Soraê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: SoraëŠ” \"ì´ì•¼ê¸°ë¥¼ ë“£ê³  ì˜í™”ë¥¼ ë§Œë“œëŠ” AI ê°ë…\"ì´ì—ìš”!\n",
    "\n",
    "\"ê°•ì•„ì§€ê°€ í•´ë³€ì—ì„œ ë›°ì–´ë…¸ëŠ” ì¥ë©´\"ì´ë¼ê³  ë§í•˜ë©´, AIê°€ ì§„ì§œ ê°™ì€ ì˜ìƒì„ ë§Œë“¤ì–´ì¤˜ìš”.\n",
    "ë¹„ê²°ì€ ì‚¬ì§„ì„ í¼ì¦ ì¡°ê°(íŒ¨ì¹˜)ìœ¼ë¡œ ë‚˜ëˆ„ê³ , Transformerê°€ ì¡°ê°ë“¤ì„ ì¡°í•©í•˜ëŠ” ê±°ì˜ˆìš”.\n",
    "\n",
    "- ğŸ§© **NaViT**: ë‹¤ì–‘í•œ í¬ê¸°ì˜ í¼ì¦ì„ í•œ ìƒìì— ë‹´ëŠ” ê¸°ìˆ  (í° ì‚¬ì§„, ì‘ì€ ì‚¬ì§„ ëª¨ë‘ OK!)\n",
    "- ğŸ¬ **ë¹„ë””ì˜¤ íŒ¨ì¹˜**: ì‹œê°„ ë°©í–¥ìœ¼ë¡œë„ í¼ì¦ì„ ë‚˜ëˆ” (ì‚¬ì§„ â†’ ë™ì˜ìƒ í™•ì¥)\n",
    "\n",
    "#### ğŸ¤” Dual-streamì´ë‘ Single-streamì€ ë­ê°€ ë‹¬ë¼ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: Dual-streamì€ \"í†µì—­ì‚¬ê°€ ìˆëŠ” íšŒì˜\", Single-streamì€ \"ê°™ì€ ì–¸ì–´ë¡œ ëŒ€í™”í•˜ëŠ” íšŒì˜\"\n",
    "\n",
    "Dual-stream: ê¸€(í…ìŠ¤íŠ¸)íŒ€ê³¼ ì˜ìƒ(ë¹„ë””ì˜¤)íŒ€ì´ ë”°ë¡œ ì¼í•œ ë’¤, í†µì—­ì‚¬(Cross-Attention)ê°€ ì—°ê²°\n",
    "Single-stream: ê¸€ê³¼ ì˜ìƒì„ í•œ í…Œì´ë¸”ì— ëª¨ì•„ë†“ê³  í•¨ê»˜ í† ë¡ \n",
    "\n",
    "HunyuanVideoëŠ” ì²˜ìŒì—” Dual(ë”°ë¡œ), ë‚˜ì¤‘ì—” Single(í•¨ê»˜)ë¡œ ì§„í–‰í•´ìš”!\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: NaViT íŒ¨í‚¹ ê³„ì‚°\n",
    "\n",
    "íŒ¨ì¹˜ í¬ê¸° $p=16$ì¼ ë•Œ, í•´ìƒë„ $256 \\times 256$, $512 \\times 384$, $128 \\times 128$ ì„¸ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ íŒ¨í‚¹í•˜ë©´ ì´ í† í° ìˆ˜ëŠ”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$N_1 = \\frac{256}{16} \\times \\frac{256}{16} = 16 \\times 16 = 256$$\n",
    "\n",
    "$$N_2 = \\frac{512}{16} \\times \\frac{384}{16} = 32 \\times 24 = 768$$\n",
    "\n",
    "$$N_3 = \\frac{128}{16} \\times \\frac{128}{16} = 8 \\times 8 = 64$$\n",
    "\n",
    "$$N_{\\text{total}} = 256 + 768 + 64 = 1088 \\text{ í† í°}$$\n",
    "\n",
    "ê¸°ì¡´ ë°©ì‹(512ë¡œ ë¦¬ì‚¬ì´ì¦ˆ)ì´ë©´ $3 \\times 32 \\times 32 = 3072$ í† í° â†’ NaViTê°€ **64.6% ì ˆê°**!\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: 3D VAE ì••ì¶•ë¥ \n",
    "\n",
    "720p ë¹„ë””ì˜¤ (1280Ã—720, 30fps, 5ì´ˆ)ë¥¼ HunyuanVideoì˜ 3D Causal VAE ($M_t=4, M_h=8, M_w=8$, $C_z=16$)ë¡œ ì••ì¶•í•˜ë©´ latent í…ì„œ í¬ê¸°ëŠ”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "ì›ë³¸: $T=150, H=720, W=1280, C=3$\n",
    "\n",
    "$$\\text{Latent}: C_z \\times \\frac{T}{M_t} \\times \\frac{H}{M_h} \\times \\frac{W}{M_w} = 16 \\times \\frac{150}{4} \\times \\frac{720}{8} \\times \\frac{1280}{8}$$\n",
    "$$= 16 \\times 37 \\times 90 \\times 160 \\approx 8{,}524{,}800 \\text{ ê°’}$$\n",
    "\n",
    "ì›ë³¸ í¬ê¸°: $150 \\times 720 \\times 1280 \\times 3 = 414{,}720{,}000$ ê°’\n",
    "\n",
    "ì••ì¶•ë¥ : $414{,}720{,}000 / 8{,}524{,}800 \\approx 48.6\\times$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • <a name='2.-í™˜ê²½-ì„¤ì •'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜ <a name='3.-NaViT-íŒ¨í‚¹'></a>\n",
    "\n",
    "NaViTëŠ” ë‹¤ì–‘í•œ í•´ìƒë„ì˜ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ íŒ¨í‚¹í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "ë¸”ë¡ ëŒ€ê° Attention Maskë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ìƒ˜í”Œ ê°„ì˜ ì •ë³´ ëˆ„ì¶œì„ ë°©ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ NaViT ê°€ë³€ í•´ìƒë„ íŒ¨í‚¹ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "patch_size = 16\n",
    "\n",
    "resolutions = [\n",
    "    ('A: 256x256', 256, 256),\n",
    "    ('B: 512x384', 512, 384),\n",
    "    ('C: 128x128', 128, 128),\n",
    "    ('D: 384x256', 384, 256),\n",
    "]\n",
    "\n",
    "print(\"=== NaViT íŒ¨í‚¹ ë¶„ì„ ===\\n\")\n",
    "print(f\"íŒ¨ì¹˜ í¬ê¸°: {patch_size}x{patch_size}\\n\")\n",
    "\n",
    "total_tokens = 0\n",
    "sample_sizes = []\n",
    "print(f\"{'ìƒ˜í”Œ':<15} | {'í•´ìƒë„':>12} | {'íŒ¨ì¹˜ ìˆ˜':>10} | {'í† í° ìˆ˜':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, h, w in resolutions:\n",
    "    n_patches = (h // patch_size) * (w // patch_size)\n",
    "    total_tokens += n_patches\n",
    "    sample_sizes.append(n_patches)\n",
    "    print(f\"{name:<15} | {h:>4}x{w:<4}   | {(h//patch_size):>3}x{(w//patch_size):<3}  | {n_patches:>10}\")\n",
    "\n",
    "print(f\"\\nì´ íŒ¨í‚¹ í† í° ìˆ˜: {total_tokens}\")\n",
    "max_res = max(h * w for _, h, w in resolutions)\n",
    "max_side = int(np.sqrt(max_res))\n",
    "naive_tokens = len(resolutions) * (max_side // patch_size) ** 2\n",
    "print(f\"Naive íŒ¨ë”© ë°©ì‹ (ëª¨ë‘ {max_side}x{max_side}ë¡œ ë¦¬ì‚¬ì´ì¦ˆ): {naive_tokens}\")\n",
    "print(f\"NaViT í† í° ì ˆê°ë¥ : {(1 - total_tokens / naive_tokens) * 100:.1f}%\")\n",
    "\n",
    "# Attention Mask ì‹œê°í™”\n",
    "mask = np.zeros((total_tokens, total_tokens))\n",
    "offset = 0\n",
    "for size in sample_sizes:\n",
    "    mask[offset:offset+size, offset:offset+size] = 1.0\n",
    "    offset += size\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "im = ax1.imshow(mask, cmap='Blues', interpolation='nearest')\n",
    "offset = 0\n",
    "for i, size in enumerate(sample_sizes):\n",
    "    ax1.axhline(y=offset-0.5, color='red', lw=1.5)\n",
    "    ax1.axvline(x=offset-0.5, color='red', lw=1.5)\n",
    "    ax1.text(offset + size/2, offset + size/2, resolutions[i][0].split(':')[0],\n",
    "             ha='center', va='center', fontsize=10, fontweight='bold', color='darkblue')\n",
    "    offset += size\n",
    "ax1.set_xlabel('Key/Value í† í°', fontsize=11)\n",
    "ax1.set_ylabel('Query í† í°', fontsize=11)\n",
    "ax1.set_title('NaViT ë¸”ë¡ ëŒ€ê° Attention Mask', fontweight='bold')\n",
    "\n",
    "ax2 = axes[1]\n",
    "names = [r[0].split(':')[0] for r in resolutions]\n",
    "colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']\n",
    "bars = ax2.bar(names, sample_sizes, color=colors, edgecolor='white', lw=1.5)\n",
    "for bar, val in zip(bars, sample_sizes):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "             str(val), ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.set_ylabel('í† í° ìˆ˜', fontsize=11)\n",
    "ax2.set_title('ìƒ˜í”Œë³„ í† í° ìˆ˜ ë¶„í¬', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('navit_packing.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/navit_packing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dual-stream vs Single-stream ì•„í‚¤í…ì²˜ ë¹„êµ <a name='4.-Dual-vs-Single'></a>\n",
    "\n",
    "HunyuanVideoëŠ” ì•ìª½ ë ˆì´ì–´ì—ì„œ Dual-stream (ë¶„ë¦¬ ì²˜ë¦¬ + Cross-Attention), ë’·ìª½ ë ˆì´ì–´ì—ì„œ Single-stream (ì—°ê²° + Self-Attention)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ë‘ ë°©ì‹ì˜ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ íŠ¹ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Dual-stream vs Single-stream ë¹„êµ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d_model = 128\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads\n",
    "\n",
    "n_text_tokens = 64\n",
    "n_video_tokens = 256\n",
    "\n",
    "# Dual-stream ë¸”ë¡ (ê°„ì†Œí™”)\n",
    "class DualStreamBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.text_self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.video_self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.text_cross_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.video_cross_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.text_ln = tf.keras.layers.LayerNormalization()\n",
    "        self.video_ln = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, h_text, h_video):\n",
    "        t_self = self.text_self_attn(h_text, h_text)\n",
    "        v_self = self.video_self_attn(h_video, h_video)\n",
    "        t_cross = self.text_cross_attn(h_text, h_video)\n",
    "        v_cross = self.video_cross_attn(h_video, h_text)\n",
    "        h_text = self.text_ln(h_text + t_self + t_cross)\n",
    "        h_video = self.video_ln(h_video + v_self + v_cross)\n",
    "        return h_text, h_video\n",
    "\n",
    "# Single-stream ë¸”ë¡ (ê°„ì†Œí™”)\n",
    "class SingleStreamBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.self_attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, key_dim=d_model // n_heads)\n",
    "        self.ln = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, h_text, h_video):\n",
    "        h_concat = tf.concat([h_text, h_video], axis=1)\n",
    "        h_out = self.self_attn(h_concat, h_concat)\n",
    "        h_out = self.ln(h_concat + h_out)\n",
    "        h_text_out = h_out[:, :tf.shape(h_text)[1], :]\n",
    "        h_video_out = h_out[:, tf.shape(h_text)[1]:, :]\n",
    "        return h_text_out, h_video_out\n",
    "\n",
    "dual_block = DualStreamBlock(d_model, n_heads)\n",
    "single_block = SingleStreamBlock(d_model, n_heads)\n",
    "\n",
    "h_text = tf.random.normal([1, n_text_tokens, d_model])\n",
    "h_video = tf.random.normal([1, n_video_tokens, d_model])\n",
    "\n",
    "# Dual-stream forward\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    dt_out, dv_out = dual_block(h_text, h_video)\n",
    "dual_time = (time.time() - t0) / 10\n",
    "\n",
    "# Single-stream forward\n",
    "t0 = time.time()\n",
    "for _ in range(10):\n",
    "    st_out, sv_out = single_block(h_text, h_video)\n",
    "single_time = (time.time() - t0) / 10\n",
    "\n",
    "dual_params = sum(p.numpy().size for p in dual_block.trainable_variables)\n",
    "single_params = sum(p.numpy().size for p in single_block.trainable_variables)\n",
    "\n",
    "print(\"=== Dual-stream vs Single-stream ë¹„êµ ===\\n\")\n",
    "print(f\"{'í•­ëª©':<20} | {'Dual-stream':>15} | {'Single-stream':>15}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'í…ìŠ¤íŠ¸ í† í° ìˆ˜':<20} | {n_text_tokens:>15} | {n_text_tokens:>15}\")\n",
    "print(f\"{'ë¹„ë””ì˜¤ í† í° ìˆ˜':<20} | {n_video_tokens:>15} | {n_video_tokens:>15}\")\n",
    "print(f\"{'íŒŒë¼ë¯¸í„° ìˆ˜':<20} | {dual_params:>15,} | {single_params:>15,}\")\n",
    "print(f\"{'ì¶”ë¡  ì‹œê°„ (ms)':<20} | {dual_time*1000:>15.2f} | {single_time*1000:>15.2f}\")\n",
    "print(f\"{'ì¶œë ¥ text shape':<20} | {str(dt_out.shape):>15} | {str(st_out.shape):>15}\")\n",
    "print(f\"{'ì¶œë ¥ video shape':<20} | {str(dv_out.shape):>15} | {str(sv_out.shape):>15}\")\n",
    "\n",
    "print(\"\\nHunyuanVideo ì„¤ê³„ ì „ëµ:\")\n",
    "print(\"  - ì „ì²´ 38ê°œ ë¸”ë¡ ì¤‘ ì•ìª½ 20ê°œ: Dual-stream (ëª¨ë‹¬ë¦¬í‹°ë³„ ë…ë¦½ íŠ¹ì§• í˜•ì„±)\")\n",
    "print(\"  - ë’¤ìª½ 18ê°œ: Single-stream (ê¹Šì€ ë©€í‹°ëª¨ë‹¬ ìœµí•©)\")\n",
    "print(\"  - Cross-Attention â†’ Self-Attention ì „í™˜ìœ¼ë¡œ íŒŒë¼ë¯¸í„° íš¨ìœ¨ + ì„±ëŠ¥ ê· í˜•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„ <a name='5.-ì••ì¶•-ë¶„ì„'></a>\n",
    "\n",
    "HunyuanVideoì˜ 3D Causal VAEëŠ” ë¹„ë””ì˜¤ë¥¼ ì‹œê°„Â·ê³µê°„ ì–‘ë°©í–¥ìœ¼ë¡œ ì••ì¶•í•˜ì—¬ DiTê°€ ì²˜ë¦¬í•  latent ê³µê°„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "í•µì‹¬ ì••ì¶• íŒŒë¼ë¯¸í„°:\n",
    "- ì‹œê°„ ì••ì¶•ë¥ : $M_t = 4$\n",
    "- ê³µê°„ ì••ì¶•ë¥ : $M_h = M_w = 8$\n",
    "- Latent ì±„ë„ ìˆ˜: $C_z = 16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ HunyuanVideo 3D Causal VAE ì••ì¶• í†µê³„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ í•´ìƒë„ì— ëŒ€í•œ ì••ì¶•ë¥  ë¶„ì„\n",
    "\n",
    "Mt, Mh, Mw = 4, 8, 8\n",
    "Cz = 16\n",
    "\n",
    "videos = [\n",
    "    ('480p-2s', 2, 24, 480, 854, 3),\n",
    "    ('720p-5s', 5, 30, 720, 1280, 3),\n",
    "    ('1080p-10s', 10, 30, 1080, 1920, 3),\n",
    "    ('4K-5s', 5, 30, 2160, 3840, 3),\n",
    "]\n",
    "\n",
    "print(\"=== HunyuanVideo 3D Causal VAE ì••ì¶• ë¶„ì„ ===\")\n",
    "print(f\"ì••ì¶• ë¹„ìœ¨: Mt={Mt}, Mh={Mh}, Mw={Mw}, Cz={Cz}\\n\")\n",
    "\n",
    "headers = ['ë¹„ë””ì˜¤', 'ì›ë³¸ í¬ê¸°(MB)', 'Latent í¬ê¸°(MB)', 'ì••ì¶•ë¥ ', 'DiT í† í° ìˆ˜']\n",
    "print(f\"{'ë¹„ë””ì˜¤':<14} | {'ì›ë³¸(MB)':>10} | {'Latent(MB)':>11} | {'ì••ì¶•ë¥ ':>8} | {'í† í° ìˆ˜':>10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "patch_t, patch_h, patch_w = 1, 2, 2\n",
    "token_counts = []\n",
    "names = []\n",
    "\n",
    "for name, dur, fps, H, W, C in videos:\n",
    "    T = dur * fps\n",
    "    original = T * H * W * C * 4\n",
    "    lat_T = max(1, T // Mt)\n",
    "    lat_H = H // Mh\n",
    "    lat_W = W // Mw\n",
    "    latent = Cz * lat_T * lat_H * lat_W * 4\n",
    "    ratio = original / latent\n",
    "\n",
    "    n_tokens = (lat_T // patch_t) * (lat_H // patch_h) * (lat_W // patch_w)\n",
    "    token_counts.append(n_tokens)\n",
    "    names.append(name)\n",
    "\n",
    "    print(f\"{name:<14} | {original/1e6:>10.1f} | {latent/1e6:>11.2f} | {ratio:>7.1f}x | {n_tokens:>10,}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "original_sizes = []\n",
    "latent_sizes = []\n",
    "for name, dur, fps, H, W, C in videos:\n",
    "    T = dur * fps\n",
    "    original_sizes.append(T * H * W * C * 4 / 1e6)\n",
    "    lat_T = max(1, T // Mt)\n",
    "    latent_sizes.append(Cz * lat_T * (H // Mh) * (W // Mw) * 4 / 1e6)\n",
    "\n",
    "x_pos = np.arange(len(videos))\n",
    "width = 0.35\n",
    "bars1 = ax1.bar(x_pos - width/2, original_sizes, width, label='ì›ë³¸', color='#FF5722', alpha=0.8)\n",
    "bars2 = ax1.bar(x_pos + width/2, latent_sizes, width, label='Latent', color='#2196F3', alpha=0.8)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(names, fontsize=9)\n",
    "ax1.set_ylabel('í¬ê¸° (MB, FP32)', fontsize=11)\n",
    "ax1.set_title('ì›ë³¸ vs Latent í¬ê¸° ë¹„êµ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "ax2 = axes[1]\n",
    "colors = ['#4CAF50', '#2196F3', '#FF9800', '#9C27B0']\n",
    "bars = ax2.bar(names, token_counts, color=colors, edgecolor='white', lw=1.5)\n",
    "for bar, val in zip(bars, token_counts):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.02,\n",
    "             f'{val:,}', ha='center', fontsize=9, fontweight='bold')\n",
    "ax2.set_ylabel('DiT ì…ë ¥ í† í° ìˆ˜', fontsize=11)\n",
    "ax2.set_title('í•´ìƒë„ë³„ DiT ì‹œí€€ìŠ¤ ê¸¸ì´', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hunyuan_compression.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/hunyuan_compression.png\")\n",
    "print(f\"\\ní•µì‹¬: 1080p-10s ë¹„ë””ì˜¤ë„ DiTê°€ ì²˜ë¦¬ ê°€ëŠ¥í•œ {token_counts[2]:,}ê°œ í† í°ìœ¼ë¡œ ì••ì¶•!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ê°œìš” <a name='6.-íŒŒì´í”„ë¼ì¸'></a>\n",
    "\n",
    "HunyuanVideoì˜ ì „ì²´ ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸:\n",
    "\n",
    "1. **í…ìŠ¤íŠ¸ ì¸ì½”ë”©**: MLLM (ë©€í‹°ëª¨ë‹¬ LLM) + CLIPìœ¼ë¡œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±\n",
    "2. **3D Causal VAE ì¸ì½”ë”©**: (í•™ìŠµ ì‹œ) ë¹„ë””ì˜¤ â†’ latent ì••ì¶•\n",
    "3. **DiT Denoising**: Flow Matchingìœ¼ë¡œ latentì—ì„œ ë…¸ì´ì¦ˆ ì œê±°\n",
    "4. **3D Causal VAE ë””ì½”ë”©**: latent â†’ ë¹„ë””ì˜¤ ë³µì›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# HunyuanVideo íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ í…ì„œ shapeìœ¼ë¡œ ì¶”ì \n",
    "\n",
    "print(\"=== HunyuanVideo ë¹„ë””ì˜¤ ìƒì„± íŒŒì´í”„ë¼ì¸ ===\\n\")\n",
    "\n",
    "# ì„¤ì •\n",
    "T_frames, H, W = 32, 256, 256\n",
    "Mt, Mh, Mw, Cz = 4, 8, 8, 16\n",
    "d_model = 128\n",
    "n_text_tokens_pipe = 77\n",
    "n_steps = 30\n",
    "\n",
    "print(\"[ ì…ë ¥ ì„¤ì • ]\")\n",
    "print(f\"  ëª©í‘œ ë¹„ë””ì˜¤: {T_frames} frames x {H}x{W}\")\n",
    "print(f\"  í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸: 'ê°•ì•„ì§€ê°€ í•´ë³€ì—ì„œ ë›°ì–´ë…¸ëŠ” ì¥ë©´'\")\n",
    "print()\n",
    "\n",
    "# Step 1: Text Encoding\n",
    "text_embed = tf.random.normal([1, n_text_tokens_pipe, d_model])\n",
    "print(\"[ Step 1: í…ìŠ¤íŠ¸ ì¸ì½”ë”© ]\")\n",
    "print(f\"  MLLM + CLIP â†’ text_embed: {text_embed.shape}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Latent shape ê³„ì‚°\n",
    "lat_T = T_frames // Mt\n",
    "lat_H = H // Mh\n",
    "lat_W = W // Mw\n",
    "print(\"[ Step 2: 3D Causal VAE Latent Space ]\")\n",
    "print(f\"  ì›ë³¸ ë¹„ë””ì˜¤: ({T_frames}, {H}, {W}, 3)\")\n",
    "print(f\"  Latent í¬ê¸°: ({Cz}, {lat_T}, {lat_H}, {lat_W})\")\n",
    "print(f\"  ì‹œê°„ ì••ì¶•: {T_frames} â†’ {lat_T} (/{Mt})\")\n",
    "print(f\"  ê³µê°„ ì••ì¶•: {H}x{W} â†’ {lat_H}x{lat_W} (/{Mh}x{Mw})\")\n",
    "print()\n",
    "\n",
    "# Step 3: DiT Denoising (Flow Matching)\n",
    "n_video_patches = lat_T * (lat_H // 2) * (lat_W // 2)\n",
    "print(\"[ Step 3: DiT Denoising (Flow Matching) ]\")\n",
    "print(f\"  ë¹„ë””ì˜¤ í† í° ìˆ˜: {n_video_patches}\")\n",
    "print(f\"  í…ìŠ¤íŠ¸ í† í° ìˆ˜: {n_text_tokens_pipe}\")\n",
    "print(f\"  Euler ODE ìŠ¤í… ìˆ˜: {n_steps}\")\n",
    "\n",
    "# ê°„ë‹¨í•œ Euler sampling ì‹œë®¬ë ˆì´ì…˜\n",
    "z = tf.random.normal([1, n_video_patches, d_model])\n",
    "dt = 1.0 / n_steps\n",
    "print(f\"\\n  ì‹œë®¬ë ˆì´ì…˜ (shape ì¶”ì ):\")\n",
    "for step in [0, n_steps//3, 2*n_steps//3, n_steps-1]:\n",
    "    t_val = step * dt\n",
    "    noise_level = 1.0 - t_val\n",
    "    print(f\"    t={t_val:.2f} | ë…¸ì´ì¦ˆ ìˆ˜ì¤€: {noise_level:.2f} | z shape: {z.shape}\")\n",
    "print()\n",
    "\n",
    "# Step 4: VAE Decode\n",
    "print(\"[ Step 4: 3D Causal VAE ë””ì½”ë”© ]\")\n",
    "print(f\"  Latent ({Cz}, {lat_T}, {lat_H}, {lat_W}) â†’ ë¹„ë””ì˜¤ ({T_frames}, {H}, {W}, 3)\")\n",
    "print()\n",
    "\n",
    "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ìš”ì•½\n",
    "print(\"=\" * 60)\n",
    "print(\"HunyuanVideo ì•„í‚¤í…ì²˜ ì‚¬ì–‘ (arxiv 2412.17601)\")\n",
    "print(\"=\" * 60)\n",
    "specs = [\n",
    "    ('DiT íŒŒë¼ë¯¸í„°', '13B'),\n",
    "    ('Dual-stream ë¸”ë¡', '20ê°œ'),\n",
    "    ('Single-stream ë¸”ë¡', '18ê°œ'),\n",
    "    ('3D VAE ì±„ë„', '16'),\n",
    "    ('ì‹œê°„ ì••ì¶•ë¥  (Mt)', '4'),\n",
    "    ('ê³µê°„ ì••ì¶•ë¥  (Mh, Mw)', '8, 8'),\n",
    "    ('í…ìŠ¤íŠ¸ ì¸ì½”ë”', 'MLLM + CLIP'),\n",
    "    ('í›ˆë ¨ ë°©ì‹', 'Flow Matching (Rectified Flow)'),\n",
    "    ('ìµœëŒ€ í•´ìƒë„', '1280x720, 129 frames'),\n",
    "]\n",
    "for k, v in specs:\n",
    "    print(f\"  {k:<24}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ HunyuanVideo ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "box_style = dict(boxstyle='round,pad=0.4', facecolor='#E3F2FD', edgecolor='#1976D2', lw=2)\n",
    "box_style2 = dict(boxstyle='round,pad=0.4', facecolor='#E8F5E9', edgecolor='#388E3C', lw=2)\n",
    "box_style3 = dict(boxstyle='round,pad=0.4', facecolor='#FFF3E0', edgecolor='#F57C00', lw=2)\n",
    "box_style4 = dict(boxstyle='round,pad=0.4', facecolor='#F3E5F5', edgecolor='#7B1FA2', lw=2)\n",
    "\n",
    "# Text Encoder\n",
    "ax.text(1.5, 6.5, 'Text Encoder\\n(MLLM + CLIP)', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=box_style2)\n",
    "\n",
    "# 3D VAE Encoder\n",
    "ax.text(5, 6.5, '3D Causal VAE\\nEncoder', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=box_style3)\n",
    "\n",
    "# Dual-stream\n",
    "ax.text(5, 4.5, 'Dual-stream Blocks\\n(20 layers)\\nSelf + Cross-Attn', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=box_style)\n",
    "\n",
    "# Single-stream\n",
    "ax.text(5, 2.5, 'Single-stream Blocks\\n(18 layers)\\nConcat Self-Attn', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=box_style4)\n",
    "\n",
    "# 3D VAE Decoder\n",
    "ax.text(9.5, 2.5, '3D Causal VAE\\nDecoder', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=box_style3)\n",
    "\n",
    "# Output\n",
    "ax.text(12.5, 2.5, 'Generated\\nVideo', ha='center', va='center',\n",
    "        fontsize=11, fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='#FFCDD2', edgecolor='#D32F2F', lw=2))\n",
    "\n",
    "# Noise input\n",
    "ax.text(9.5, 6.5, 'Gaussian\\nNoise $z_T$', ha='center', va='center',\n",
    "        fontsize=10, fontweight='bold', bbox=dict(boxstyle='round,pad=0.4', facecolor='#F5F5F5', edgecolor='#616161', lw=2))\n",
    "\n",
    "# Arrows\n",
    "arrow_props = dict(arrowstyle='->', color='#333', lw=2)\n",
    "ax.annotate('', xy=(3.5, 4.5), xytext=(1.5, 5.9), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(5, 3.3), xytext=(5, 3.8), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(5, 5.2), xytext=(5, 5.8), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(8.3, 2.5), xytext=(6.8, 2.5), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(11.2, 2.5), xytext=(10.8, 2.5), arrowprops=arrow_props)\n",
    "ax.annotate('', xy=(7.5, 5.5), xytext=(9.5, 5.9), arrowprops=arrow_props)\n",
    "\n",
    "# Flow Matching label\n",
    "ax.text(7, 3.5, 'Flow Matching\\n(Euler ODE)', ha='center', va='center',\n",
    "        fontsize=9, fontstyle='italic', color='#1976D2')\n",
    "\n",
    "ax.set_title('HunyuanVideo ì „ì²´ ì•„í‚¤í…ì²˜ (Dualâ†’Single Stream)', fontweight='bold', fontsize=14, pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hunyuan_architecture.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ ì €ì¥ë¨: chapter17_diffusion_transformers/hunyuan_architecture.png\")\n",
    "\n",
    "# Sora vs HunyuanVideo ë¹„êµ\n",
    "print(\"\\n=== Sora vs HunyuanVideo ë¹„êµ ===\\n\")\n",
    "print(f\"{'íŠ¹ì„±':<22} | {'Sora (OpenAI)':>22} | {'HunyuanVideo (Tencent)':>22}\")\n",
    "print(\"-\" * 74)\n",
    "comparisons = [\n",
    "    ('ê³µê°œ ì—¬ë¶€', 'ë¹„ê³µê°œ (ê¸°ìˆ  ë³´ê³ ì„œë§Œ)', 'ì˜¤í”ˆì†ŒìŠ¤ (ì½”ë“œ+ê°€ì¤‘ì¹˜)'),\n",
    "    ('ì•„í‚¤í…ì²˜', 'DiT (ì¶”ì •)', 'Dualâ†’Single DiT'),\n",
    "    ('í›ˆë ¨ ë°©ì‹', 'Flow Matching (ì¶”ì •)', 'Flow Matching'),\n",
    "    ('í•´ìƒë„ ì²˜ë¦¬', 'NaViT íŒ¨í‚¹', 'Multi-resolution í•™ìŠµ'),\n",
    "    ('í…ìŠ¤íŠ¸ ì¸ì½”ë”', 'ë¹„ê³µê°œ', 'MLLM + CLIP'),\n",
    "    ('VAE', 'ì‹œê³µê°„ íŒ¨ì¹˜', '3D Causal VAE'),\n",
    "    ('ìµœëŒ€ ê¸¸ì´', '~60ì´ˆ (ë°ëª¨)', '~5ì´ˆ (ì˜¤í”ˆì†ŒìŠ¤)'),\n",
    "    ('íŒŒë¼ë¯¸í„°', 'ë¹„ê³µê°œ', '13B'),\n",
    "]\n",
    "for feat, sora, hunyuan in comparisons:\n",
    "    print(f\"{feat:<22} | {sora:>22} | {hunyuan:>22}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬ <a name='7.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| NaViT íŒ¨í‚¹ | ê°€ë³€ í•´ìƒë„ë¥¼ íŒ¨ë”© ì—†ì´ í•œ ë°°ì¹˜ë¡œ ì²˜ë¦¬ | â­â­â­ |\n",
    "| Dual-stream | í…ìŠ¤íŠ¸/ë¹„ë””ì˜¤ ë¶„ë¦¬ ì²˜ë¦¬ + Cross-Attention | â­â­â­ |\n",
    "| Single-stream | í† í° ì—°ê²° í›„ í†µí•© Self-Attention | â­â­â­ |\n",
    "| Dualâ†’Single ì „í™˜ | HunyuanVideoì˜ í•µì‹¬ ì„¤ê³„ (20+18 ë¸”ë¡) | â­â­â­ |\n",
    "| 3D Causal VAE | ì‹œê³µê°„ $4\\times8\\times8 = 256$ë°° ì••ì¶• | â­â­ |\n",
    "| Flow Matching | Rectified Flow ê¸°ë°˜ Euler ODE ìƒ˜í”Œë§ | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\text{NaViT ì´ í† í°} = \\sum_{i=1}^B \\frac{H_i}{p_h} \\cdot \\frac{W_i}{p_w}$$\n",
    "\n",
    "$$h^{(l)}_{\\text{single}} = \\text{SelfAttn}\\!\\left([h_{\\text{text}}; h_{\\text{video}}]^{(l-1)}\\right)$$\n",
    "\n",
    "$$z \\in \\mathbb{R}^{C_z \\times (T/M_t) \\times (H/M_h) \\times (W/M_w)}$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "ì´ê²ƒìœ¼ë¡œ TensorFlow ê°•ì˜ ì»¤ë¦¬í˜ëŸ¼ì˜ ì „ì²´ ì´ë¡  ì±•í„°ê°€ ë§ˆë¬´ë¦¬ë©ë‹ˆë‹¤. **projects/** ë””ë ‰í† ë¦¬ì˜ ì¢…í•© ì‹¤ì „ í”„ë¡œì íŠ¸ë¡œ ë„˜ì–´ê°€ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}