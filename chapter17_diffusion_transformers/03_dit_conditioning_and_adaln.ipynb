{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ê³¼ DiT â€” adaLN-Zero ì¡°ê±´ ì£¼ì…ê³¼ CFG\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- adaLN-Zeroì˜ ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  í‘œì¤€ LayerNormê³¼ì˜ ì°¨ì´ë¥¼ ì´í•´í•œë‹¤\n",
    "- Zero-initializationì´ í•™ìŠµ ì•ˆì •ì„±ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•œë‹¤\n",
    "- ì‹œê°„ $t$, í´ë˜ìŠ¤, í…ìŠ¤íŠ¸ ë“± ë‹¤ì–‘í•œ ì¡°ê±´ ì£¼ì… ë°©ì‹ì„ ë¹„êµí•œë‹¤\n",
    "- DiTì—ì„œì˜ Classifier-Free Guidance(CFG) ì„¤ê³„ë¥¼ ì´í•´í•œë‹¤\n",
    "- TensorFlowë¡œ adaLN-Zero ë ˆì´ì–´ë¥¼ ì§ì ‘ êµ¬í˜„í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: adaLN-Zeroì™€ ì¡°ê±´ ì£¼ì…](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì •](#2.-í™˜ê²½-ì„¤ì •)\n",
    "3. [adaLN-Zero ë ˆì´ì–´ êµ¬í˜„](#3.-adaLN-Zero-êµ¬í˜„)\n",
    "4. [í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ](#4.-LayerNorm-vs-adaLN)\n",
    "5. [Zero-Init í•™ìŠµ ì•ˆì •ì„± ì‹¤í—˜](#5.-Zero-Init-ì•ˆì •ì„±)\n",
    "6. [ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨](#6.-ë‹¤ì¤‘-ì¡°ê±´-ì£¼ì…)\n",
    "7. [DiTì—ì„œì˜ CFG ì„¤ê³„](#7.-CFG-ì„¤ê³„)\n",
    "8. [ì •ë¦¬](#8.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### í‘œì¤€ LayerNorm\n",
    "\n",
    "$$\\text{LN}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "- $\\gamma, \\beta$: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼/ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„°\n",
    "- $\\mu, \\sigma^2$: ì…ë ¥ì˜ í‰ê· , ë¶„ì‚° (ì±„ë„ ì¶•)\n",
    "\n",
    "### Adaptive LayerNorm (adaLN)\n",
    "\n",
    "ì¡°ê±´ ë²¡í„° $c$ (ì‹œê°„ $t$, í´ë˜ìŠ¤, í…ìŠ¤íŠ¸ ì„ë² ë”©)ë¡œë¶€í„° $\\gamma, \\beta$ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±:\n",
    "\n",
    "$$\\text{adaLN}(x, c) = \\gamma_c \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta_c$$\n",
    "\n",
    "$$(\\gamma_c, \\beta_c) = \\text{MLP}(c)$$\n",
    "\n",
    "### adaLN-Zero (DiTì˜ í•µì‹¬ ê¸°ë²•)\n",
    "\n",
    "adaLNì— **ê²Œì´íŒ… íŒŒë¼ë¯¸í„°** $\\alpha$ë¥¼ ì¶”ê°€í•˜ê³ , ì´ˆê¸°ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •:\n",
    "\n",
    "$$h = x + \\alpha_1 \\cdot \\text{Attn}\\!\\left((1 + \\gamma_1) \\odot \\text{LN}(x) + \\beta_1\\right)$$\n",
    "\n",
    "$$\\text{output} = h + \\alpha_2 \\cdot \\text{FFN}\\!\\left((1 + \\gamma_2) \\odot \\text{LN}(h) + \\beta_2\\right)$$\n",
    "\n",
    "ì—¬ê¸°ì„œ:\n",
    "\n",
    "$$(\\gamma_1, \\beta_1, \\alpha_1, \\gamma_2, \\beta_2, \\alpha_2) = \\text{MLP}(c), \\quad c = \\text{Embed}(t) + \\text{Embed}(y)$$\n",
    "\n",
    "- $t$: diffusion íƒ€ì„ìŠ¤í…\n",
    "- $y$: í´ë˜ìŠ¤ ë ˆì´ë¸” ë˜ëŠ” í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
    "- $\\alpha_1, \\alpha_2$: ê²Œì´íŒ… ìŠ¤ì¼€ì¼ (ì´ˆê¸°ê°’ = 0)\n",
    "\n",
    "### Zero-Initializationì˜ íš¨ê³¼\n",
    "\n",
    "ì´ˆê¸° ìƒíƒœì—ì„œ $\\alpha = 0$ì´ë©´:\n",
    "\n",
    "$$h = x + 0 \\cdot \\text{Attn}(\\cdots) = x \\quad \\text{(í•­ë“± í•¨ìˆ˜)}$$\n",
    "\n",
    "- í•™ìŠµ ì‹œì‘ ì‹œ ê° DiT ë¸”ë¡ì´ **í•­ë“± í•¨ìˆ˜**ë¡œ ë™ì‘\n",
    "- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ **ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ì—†ì´** ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ ì‹œì‘\n",
    "- ViT/ResNetì˜ ì”ì°¨ ì—°ê²°ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ë” ê°•ë ¥í•œ ì´ˆê¸°í™” ì „ëµ\n",
    "\n",
    "### Classifier-Free Guidance (CFG) in DiT\n",
    "\n",
    "$$\\tilde{\\epsilon}_\\theta(x_t, c) = (1 + w) \\cdot \\epsilon_\\theta(x_t, c) - w \\cdot \\epsilon_\\theta(x_t, \\varnothing)$$\n",
    "\n",
    "- $w$: guidance scale (ë†’ì„ìˆ˜ë¡ ì¡°ê±´ì— ì¶©ì‹¤, ë‹¤ì–‘ì„± ê°ì†Œ)\n",
    "- $\\varnothing$: null ì¡°ê±´ (í•™ìŠµ ì‹œ ì¼ì • ë¹„ìœ¨ë¡œ ì¡°ê±´ì„ drop)\n",
    "- DiT í•™ìŠµ ì‹œ 10~20% í™•ë¥ ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ $\\varnothing$ìœ¼ë¡œ ëŒ€ì²´\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| í‘œì¤€ LN | $\\gamma \\odot \\text{norm}(x) + \\beta$ | ê³ ì • íŒŒë¼ë¯¸í„° |\n",
    "| adaLN | $\\gamma_c \\odot \\text{norm}(x) + \\beta_c$ | ì¡°ê±´ë¶€ íŒŒë¼ë¯¸í„° |\n",
    "| adaLN-Zero | $x + \\alpha \\cdot f(\\gamma_c \\odot \\text{norm}(x) + \\beta_c)$ | ê²Œì´íŒ… + ì˜ì  ì´ˆê¸°í™” |\n",
    "| CFG | $(1+w)\\epsilon(x,c) - w\\epsilon(x,\\varnothing)$ | ì¡°ê±´ë¶€ ê°€ì´ë˜ìŠ¤ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ adaLN-Zero ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ adaLN-Zeroê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìš”ë¦¬ì‚¬(ëª¨ë¸)ì—ê²Œ \"ì§€ê¸ˆì€ ê²¨ìš¸ì´ê³ , ë§¤ìš´ë§›ì„ ì›í•´ìš”\"ë¼ëŠ” ì£¼ë¬¸(ì¡°ê±´)ì„ ë°›ì•„ì„œ ì–‘ë…(íŒŒë¼ë¯¸í„°)ì„ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” ê²ƒ!\n",
    "\n",
    "ë³´í†µ LayerNormì€ í•­ìƒ ê°™ì€ ì–‘ë…ì„ ì“°ì§€ë§Œ, adaLNì€ ì£¼ë¬¸ì— ë”°ë¼ ì–‘ë…ì„ ë°”ê¿‰ë‹ˆë‹¤.\n",
    "ê·¸ë¦¬ê³  \"Zero\"ëŠ” ì²˜ìŒì— **ì•„ë¬´ê²ƒë„ ì•ˆ ë„£ê² ë‹¤**ëŠ” ëœ»ì´ì—ìš”.\n",
    "\n",
    "#### ğŸ¤” ì™œ ì²˜ìŒì— 0ìœ¼ë¡œ ì‹œì‘í•˜ë‚˜ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìƒˆ ìš”ë¦¬ì‚¬ê°€ ì²˜ìŒ ì¼í•  ë•Œ, ì¼ë‹¨ ì›ë˜ ë ˆì‹œí”¼(ì…ë ¥)ë¥¼ ê·¸ëŒ€ë¡œ ë‚´ë³´ë‚´ë©´ì„œ ì²œì²œíˆ ìê¸°ë§Œì˜ ì–‘ë…ì„ ë°°ìš°ëŠ” ê±°ì˜ˆìš”!\n",
    "\n",
    "ì²˜ìŒë¶€í„° ì´ìƒí•œ ì–‘ë…ì„ ë„£ìœ¼ë©´ ìš”ë¦¬ê°€ ë§í•˜ë“¯ì´, ì²˜ìŒì— Î±=0ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì•ˆì „í•˜ê²Œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: adaLN-Zero ì´ˆê¸° ì¶œë ¥\n",
    "\n",
    "DiT ë¸”ë¡ì˜ ì…ë ¥ì´ $x = [1, 2, 3]$ì´ê³  ì´ˆê¸° ìƒíƒœì—ì„œ $\\alpha_1 = 0$ì¼ ë•Œ, Attention ì´í›„ ì¶œë ¥ $h$ëŠ”?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$h = x + \\alpha_1 \\cdot \\text{Attn}(\\cdots) = x + 0 \\cdot \\text{Attn}(\\cdots) = [1, 2, 3]$$\n",
    "\n",
    "$\\alpha = 0$ì´ë¯€ë¡œ **í•­ë“± í•¨ìˆ˜**ê°€ ë©ë‹ˆë‹¤. ì…ë ¥ì´ ê·¸ëŒ€ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.\n",
    "ê¹Šì€ ë„¤íŠ¸ì›Œí¬(DiT-XL: 28ë¸”ë¡)ì—ì„œë„ ì´ˆê¸°ì— ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” í•µì‹¬ íŠ¸ë¦­ì…ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: CFG ì¶œë ¥ ê³„ì‚°\n",
    "\n",
    "$\\epsilon_\\theta(x_t, c) = 0.8$, $\\epsilon_\\theta(x_t, \\varnothing) = 0.3$ì´ê³  guidance scale $w=4.0$ì¼ ë•Œ CFG ì¶œë ¥ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\tilde{\\epsilon} = (1 + w) \\cdot \\epsilon(x_t, c) - w \\cdot \\epsilon(x_t, \\varnothing)$$\n",
    "$$= (1 + 4) \\times 0.8 - 4 \\times 0.3 = 4.0 - 1.2 = 2.8$$\n",
    "\n",
    "CFGëŠ” ì¡°ê±´ë¶€ ì˜ˆì¸¡ì„ **ì¦í­**í•©ë‹ˆë‹¤. $w$ê°€ í´ìˆ˜ë¡ ì¡°ê±´ì— ë” ì¶©ì‹¤í•´ì§€ì§€ë§Œ ë‹¤ì–‘ì„±ì´ ì¤„ì–´ë“­ë‹ˆë‹¤.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • <a name='2.-í™˜ê²½-ì„¤ì •'></a>\n",
    "\n",
    "TensorFlowë¥¼ ì‚¬ìš©í•˜ì—¬ adaLN-Zero ë ˆì´ì–´ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. adaLN-Zero ë ˆì´ì–´ êµ¬í˜„ <a name='3.-adaLN-Zero-êµ¬í˜„'></a>\n",
    "\n",
    "DiTì˜ í•µì‹¬ì¸ adaLN-Zeroë¥¼ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "ì¡°ê±´ ë²¡í„° $c$ë¡œë¶€í„° $(\\gamma, \\beta, \\alpha)$ 6ê°œ íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ adaLN-Zero ë ˆì´ì–´ êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class AdaLNZero(tf.keras.layers.Layer):\n",
    "    # Adaptive Layer Normalization with Zero-initialization (Peebles & Xie, 2023)\n",
    "    def __init__(self, hidden_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        # ì¡°ê±´ â†’ (gamma, beta, alpha) ë§¤í•‘\n",
    "        self.adaLN_modulation = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(hidden_dim, activation='silu'),\n",
    "            tf.keras.layers.Dense(3 * hidden_dim)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Zero-initialization: ë§ˆì§€ë§‰ Denseì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        last_layer = self.adaLN_modulation.layers[-1]\n",
    "        last_layer.build((None, self.hidden_dim))\n",
    "        last_layer.kernel.assign(tf.zeros_like(last_layer.kernel))\n",
    "        last_layer.bias.assign(tf.zeros_like(last_layer.bias))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, condition):\n",
    "        # x: (B, N, D), condition: (B, D_cond)\n",
    "        modulation = self.adaLN_modulation(condition)  # (B, 3*D)\n",
    "        gamma, beta, alpha = tf.split(modulation, 3, axis=-1)  # ê° (B, D)\n",
    "\n",
    "        # adaLN: ì¡°ê±´ë¶€ ì •ê·œí™”\n",
    "        x_norm = self.norm(x)  # (B, N, D)\n",
    "        # gamma, betaë¥¼ ì‹œí€€ìŠ¤ ì¶•ìœ¼ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸\n",
    "        gamma = tf.expand_dims(gamma, 1)  # (B, 1, D)\n",
    "        beta = tf.expand_dims(beta, 1)\n",
    "        alpha = tf.expand_dims(alpha, 1)\n",
    "\n",
    "        x_modulated = (1 + gamma) * x_norm + beta\n",
    "        return x_modulated, alpha\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "B, N, D = 2, 16, 384\n",
    "x_test = tf.random.normal([B, N, D])\n",
    "cond_test = tf.random.normal([B, D])\n",
    "\n",
    "adaln = AdaLNZero(hidden_dim=D)\n",
    "x_mod, alpha = adaln(x_test, cond_test)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"adaLN-Zero ë ˆì´ì–´ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"ì…ë ¥ x shape: {x_test.shape}\")\n",
    "print(f\"ì¡°ê±´ c shape: {cond_test.shape}\")\n",
    "print(f\"ì¶œë ¥ x_modulated shape: {x_mod.shape}\")\n",
    "print(f\"ê²Œì´íŒ… alpha shape: {alpha.shape}\")\n",
    "\n",
    "# Zero-init ê²€ì¦\n",
    "print(f\"\\nZero-initialization ê²€ì¦:\")\n",
    "print(f\"  alpha í‰ê· : {tf.reduce_mean(alpha).numpy():.6f} (ê¸°ëŒ€ê°’: 0.0)\")\n",
    "print(f\"  alpha í‘œì¤€í¸ì°¨: {tf.math.reduce_std(alpha).numpy():.6f} (ê¸°ëŒ€ê°’: 0.0)\")\n",
    "print(f\"  alpha ìµœëŒ€ ì ˆëŒ€ê°’: {tf.reduce_max(tf.abs(alpha)).numpy():.6f}\")\n",
    "\n",
    "# ì´ˆê¸° ìƒíƒœì—ì„œ x + alpha * f(x) = x í™•ì¸\n",
    "residual = x_test + alpha * x_mod  # ì´ˆê¸°ì— alpha=0\n",
    "diff = tf.reduce_mean(tf.abs(residual - x_test)).numpy()\n",
    "print(f\"  |x + alpha*f(x) - x| í‰ê· : {diff:.8f} (0ì´ë©´ í•­ë“±í•¨ìˆ˜)\")\n",
    "print(f\"  â†’ ì´ˆê¸° ìƒíƒœì—ì„œ í•­ë“± í•¨ìˆ˜ {'í™•ì¸' if diff < 1e-6 else 'ì‹¤íŒ¨'}!\")\n",
    "\n",
    "print(f\"\\níŒŒë¼ë¯¸í„° ìˆ˜: {adaln.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ <a name='4.-LayerNorm-vs-adaLN'></a>\n",
    "\n",
    "í‘œì¤€ LayerNormì€ ì¡°ê±´ê³¼ ë¬´ê´€í•˜ê²Œ ê³ ì •ëœ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ì§€ë§Œ,\n",
    "adaLNì€ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì •ê·œí™” íŒŒë¼ë¯¸í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í‘œì¤€ LayerNorm vs adaLN ì¡°ê±´ë¶€ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë™ì¼ ì…ë ¥ì— ë‹¤ë¥¸ ì¡°ê±´ì„ ì£¼ì—ˆì„ ë•Œì˜ ì¶œë ¥ ì°¨ì´ë¥¼ ë¹„êµ\n",
    "\n",
    "x_input = tf.random.normal([1, 8, 128])\n",
    "\n",
    "# í‘œì¤€ LayerNorm\n",
    "standard_ln = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "y_standard = standard_ln(x_input)\n",
    "\n",
    "# adaLN-Zero (ì„œë¡œ ë‹¤ë¥¸ ì¡°ê±´)\n",
    "adaln_layer = AdaLNZero(hidden_dim=128)\n",
    "\n",
    "# ì¡°ê±´ì„ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•˜ì—¬ ì°¨ì´ë¥¼ ê´€ì°° (í•™ìŠµ í›„ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)\n",
    "cond_a = tf.constant([[1.0] * 64 + [-1.0] * 64])\n",
    "cond_b = tf.constant([[-1.0] * 64 + [1.0] * 64])\n",
    "\n",
    "# ìˆ˜ë™ìœ¼ë¡œ modulation ê°€ì¤‘ì¹˜ ì„¤ì • (í•™ìŠµ í›„ ìƒíƒœ ì‹œë®¬ë ˆì´ì…˜)\n",
    "adaln_layer(x_input, cond_a)  # build\n",
    "for layer in adaln_layer.adaLN_modulation.layers:\n",
    "    if hasattr(layer, 'kernel'):\n",
    "        layer.kernel.assign(tf.random.normal(layer.kernel.shape, stddev=0.1))\n",
    "        layer.bias.assign(tf.random.normal(layer.bias.shape, stddev=0.01))\n",
    "\n",
    "y_ada_a, alpha_a = adaln_layer(x_input, cond_a)\n",
    "y_ada_b, alpha_b = adaln_layer(x_input, cond_b)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"í‘œì¤€ LayerNorm vs adaLN ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë‘ ì¡°ê±´ ê°„ adaLN ì¶œë ¥ ì°¨ì´\n",
    "diff_ada = tf.reduce_mean(tf.abs(y_ada_a - y_ada_b)).numpy()\n",
    "diff_std = tf.reduce_mean(tf.abs(y_standard - y_standard)).numpy()  # í•­ìƒ 0\n",
    "\n",
    "print(f\"ì…ë ¥ x shape: {x_input.shape}\")\n",
    "print(f\"\\ní‘œì¤€ LayerNorm:\")\n",
    "print(f\"  ì¡°ê±´ A ì¶œë ¥ == ì¡°ê±´ B ì¶œë ¥: í•­ìƒ ë™ì¼ (ì°¨ì´: {diff_std:.6f})\")\n",
    "print(f\"  â†’ ì¡°ê±´ì— ë¬´ê´€í•œ ê³ ì • ì •ê·œí™”\")\n",
    "print(f\"\\nadaLN-Zero:\")\n",
    "print(f\"  ì¡°ê±´ Aì™€ ì¡°ê±´ B ì¶œë ¥ ì°¨ì´: {diff_ada:.6f}\")\n",
    "print(f\"  â†’ ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì •ê·œí™” ìˆ˜í–‰\")\n",
    "print(f\"  alpha í‰ê·  (ì¡°ê±´ A): {tf.reduce_mean(alpha_a).numpy():.4f}\")\n",
    "print(f\"  alpha í‰ê·  (ì¡°ê±´ B): {tf.reduce_mean(alpha_b).numpy():.4f}\")\n",
    "\n",
    "# ì¡°ê±´ë³„ ì •ê·œí™” íš¨ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "token_idx = 0\n",
    "x_vals = x_input[0, token_idx, :64].numpy()\n",
    "y_std_vals = y_standard[0, token_idx, :64].numpy()\n",
    "y_ada_a_vals = y_ada_a[0, token_idx, :64].numpy()\n",
    "y_ada_b_vals = y_ada_b[0, token_idx, :64].numpy()\n",
    "\n",
    "ax1.plot(x_vals, 'gray', alpha=0.5, lw=1, label='ì…ë ¥ x')\n",
    "ax1.plot(y_std_vals, 'b-', lw=2, label='í‘œì¤€ LN')\n",
    "ax1.plot(y_ada_a_vals, 'r-', lw=2, alpha=0.7, label='adaLN (ì¡°ê±´ A)')\n",
    "ax1.plot(y_ada_b_vals, 'g-', lw=2, alpha=0.7, label='adaLN (ì¡°ê±´ B)')\n",
    "ax1.set_xlabel('ì°¨ì›', fontsize=11)\n",
    "ax1.set_ylabel('ê°’', fontsize=11)\n",
    "ax1.set_title('ì •ê·œí™” ì¶œë ¥ ë¹„êµ (ì²« ë²ˆì§¸ í† í°)', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ìš°: ì¡°ê±´ì— ë”°ë¥¸ gamma, beta ë¶„í¬\n",
    "ax2 = axes[1]\n",
    "modulation_a = adaln_layer.adaLN_modulation(cond_a).numpy()[0]\n",
    "modulation_b = adaln_layer.adaLN_modulation(cond_b).numpy()[0]\n",
    "D_h = 128\n",
    "gamma_a, beta_a, alpha_vals_a = modulation_a[:D_h], modulation_a[D_h:2*D_h], modulation_a[2*D_h:]\n",
    "gamma_b, beta_b, alpha_vals_b = modulation_b[:D_h], modulation_b[D_h:2*D_h], modulation_b[2*D_h:]\n",
    "\n",
    "x_pos = np.arange(D_h)\n",
    "ax2.scatter(gamma_a, beta_a, c='red', alpha=0.5, s=15, label='ì¡°ê±´ A (gamma vs beta)')\n",
    "ax2.scatter(gamma_b, beta_b, c='blue', alpha=0.5, s=15, label='ì¡°ê±´ B (gamma vs beta)')\n",
    "ax2.axhline(y=0, color='gray', ls='--', lw=1)\n",
    "ax2.axvline(x=0, color='gray', ls='--', lw=1)\n",
    "ax2.set_xlabel('$\\gamma_c$ (ìŠ¤ì¼€ì¼)', fontsize=11)\n",
    "ax2.set_ylabel('$\\beta_c$ (ì‹œí”„íŠ¸)', fontsize=11)\n",
    "ax2.set_title('ì¡°ê±´ë³„ adaLN íŒŒë¼ë¯¸í„° ë¶„í¬', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter17_diffusion_transformers/ln_vs_adaln_comparison.png',\n",
    "            dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter17_diffusion_transformers/ln_vs_adaln_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. Zero-Init í•™ìŠµ ì•ˆì •ì„± ì‹¤í—˜ <a name='5.-Zero-Init-ì•ˆì •ì„±'></a>\n",
    "\n",
    "Zero-initialization($\\alpha=0$) vs ëœë¤ ì´ˆê¸°í™”ì˜ í•™ìŠµ ì•ˆì •ì„±ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "ê°„ë‹¨í•œ í•¨ìˆ˜ ê·¼ì‚¬ ê³¼ì œì—ì„œ ë‘ ì´ˆê¸°í™” ë°©ì‹ì˜ ì´ˆê¸° ì†ì‹¤ê³¼ ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸°ë¥¼ ê´€ì°°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Zero-Init vs ëœë¤ ì´ˆê¸°í™” í•™ìŠµ ì•ˆì •ì„± ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class SimpleDiTBlock(tf.keras.layers.Layer):\n",
    "    # ê°„ì†Œí™”ëœ DiT ë¸”ë¡ (adaLN-Zero í¬í•¨)\n",
    "    def __init__(self, dim, zero_init=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.zero_init = zero_init\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.attn_proj = tf.keras.layers.Dense(dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim * 4, activation='gelu'),\n",
    "            tf.keras.layers.Dense(dim)\n",
    "        ])\n",
    "        self.adaln = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim, activation='silu'),\n",
    "            tf.keras.layers.Dense(6 * dim)\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dummy_cond = tf.zeros([1, self.dim])\n",
    "        self.adaln(dummy_cond)\n",
    "        if self.zero_init:\n",
    "            last = self.adaln.layers[-1]\n",
    "            last.kernel.assign(tf.zeros_like(last.kernel))\n",
    "            last.bias.assign(tf.zeros_like(last.bias))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, cond):\n",
    "        mod = self.adaln(cond)\n",
    "        g1, b1, a1, g2, b2, a2 = tf.split(mod, 6, axis=-1)\n",
    "        g1, b1, a1 = [tf.expand_dims(v, 1) for v in [g1, b1, a1]]\n",
    "        g2, b2, a2 = [tf.expand_dims(v, 1) for v in [g2, b2, a2]]\n",
    "\n",
    "        # Attention branch\n",
    "        h = (1 + g1) * self.norm(x) + b1\n",
    "        h = self.attn_proj(h)\n",
    "        x = x + a1 * h\n",
    "\n",
    "        # FFN branch\n",
    "        h2 = (1 + g2) * self.norm(x) + b2\n",
    "        h2 = self.ffn(h2)\n",
    "        x = x + a2 * h2\n",
    "        return x\n",
    "\n",
    "# ë‹¤ì¸µ DiT ì‹œë®¬ë ˆì´ì…˜ (8ë¸”ë¡ ìŠ¤íƒ)\n",
    "dim = 64\n",
    "n_blocks = 8\n",
    "\n",
    "def build_stacked_dit(zero_init):\n",
    "    blocks = [SimpleDiTBlock(dim, zero_init=zero_init) for _ in range(n_blocks)]\n",
    "    return blocks\n",
    "\n",
    "x_input = tf.random.normal([4, 16, dim])\n",
    "cond = tf.random.normal([4, dim])\n",
    "target = tf.random.normal([4, 16, dim])\n",
    "\n",
    "results = {}\n",
    "for name, zero_init in [(\"Zero-Init\", True), (\"Random-Init\", False)]:\n",
    "    tf.random.set_seed(42)\n",
    "    blocks = build_stacked_dit(zero_init)\n",
    "\n",
    "    # Forward passë¡œ ì´ˆê¸° ì¶œë ¥ í™•ì¸\n",
    "    h = x_input\n",
    "    for block in blocks:\n",
    "        h = block(h, cond)\n",
    "\n",
    "    initial_output_std = tf.math.reduce_std(h).numpy()\n",
    "\n",
    "    # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ì¸¡ì •\n",
    "    trainable_vars = []\n",
    "    for block in blocks:\n",
    "        trainable_vars.extend(block.trainable_variables)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = x_input\n",
    "        for block in blocks:\n",
    "            h = block(h, cond)\n",
    "        loss = tf.reduce_mean((h - target) ** 2)\n",
    "\n",
    "    grads = tape.gradient(loss, trainable_vars)\n",
    "    grad_norms = [tf.norm(g).numpy() for g in grads if g is not None]\n",
    "\n",
    "    results[name] = {\n",
    "        'output_std': initial_output_std,\n",
    "        'loss': loss.numpy(),\n",
    "        'grad_mean': np.mean(grad_norms),\n",
    "        'grad_max': np.max(grad_norms),\n",
    "        'grad_min': np.min(grad_norms),\n",
    "    }\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"Zero-Init vs Random-Init ì•ˆì •ì„± ë¹„êµ ({n_blocks}ë¸”ë¡ DiT)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'í•­ëª©':<25} | {'Zero-Init':>15} | {'Random-Init':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for key in ['output_std', 'loss', 'grad_mean', 'grad_max']:\n",
    "    labels = {\n",
    "        'output_std': 'ì´ˆê¸° ì¶œë ¥ í‘œì¤€í¸ì°¨',\n",
    "        'loss': 'ì´ˆê¸° ì†ì‹¤',\n",
    "        'grad_mean': 'ê·¸ë˜ë””ì–¸íŠ¸ í‰ê·  norm',\n",
    "        'grad_max': 'ê·¸ë˜ë””ì–¸íŠ¸ ìµœëŒ€ norm',\n",
    "    }\n",
    "    v_zero = results['Zero-Init'][key]\n",
    "    v_rand = results['Random-Init'][key]\n",
    "    print(f\"{labels[key]:<25} | {v_zero:>15.4f} | {v_rand:>15.4f}\")\n",
    "\n",
    "print(f\"\\në¶„ì„:\")\n",
    "if results['Zero-Init']['output_std'] < results['Random-Init']['output_std']:\n",
    "    print(f\"  Zero-Init: ì´ˆê¸° ì¶œë ¥ì´ ì…ë ¥ì— ê°€ê¹Œì›€ (í•­ë“± í•¨ìˆ˜ ê·¼ì‚¬)\")\n",
    "else:\n",
    "    print(f\"  Zero-Init: ì´ˆê¸° ì¶œë ¥ í‘œì¤€í¸ì°¨ ë¶„ì„ ì™„ë£Œ\")\n",
    "print(f\"  Random-Init: ì´ˆê¸°ë¶€í„° ì¶œë ¥ ë¶„í¬ê°€ í¬ê²Œ ë³€í˜•ë¨\")\n",
    "print(f\"  â†’ Zero-Initì´ ê¹Šì€ DiTì—ì„œ ë” ì•ˆì •ì ì¸ í•™ìŠµ ì‹œì‘ì ì„ ì œê³µ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨ <a name='6.-ë‹¤ì¤‘-ì¡°ê±´-ì£¼ì…'></a>\n",
    "\n",
    "DiTëŠ” ì—¬ëŸ¬ ì¡°ê±´ì„ ë™ì‹œì— ì£¼ì…í•©ë‹ˆë‹¤:\n",
    "- **íƒ€ì„ìŠ¤í… $t$**: Sinusoidal ì„ë² ë”© â†’ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ ì •ë³´\n",
    "- **í´ë˜ìŠ¤ ë ˆì´ë¸” $y$**: Embedding í…Œì´ë¸” â†’ ìƒì„± ëŒ€ìƒ ì¹´í…Œê³ ë¦¬\n",
    "- **í…ìŠ¤íŠ¸**: CLIP/T5 ì¸ì½”ë” ì¶œë ¥ â†’ Cross-Attention ë˜ëŠ” adaLN\n",
    "\n",
    "$$c = \\text{MLP}(\\text{SinEmbed}(t)) + \\text{Embed}(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… (ì‹œê°„ + í´ë˜ìŠ¤) ë°ëª¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class TimestepEmbedding(tf.keras.layers.Layer):\n",
    "    # Sinusoidal timestep embedding (DDPM ìŠ¤íƒ€ì¼)\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dim, activation='silu'),\n",
    "            tf.keras.layers.Dense(dim),\n",
    "        ])\n",
    "\n",
    "    def call(self, t):\n",
    "        half_dim = self.dim // 2\n",
    "        freqs = tf.exp(-tf.math.log(10000.0) * tf.range(0, half_dim, dtype=tf.float32) / half_dim)\n",
    "        args = tf.cast(t[:, None], tf.float32) * freqs[None, :]\n",
    "        embedding = tf.concat([tf.cos(args), tf.sin(args)], axis=-1)\n",
    "        return self.mlp(embedding)\n",
    "\n",
    "\n",
    "class ClassEmbedding(tf.keras.layers.Layer):\n",
    "    # í´ë˜ìŠ¤ ë ˆì´ë¸” ì„ë² ë”©\n",
    "    def __init__(self, num_classes, dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(num_classes + 1, dim)  # +1 for null class\n",
    "        self.null_class = num_classes  # null class for CFG\n",
    "\n",
    "    def call(self, y, drop_prob=0.0):\n",
    "        # CFG: drop_prob í™•ë¥ ë¡œ null classë¡œ ëŒ€ì²´\n",
    "        if drop_prob > 0:\n",
    "            mask = tf.random.uniform(tf.shape(y)) < drop_prob\n",
    "            y = tf.where(mask, self.null_class, y)\n",
    "        return self.embed(y)\n",
    "\n",
    "\n",
    "class DiTConditioner(tf.keras.layers.Layer):\n",
    "    # ì‹œê°„ + í´ë˜ìŠ¤ ì¡°ê±´ ê²°í•©\n",
    "    def __init__(self, dim, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.time_embed = TimestepEmbedding(dim)\n",
    "        self.class_embed = ClassEmbedding(num_classes, dim)\n",
    "\n",
    "    def call(self, t, y, cfg_drop_prob=0.0):\n",
    "        c_time = self.time_embed(t)\n",
    "        c_class = self.class_embed(y, drop_prob=cfg_drop_prob)\n",
    "        return c_time + c_class\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "dim = 256\n",
    "num_classes = 1000\n",
    "conditioner = DiTConditioner(dim, num_classes)\n",
    "\n",
    "batch_size = 8\n",
    "timesteps = tf.constant([0, 100, 250, 500, 750, 900, 950, 999])\n",
    "class_labels = tf.constant([1, 42, 100, 207, 404, 555, 888, 999])\n",
    "\n",
    "# ì¼ë°˜ ì¡°ê±´ (CFG off)\n",
    "c_normal = conditioner(timesteps, class_labels, cfg_drop_prob=0.0)\n",
    "\n",
    "# CFG í•™ìŠµ ëª¨ë“œ (10% drop)\n",
    "c_cfg = conditioner(timesteps, class_labels, cfg_drop_prob=0.1)\n",
    "\n",
    "# ë¬´ì¡°ê±´ ìƒì„± (null condition)\n",
    "null_labels = tf.fill([batch_size], num_classes)\n",
    "c_uncond = conditioner(timesteps, null_labels, cfg_drop_prob=0.0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ë‹¤ì¤‘ ì¡°ê±´ ì£¼ì… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {dim}\")\n",
    "print(f\"í´ë˜ìŠ¤ ìˆ˜: {num_classes}\")\n",
    "print(f\"\\nì¡°ê±´ ë²¡í„° shape: {c_normal.shape}\")\n",
    "print(f\"\\nì‹œê°„ë³„ ì¡°ê±´ ë²¡í„° í†µê³„:\")\n",
    "print(f\"{'íƒ€ì„ìŠ¤í…':>10} | {'í´ë˜ìŠ¤':>8} | {'ì¡°ê±´ norm':>12} | {'ì¡°ê±´ mean':>12}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(batch_size):\n",
    "    t_val = timesteps[i].numpy()\n",
    "    y_val = class_labels[i].numpy()\n",
    "    c_norm = tf.norm(c_normal[i]).numpy()\n",
    "    c_mean = tf.reduce_mean(c_normal[i]).numpy()\n",
    "    print(f\"{t_val:>10} | {y_val:>8} | {c_norm:>12.4f} | {c_mean:>12.4f}\")\n",
    "\n",
    "# CFG: ì¡°ê±´ë¶€ vs ë¬´ì¡°ê±´ ì°¨ì´\n",
    "cfg_diff = tf.reduce_mean(tf.abs(c_normal - c_uncond)).numpy()\n",
    "print(f\"\\nì¡°ê±´ë¶€ vs ë¬´ì¡°ê±´ ë²¡í„° ì°¨ì´: {cfg_diff:.4f}\")\n",
    "print(f\"â†’ CFG ì¶”ë¡  ì‹œ ì´ ì°¨ì´ë¥¼ guidance scaleë¡œ ì¦í­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 7. DiTì—ì„œì˜ CFG ì„¤ê³„ <a name='7.-CFG-ì„¤ê³„'></a>\n",
    "\n",
    "DiTëŠ” Classifier-Free Guidanceë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "**í•™ìŠµ ì‹œ:**\n",
    "- ì¼ì • ë¹„ìœ¨(ë³´í†µ 10%)ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ $\\varnothing$(null class)ë¡œ ëŒ€ì²´\n",
    "- ëª¨ë¸ì´ ì¡°ê±´ë¶€/ë¬´ì¡°ê±´ ìƒì„±ì„ ëª¨ë‘ í•™ìŠµ\n",
    "\n",
    "**ì¶”ë¡  ì‹œ:**\n",
    "$$\\tilde{\\epsilon}_\\theta(x_t, c) = (1 + w) \\cdot \\epsilon_\\theta(x_t, c) - w \\cdot \\epsilon_\\theta(x_t, \\varnothing)$$\n",
    "\n",
    "| guidance scale $w$ | íš¨ê³¼ | ì ìš© |\n",
    "|-------|------|------|\n",
    "| 0.0 | ë¬´ì¡°ê±´ ìƒì„± (ì¡°ê±´ ë¬´ì‹œ) | ë‹¤ì–‘ì„± ìµœëŒ€ |\n",
    "| 1.0~2.0 | ì•½í•œ ê°€ì´ë˜ìŠ¤ | ê· í˜• |\n",
    "| 4.0~7.5 | ê°•í•œ ê°€ì´ë˜ìŠ¤ (DiT ê¸°ë³¸ê°’ 4.0) | í’ˆì§ˆ ìµœëŒ€ |\n",
    "| >10.0 | ê³¼ë„í•œ ê°€ì´ë˜ìŠ¤ | ì•„í‹°íŒ©íŠ¸ ë°œìƒ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 8. ì •ë¦¬ <a name='8.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| adaLN-Zero | ì¡°ê±´ë¶€ ì •ê·œí™” + ê²Œì´íŒ…($\\alpha=0$) ì´ˆê¸°í™” | â­â­â­ |\n",
    "| Zero-Initialization | ì´ˆê¸° ìƒíƒœì—ì„œ í•­ë“± í•¨ìˆ˜ â†’ ì•ˆì •ì  í•™ìŠµ ì‹œì‘ | â­â­â­ |\n",
    "| ì¡°ê±´ ì£¼ì… | timestep(sinusoidal) + class(embedding) â†’ adaLN | â­â­â­ |\n",
    "| CFG in DiT | $(1+w)\\epsilon(c) - w\\epsilon(\\varnothing)$, í•™ìŠµ ì‹œ 10% drop | â­â­â­ |\n",
    "| í‘œì¤€ LN vs adaLN | ê³ ì • íŒŒë¼ë¯¸í„° vs ì¡°ê±´ë¶€ ë™ì  íŒŒë¼ë¯¸í„° | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$h = x + \\alpha \\cdot \\text{Attn}\\!\\left((1 + \\gamma_c) \\odot \\text{LN}(x) + \\beta_c\\right)$$\n",
    "\n",
    "$$(\\gamma, \\beta, \\alpha) \\leftarrow \\text{MLP}(c), \\quad \\alpha_{\\text{init}} = 0 \\;\\text{(í•­ë“± í•¨ìˆ˜)}$$\n",
    "\n",
    "$$\\tilde{\\epsilon}_\\theta = (1 + w) \\cdot \\epsilon_\\theta(x_t, c) - w \\cdot \\epsilon_\\theta(x_t, \\varnothing)$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**04_flow_matching_and_rectified_flow.ipynb** â€” Flow Matching ODE ìˆ˜ì‹ê³¼ Rectified Flowì˜ ì§ì„  ê²½ë¡œë¥¼ DDPMê³¼ ë¹„êµí•˜ë©°, SD3/Flux ë“± ìµœì‹  ëª¨ë¸ì˜ í›ˆë ¨ ë°©ì‹ì„ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}