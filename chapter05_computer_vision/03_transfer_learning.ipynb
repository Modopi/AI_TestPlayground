{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 05-03: 전이학습 — Feature Extraction & Fine-Tuning\n",
    "\n",
    "## 학습 목표\n",
    "- 전이학습(Transfer Learning)의 개념과 두 가지 전략을 이해한다\n",
    "- EfficientNetB0로 Feature Extraction 파이프라인을 구현한다\n",
    "- 새 분류 헤드를 추가하고 점진적으로 Fine-Tuning을 적용한다\n",
    "- 단계별 학습률 설정의 중요성을 이해하고 적용한다\n",
    "\n",
    "## 목차\n",
    "1. [전이학습 전략 개요](#1)\n",
    "2. [Feature Extraction 단계](#2)\n",
    "3. [새 분류 헤드 추가](#3)\n",
    "4. [Fine-Tuning 단계](#4)\n",
    "5. [단계별 학습률 설정 가이드](#5)\n",
    "6. [정리](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 재현성 시드\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'TensorFlow 버전: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전이학습 전략 개요 <a id='1'></a>\n",
    "\n",
    "전이학습 전략\n",
    "\n",
    "**Feature Extraction**: 사전 학습 모델을 고정(freeze)하고 새 분류 헤드만 학습\n",
    "**Fine-Tuning**: 일부 레이어를 해동(unfreeze)하여 미세조정\n",
    "\n",
    "데이터가 적으면 Feature Extraction, 많으면 Fine-Tuning이 유리\n",
    "\n",
    "### 전략 선택 기준\n",
    "\n",
    "| 상황 | 권장 전략 |\n",
    "|------|----------|\n",
    "| 데이터 적음 + 도메인 유사 | Feature Extraction |\n",
    "| 데이터 적음 + 도메인 다름 | Feature Extraction + 조심스러운 Fine-Tuning |\n",
    "| 데이터 많음 + 도메인 유사 | Fine-Tuning (전체 또는 상위 레이어) |\n",
    "| 데이터 많음 + 도메인 다름 | 처음부터 학습 (또는 전체 Fine-Tuning) |\n",
    "\n",
    "### 왜 낮은 학습률인가?\n",
    "사전 학습 가중치는 이미 잘 최적화되어 있다. 높은 학습률로 Fine-Tuning하면 좋은 가중치를 파괴한다.\n",
    "일반적으로 Feature Extraction 단계보다 **10~100배 낮은 학습률**을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction 단계 <a id='2'></a>\n",
    "\n",
    "**기본 모델(Base Model)을 완전히 고정**하고, 우리가 추가하는 새 분류 헤드만 학습한다.\n",
    "\n",
    "핵심 설정:\n",
    "- `include_top=False`: ImageNet 분류 헤드 제거\n",
    "- `base_model.trainable = False`: 기본 모델 가중치 동결\n",
    "- `training=False`: BatchNormalization을 추론 모드로 실행 (중요!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0 기본 모델 로드 (ImageNet 사전 학습 가중치)\n",
    "# include_top=False: 분류 헤드 제거, 특징 추출기만 사용\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 5  # 예: Flowers 데이터셋 5개 클래스\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# 기본 모델 동결 (Feature Extraction 단계)\n",
    "base_model.trainable = False\n",
    "\n",
    "# 동결 확인\n",
    "total_layers = len(base_model.layers)\n",
    "trainable_layers = sum(1 for l in base_model.layers if l.trainable)\n",
    "frozen_layers = total_layers - trainable_layers\n",
    "\n",
    "print(f'EfficientNetB0 기본 모델 정보:')\n",
    "print(f'  전체 레이어 수: {total_layers}')\n",
    "print(f'  학습 가능 레이어: {trainable_layers}')\n",
    "print(f'  동결된 레이어: {frozen_layers}')\n",
    "print(f'  총 파라미터: {base_model.count_params():,}')\n",
    "print(f'  학습 가능 파라미터: {sum([tf.size(w).numpy() for w in base_model.trainable_weights]):,}')\n",
    "print()\n",
    "\n",
    "# 기본 모델의 출력 형태 확인\n",
    "dummy_input = tf.random.normal([1, IMG_SIZE, IMG_SIZE, 3])\n",
    "base_output = base_model(dummy_input, training=False)\n",
    "print(f'기본 모델 출력 형태: {base_output.shape}')\n",
    "print(f'  → {base_output.shape[1]}×{base_output.shape[2]} 공간 그리드, {base_output.shape[3]} 채널')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 새 분류 헤드 추가 <a id='3'></a>\n",
    "\n",
    "동결된 기본 모델 위에 작업별 분류 헤드를 추가한다.\n",
    "\n",
    "권장 분류 헤드 구조:\n",
    "```\n",
    "GlobalAveragePooling2D → Dense(256, relu) → Dropout(0.3) → Dense(num_classes, softmax)\n",
    "```\n",
    "\n",
    "**왜 GlobalAveragePooling인가?**\n",
    "- Flatten보다 파라미터 수가 훨씬 적다\n",
    "- 공간 정보를 채널별로 압축하여 과적합 방지\n",
    "- 입력 해상도에 무관하게 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새 분류 헤드 추가 및 완전한 전이학습 모델 구성\n",
    "\n",
    "def build_transfer_model(base_model, num_classes, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    전이학습 모델 구성 함수\n",
    "    \n",
    "    Args:\n",
    "        base_model: 동결된 사전 학습 기본 모델\n",
    "        num_classes: 분류 클래스 수\n",
    "        dropout_rate: 드롭아웃 비율\n",
    "    \n",
    "    Returns:\n",
    "        완성된 전이학습 모델\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # 기본 모델 통과 (training=False: BN을 추론 모드로 실행)\n",
    "    # BN 레이어는 training=True이면 배치 통계를 업데이트하므로\n",
    "    # 동결된 모델에서는 반드시 training=False로 설정\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # 분류 헤드\n",
    "    # GlobalAveragePooling: (None, 7, 7, 1280) → (None, 1280)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    \n",
    "    # 중간 Dense 레이어\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', name='dense_1')(x)\n",
    "    \n",
    "    # Dropout으로 과적합 방지\n",
    "    x = tf.keras.layers.Dropout(dropout_rate, name='dropout')(x)\n",
    "    \n",
    "    # 최종 분류층\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes, activation='softmax', name='classifier'\n",
    "    )(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs, name='EfficientNetB0_Transfer')\n",
    "\n",
    "\n",
    "# Feature Extraction 모델 생성\n",
    "transfer_model = build_transfer_model(base_model, NUM_CLASSES, dropout_rate=0.3)\n",
    "transfer_model.summary()\n",
    "\n",
    "# 파라미터 요약\n",
    "total_params     = transfer_model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in transfer_model.trainable_weights])\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f'\\n파라미터 요약:')\n",
    "print(f'  전체 파라미터:    {total_params:>10,}')\n",
    "print(f'  학습 가능 파라미터: {trainable_params:>10,}  ← 새 분류 헤드만')\n",
    "print(f'  동결된 파라미터:   {frozen_params:>10,}  ← EfficientNetB0 본체')\n",
    "print(f'  학습 비율:         {trainable_params/total_params*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction 단계 학습 설정\n",
    "\n",
    "# CIFAR-10을 예시로 사용 (실제로는 도메인 특화 데이터셋 사용)\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.squeeze()\n",
    "y_test  = y_test.squeeze()\n",
    "\n",
    "# 일부 클래스만 사용 (동물 5종: 비행기=0, 자동차=1, 새=2, 고양이=3, 사슴=4)\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# 클래스 0~4만 필터링\n",
    "train_mask = y_train < NUM_CLASSES\n",
    "test_mask  = y_test  < NUM_CLASSES\n",
    "\n",
    "x_train_sub = x_train_raw[train_mask]\n",
    "y_train_sub = y_train[train_mask]\n",
    "x_test_sub  = x_test_raw[test_mask]\n",
    "y_test_sub  = y_test[test_mask]\n",
    "\n",
    "# 224x224로 리사이즈 + 정규화 (EfficientNetB0 입력 형식)\n",
    "def preprocess(images, labels):\n",
    "    # float32 변환 및 정규화\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    # EfficientNet 전처리 (내장 전처리 사용 안할 경우 0-255 그대로 입력)\n",
    "    images = tf.image.resize(images, [IMG_SIZE, IMG_SIZE])\n",
    "    return images, labels\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub))\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test_sub, y_test_sub))\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f'학습 샘플 수: {len(x_train_sub):,}')\n",
    "print(f'검증 샘플 수: {len(x_test_sub):,}')\n",
    "\n",
    "# Feature Extraction 단계 컴파일 (높은 학습률 가능 - 헤드만 학습)\n",
    "transfer_model_fe = build_transfer_model(base_model, NUM_CLASSES)\n",
    "transfer_model_fe.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # 헤드 학습 시 일반 학습률\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Feature Extraction 학습 (빠른 수렴)\n",
    "history_fe = transfer_model_fe.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\nFeature Extraction 최종 검증 정확도: {history_fe.history[\"val_accuracy\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning 단계 <a id='4'></a>\n",
    "\n",
    "Feature Extraction 학습 후, 기본 모델의 **상위 레이어를 해동**하여 전체 모델을 더 세밀하게 조정한다.\n",
    "\n",
    "### Fine-Tuning 전략\n",
    "1. Feature Extraction이 수렴할 때까지 먼저 학습\n",
    "2. 기본 모델의 **하위 레이어는 유지** (저수준 특징 — 엣지, 텍스처)\n",
    "3. **상위 레이어만 해동** (고수준 특징 — 형태, 의미론적 특징)\n",
    "4. **매우 낮은 학습률** 사용 (1e-4 ~ 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning 단계 설정\n",
    "\n",
    "# Feature Extraction으로 학습된 모델을 Fine-Tuning\n",
    "fine_tune_model = transfer_model_fe\n",
    "\n",
    "# 기본 모델 전체 해동\n",
    "fine_tune_model.layers[1].trainable = True  # base_model은 두 번째 레이어\n",
    "\n",
    "# 상위 레이어만 학습 가능하게 설정 (하위 레이어는 재동결)\n",
    "# EfficientNetB0의 전체 레이어 중 마지막 30개만 해동\n",
    "FINE_TUNE_AT = len(base_model.layers) - 30  # 상위 30개 레이어만 해동\n",
    "\n",
    "for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False  # 하위 레이어 재동결\n",
    "\n",
    "for layer in base_model.layers[FINE_TUNE_AT:]:\n",
    "    layer.trainable = True   # 상위 레이어 해동\n",
    "\n",
    "# Fine-Tuning 후 파라미터 상태 확인\n",
    "total_params     = fine_tune_model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in fine_tune_model.trainable_weights])\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f'Fine-Tuning 설정:')\n",
    "print(f'  해동 시작 레이어: {FINE_TUNE_AT} (전체 {len(base_model.layers)}개 중)')\n",
    "print(f'  학습 가능 파라미터: {trainable_params:>10,}')\n",
    "print(f'  동결된 파라미터:   {frozen_params:>10,}')\n",
    "print(f'  학습 비율:         {trainable_params/total_params*100:.2f}%')\n",
    "print()\n",
    "\n",
    "# Fine-Tuning 컴파일 (중요: Feature Extraction보다 훨씬 낮은 학습률)\n",
    "# 학습률을 10배 낮게 설정하여 기존 가중치 보존\n",
    "fine_tune_lr = 1e-4  # Feature Extraction의 1/10\n",
    "\n",
    "fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-Tuning 학습\n",
    "history_ft = fine_tune_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    initial_epoch=5,        # Feature Extraction 이어서 학습\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\nFine-Tuning 최종 검증 정확도: {history_ft.history[\"val_accuracy\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction + Fine-Tuning 학습 곡선 통합 시각화\n",
    "\n",
    "# 두 단계의 히스토리 합치기\n",
    "acc     = history_fe.history['accuracy']     + history_ft.history['accuracy']\n",
    "val_acc = history_fe.history['val_accuracy'] + history_ft.history['val_accuracy']\n",
    "loss     = history_fe.history['loss']        + history_ft.history['loss']\n",
    "val_loss = history_fe.history['val_loss']    + history_ft.history['val_loss']\n",
    "\n",
    "epochs_fe = len(history_fe.history['accuracy'])\n",
    "epochs_total = len(acc)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "ax1.plot(acc, label='학습 정확도', color='blue')\n",
    "ax1.plot(val_acc, label='검증 정확도', color='orange')\n",
    "ax1.axvline(x=epochs_fe - 0.5, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Fine-Tuning 시작 (에폭 {epochs_fe})')\n",
    "ax1.fill_betweenx([0, 1], 0, epochs_fe - 0.5, alpha=0.1, color='blue',\n",
    "                   label='Feature Extraction 구간')\n",
    "ax1.fill_betweenx([0, 1], epochs_fe - 0.5, epochs_total, alpha=0.1, color='green',\n",
    "                   label='Fine-Tuning 구간')\n",
    "ax1.set_xlabel('에폭')\n",
    "ax1.set_ylabel('정확도')\n",
    "ax1.set_title('학습/검증 정확도')\n",
    "ax1.legend(loc='lower right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 손실 그래프\n",
    "ax2.plot(loss, label='학습 손실', color='blue')\n",
    "ax2.plot(val_loss, label='검증 손실', color='orange')\n",
    "ax2.axvline(x=epochs_fe - 0.5, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Fine-Tuning 시작')\n",
    "ax2.fill_betweenx([0, max(loss) * 1.1], 0, epochs_fe - 0.5,\n",
    "                   alpha=0.1, color='blue')\n",
    "ax2.fill_betweenx([0, max(loss) * 1.1], epochs_fe - 0.5, epochs_total,\n",
    "                   alpha=0.1, color='green')\n",
    "ax2.set_xlabel('에폭')\n",
    "ax2.set_ylabel('손실')\n",
    "ax2.set_title('학습/검증 손실')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('전이학습: Feature Extraction → Fine-Tuning', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 단계별 학습률 설정 가이드 <a id='5'></a>\n",
    "\n",
    "### 학습률 선택 원칙\n",
    "\n",
    "| 단계 | 학습률 범위 | 이유 |\n",
    "|------|------------|------|\n",
    "| Feature Extraction | $1 \\times 10^{-3}$ ~ $1 \\times 10^{-2}$ | 새 헤드만 학습, 자유롭게 설정 가능 |\n",
    "| Fine-Tuning (상위 레이어) | $1 \\times 10^{-5}$ ~ $1 \\times 10^{-4}$ | 기존 가중치 보존, 조심스럽게 |\n",
    "| 전체 Fine-Tuning | $1 \\times 10^{-6}$ ~ $1 \\times 10^{-5}$ | 매우 조심스럽게, 대용량 데이터 필요 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단계별 학습률 설정 가이드 코드\n",
    "\n",
    "def get_transfer_learning_optimizer(stage='feature_extraction', base_lr=1e-3):\n",
    "    \"\"\"\n",
    "    전이학습 단계에 맞는 최적화기 반환\n",
    "    \n",
    "    Args:\n",
    "        stage: 학습 단계 ('feature_extraction' | 'fine_tuning' | 'full_fine_tuning')\n",
    "        base_lr: Feature Extraction 기준 학습률\n",
    "    \n",
    "    Returns:\n",
    "        Adam 최적화기\n",
    "    \"\"\"\n",
    "    lr_scale = {\n",
    "        'feature_extraction': 1.0,     # 기준 학습률 그대로\n",
    "        'fine_tuning':        0.1,     # 1/10 수준\n",
    "        'full_fine_tuning':   0.01,    # 1/100 수준\n",
    "    }\n",
    "    \n",
    "    lr = base_lr * lr_scale.get(stage, 1.0)\n",
    "    print(f'단계: {stage}, 학습률: {lr:.2e}')\n",
    "    return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "def apply_fine_tuning_schedule(model, base_model, fine_tune_at_percent=0.7):\n",
    "    \"\"\"\n",
    "    모델의 상위 (1 - fine_tune_at_percent) 비율 레이어만 해동\n",
    "    \n",
    "    Args:\n",
    "        model: 전이학습 모델\n",
    "        base_model: 기본 모델 (model 내부의 레이어)\n",
    "        fine_tune_at_percent: 동결 유지 비율 (0.7 = 하위 70% 동결)\n",
    "    \"\"\"\n",
    "    total_layers = len(base_model.layers)\n",
    "    freeze_until = int(total_layers * fine_tune_at_percent)\n",
    "    \n",
    "    # 기본 모델 전체 해동 후 선택적 재동결\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if i < freeze_until:\n",
    "            layer.trainable = False  # 하위 레이어 동결\n",
    "    \n",
    "    n_frozen   = sum(1 for l in base_model.layers if not l.trainable)\n",
    "    n_trainable = total_layers - n_frozen\n",
    "    \n",
    "    print(f'Fine-Tuning 레이어 설정:')\n",
    "    print(f'  전체: {total_layers}개')\n",
    "    print(f'  동결: {n_frozen}개 (하위 {fine_tune_at_percent*100:.0f}%)')\n",
    "    print(f'  학습: {n_trainable}개 (상위 {(1-fine_tune_at_percent)*100:.0f}%)')\n",
    "\n",
    "\n",
    "# 단계별 학습률 확인\n",
    "print('===== 전이학습 단계별 학습률 =====\\n')\n",
    "for stage in ['feature_extraction', 'fine_tuning', 'full_fine_tuning']:\n",
    "    opt = get_transfer_learning_optimizer(stage, base_lr=1e-3)\n",
    "\n",
    "print()\n",
    "\n",
    "# Fine-Tuning 레이어 설정 예시\n",
    "print('===== Fine-Tuning 레이어 설정 예시 =====\\n')\n",
    "temp_base = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, weights=None, input_shape=(224, 224, 3)\n",
    ")\n",
    "temp_model = build_transfer_model(temp_base, NUM_CLASSES)\n",
    "apply_fine_tuning_schedule(temp_model, temp_base, fine_tune_at_percent=0.7)\n",
    "\n",
    "# 학습률 스케줄러 예시\n",
    "print('\\n===== 학습률 스케줄러 예시 =====\\n')\n",
    "\n",
    "# Cosine Decay (Fine-Tuning 시 권장)\n",
    "cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=1000,\n",
    "    alpha=1e-6  # 최솟값\n",
    ")\n",
    "\n",
    "# 학습률 시각화\n",
    "steps = np.arange(0, 1000)\n",
    "lrs = [cosine_decay(step).numpy() for step in steps]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(steps, lrs)\n",
    "plt.xlabel('학습 스텝')\n",
    "plt.ylabel('학습률')\n",
    "plt.title('Cosine Decay 학습률 스케줄')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 정리 <a id='6'></a>\n",
    "\n",
    "### 전이학습 핵심 원칙\n",
    "\n",
    "1. **Feature Extraction 먼저**: 항상 기본 모델을 동결한 채 분류 헤드를 먼저 학습시켜 수렴시킨다\n",
    "2. **점진적 해동**: 상위 레이어부터 조금씩 해동하여 Fine-Tuning\n",
    "3. **낮은 학습률**: Fine-Tuning 시 Feature Extraction의 1/10 ~ 1/100 학습률 사용\n",
    "4. **BatchNorm 주의**: 동결된 모델의 BN은 `training=False`로 실행\n",
    "5. **데이터 증강**: 적은 데이터에서 전이학습 시 데이터 증강 필수\n",
    "\n",
    "### 학습률 요약\n",
    "| 단계 | 권장 학습률 |\n",
    "|------|------------|\n",
    "| Feature Extraction | $10^{-3}$ |\n",
    "| Fine-Tuning (상위) | $10^{-4}$ |\n",
    "| Full Fine-Tuning | $10^{-5}$ |\n",
    "\n",
    "### 다음 챕터 예고\n",
    "**Chapter 05-04**: 객체 탐지(Object Detection) 입문 — 분류를 넘어 이미지에서 객체의 위치까지 찾아내는 기술을 학습한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
