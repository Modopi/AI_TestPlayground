{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 05-03: ì „ì´í•™ìŠµ â€” Feature Extraction & Fine-Tuning\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ì „ì´í•™ìŠµ(Transfer Learning)ì˜ ê°œë…ê³¼ ë‘ ê°€ì§€ ì „ëµì„ ì´í•´í•œë‹¤\n",
    "- EfficientNetV2B0ë¡œ Feature Extraction íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•œë‹¤\n",
    "- ìƒˆ ë¶„ë¥˜ í—¤ë“œë¥¼ ì¶”ê°€í•˜ê³  ì ì§„ì ìœ¼ë¡œ Fine-Tuningì„ ì ìš©í•œë‹¤\n",
    "- ë‹¨ê³„ë³„ í•™ìŠµë¥  ì„¤ì •ì˜ ì¤‘ìš”ì„±ì„ ì´í•´í•˜ê³  ì ìš©í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ì „ì´í•™ìŠµ ì „ëµ ê°œìš”](#1)\n",
    "2. [Feature Extraction ë‹¨ê³„](#2)\n",
    "3. [ìƒˆ ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€](#3)\n",
    "4. [Fine-Tuning ë‹¨ê³„](#4)\n",
    "5. [ë‹¨ê³„ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ](#5)\n",
    "6. [ì •ë¦¬](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì „ì´í•™ìŠµ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ§  ì „ì´í•™ìŠµ(Transfer Learning)ì´ ë­ì˜ˆìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: í”¼ì•„ë…¸ë¥¼ ë°°ìš´ ì‚¬ëŒì´ ê¸°íƒ€ë¥¼ ë°°ìš¸ ë•Œ!\n",
    "> ìŒì•… ì´ë¡ , ë°•ì ê°ê°, ì† ê·¼ìœ¡ ê¸°ì–µ ë“±ì´ ì´ë¯¸ ìˆì–´ì„œ\n",
    "> ì²˜ìŒ ì‹œì‘í•˜ëŠ” ì‚¬ëŒë³´ë‹¤ **í›¨ì”¬ ë¹¨ë¦¬** ë°°ìš¸ ìˆ˜ ìˆì–´ìš”!\n",
    "\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ, ImageNet(1400ë§Œ ì¥) í•™ìŠµëœ ëª¨ë¸ì€\n",
    "ì—£ì§€, ìƒ‰ê¹”, í˜•íƒœ, ë¬¼ì²´ ë“± **ì¼ë°˜ì ì¸ ì‹œê° ì§€ì‹**ì„ ì´ë¯¸ ì•Œì•„ìš”!\n",
    "ì´ ì§€ì‹ì„ **ìš°ë¦¬ ë¬¸ì œì— ê·¸ëŒ€ë¡œ í™œìš©**í•˜ë©´ ë¼ìš”.\n",
    "\n",
    "#### ğŸ“‹ ë‘ ê°€ì§€ ì „ëµ ë¹„êµ\n",
    "\n",
    "**1. Feature Extraction (íŠ¹ì§• ì¶”ì¶œ)**:\n",
    "```\n",
    "ì‚¬ì „í•™ìŠµ ëª¨ë¸ (ë™ê²° ğŸ”’)   â†’   ìƒˆ ë¶„ë¥˜ í—¤ë“œ (í•™ìŠµ âœ…)\n",
    "\"ImageNet ì§€ì‹ ê·¸ëŒ€ë¡œ\"       \"ìš°ë¦¬ ë¬¸ì œì— ë§ê²Œ\"\n",
    "```\n",
    "- ë°ì´í„°ê°€ ì ì„ ë•Œ ê¶Œì¥\n",
    "- í•™ìŠµ ë¹ ë¥´ê³  ê³¼ì í•© ì ìŒ\n",
    "\n",
    "**2. Fine-Tuning (ë¯¸ì„¸ ì¡°ì •)**:\n",
    "```\n",
    "ì‚¬ì „í•™ìŠµ ëª¨ë¸ ìƒìœ„ ì¸µë§Œ (í•´ë™ ğŸ”“)   â†’   ìƒˆ ë¶„ë¥˜ í—¤ë“œ (í•™ìŠµ âœ…)\n",
    "\"ì•½ê°„ ì¡°ì •\"                             \"ìš°ë¦¬ ë¬¸ì œì— ë§ê²Œ\"\n",
    "```\n",
    "- ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ\n",
    "- Feature Extraction í›„ì— ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©\n",
    "- **ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ** í•„ìˆ˜! (ê¸°ì¡´ ì§€ì‹ íŒŒê´´ ë°©ì§€)\n",
    "\n",
    "> ğŸ’¡ **ì™œ ë‚®ì€ í•™ìŠµë¥ ?**\n",
    "> ì˜¤ë«ë™ì•ˆ ìŒ“ì¸ ì¢‹ì€ ê°€ì¤‘ì¹˜ë¥¼ ë„ˆë¬´ ë¹ ë¥´ê²Œ ë°”ê¾¸ë©´\n",
    "> ImageNetì—ì„œ ë°°ìš´ ê·€ì¤‘í•œ ì§€ì‹ì´ ì‚¬ë¼ì ¸ìš”!\n",
    "> ë§ˆì¹˜ í”¼ì•„ë…¸ ì‹¤ë ¥ì„ ìƒì§€ ì•Šìœ¼ë©´ì„œ ê¸°íƒ€ë¥¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë°°ìš°ëŠ” ê²ƒì²˜ëŸ¼ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ì¬í˜„ì„± ì‹œë“œ\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f'TensorFlow ë²„ì „: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì „ì´í•™ìŠµ ì „ëµ ê°œìš” <a id='1'></a>\n",
    "\n",
    "ì „ì´í•™ìŠµ ì „ëµ\n",
    "\n",
    "**Feature Extraction**: ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ê³ ì •(freeze)í•˜ê³  ìƒˆ ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµ\n",
    "**Fine-Tuning**: ì¼ë¶€ ë ˆì´ì–´ë¥¼ í•´ë™(unfreeze)í•˜ì—¬ ë¯¸ì„¸ì¡°ì •\n",
    "\n",
    "ë°ì´í„°ê°€ ì ìœ¼ë©´ Feature Extraction, ë§ìœ¼ë©´ Fine-Tuningì´ ìœ ë¦¬\n",
    "\n",
    "### ì „ëµ ì„ íƒ ê¸°ì¤€\n",
    "\n",
    "| ìƒí™© | ê¶Œì¥ ì „ëµ |\n",
    "|------|----------|\n",
    "| ë°ì´í„° ì ìŒ + ë„ë©”ì¸ ìœ ì‚¬ | Feature Extraction |\n",
    "| ë°ì´í„° ì ìŒ + ë„ë©”ì¸ ë‹¤ë¦„ | Feature Extraction + ì¡°ì‹¬ìŠ¤ëŸ¬ìš´ Fine-Tuning |\n",
    "| ë°ì´í„° ë§ìŒ + ë„ë©”ì¸ ìœ ì‚¬ | Fine-Tuning (ì „ì²´ ë˜ëŠ” ìƒìœ„ ë ˆì´ì–´) |\n",
    "| ë°ì´í„° ë§ìŒ + ë„ë©”ì¸ ë‹¤ë¦„ | ì²˜ìŒë¶€í„° í•™ìŠµ (ë˜ëŠ” ì „ì²´ Fine-Tuning) |\n",
    "\n",
    "### ì™œ ë‚®ì€ í•™ìŠµë¥ ì¸ê°€?\n",
    "ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜ëŠ” ì´ë¯¸ ì˜ ìµœì í™”ë˜ì–´ ìˆë‹¤. ë†’ì€ í•™ìŠµë¥ ë¡œ Fine-Tuningí•˜ë©´ ì¢‹ì€ ê°€ì¤‘ì¹˜ë¥¼ íŒŒê´´í•œë‹¤.\n",
    "ì¼ë°˜ì ìœ¼ë¡œ Feature Extraction ë‹¨ê³„ë³´ë‹¤ **10~100ë°° ë‚®ì€ í•™ìŠµë¥ **ì„ ì‚¬ìš©í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction ë‹¨ê³„ <a id='2'></a>\n",
    "\n",
    "**ê¸°ë³¸ ëª¨ë¸(Base Model)ì„ ì™„ì „íˆ ê³ ì •**í•˜ê³ , ìš°ë¦¬ê°€ ì¶”ê°€í•˜ëŠ” ìƒˆ ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµí•œë‹¤.\n",
    "\n",
    "í•µì‹¬ ì„¤ì •:\n",
    "- `include_top=False`: ImageNet ë¶„ë¥˜ í—¤ë“œ ì œê±°\n",
    "- `base_model.trainable = False`: ê¸°ë³¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë™ê²°\n",
    "- `training=False`: BatchNormalizationì„ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰ (ì¤‘ìš”!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetV2B0 ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (ImageNet ì‚¬ì „ í•™ìŠµ ê°€ì¤‘ì¹˜)\n",
    "# include_top=False: ë¶„ë¥˜ í—¤ë“œ ì œê±°, íŠ¹ì§• ì¶”ì¶œê¸°ë§Œ ì‚¬ìš©\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 5  # ì˜ˆ: Flowers ë°ì´í„°ì…‹ 5ê°œ í´ë˜ìŠ¤\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3)\n",
    ")\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ ë™ê²° (Feature Extraction ë‹¨ê³„)\n",
    "base_model.trainable = False\n",
    "\n",
    "# ë™ê²° í™•ì¸\n",
    "total_layers = len(base_model.layers)\n",
    "trainable_layers = sum(1 for l in base_model.layers if l.trainable)\n",
    "frozen_layers = total_layers - trainable_layers\n",
    "\n",
    "print(f'EfficientNetV2B0 ê¸°ë³¸ ëª¨ë¸ ì •ë³´:')\n",
    "print(f'  ì „ì²´ ë ˆì´ì–´ ìˆ˜: {total_layers}')\n",
    "print(f'  í•™ìŠµ ê°€ëŠ¥ ë ˆì´ì–´: {trainable_layers}')\n",
    "print(f'  ë™ê²°ëœ ë ˆì´ì–´: {frozen_layers}')\n",
    "print(f'  ì´ íŒŒë¼ë¯¸í„°: {base_model.count_params():,}')\n",
    "print(f'  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {sum([tf.size(w).numpy() for w in base_model.trainable_weights]):,}')\n",
    "print()\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ì˜ ì¶œë ¥ í˜•íƒœ í™•ì¸\n",
    "dummy_input = tf.random.normal([1, IMG_SIZE, IMG_SIZE, 3])\n",
    "base_output = base_model(dummy_input, training=False)\n",
    "print(f'ê¸°ë³¸ ëª¨ë¸ ì¶œë ¥ í˜•íƒœ: {base_output.shape}')\n",
    "print(f'  â†’ {base_output.shape[1]}Ã—{base_output.shape[2]} ê³µê°„ ê·¸ë¦¬ë“œ, {base_output.shape[3]} ì±„ë„')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ìƒˆ ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€ <a id='3'></a>\n",
    "\n",
    "ë™ê²°ëœ ê¸°ë³¸ ëª¨ë¸ ìœ„ì— ì‘ì—…ë³„ ë¶„ë¥˜ í—¤ë“œë¥¼ ì¶”ê°€í•œë‹¤.\n",
    "\n",
    "ê¶Œì¥ ë¶„ë¥˜ í—¤ë“œ êµ¬ì¡°:\n",
    "```\n",
    "GlobalAveragePooling2D â†’ Dense(256, relu) â†’ Dropout(0.3) â†’ Dense(num_classes, softmax)\n",
    "```\n",
    "\n",
    "**ì™œ GlobalAveragePoolingì¸ê°€?**\n",
    "- Flattenë³´ë‹¤ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ í›¨ì”¬ ì ë‹¤\n",
    "- ê³µê°„ ì •ë³´ë¥¼ ì±„ë„ë³„ë¡œ ì••ì¶•í•˜ì—¬ ê³¼ì í•© ë°©ì§€\n",
    "- ì…ë ¥ í•´ìƒë„ì— ë¬´ê´€í•˜ê²Œ ë™ì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆ ë¶„ë¥˜ í—¤ë“œ ì¶”ê°€ ë° ì™„ì „í•œ ì „ì´í•™ìŠµ ëª¨ë¸ êµ¬ì„±\n",
    "\n",
    "def build_transfer_model(base_model, num_classes, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    ì „ì´í•™ìŠµ ëª¨ë¸ êµ¬ì„± í•¨ìˆ˜\n",
    "    \n",
    "    Args:\n",
    "        base_model: ë™ê²°ëœ ì‚¬ì „ í•™ìŠµ ê¸°ë³¸ ëª¨ë¸\n",
    "        num_classes: ë¶„ë¥˜ í´ë˜ìŠ¤ ìˆ˜\n",
    "        dropout_rate: ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "    \n",
    "    Returns:\n",
    "        ì™„ì„±ëœ ì „ì´í•™ìŠµ ëª¨ë¸\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    # ê¸°ë³¸ ëª¨ë¸ í†µê³¼ (training=False: BNì„ ì¶”ë¡  ëª¨ë“œë¡œ ì‹¤í–‰)\n",
    "    # BN ë ˆì´ì–´ëŠ” training=Trueì´ë©´ ë°°ì¹˜ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•˜ë¯€ë¡œ\n",
    "    # ë™ê²°ëœ ëª¨ë¸ì—ì„œëŠ” ë°˜ë“œì‹œ training=Falseë¡œ ì„¤ì •\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # ë¶„ë¥˜ í—¤ë“œ\n",
    "    # GlobalAveragePooling: (None, 7, 7, 1280) â†’ (None, 1280)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D(name='gap')(x)\n",
    "    \n",
    "    # ì¤‘ê°„ Dense ë ˆì´ì–´\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', name='dense_1')(x)\n",
    "    \n",
    "    # Dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€\n",
    "    x = tf.keras.layers.Dropout(dropout_rate, name='dropout')(x)\n",
    "    \n",
    "    # ìµœì¢… ë¶„ë¥˜ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        num_classes, activation='softmax', name='classifier'\n",
    "    )(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs, name='EfficientNetV2B0_Transfer')\n",
    "\n",
    "\n",
    "# Feature Extraction ëª¨ë¸ ìƒì„±\n",
    "transfer_model = build_transfer_model(base_model, NUM_CLASSES, dropout_rate=0.3)\n",
    "transfer_model.summary()\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìš”ì•½\n",
    "total_params     = transfer_model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in transfer_model.trainable_weights])\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f'\\níŒŒë¼ë¯¸í„° ìš”ì•½:')\n",
    "print(f'  ì „ì²´ íŒŒë¼ë¯¸í„°:    {total_params:>10,}')\n",
    "print(f'  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:>10,}  â† ìƒˆ ë¶„ë¥˜ í—¤ë“œë§Œ')\n",
    "print(f'  ë™ê²°ëœ íŒŒë¼ë¯¸í„°:   {frozen_params:>10,}  â† EfficientNetV2B0 ë³¸ì²´')\n",
    "print(f'  í•™ìŠµ ë¹„ìœ¨:         {trainable_params/total_params*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction ë‹¨ê³„ í•™ìŠµ ì„¤ì •\n",
    "\n",
    "# CIFAR-10ì„ ì˜ˆì‹œë¡œ ì‚¬ìš© (ì‹¤ì œë¡œëŠ” ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°ì…‹ ì‚¬ìš©)\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.squeeze()\n",
    "y_test  = y_test.squeeze()\n",
    "\n",
    "# ì¼ë¶€ í´ë˜ìŠ¤ë§Œ ì‚¬ìš© (ë™ë¬¼ 5ì¢…: ë¹„í–‰ê¸°=0, ìë™ì°¨=1, ìƒˆ=2, ê³ ì–‘ì´=3, ì‚¬ìŠ´=4)\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# í´ë˜ìŠ¤ 0~4ë§Œ í•„í„°ë§\n",
    "train_mask = y_train < NUM_CLASSES\n",
    "test_mask  = y_test  < NUM_CLASSES\n",
    "\n",
    "x_train_sub = x_train_raw[train_mask]\n",
    "y_train_sub = y_train[train_mask]\n",
    "x_test_sub  = x_test_raw[test_mask]\n",
    "y_test_sub  = y_test[test_mask]\n",
    "\n",
    "# 224x224ë¡œ ë¦¬ì‚¬ì´ì¦ˆ + ì •ê·œí™” (EfficientNetV2B0 ì…ë ¥ í˜•ì‹)\n",
    "def preprocess(images, labels):\n",
    "    # float32 ë³€í™˜ ë° ì •ê·œí™”\n",
    "    images = tf.cast(images, tf.float32)\n",
    "    # EfficientNet ì „ì²˜ë¦¬ (ë‚´ì¥ ì „ì²˜ë¦¬ ì‚¬ìš© ì•ˆí•  ê²½ìš° 0-255 ê·¸ëŒ€ë¡œ ì…ë ¥)\n",
    "    images = tf.image.resize(images, [IMG_SIZE, IMG_SIZE])\n",
    "    return images, labels\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train_sub, y_train_sub))\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_test_sub, y_test_sub))\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f'í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(x_train_sub):,}')\n",
    "print(f'ê²€ì¦ ìƒ˜í”Œ ìˆ˜: {len(x_test_sub):,}')\n",
    "\n",
    "# Feature Extraction ë‹¨ê³„ ì»´íŒŒì¼ (ë†’ì€ í•™ìŠµë¥  ê°€ëŠ¥ - í—¤ë“œë§Œ í•™ìŠµ)\n",
    "transfer_model_fe = build_transfer_model(base_model, NUM_CLASSES)\n",
    "transfer_model_fe.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # í—¤ë“œ í•™ìŠµ ì‹œ ì¼ë°˜ í•™ìŠµë¥ \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Feature Extraction í•™ìŠµ (ë¹ ë¥¸ ìˆ˜ë ´)\n",
    "history_fe = transfer_model_fe.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\nFeature Extraction ìµœì¢… ê²€ì¦ ì •í™•ë„: {history_fe.history[\"val_accuracy\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning ë‹¨ê³„ <a id='4'></a>\n",
    "\n",
    "Feature Extraction í•™ìŠµ í›„, ê¸°ë³¸ ëª¨ë¸ì˜ **ìƒìœ„ ë ˆì´ì–´ë¥¼ í•´ë™**í•˜ì—¬ ì „ì²´ ëª¨ë¸ì„ ë” ì„¸ë°€í•˜ê²Œ ì¡°ì •í•œë‹¤.\n",
    "\n",
    "### Fine-Tuning ì „ëµ\n",
    "1. Feature Extractionì´ ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë¨¼ì € í•™ìŠµ\n",
    "2. ê¸°ë³¸ ëª¨ë¸ì˜ **í•˜ìœ„ ë ˆì´ì–´ëŠ” ìœ ì§€** (ì €ìˆ˜ì¤€ íŠ¹ì§• â€” ì—£ì§€, í…ìŠ¤ì²˜)\n",
    "3. **ìƒìœ„ ë ˆì´ì–´ë§Œ í•´ë™** (ê³ ìˆ˜ì¤€ íŠ¹ì§• â€” í˜•íƒœ, ì˜ë¯¸ë¡ ì  íŠ¹ì§•)\n",
    "4. **ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ ** ì‚¬ìš© (1e-4 ~ 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning ë‹¨ê³„ ì„¤ì •\n",
    "\n",
    "# Feature Extractionìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ Fine-Tuning\n",
    "fine_tune_model = transfer_model_fe\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ ì „ì²´ í•´ë™\n",
    "fine_tune_model.layers[1].trainable = True  # base_modelì€ ë‘ ë²ˆì§¸ ë ˆì´ì–´\n",
    "\n",
    "# ìƒìœ„ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì • (í•˜ìœ„ ë ˆì´ì–´ëŠ” ì¬ë™ê²°)\n",
    "# EfficientNetV2B0ì˜ ì „ì²´ ë ˆì´ì–´ ì¤‘ ë§ˆì§€ë§‰ 30ê°œë§Œ í•´ë™\n",
    "FINE_TUNE_AT = len(base_model.layers) - 30  # ìƒìœ„ 30ê°œ ë ˆì´ì–´ë§Œ í•´ë™\n",
    "\n",
    "for layer in base_model.layers[:FINE_TUNE_AT]:\n",
    "    layer.trainable = False  # í•˜ìœ„ ë ˆì´ì–´ ì¬ë™ê²°\n",
    "\n",
    "for layer in base_model.layers[FINE_TUNE_AT:]:\n",
    "    layer.trainable = True   # ìƒìœ„ ë ˆì´ì–´ í•´ë™\n",
    "\n",
    "# Fine-Tuning í›„ íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸\n",
    "total_params     = fine_tune_model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in fine_tune_model.trainable_weights])\n",
    "frozen_params    = total_params - trainable_params\n",
    "\n",
    "print(f'Fine-Tuning ì„¤ì •:')\n",
    "print(f'  í•´ë™ ì‹œì‘ ë ˆì´ì–´: {FINE_TUNE_AT} (ì „ì²´ {len(base_model.layers)}ê°œ ì¤‘)')\n",
    "print(f'  í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:>10,}')\n",
    "print(f'  ë™ê²°ëœ íŒŒë¼ë¯¸í„°:   {frozen_params:>10,}')\n",
    "print(f'  í•™ìŠµ ë¹„ìœ¨:         {trainable_params/total_params*100:.2f}%')\n",
    "print()\n",
    "\n",
    "# Fine-Tuning ì»´íŒŒì¼ (ì¤‘ìš”: Feature Extractionë³´ë‹¤ í›¨ì”¬ ë‚®ì€ í•™ìŠµë¥ )\n",
    "# í•™ìŠµë¥ ì„ 10ë°° ë‚®ê²Œ ì„¤ì •í•˜ì—¬ ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³´ì¡´\n",
    "fine_tune_lr = 1e-4  # Feature Extractionì˜ 1/10\n",
    "\n",
    "fine_tune_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-Tuning í•™ìŠµ\n",
    "history_ft = fine_tune_model.fit(\n",
    "    train_ds,\n",
    "    epochs=5,\n",
    "    initial_epoch=5,        # Feature Extraction ì´ì–´ì„œ í•™ìŠµ\n",
    "    validation_data=val_ds,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f'\\nFine-Tuning ìµœì¢… ê²€ì¦ ì •í™•ë„: {history_ft.history[\"val_accuracy\"][-1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction + Fine-Tuning í•™ìŠµ ê³¡ì„  í†µí•© ì‹œê°í™”\n",
    "\n",
    "# ë‘ ë‹¨ê³„ì˜ íˆìŠ¤í† ë¦¬ í•©ì¹˜ê¸°\n",
    "acc     = history_fe.history['accuracy']     + history_ft.history['accuracy']\n",
    "val_acc = history_fe.history['val_accuracy'] + history_ft.history['val_accuracy']\n",
    "loss     = history_fe.history['loss']        + history_ft.history['loss']\n",
    "val_loss = history_fe.history['val_loss']    + history_ft.history['val_loss']\n",
    "\n",
    "epochs_fe = len(history_fe.history['accuracy'])\n",
    "epochs_total = len(acc)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ì •í™•ë„ ê·¸ë˜í”„\n",
    "ax1.plot(acc, label='í•™ìŠµ ì •í™•ë„', color='blue')\n",
    "ax1.plot(val_acc, label='ê²€ì¦ ì •í™•ë„', color='orange')\n",
    "ax1.axvline(x=epochs_fe - 0.5, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Fine-Tuning ì‹œì‘ (ì—í­ {epochs_fe})')\n",
    "ax1.fill_betweenx([0, 1], 0, epochs_fe - 0.5, alpha=0.1, color='blue',\n",
    "                   label='Feature Extraction êµ¬ê°„')\n",
    "ax1.fill_betweenx([0, 1], epochs_fe - 0.5, epochs_total, alpha=0.1, color='green',\n",
    "                   label='Fine-Tuning êµ¬ê°„')\n",
    "ax1.set_xlabel('ì—í­')\n",
    "ax1.set_ylabel('ì •í™•ë„')\n",
    "ax1.set_title('í•™ìŠµ/ê²€ì¦ ì •í™•ë„')\n",
    "ax1.legend(loc='lower right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì†ì‹¤ ê·¸ë˜í”„\n",
    "ax2.plot(loss, label='í•™ìŠµ ì†ì‹¤', color='blue')\n",
    "ax2.plot(val_loss, label='ê²€ì¦ ì†ì‹¤', color='orange')\n",
    "ax2.axvline(x=epochs_fe - 0.5, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Fine-Tuning ì‹œì‘')\n",
    "ax2.fill_betweenx([0, max(loss) * 1.1], 0, epochs_fe - 0.5,\n",
    "                   alpha=0.1, color='blue')\n",
    "ax2.fill_betweenx([0, max(loss) * 1.1], epochs_fe - 0.5, epochs_total,\n",
    "                   alpha=0.1, color='green')\n",
    "ax2.set_xlabel('ì—í­')\n",
    "ax2.set_ylabel('ì†ì‹¤')\n",
    "ax2.set_title('í•™ìŠµ/ê²€ì¦ ì†ì‹¤')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ì „ì´í•™ìŠµ: Feature Extraction â†’ Fine-Tuning', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë‹¨ê³„ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ <a id='5'></a>\n",
    "\n",
    "### í•™ìŠµë¥  ì„ íƒ ì›ì¹™\n",
    "\n",
    "| ë‹¨ê³„ | í•™ìŠµë¥  ë²”ìœ„ | ì´ìœ  |\n",
    "|------|------------|------|\n",
    "| Feature Extraction | $1 \\times 10^{-3}$ ~ $1 \\times 10^{-2}$ | ìƒˆ í—¤ë“œë§Œ í•™ìŠµ, ììœ ë¡­ê²Œ ì„¤ì • ê°€ëŠ¥ |\n",
    "| Fine-Tuning (ìƒìœ„ ë ˆì´ì–´) | $1 \\times 10^{-5}$ ~ $1 \\times 10^{-4}$ | ê¸°ì¡´ ê°€ì¤‘ì¹˜ ë³´ì¡´, ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ |\n",
    "| ì „ì²´ Fine-Tuning | $1 \\times 10^{-6}$ ~ $1 \\times 10^{-5}$ | ë§¤ìš° ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ, ëŒ€ìš©ëŸ‰ ë°ì´í„° í•„ìš” |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¨ê³„ë³„ í•™ìŠµë¥  ì„¤ì • ê°€ì´ë“œ ì½”ë“œ\n",
    "\n",
    "def get_transfer_learning_optimizer(stage='feature_extraction', base_lr=1e-3):\n",
    "    \"\"\"\n",
    "    ì „ì´í•™ìŠµ ë‹¨ê³„ì— ë§ëŠ” ìµœì í™”ê¸° ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        stage: í•™ìŠµ ë‹¨ê³„ ('feature_extraction' | 'fine_tuning' | 'full_fine_tuning')\n",
    "        base_lr: Feature Extraction ê¸°ì¤€ í•™ìŠµë¥ \n",
    "    \n",
    "    Returns:\n",
    "        Adam ìµœì í™”ê¸°\n",
    "    \"\"\"\n",
    "    lr_scale = {\n",
    "        'feature_extraction': 1.0,     # ê¸°ì¤€ í•™ìŠµë¥  ê·¸ëŒ€ë¡œ\n",
    "        'fine_tuning':        0.1,     # 1/10 ìˆ˜ì¤€\n",
    "        'full_fine_tuning':   0.01,    # 1/100 ìˆ˜ì¤€\n",
    "    }\n",
    "    \n",
    "    lr = base_lr * lr_scale.get(stage, 1.0)\n",
    "    print(f'ë‹¨ê³„: {stage}, í•™ìŠµë¥ : {lr:.2e}')\n",
    "    return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "\n",
    "def apply_fine_tuning_schedule(model, base_model, fine_tune_at_percent=0.7):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì˜ ìƒìœ„ (1 - fine_tune_at_percent) ë¹„ìœ¨ ë ˆì´ì–´ë§Œ í•´ë™\n",
    "    \n",
    "    Args:\n",
    "        model: ì „ì´í•™ìŠµ ëª¨ë¸\n",
    "        base_model: ê¸°ë³¸ ëª¨ë¸ (model ë‚´ë¶€ì˜ ë ˆì´ì–´)\n",
    "        fine_tune_at_percent: ë™ê²° ìœ ì§€ ë¹„ìœ¨ (0.7 = í•˜ìœ„ 70% ë™ê²°)\n",
    "    \"\"\"\n",
    "    total_layers = len(base_model.layers)\n",
    "    freeze_until = int(total_layers * fine_tune_at_percent)\n",
    "    \n",
    "    # ê¸°ë³¸ ëª¨ë¸ ì „ì²´ í•´ë™ í›„ ì„ íƒì  ì¬ë™ê²°\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        if i < freeze_until:\n",
    "            layer.trainable = False  # í•˜ìœ„ ë ˆì´ì–´ ë™ê²°\n",
    "    \n",
    "    n_frozen   = sum(1 for l in base_model.layers if not l.trainable)\n",
    "    n_trainable = total_layers - n_frozen\n",
    "    \n",
    "    print(f'Fine-Tuning ë ˆì´ì–´ ì„¤ì •:')\n",
    "    print(f'  ì „ì²´: {total_layers}ê°œ')\n",
    "    print(f'  ë™ê²°: {n_frozen}ê°œ (í•˜ìœ„ {fine_tune_at_percent*100:.0f}%)')\n",
    "    print(f'  í•™ìŠµ: {n_trainable}ê°œ (ìƒìœ„ {(1-fine_tune_at_percent)*100:.0f}%)')\n",
    "\n",
    "\n",
    "# ë‹¨ê³„ë³„ í•™ìŠµë¥  í™•ì¸\n",
    "print('===== ì „ì´í•™ìŠµ ë‹¨ê³„ë³„ í•™ìŠµë¥  =====\\n')\n",
    "for stage in ['feature_extraction', 'fine_tuning', 'full_fine_tuning']:\n",
    "    opt = get_transfer_learning_optimizer(stage, base_lr=1e-3)\n",
    "\n",
    "print()\n",
    "\n",
    "# Fine-Tuning ë ˆì´ì–´ ì„¤ì • ì˜ˆì‹œ\n",
    "print('===== Fine-Tuning ë ˆì´ì–´ ì„¤ì • ì˜ˆì‹œ =====\\n')\n",
    "temp_base = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False, weights=None, input_shape=(224, 224, 3)\n",
    ")\n",
    "temp_model = build_transfer_model(temp_base, NUM_CLASSES)\n",
    "apply_fine_tuning_schedule(temp_model, temp_base, fine_tune_at_percent=0.7)\n",
    "\n",
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì˜ˆì‹œ\n",
    "print('\\n===== í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì˜ˆì‹œ =====\\n')\n",
    "\n",
    "# Cosine Decay (Fine-Tuning ì‹œ ê¶Œì¥)\n",
    "cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=1000,\n",
    "    alpha=1e-6  # ìµœì†Ÿê°’\n",
    ")\n",
    "\n",
    "# í•™ìŠµë¥  ì‹œê°í™”\n",
    "steps = np.arange(0, 1000)\n",
    "lrs = [cosine_decay(step).numpy() for step in steps]\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(steps, lrs)\n",
    "plt.xlabel('í•™ìŠµ ìŠ¤í…')\n",
    "plt.ylabel('í•™ìŠµë¥ ')\n",
    "plt.title('Cosine Decay í•™ìŠµë¥  ìŠ¤ì¼€ì¤„')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬ <a id='6'></a>\n",
    "\n",
    "### ì „ì´í•™ìŠµ í•µì‹¬ ì›ì¹™\n",
    "\n",
    "1. **Feature Extraction ë¨¼ì €**: í•­ìƒ ê¸°ë³¸ ëª¨ë¸ì„ ë™ê²°í•œ ì±„ ë¶„ë¥˜ í—¤ë“œë¥¼ ë¨¼ì € í•™ìŠµì‹œì¼œ ìˆ˜ë ´ì‹œí‚¨ë‹¤\n",
    "2. **ì ì§„ì  í•´ë™**: ìƒìœ„ ë ˆì´ì–´ë¶€í„° ì¡°ê¸ˆì”© í•´ë™í•˜ì—¬ Fine-Tuning\n",
    "3. **ë‚®ì€ í•™ìŠµë¥ **: Fine-Tuning ì‹œ Feature Extractionì˜ 1/10 ~ 1/100 í•™ìŠµë¥  ì‚¬ìš©\n",
    "4. **BatchNorm ì£¼ì˜**: ë™ê²°ëœ ëª¨ë¸ì˜ BNì€ `training=False`ë¡œ ì‹¤í–‰\n",
    "5. **ë°ì´í„° ì¦ê°•**: ì ì€ ë°ì´í„°ì—ì„œ ì „ì´í•™ìŠµ ì‹œ ë°ì´í„° ì¦ê°• í•„ìˆ˜\n",
    "\n",
    "### í•™ìŠµë¥  ìš”ì•½\n",
    "| ë‹¨ê³„ | ê¶Œì¥ í•™ìŠµë¥  |\n",
    "|------|------------|\n",
    "| Feature Extraction | $10^{-3}$ |\n",
    "| Fine-Tuning (ìƒìœ„) | $10^{-4}$ |\n",
    "| Full Fine-Tuning | $10^{-5}$ |\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 05-04**: ê°ì²´ íƒì§€(Object Detection) ì…ë¬¸ â€” ë¶„ë¥˜ë¥¼ ë„˜ì–´ ì´ë¯¸ì§€ì—ì„œ ê°ì²´ì˜ ìœ„ì¹˜ê¹Œì§€ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì„ í•™ìŠµí•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}