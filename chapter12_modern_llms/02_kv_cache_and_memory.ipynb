{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: ìµœì‹  LLM ì•„í‚¤í…ì²˜ â€” KV Cacheì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Autoregressive ìƒì„±ì—ì„œ **KV Cacheì˜ ì—­í• ê³¼ ë©”ëª¨ë¦¬ ê³µì‹**ì„ ì •í™•íˆ ì´í•´í•˜ê³  ê³„ì‚°í•œë‹¤\n",
    "- Llama 3 8B ì‹¤ì œ íŒŒë¼ë¯¸í„°ë¡œ **ë°°ì¹˜/ì‹œí€€ìŠ¤ë³„ KV ìºì‹œ ë©”ëª¨ë¦¬**ë¥¼ ê³„ì‚°í•œë‹¤\n",
    "- **Rolling Buffer**(Sliding Window) ë°©ì‹ìœ¼ë¡œ ê³ ì • ë©”ëª¨ë¦¬ KV ê´€ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- **Multi-Turn ëŒ€í™”**ì—ì„œ KV ìºì‹œ ë©”ëª¨ë¦¬ ì¦ê°€ íŒ¨í„´ì„ ì‹œê°í™”í•˜ê³  ë¶„ì„í•œë‹¤\n",
    "- **Prefix Caching** ê°œìš”ì™€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¬ì‚¬ìš© ì´ì ì„ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: KV Cache ë©”ëª¨ë¦¬ ê³µì‹](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [Autoregressive ìƒì„±ê³¼ KV Cache](#2.-KV-Cache-ì›ë¦¬)\n",
    "3. [Rolling Buffer (Sliding Window)](#3.-Rolling-Buffer)\n",
    "4. [Multi-Turn ëŒ€í™” ë©”ëª¨ë¦¬ ë¶„ì„](#4.-Multi-Turn-ë¶„ì„)\n",
    "5. [Prefix Caching ì‹œë®¬ë ˆì´ì…˜](#5.-Prefix-Caching)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### KV Cache ë©”ëª¨ë¦¬ ê³µì‹\n",
    "\n",
    "Autoregressive ë””ì½”ë”©ì—ì„œ ë§¤ í† í° ìƒì„± ì‹œ ì´ì „ í† í°ë“¤ì˜ K, Vë¥¼ **ì¬ê³„ì‚°í•˜ì§€ ì•Šê³  ìºì‹œ**í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$M_{KV} = 2 \\times L \\times H_{kv} \\times d_{head} \\times S \\times B \\times \\text{bytes\\_per\\_element}$$\n",
    "\n",
    "- $2$: Kì™€ V ë‘ í…ì„œ\n",
    "- $L$: ë ˆì´ì–´(ë¸”ë¡) ìˆ˜\n",
    "- $H_{kv}$: KV í—¤ë“œ ìˆ˜ (GQAì—ì„œ Q í—¤ë“œë³´ë‹¤ ì ìŒ)\n",
    "- $d_{head}$: í—¤ë“œ ì°¨ì›\n",
    "- $S$: í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "- $B$: ë°°ì¹˜ í¬ê¸°\n",
    "- bytes: FP16=2, FP32=4, INT8=1\n",
    "\n",
    "**Llama 3 8B ì˜ˆì‹œ ($L=32, H_{kv}=8, d_{head}=128$, FP16):**\n",
    "\n",
    "| ì‹œí€€ìŠ¤ ê¸¸ì´ | ë°°ì¹˜=1 | ë°°ì¹˜=8 | ë°°ì¹˜=32 |\n",
    "|------------|--------|--------|---------|\n",
    "| $S=512$ | 64 MB | 512 MB | 2.0 GB |\n",
    "| $S=2048$ | 256 MB | 2.0 GB | 8.0 GB |\n",
    "| $S=8192$ | 1.0 GB | 8.0 GB | 32 GB |\n",
    "\n",
    "### KV Cache ì—†ì´ vs ìˆì„ ë•Œ ë³µì¡ë„\n",
    "\n",
    "| í•­ëª© | Cache ì—†ìŒ | Cache ìˆìŒ |\n",
    "|------|-----------|-----------|\n",
    "| í† í°ë‹¹ K,V ê³„ì‚° | $O(S \\cdot d)$ ì „ì²´ ì¬ê³„ì‚° | $O(d)$ ìƒˆ í† í°ë§Œ |\n",
    "| Attention FLOPs | $O(S^2 \\cdot d)$ ë§¤ë²ˆ | $O(S \\cdot d)$ ìƒˆ Që§Œ |\n",
    "| ì¶”ê°€ ë©”ëª¨ë¦¬ | ì—†ìŒ | $O(L \\cdot H_{kv} \\cdot d \\cdot S)$ |\n",
    "| ì´ ìƒì„± FLOPs ($T$í† í°) | $O(T \\cdot S^2 \\cdot d)$ | $O(T \\cdot S \\cdot d)$ |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ KV Cache ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ“ KV Cacheê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìˆ˜í•™ ì‹œí—˜ì—ì„œ **ì• ë¬¸ì œì˜ í’€ì´ë¥¼ ë©”ëª¨ì¥ì— ì ì–´ë‘ëŠ” ê²ƒ**ê³¼ ê°™ì•„ìš”!\n",
    "> \n",
    "> AIê°€ ê¸€ì„ ì“¸ ë•Œ, \"ë‚˜ëŠ” ì˜¤ëŠ˜ í•™êµì—...\"ê¹Œì§€ ì“°ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë ¤ë©´ ì• ë‹¨ì–´ë“¤ì˜ ì •ë³´(K,V)ê°€ í•„ìš”í•´ìš”.\n",
    "> **ë©”ëª¨ì¥ ì—†ìœ¼ë©´**: ë§¤ë²ˆ ì• ë‹¨ì–´ë“¤ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ê³„ì‚° â†’ ëŠë¦¼! ğŸ¢\n",
    "> **ë©”ëª¨ì¥ ìˆìœ¼ë©´**: ì´ì „ ê³„ì‚°ì„ ê¸°ì–µí•˜ê³  ìƒˆ ë‹¨ì–´ë§Œ ì¶”ê°€ ê³„ì‚° â†’ ë¹ ë¦„! ğŸš€\n",
    "\n",
    "| ìƒí™© | ë¹„ìœ  | ì†ë„ |\n",
    "|------|------|------|\n",
    "| Cache ì—†ìŒ | ë§¤ë²ˆ 1ë²ˆ ë¬¸ì œë¶€í„° ë‹¤ì‹œ í’€ê¸° | ğŸ¢ğŸ¢ğŸ¢ |\n",
    "| Cache ìˆìŒ | ë©”ëª¨ì¥ ë³´ê³  ë§ˆì§€ë§‰ ë¬¸ì œë§Œ í’€ê¸° | ğŸš€ |\n",
    "| ë©”ëª¨ì¥ì´ ê½‰ ì°¸ | ì˜¤ë˜ëœ ë©”ëª¨ ì§€ìš°ê¸° (Rolling Buffer) | ğŸš€ (ê³ ì • ë©”ëª¨ë¦¬) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: KV Cache ë©”ëª¨ë¦¬ ê³„ì‚°\n",
    "\n",
    "Llama 3 8B ($L=32, H_{kv}=8, d_{head}=128$)ì—ì„œ ë°°ì¹˜ í¬ê¸° $B=4$, ì‹œí€€ìŠ¤ ê¸¸ì´ $S=4096$ì¼ ë•Œ FP16 KV ìºì‹œ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$M_{KV} = 2 \\times 32 \\times 8 \\times 128 \\times 4096 \\times 4 \\times 2 \\text{ bytes}$$\n",
    "$$= 2 \\times 32 \\times 8 \\times 128 \\times 4096 \\times 4 \\times 2$$\n",
    "$$= 2,147,483,648 \\text{ bytes} = 2.0 \\text{ GB}$$\n",
    "\n",
    "ë°°ì¹˜ 1 ê¸°ì¤€ 512 MB Ã— ë°°ì¹˜ 4 = 2.0 GB\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: GQA vs MHA KV Cache ë¹„êµ\n",
    "\n",
    "ë™ì¼ ëª¨ë¸ì—ì„œ MHA($H_{kv}=32$)ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ KV CacheëŠ” ëª‡ GBì¸ê°€? GQA ëŒ€ë¹„ ëª‡ ë°°ì¸ê°€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$M_{MHA} = 2 \\times 32 \\times 32 \\times 128 \\times 4096 \\times 4 \\times 2 = 8.0 \\text{ GB}$$\n",
    "\n",
    "$$\\frac{M_{MHA}}{M_{GQA}} = \\frac{32}{8} = 4\\text{ë°°}$$\n",
    "\n",
    "GQA($H_{kv}=8$)ê°€ MHA($H_{kv}=32$) ëŒ€ë¹„ **KV ë©”ëª¨ë¦¬ 75% ì ˆê°!**\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoregressive ìƒì„±ê³¼ KV Cache <a name='2.-KV-Cache-ì›ë¦¬'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ KV Cache ìˆ/ì—†ì´ Autoregressive ìƒì„± ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê°„ë‹¨í•œ Attention ì—°ì‚°ì—ì„œ KV Cacheì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤\n",
    "\n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "d_head = d_model // n_heads\n",
    "seq_len = 64\n",
    "gen_len = 32  # ìƒì„±í•  í† í° ìˆ˜\n",
    "\n",
    "# ëœë¤ ê°€ì¤‘ì¹˜ (ë°ëª¨ìš©)\n",
    "Wq = tf.random.normal((d_model, d_model))\n",
    "Wk = tf.random.normal((d_model, d_model))\n",
    "Wv = tf.random.normal((d_model, d_model))\n",
    "\n",
    "# === ë°©ë²• 1: KV Cache ì—†ì´ (ë§¤ë²ˆ ì „ì²´ ì¬ê³„ì‚°) ===\n",
    "def generate_no_cache(prompt, gen_len):\n",
    "    tokens = tf.identity(prompt)  # [1, prompt_len, d]\n",
    "    flops_total = 0\n",
    "    for step in range(gen_len):\n",
    "        S = tokens.shape[1]\n",
    "        Q = tokens @ Wq  # ì „ì²´ ì‹œí€€ìŠ¤ Q\n",
    "        K = tokens @ Wk  # ì „ì²´ ì‹œí€€ìŠ¤ K (ë§¤ë²ˆ ì¬ê³„ì‚°!)\n",
    "        V = tokens @ Wv  # ì „ì²´ ì‹œí€€ìŠ¤ V (ë§¤ë²ˆ ì¬ê³„ì‚°!)\n",
    "        attn = tf.nn.softmax(Q @ tf.transpose(K, [0, 2, 1]) / tf.sqrt(float(d_model)))\n",
    "        out = attn @ V\n",
    "        new_token = out[:, -1:, :]  # ë§ˆì§€ë§‰ í† í°ì˜ ì¶œë ¥\n",
    "        tokens = tf.concat([tokens, new_token], axis=1)\n",
    "        flops_total += S * S * d_model * 2  # ëŒ€ëµì  FLOPs\n",
    "    return tokens, flops_total\n",
    "\n",
    "# === ë°©ë²• 2: KV Cache ì‚¬ìš© ===\n",
    "def generate_with_cache(prompt, gen_len):\n",
    "    B = prompt.shape[0]\n",
    "    # í”„ë¡¬í”„íŠ¸ K,V ë¯¸ë¦¬ ê³„ì‚° (Prefill)\n",
    "    k_cache = prompt @ Wk  # [1, prompt_len, d]\n",
    "    v_cache = prompt @ Wv\n",
    "    tokens = tf.identity(prompt)\n",
    "    flops_total = 0\n",
    "    for step in range(gen_len):\n",
    "        # ìƒˆ í† í°ì˜ Që§Œ ê³„ì‚°\n",
    "        q_new = tokens[:, -1:, :] @ Wq  # [1, 1, d]\n",
    "        k_new = tokens[:, -1:, :] @ Wk  # [1, 1, d]\n",
    "        v_new = tokens[:, -1:, :] @ Wv\n",
    "        # ìºì‹œì— ì¶”ê°€\n",
    "        k_cache = tf.concat([k_cache, k_new], axis=1)\n",
    "        v_cache = tf.concat([v_cache, v_new], axis=1)\n",
    "        S = k_cache.shape[1]\n",
    "        # Attention: ìƒˆ Q Ã— ì „ì²´ cached K\n",
    "        attn = tf.nn.softmax(q_new @ tf.transpose(k_cache, [0, 2, 1]) / tf.sqrt(float(d_model)))\n",
    "        out = attn @ v_cache  # [1, 1, d]\n",
    "        tokens = tf.concat([tokens, out], axis=1)\n",
    "        flops_total += S * d_model * 2  # SÃ—d (ìºì‹œ ë°©ì‹: S^2 â†’ S)\n",
    "    return tokens, flops_total\n",
    "\n",
    "prompt = tf.random.normal((1, seq_len, d_model))\n",
    "\n",
    "# ì›Œë°ì—…\n",
    "_, _ = generate_no_cache(prompt, 2)\n",
    "_, _ = generate_with_cache(prompt, 2)\n",
    "\n",
    "# ë²¤ì¹˜ë§ˆí¬\n",
    "start = time.perf_counter()\n",
    "_, flops_no = generate_no_cache(prompt, gen_len)\n",
    "time_no = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "_, flops_with = generate_with_cache(prompt, gen_len)\n",
    "time_with = time.perf_counter() - start\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Autoregressive ìƒì„± ë¹„êµ (í”„ë¡¬í”„íŠ¸={seq_len}, ìƒì„±={gen_len}í† í°)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'ë°©ë²•':<25} | {'ì‹œê°„ (ms)':>12} | {'ëŒ€ëµ FLOPs':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'KV Cache ì—†ìŒ':<25} | {time_no*1000:>12.1f} | {flops_no:>15,}\")\n",
    "print(f\"{'KV Cache ìˆìŒ':<25} | {time_with*1000:>12.1f} | {flops_with:>15,}\")\n",
    "print(f\"{'ì†ë„ í–¥ìƒ':<25} | {time_no/time_with:>11.1f}x | {flops_no/flops_with:>14.1f}x\")\n",
    "print()\n",
    "print(\"KV Cacheë¡œ ì—°ì‚°ëŸ‰ ëŒ€í­ ê°ì†Œ: ë§¤ ìŠ¤í… O(S^2) â†’ O(S)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Llama 3 8B KV Cache ë©”ëª¨ë¦¬ ê³„ì‚°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì‹¤ì œ Llama 3 8B íŒŒë¼ë¯¸í„°ë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì˜ ë©”ëª¨ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤\n",
    "\n",
    "def kv_cache_memory_bytes(n_layers, n_kv_heads, d_head, seq_len, batch_size, dtype_bytes=2):\n",
    "    return 2 * n_layers * n_kv_heads * d_head * seq_len * batch_size * dtype_bytes\n",
    "\n",
    "# Llama 3 8B íŒŒë¼ë¯¸í„°\n",
    "L, H_kv, d_h = 32, 8, 128\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Llama 3 8B KV Cache ë©”ëª¨ë¦¬ ê³„ì‚° (FP16)\")\n",
    "print(f\"  L={L}, H_kv={H_kv}, d_head={d_h}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "\n",
    "# í‘œ í—¤ë”\n",
    "header = f\"{'S \\\\ B':<8}\"\n",
    "for b in batch_sizes:\n",
    "    header += f\" | {'B='+str(b):>8}\"\n",
    "print(header)\n",
    "print(\"-\" * (8 + 11 * len(batch_sizes)))\n",
    "\n",
    "for s in seq_lengths:\n",
    "    row = f\"{s:<8}\"\n",
    "    for b in batch_sizes:\n",
    "        mem = kv_cache_memory_bytes(L, H_kv, d_h, s, b)\n",
    "        if mem >= 1e9:\n",
    "            row += f\" | {mem/1e9:>6.1f} GB\"\n",
    "        else:\n",
    "            row += f\" | {mem/1e6:>5.0f} MB \"\n",
    "        \n",
    "    print(row)\n",
    "\n",
    "print()\n",
    "print(\"âš ï¸ ì£¼ì˜: ëª¨ë¸ ê°€ì¤‘ì¹˜(~16GB FP16) + KV Cache + í™œì„±í™” ë©”ëª¨ë¦¬ í•©ê³„ê°€ GPU VRAMì„ ì´ˆê³¼í•˜ë©´ OOM!\")\n",
    "print()\n",
    "\n",
    "# 80GB A100 ì˜ˆì‹œ\n",
    "model_weights_gb = 16  # Llama 3 8B FP16\n",
    "vram_gb = 80\n",
    "available_kv = vram_gb - model_weights_gb - 5  # 5GB ì—¬ìœ \n",
    "\n",
    "print(f\"A100 80GB ê¸°ì¤€ ê°€ìš© KV ë©”ëª¨ë¦¬: ~{available_kv} GB\")\n",
    "max_batch_4096 = int(available_kv * 1e9 / kv_cache_memory_bytes(L, H_kv, d_h, 4096, 1))\n",
    "print(f\"  S=4096ì—ì„œ ìµœëŒ€ ë°°ì¹˜: ~{max_batch_4096}\")\n",
    "max_batch_8192 = int(available_kv * 1e9 / kv_cache_memory_bytes(L, H_kv, d_h, 8192, 1))\n",
    "print(f\"  S=8192ì—ì„œ ìµœëŒ€ ë°°ì¹˜: ~{max_batch_8192}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Rolling Buffer (Sliding Window) <a name='3.-Rolling-Buffer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Rolling Buffer KV Cache êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê³ ì • í¬ê¸° ë²„í¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ì œí•œí•˜ë©´ì„œ ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤\n",
    "\n",
    "class RollingKVCache:\n",
    "    # ê³ ì • í¬ê¸° Rolling Buffer KV Cache (Mistral ìŠ¤íƒ€ì¼)\n",
    "    def __init__(self, max_size, d_model, n_layers=1):\n",
    "        self.max_size = max_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        # [n_layers, 2(K,V), max_size, d_model]\n",
    "        self.buffer = np.zeros((n_layers, 2, max_size, d_model))\n",
    "        self.write_pos = 0  # ë‹¤ìŒ ì“°ê¸° ìœ„ì¹˜ (circular)\n",
    "        self.length = 0     # í˜„ì¬ ì €ì¥ëœ í† í° ìˆ˜\n",
    "\n",
    "    def update(self, k_new, v_new, layer=0):\n",
    "        # k_new, v_new: [1, d_model]\n",
    "        pos = self.write_pos % self.max_size  # ìˆœí™˜ ìœ„ì¹˜\n",
    "        self.buffer[layer, 0, pos] = k_new\n",
    "        self.buffer[layer, 1, pos] = v_new\n",
    "        self.write_pos += 1\n",
    "        self.length = min(self.length + 1, self.max_size)\n",
    "        return pos\n",
    "\n",
    "    def get_kv(self, layer=0):\n",
    "        # í˜„ì¬ ì €ì¥ëœ K,V ë°˜í™˜ (ìˆœì„œ ë³´ì •)\n",
    "        if self.length < self.max_size:\n",
    "            return self.buffer[layer, 0, :self.length], self.buffer[layer, 1, :self.length]\n",
    "        else:\n",
    "            # ìˆœí™˜ ë²„í¼: ê°€ì¥ ì˜¤ë˜ëœ ê²ƒë¶€í„° ìˆœì„œëŒ€ë¡œ\n",
    "            start = self.write_pos % self.max_size\n",
    "            indices = [(start + i) % self.max_size for i in range(self.max_size)]\n",
    "            return self.buffer[layer, 0, indices], self.buffer[layer, 1, indices]\n",
    "\n",
    "    def memory_bytes(self, dtype_bytes=2):\n",
    "        return self.buffer.nbytes  # ì‹¤ì œ ê³ ì • ë©”ëª¨ë¦¬\n",
    "\n",
    "\n",
    "# ì‹œë®¬ë ˆì´ì…˜: Rolling Buffer vs ë¬´ì œí•œ ìºì‹œ\n",
    "d = 128\n",
    "window_size = 64  # ìµœê·¼ 64 í† í°ë§Œ ìœ ì§€\n",
    "total_tokens = 200\n",
    "\n",
    "cache_rolling = RollingKVCache(max_size=window_size, d_model=d)\n",
    "rolling_memory = []\n",
    "unlimited_memory = []\n",
    "\n",
    "for t in range(total_tokens):\n",
    "    k_new = np.random.randn(d)\n",
    "    v_new = np.random.randn(d)\n",
    "    cache_rolling.update(k_new, v_new)\n",
    "    \n",
    "    rolling_memory.append(cache_rolling.length * d * 2 * 2)  # K+V, float16\n",
    "    unlimited_memory.append((t + 1) * d * 2 * 2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rolling Buffer KV Cache ì‹œë®¬ë ˆì´ì…˜ (window={window_size})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì´ ìƒì„± í† í°: {total_tokens}\")\n",
    "print(f\"Rolling ìµœì¢… ë©”ëª¨ë¦¬: {rolling_memory[-1]:,} bytes ({rolling_memory[-1]/1024:.1f} KB)\")\n",
    "print(f\"ë¬´ì œí•œ ìµœì¢… ë©”ëª¨ë¦¬: {unlimited_memory[-1]:,} bytes ({unlimited_memory[-1]/1024:.1f} KB)\")\n",
    "print(f\"ë©”ëª¨ë¦¬ ì ˆê°: {(1 - rolling_memory[-1]/unlimited_memory[-1])*100:.0f}%\")\n",
    "print()\n",
    "\n",
    "# ì €ì¥ëœ í† í° í™•ì¸\n",
    "k_stored, v_stored = cache_rolling.get_kv()\n",
    "print(f\"Rolling Buffer ì €ì¥ í† í° ìˆ˜: {len(k_stored)} (ìµœê·¼ {window_size}ê°œë§Œ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Rolling Buffer vs ë¬´ì œí•œ ìºì‹œ ë©”ëª¨ë¦¬ ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# ì™¼ìª½: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "tokens = np.arange(1, total_tokens + 1)\n",
    "ax1.plot(tokens, np.array(unlimited_memory) / 1024, 'r-', lw=2.5, label='ë¬´ì œí•œ ìºì‹œ')\n",
    "ax1.plot(tokens, np.array(rolling_memory) / 1024, 'b-', lw=2.5, label=f'Rolling Buffer (W={window_size})')\n",
    "ax1.axhline(y=window_size * d * 2 * 2 / 1024, color='blue', ls='--', lw=1.5, alpha=0.5)\n",
    "ax1.fill_between(tokens, np.array(rolling_memory) / 1024, np.array(unlimited_memory) / 1024,\n",
    "                  alpha=0.15, color='red', label='ì ˆì•½ëœ ë©”ëª¨ë¦¬')\n",
    "ax1.set_xlabel('ìƒì„±ëœ í† í° ìˆ˜', fontsize=11)\n",
    "ax1.set_ylabel('KV Cache ë©”ëª¨ë¦¬ (KB)', fontsize=11)\n",
    "ax1.set_title('KV Cache ë©”ëª¨ë¦¬ ì¦ê°€ íŒ¨í„´', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: Rolling Buffer ë™ì‘ ì›ë¦¬ (circular)\n",
    "ax2 = axes[1]\n",
    "n_slots = 8  # ì‹œê°í™”ìš© ì‘ì€ ë²„í¼\n",
    "positions = np.arange(n_slots)\n",
    "current_write = 5  # í˜„ì¬ ì“°ê¸° ìœ„ì¹˜\n",
    "\n",
    "colors_slot = ['#43A047' if i < current_write else '#E0E0E0' for i in range(n_slots)]\n",
    "colors_slot[current_write % n_slots] = '#E53935'  # ë‹¤ìŒ ì“°ê¸° ìœ„ì¹˜\n",
    "\n",
    "bars = ax2.bar(positions, [1]*n_slots, color=colors_slot, edgecolor='black', lw=1.5)\n",
    "\n",
    "for i in range(n_slots):\n",
    "    if i < current_write:\n",
    "        age = current_write - i\n",
    "        ax2.text(i, 0.5, f't-{age}', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "    elif i == current_write:\n",
    "        ax2.text(i, 0.5, 'â†’next', ha='center', va='center', fontsize=8, color='white', fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('ë²„í¼ ìŠ¬ë¡¯', fontsize=11)\n",
    "ax2.set_title('Rolling Buffer ìˆœí™˜ êµ¬ì¡°', fontweight='bold')\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xticks(positions)\n",
    "\n",
    "# ë²”ë¡€ íŒ¨ì¹˜\n",
    "import matplotlib.patches as mpatches\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor='#43A047', label='ìºì‹œëœ í† í°'),\n",
    "    mpatches.Patch(facecolor='#E53935', label='ë‹¤ìŒ ì“°ê¸° ìœ„ì¹˜'),\n",
    "    mpatches.Patch(facecolor='#E0E0E0', label='ë¹ˆ ìŠ¬ë¡¯'),\n",
    "]\n",
    "ax2.legend(handles=legend_elements, fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter12_modern_llms/kv_cache_rolling_buffer.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/kv_cache_rolling_buffer.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Turn ëŒ€í™” ë©”ëª¨ë¦¬ ë¶„ì„ <a name='4.-Multi-Turn-ë¶„ì„'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Multi-Turn ëŒ€í™”ì˜ KV Cache ë©”ëª¨ë¦¬ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì‹¤ì œ ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í„´ë§ˆë‹¤ KV Cacheê°€ ì–´ë–»ê²Œ ì¦ê°€í•˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤\n",
    "\n",
    "# Llama 3 8B ê¸°ì¤€\n",
    "L, H_kv, d_h = 32, 8, 128\n",
    "\n",
    "def kv_memory_gb(seq_len, batch=1, dtype_bytes=2):\n",
    "    return 2 * L * H_kv * d_h * seq_len * batch * dtype_bytes / 1e9\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤: 5í„´ ëŒ€í™”\n",
    "turns = [\n",
    "    (\"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\", 200),\n",
    "    (\"ì‚¬ìš©ì ì§ˆë¬¸ 1\", 50),\n",
    "    (\"AI ì‘ë‹µ 1\", 300),\n",
    "    (\"ì‚¬ìš©ì ì§ˆë¬¸ 2\", 80),\n",
    "    (\"AI ì‘ë‹µ 2\", 500),\n",
    "    (\"ì‚¬ìš©ì ì§ˆë¬¸ 3\", 60),\n",
    "    (\"AI ì‘ë‹µ 3\", 800),\n",
    "]\n",
    "\n",
    "cumulative_tokens = 0\n",
    "turn_data = []\n",
    "print(\"=\" * 70)\n",
    "print(\"Multi-Turn ëŒ€í™” KV Cache ë©”ëª¨ë¦¬ ë¶„ì„ (Llama 3 8B, FP16, B=1)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'í„´':<25} | {'í† í°':>6} | {'ëˆ„ì ':>6} | {'KV ë©”ëª¨ë¦¬':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, tokens in turns:\n",
    "    cumulative_tokens += tokens\n",
    "    mem = kv_memory_gb(cumulative_tokens)\n",
    "    turn_data.append((name, tokens, cumulative_tokens, mem))\n",
    "    print(f\"{name:<25} | {tokens:>6} | {cumulative_tokens:>6} | {mem*1000:>8.1f} MB\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'ì´ê³„':<25} | {cumulative_tokens:>6} |        | {kv_memory_gb(cumulative_tokens)*1000:>8.1f} MB\")\n",
    "print()\n",
    "print(\"ë°°ì¹˜ í¬ê¸°ë³„ ìµœì¢… ë©”ëª¨ë¦¬:\")\n",
    "for b in [1, 4, 8, 16]:\n",
    "    mem = kv_memory_gb(cumulative_tokens, batch=b)\n",
    "    print(f\"  B={b:>2}: {mem:.3f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Multi-Turn KV Cache ë©”ëª¨ë¦¬ ì¦ê°€ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "names = [d[0] for d in turn_data]\n",
    "cumulative = [d[2] for d in turn_data]\n",
    "memories = [d[3] * 1000 for d in turn_data]  # MB\n",
    "tokens_per_turn = [d[1] for d in turn_data]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# ì™¼ìª½: ëˆ„ì  KV Cache ë©”ëª¨ë¦¬\n",
    "ax1 = axes[0]\n",
    "colors_turn = ['#1565C0', '#43A047', '#E53935', '#43A047', '#E53935', '#43A047', '#E53935']\n",
    "labels_turn = ['System', 'User', 'AI', 'User', 'AI', 'User', 'AI']\n",
    "\n",
    "ax1.bar(range(len(turn_data)), memories, color=colors_turn, edgecolor='black', lw=1)\n",
    "ax1.set_xticks(range(len(turn_data)))\n",
    "ax1.set_xticklabels([f'Turn {i}' for i in range(len(turn_data))], fontsize=8, rotation=30)\n",
    "ax1.set_ylabel('KV Cache ë©”ëª¨ë¦¬ (MB)', fontsize=11)\n",
    "ax1.set_title('í„´ë³„ ëˆ„ì  KV Cache ë©”ëª¨ë¦¬', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, (m, l) in enumerate(zip(memories, labels_turn)):\n",
    "    ax1.text(i, m + 2, f'{m:.0f}MB\\n({l})', ha='center', fontsize=7)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: ë°°ì¹˜ë³„ ë©”ëª¨ë¦¬ (ìµœì¢… ìƒíƒœ)\n",
    "ax2 = axes[1]\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "final_seq = cumulative[-1]\n",
    "batch_memories = [kv_memory_gb(final_seq, b) for b in batch_sizes]\n",
    "\n",
    "ax2.plot(batch_sizes, batch_memories, 'r-o', lw=2.5, ms=8)\n",
    "ax2.axhline(y=80, color='gray', ls='--', lw=1.5, label='A100 80GB VRAM')\n",
    "ax2.axhline(y=24, color='orange', ls='--', lw=1.5, label='RTX 4090 24GB VRAM')\n",
    "ax2.fill_between(batch_sizes, batch_memories, 0, alpha=0.1, color='red')\n",
    "ax2.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=11)\n",
    "ax2.set_ylabel('KV Cache ë©”ëª¨ë¦¬ (GB)', fontsize=11)\n",
    "ax2.set_title(f'ë°°ì¹˜ë³„ KV Cache (S={final_seq})', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter12_modern_llms/kv_cache_multiturn.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/kv_cache_multiturn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prefix Caching ì‹œë®¬ë ˆì´ì…˜ <a name='5.-Prefix-Caching'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Prefix Caching ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë™ì¼í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê³µìœ í•˜ëŠ” ë‹¤ìˆ˜ì˜ ìš”ì²­ì—ì„œ KV Cacheë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤: ë™ì¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(500 í† í°)ë¡œ 100ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬\n",
    "system_prompt_tokens = 500\n",
    "user_query_avg_tokens = 100\n",
    "num_requests = 100\n",
    "\n",
    "# Llama 3 8B ê¸°ì¤€\n",
    "def kv_prefill_flops(seq_len, d_model=4096, n_layers=32):\n",
    "    # ëŒ€ëµì : ê° ë ˆì´ì–´ì—ì„œ QKV projection + attention + FFN\n",
    "    return n_layers * (3 * seq_len * d_model**2 + 2 * seq_len**2 * d_model)\n",
    "\n",
    "# === ë°©ë²• 1: Prefix Caching ì—†ìŒ (ë§¤ë²ˆ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¬ê³„ì‚°) ===\n",
    "flops_no_prefix = 0\n",
    "for _ in range(num_requests):\n",
    "    total_seq = system_prompt_tokens + user_query_avg_tokens\n",
    "    flops_no_prefix += kv_prefill_flops(total_seq)\n",
    "\n",
    "# === ë°©ë²• 2: Prefix Caching (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ 1íšŒë§Œ ê³„ì‚°) ===\n",
    "flops_prefix = kv_prefill_flops(system_prompt_tokens)  # 1íšŒ\n",
    "for _ in range(num_requests):\n",
    "    # ì‚¬ìš©ì ì¿¼ë¦¬ë§Œ ì²˜ë¦¬ (ê¸°ì¡´ prefix ìºì‹œ ì¬ì‚¬ìš©)\n",
    "    flops_prefix += kv_prefill_flops(user_query_avg_tokens)\n",
    "\n",
    "# KV ë©”ëª¨ë¦¬ë„ ë¹„êµ\n",
    "kv_per_request = kv_memory_gb(system_prompt_tokens + user_query_avg_tokens)\n",
    "kv_shared_prefix = kv_memory_gb(system_prompt_tokens)  # ê³µìœ  ë¶€ë¶„\n",
    "kv_per_query = kv_memory_gb(user_query_avg_tokens)\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(f\"Prefix Caching íš¨ê³¼ ë¶„ì„ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ {system_prompt_tokens}í† í°, {num_requests}ê°œ ìš”ì²­)\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'í•­ëª©':<30} | {'ìºì‹± ì—†ìŒ':>15} | {'Prefix ìºì‹±':>15}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'ì´ Prefill FLOPs':<30} | {flops_no_prefix:>15.2e} | {flops_prefix:>15.2e}\")\n",
    "print(f\"{'FLOPs ì ˆê°':<30} | {'-':>15} | {(1-flops_prefix/flops_no_prefix)*100:>13.1f}%\")\n",
    "print(f\"{'ìš”ì²­ë‹¹ ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸ ì¬ê³„ì‚°':<30} | {'ë§¤ë²ˆ':>15} | {'1íšŒë§Œ':>15}\")\n",
    "print()\n",
    "print(\"Prefix Caching í•µì‹¬ ì›ë¦¬:\")\n",
    "print(\"  1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ KVë¥¼ í•œ ë²ˆ ê³„ì‚° í›„ GPU ë©”ëª¨ë¦¬ì— ìœ ì§€\")\n",
    "print(\"  2. ìƒˆ ì‚¬ìš©ì ì¿¼ë¦¬ëŠ” prefix KVë¥¼ ë³µì‚¬(COW) í›„ ì´ì–´ì„œ ê³„ì‚°\")\n",
    "print(\"  3. vLLM, SGLang ë“± ì„œë¹™ í”„ë ˆì„ì›Œí¬ì—ì„œ ìë™ ì§€ì›\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| KV Cache | ì´ì „ í† í°ì˜ K,Vë¥¼ ì €ì¥í•˜ì—¬ ì¬ê³„ì‚° ë°©ì§€ â†’ ìƒì„± ì†ë„ $O(S^2) \\to O(S)$ | â­â­â­ |\n",
    "| KV ë©”ëª¨ë¦¬ ê³µì‹ | $M_{KV} = 2 \\cdot L \\cdot H_{kv} \\cdot d_{head} \\cdot S \\cdot B \\cdot \\text{bytes}$ | â­â­â­ |\n",
    "| Rolling Buffer | ê³ ì • í¬ê¸° ìˆœí™˜ ë²„í¼ë¡œ ë©”ëª¨ë¦¬ ìƒí•œ ì„¤ì • â†’ Mistral ìŠ¤íƒ€ì¼ | â­â­ |\n",
    "| Prefix Caching | ê³µí†µ ì ‘ë‘ì‚¬(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)ì˜ KVë¥¼ ê³µìœ  â†’ ì„œë¹™ ì²˜ë¦¬ëŸ‰ í–¥ìƒ | â­â­â­ |\n",
    "| GQA + KV Cache | $H_{kv} \\ll H_Q$ë¡œ ìºì‹œ ë©”ëª¨ë¦¬ë¥¼ $G/H$ë°°ë¡œ ì¤„ì„ | â­â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$M_{KV} = 2 \\times L \\times H_{kv} \\times d_{head} \\times S \\times B \\times \\text{bytes}$$\n",
    "\n",
    "Llama 3 8B ($L=32, H_{kv}=8, d_{head}=128$): $S=4096, B=1$ â†’ **512 MB**\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 12-03: Rotary Position Embedding (RoPE)** â€” ë³µì†Œìˆ˜ í‰ë©´ì—ì„œì˜ ìœ„ì¹˜ ì¸ì½”ë”© ìˆ˜ì‹ ë„ì¶œ, ì¥ê±°ë¦¬ ì˜ì¡´ì„± ë³´ì¡´, YaRNì„ ì´ìš©í•œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}