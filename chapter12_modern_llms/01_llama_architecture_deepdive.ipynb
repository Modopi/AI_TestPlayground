{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: ìµœì‹  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ ì•„í‚¤í…ì²˜ â€” Llama ì‹¬ì¸µ ë¶„ì„\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- Llama 3 ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ(RMSNorm, SwiGLU, GQA)ì˜ **ìˆ˜í•™ì  ì›ë¦¬**ë¥¼ ì´í•´í•œë‹¤\n",
    "- RMSNormì´ LayerNorm ëŒ€ë¹„ ê°–ëŠ” **ì—°ì‚° íš¨ìœ¨ ì´ì **ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•˜ê³  êµ¬í˜„í•œë‹¤\n",
    "- SwiGLU í™œì„±í™” í•¨ìˆ˜ì˜ **ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜**ì„ êµ¬í˜„í•˜ê³  GELU ëŒ€ë¹„ ì„±ëŠ¥ì„ ë¹„êµí•œë‹¤\n",
    "- MHA â†’ MQA â†’ GQAì˜ ë°œì „ ê³¼ì •ì„ ì´í•´í•˜ê³ , **KV í—¤ë“œ ìˆ˜ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ì ˆê°ë¥ **ì„ ê³„ì‚°í•œë‹¤\n",
    "- RMSNorm + SwiGLU + GQAë¥¼ ê²°í•©í•œ **ì†Œí˜• Llama Block**ì„ ë°‘ë°”ë‹¥ë¶€í„° êµ¬í˜„í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: RMSNorm, SwiGLU, GQA](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [RMSNorm vs LayerNorm ë¹„êµ êµ¬í˜„](#2.-RMSNorm-vs-LayerNorm)\n",
    "3. [SwiGLU FFN êµ¬í˜„](#3.-SwiGLU-FFN)\n",
    "4. [MHA â†’ MQA â†’ GQA ë°œì „ê³¼ êµ¬í˜„](#4.-GQA-êµ¬í˜„)\n",
    "5. [ì†Œí˜• Llama Block ì¡°ë¦½](#5.-Llama-Block)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### RMSNorm (Root Mean Square Layer Normalization)\n",
    "\n",
    "ê¸°ì¡´ LayerNormì€ í‰ê· ê³¼ ë¶„ì‚°ì„ ëª¨ë‘ ê³„ì‚°í•˜ì§€ë§Œ, RMSNormì€ **í‰ê·  ì œê±° ì—†ì´** RMS(ì œê³±í‰ê· ì œê³±ê·¼)ë§Œìœ¼ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{RMSNorm}(a_i) = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} \\cdot g_i, \\quad \\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n}\\sum_{j=1}^{n} a_j^2 + \\epsilon}$$\n",
    "\n",
    "- $a_i$: ì…ë ¥ ë²¡í„°ì˜ $i$ë²ˆì§¸ ì›ì†Œ\n",
    "- $g_i$: í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° (gain)\n",
    "- $n$: íˆë“  ì°¨ì› í¬ê¸°\n",
    "- $\\epsilon$: ìˆ˜ì¹˜ ì•ˆì •í™” ìƒìˆ˜ (ë³´í†µ $10^{-6}$)\n",
    "\n",
    "**LayerNormê³¼ ë¹„êµ:**\n",
    "\n",
    "| êµ¬ë¶„ | LayerNorm | RMSNorm |\n",
    "|------|-----------|---------|\n",
    "| ìˆ˜ì‹ | $\\frac{a_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma_i + \\beta_i$ | $\\frac{a_i}{\\text{RMS}(\\mathbf{a})} \\cdot g_i$ |\n",
    "| íŒŒë¼ë¯¸í„° | $\\gamma, \\beta$ (2nê°œ) | $g$ (nê°œ) |\n",
    "| ì—°ì‚° | í‰ê·  + ë¶„ì‚° (2-pass) | RMSë§Œ (1-pass) |\n",
    "| FLOPs | $\\sim 5n$ | $\\sim 3n$ |\n",
    "\n",
    "### SwiGLU (Swish-Gated Linear Unit)\n",
    "\n",
    "Llamaì˜ FFNì€ í‘œì¤€ 2-layer MLP ëŒ€ì‹  **ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{Swish}_\\beta(xW_1) \\otimes (xW_2)$$\n",
    "\n",
    "$$\\text{Swish}_\\beta(x) = x \\cdot \\sigma(\\beta x), \\quad \\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- $x \\in \\mathbb{R}^{d_{model}}$: ì…ë ¥\n",
    "- $W_1, W_2 \\in \\mathbb{R}^{d_{model} \\times d_{ff}}$: ê²Œì´íŠ¸/ê°’ í”„ë¡œì ì…˜ ($\\beta=1$ for Llama)\n",
    "- $W_3 \\in \\mathbb{R}^{d_{ff} \\times d_{model}}$: ì¶œë ¥ í”„ë¡œì ì…˜\n",
    "- $\\otimes$: ì›ì†Œë³„ ê³± (element-wise product)\n",
    "- $d_{ff}$: Llama 3 8Bì—ì„œ 14,336 ($= \\frac{8}{3} \\times d_{model}$ì˜ 256 ë°°ìˆ˜ ì˜¬ë¦¼)\n",
    "\n",
    "**ìµœì¢… FFN ì¶œë ¥:**\n",
    "\n",
    "$$\\text{FFN}(x) = \\text{SwiGLU}(x) \\cdot W_3 = [\\text{Swish}(xW_1) \\otimes (xW_2)] W_3$$\n",
    "\n",
    "### Grouped Query Attention (GQA)\n",
    "\n",
    "MHAì—ì„œ GQAë¡œì˜ ë°œì „ì€ **KV í—¤ë“œ ìˆ˜ë¥¼ ì¤„ì—¬** ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "| ë°©ì‹ | Q í—¤ë“œ ìˆ˜ | KV í—¤ë“œ ìˆ˜ | KV íŒŒë¼ë¯¸í„° ë¹„ìœ¨ |\n",
    "|------|----------|-----------|----------------|\n",
    "| MHA | $H$ | $H$ | $1.0$ |\n",
    "| MQA | $H$ | $1$ | $1/H$ |\n",
    "| GQA | $H$ | $G$ | $G/H$ |\n",
    "\n",
    "Llama 3 8B: $H = 32$, $G = 8$ â†’ KV íŒŒë¼ë¯¸í„°ê°€ MHA ëŒ€ë¹„ $8/32 = 25\\%$ë¡œ ì ˆê°\n",
    "\n",
    "$$\\text{KV ë©”ëª¨ë¦¬ ì ˆê°ë¥ } = 1 - \\frac{G}{H} = 1 - \\frac{n_{kv}}{n_q}$$\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ì„± ìš”ì†Œ | ìˆ˜ì‹ | Llama 3 8B ê°’ |\n",
    "|-----------|------|--------------|\n",
    "| RMSNorm | $a_i / \\text{RMS}(\\mathbf{a}) \\cdot g_i$ | $\\epsilon = 10^{-5}$ |\n",
    "| SwiGLU | $\\text{Swish}(xW_1) \\otimes (xW_2)$ | $d_{ff} = 14336$ |\n",
    "| GQA | $H_Q=32, H_{KV}=8$ | 75% KV ì ˆê° |\n",
    "| íˆë“  ì°¨ì› | $d_{model}$ | 4096 |\n",
    "| ë ˆì´ì–´ ìˆ˜ | $L$ | 32 |\n",
    "| ì–´íœ˜ í¬ê¸° | $V$ | 128,000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Llama ì•„í‚¤í…ì²˜ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ RMSNormì´ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ë°˜ ì•„ì´ë“¤ì˜ í‚¤ë¥¼ ë¹„êµí•  ë•Œ, **í‰ê·  í‚¤ë¥¼ ë¹¼ê³  ë‚˜ëˆ„ëŠ” ë°©ë²•**(LayerNorm)ê³¼ **ê·¸ëƒ¥ í‚¤ì˜ í¬ê¸°ë¡œë§Œ ë‚˜ëˆ„ëŠ” ë°©ë²•**(RMSNorm)ì´ ìˆì–´ìš”. ë‘ ë²ˆì§¸ ë°©ë²•ì´ ê³„ì‚°ì´ ë” ë¹¨ë¼ìš”!\n",
    "\n",
    "RMSNormì€ ìˆ«ìë“¤ì„ \"ì ë‹¹í•œ í¬ê¸°\"ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ë„êµ¬ì˜ˆìš”. ìˆ«ìê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ AIê°€ í•™ìŠµí•˜ê¸° ì–´ë ¤ìš´ë°, RMSNormì´ ë”± ë§ê²Œ ì¡°ì ˆí•´ì¤ë‹ˆë‹¤.\n",
    "\n",
    "#### ğŸšª SwiGLUëŠ” ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: **ë¬¸ ë‘ ê°œê°€ ë‹¬ë¦° ë³µë„**ë¥¼ ìƒìƒí•´ë³´ì„¸ìš”. ì²« ë²ˆì§¸ ë¬¸(Swish)ì€ \"ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€\" ì ìˆ˜ë¥¼ ë§¤ê¸°ê³ , ë‘ ë²ˆì§¸ ë¬¸ì€ \"ì‹¤ì œ ì •ë³´\"ë¥¼ í†µê³¼ì‹œì¼œìš”. ë‘ ë¬¸ì˜ ê²°ê³¼ë¥¼ í•©ì³ì„œ ì •ë§ ì¤‘ìš”í•œ ì •ë³´ë§Œ í†µê³¼í•©ë‹ˆë‹¤!\n",
    "\n",
    "#### ğŸ‘¥ GQAëŠ” ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ì„ ë³¼ ë•Œ, **ì§ˆë¬¸ì§€(Q)ëŠ” 32ëª…**ì´ ê°ì ë‹¤ë¥´ê²Œ ê°–ê³  ìˆì§€ë§Œ, **ë‹µì•ˆì§€(K,V)ëŠ” 8ê°œ ê·¸ë£¹ì´ ê³µìœ **í•´ìš”. ë‹µì•ˆì§€ë¥¼ ëœ ë§Œë“¤ì–´ë„ ë˜ë‹ˆê¹Œ ì¢…ì´(ë©”ëª¨ë¦¬)ê°€ ì ˆì•½ë©ë‹ˆë‹¤!\n",
    "\n",
    "| ë°©ì‹ | ë¹„ìœ  | ë©”ëª¨ë¦¬ |\n",
    "|------|------|--------|\n",
    "| MHA | í•™ìƒ 32ëª…ì´ ê°ì ë‹µì•ˆì§€ 32ì¥ | ğŸ’¸ğŸ’¸ğŸ’¸ |\n",
    "| MQA | í•™ìƒ 32ëª…ì´ ë‹µì•ˆì§€ 1ì¥ ê³µìœ  | ğŸ’¸ (ë„ˆë¬´ ì ì–´ í’ˆì§ˆâ†“) |\n",
    "| GQA | í•™ìƒ 32ëª…ì´ 8ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ  8ì¥ ê³µìœ  | ğŸ’¸ğŸ’¸ (ì ì ˆ!) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: RMSNorm ìˆ˜ë™ ê³„ì‚°\n",
    "\n",
    "ì…ë ¥ ë²¡í„° $\\mathbf{a} = [3, 4, 0]$, ê²Œì¸ $\\mathbf{g} = [1, 1, 1]$, $\\epsilon = 0$ì¼ ë•Œ RMSNormì˜ ì¶œë ¥ì„ êµ¬í•˜ì‹œì˜¤.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{3^2 + 4^2 + 0^2}{3}} = \\sqrt{\\frac{25}{3}} \\approx 2.887$$\n",
    "\n",
    "$$\\text{RMSNorm}(\\mathbf{a}) = \\left[\\frac{3}{2.887}, \\frac{4}{2.887}, \\frac{0}{2.887}\\right] \\approx [1.039, 1.386, 0]$$\n",
    "\n",
    "ë²¡í„°ì˜ \"í¬ê¸°\"ë§Œìœ¼ë¡œ ì •ê·œí™”ë˜ì–´, ì›ë˜ ë°©í–¥ì€ ìœ ì§€í•˜ë©´ì„œ ìŠ¤ì¼€ì¼ë§Œ ì¡°ì ˆë¨.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: GQA ë©”ëª¨ë¦¬ ì ˆê°ë¥  ê³„ì‚°\n",
    "\n",
    "Llama 3 70BëŠ” $H_Q = 64$, $H_{KV} = 8$ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. MHA ëŒ€ë¹„ GQAì˜ KV íŒŒë¼ë¯¸í„° ì ˆê°ë¥ ê³¼, ì‹œí€€ìŠ¤ ê¸¸ì´ $S = 4096$ì¼ ë•Œ FP16 KV ìºì‹œ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤. ($L=80, d_{head}=128$)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "**ì ˆê°ë¥ **: $1 - G/H = 1 - 8/64 = 87.5\\%$\n",
    "\n",
    "**KV ìºì‹œ í¬ê¸°**:\n",
    "$$M_{KV} = 2 \\times L \\times H_{KV} \\times d_{head} \\times S \\times 2\\text{B}$$\n",
    "$$= 2 \\times 80 \\times 8 \\times 128 \\times 4096 \\times 2 = 2 \\times 80 \\times 8 \\times 128 \\times 4096 \\times 2$$\n",
    "$$= 1,073,741,824 \\text{ bytes} = 1.07 \\text{ GB (ë°°ì¹˜ 1 ê¸°ì¤€)}$$\n",
    "\n",
    "MHAì˜€ë‹¤ë©´: $2 \\times 80 \\times 64 \\times 128 \\times 4096 \\times 2 = 8.59$ GB â†’ GQAë¡œ **7.52 GB ì ˆì•½!**\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # í—¤ë“œë¦¬ìŠ¤ í™˜ê²½ í•„ìˆ˜\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RMSNorm vs LayerNorm ë¹„êµ êµ¬í˜„ <a name='2.-RMSNorm-vs-LayerNorm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RMSNorm vs LayerNorm êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RMSNormê³¼ LayerNormì„ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ìˆ˜ì¹˜ì  ì°¨ì´ì™€ ì†ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤\n",
    "\n",
    "class RMSNorm(tf.keras.layers.Layer):\n",
    "    # Root Mean Square Layer Normalization\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.g = self.add_weight(name='gain', shape=(dim,),\n",
    "                                 initializer='ones', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        # RMS ê³„ì‚°: sqrt(mean(x^2) + eps)\n",
    "        rms = tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True) + self.eps)\n",
    "        return (x / rms) * self.g\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°\n",
    "d_model = 4096\n",
    "batch_seq = (2, 128)  # (batch, seq_len)\n",
    "x = tf.random.normal((*batch_seq, d_model))\n",
    "\n",
    "# RMSNorm\n",
    "rms_norm = RMSNorm(d_model)\n",
    "rms_out = rms_norm(x)\n",
    "\n",
    "# LayerNorm (TF ë‚´ì¥)\n",
    "layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "ln_out = layer_norm(x)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"RMSNorm vs LayerNorm ìˆ˜ì¹˜ ë¹„êµ\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"ì…ë ¥ shape: {x.shape}\")\n",
    "print(f\"ì…ë ¥ í‰ê· : {tf.reduce_mean(x).numpy():.4f}\")\n",
    "print(f\"ì…ë ¥ í‘œì¤€í¸ì°¨: {tf.math.reduce_std(x).numpy():.4f}\")\n",
    "print()\n",
    "print(f\"RMSNorm ì¶œë ¥ í‰ê· : {tf.reduce_mean(rms_out).numpy():.6f}\")\n",
    "print(f\"RMSNorm ì¶œë ¥ std:  {tf.math.reduce_std(rms_out).numpy():.6f}\")\n",
    "print(f\"LayerNorm ì¶œë ¥ í‰ê· : {tf.reduce_mean(ln_out).numpy():.6f}\")\n",
    "print(f\"LayerNorm ì¶œë ¥ std:  {tf.math.reduce_std(ln_out).numpy():.6f}\")\n",
    "print()\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "rms_params = sum(tf.size(v).numpy() for v in rms_norm.trainable_variables)\n",
    "ln_params = sum(tf.size(v).numpy() for v in layer_norm.trainable_variables)\n",
    "print(f\"RMSNorm íŒŒë¼ë¯¸í„°: {rms_params:,} (gainë§Œ)\")\n",
    "print(f\"LayerNorm íŒŒë¼ë¯¸í„°: {ln_params:,} (gamma + beta)\")\n",
    "print(f\"íŒŒë¼ë¯¸í„° ì ˆê°: {(1 - rms_params/ln_params)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ RMSNorm vs LayerNorm ì†ë„ ë²¤ì¹˜ë§ˆí¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ í‰ê·  ì‹œê°„ ì¸¡ì •\n",
    "\n",
    "n_warmup = 10\n",
    "n_runs = 100\n",
    "\n",
    "# ì›Œë°ì—…\n",
    "for _ in range(n_warmup):\n",
    "    _ = rms_norm(x)\n",
    "    _ = layer_norm(x)\n",
    "\n",
    "# RMSNorm ë²¤ì¹˜ë§ˆí¬\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_runs):\n",
    "    _ = rms_norm(x)\n",
    "rms_time = (time.perf_counter() - start) / n_runs * 1000\n",
    "\n",
    "# LayerNorm ë²¤ì¹˜ë§ˆí¬\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_runs):\n",
    "    _ = layer_norm(x)\n",
    "ln_time = (time.perf_counter() - start) / n_runs * 1000\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"ì†ë„ ë²¤ì¹˜ë§ˆí¬ (CPU, d_model=4096, batch=2, seq=128)\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'ë°©ë²•':<20} | {'ì‹œê°„ (ms)':>12} | {'ìƒëŒ€ ì†ë„':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'RMSNorm':<20} | {rms_time:>12.3f} | {'ê¸°ì¤€':>10}\")\n",
    "print(f\"{'LayerNorm':<20} | {ln_time:>12.3f} | {ln_time/rms_time:>9.2f}x\")\n",
    "print()\n",
    "print(f\"RMSNormì´ LayerNorm ëŒ€ë¹„ ì•½ {(1-rms_time/ln_time)*100:.1f}% ë¹ ë¦„ (CPU ê¸°ì¤€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SwiGLU FFN êµ¬í˜„ <a name='3.-SwiGLU-FFN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SwiGLU FFN êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Llama 3 ìŠ¤íƒ€ì¼ì˜ SwiGLU FFNì„ êµ¬í˜„í•˜ê³ , í‘œì¤€ GELU FFNê³¼ ë¹„êµí•©ë‹ˆë‹¤\n",
    "\n",
    "class SwiGLUFFN(tf.keras.layers.Layer):\n",
    "    # SwiGLU Feed-Forward Network (Llama 3 style)\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        # W1: gate projection, W2: up projection, W3: down projection\n",
    "        self.w1 = tf.keras.layers.Dense(d_ff, use_bias=False, name='gate')\n",
    "        self.w2 = tf.keras.layers.Dense(d_ff, use_bias=False, name='up')\n",
    "        self.w3 = tf.keras.layers.Dense(d_model, use_bias=False, name='down')\n",
    "\n",
    "    def call(self, x):\n",
    "        # SwiGLU: Swish(x @ W1) * (x @ W2) @ W3\n",
    "        gate = tf.nn.silu(self.w1(x))    # Swish = SiLU\n",
    "        up = self.w2(x)\n",
    "        return self.w3(gate * up)         # element-wise product â†’ down projection\n",
    "\n",
    "\n",
    "class StandardFFN(tf.keras.layers.Layer):\n",
    "    # í‘œì¤€ GELU FFN (GPT-2 ìŠ¤íƒ€ì¼)\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, use_bias=False)\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.fc2(tf.nn.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "# Llama 3 8B ê¸°ì¤€ FFN ì°¨ì›\n",
    "d_model = 4096\n",
    "d_ff_swiglu = 14336   # Llama 3: 8/3 * 4096 â†’ 256ì˜ ë°°ìˆ˜ ì˜¬ë¦¼\n",
    "d_ff_standard = 11008  # ìœ ì‚¬ íŒŒë¼ë¯¸í„° ìˆ˜ì˜ í‘œì¤€ FFN\n",
    "\n",
    "swiglu_ffn = SwiGLUFFN(d_model, d_ff_swiglu)\n",
    "std_ffn = StandardFFN(d_model, d_ff_standard)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "x_test = tf.random.normal((1, 16, d_model))\n",
    "y_swiglu = swiglu_ffn(x_test)\n",
    "y_std = std_ffn(x_test)\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "swiglu_params = sum(tf.size(v).numpy() for v in swiglu_ffn.trainable_variables)\n",
    "std_params = sum(tf.size(v).numpy() for v in std_ffn.trainable_variables)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SwiGLU FFN vs í‘œì¤€ GELU FFN ë¹„êµ\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'í•­ëª©':<25} | {'SwiGLU':>15} | {'í‘œì¤€ GELU':>15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'d_ff':<25} | {d_ff_swiglu:>15,} | {d_ff_standard:>15,}\")\n",
    "print(f\"{'íŒŒë¼ë¯¸í„° ìˆ˜':<25} | {swiglu_params:>15,} | {std_params:>15,}\")\n",
    "print(f\"{'ì¶œë ¥ shape':<25} | {str(y_swiglu.shape):>15} | {str(y_std.shape):>15}\")\n",
    "print(f\"{'ê²Œì´íŒ… ë°©ì‹':<25} | {'Swish gate':>15} | {'ì—†ìŒ':>15}\")\n",
    "print()\n",
    "print(\"SwiGLUëŠ” ê²Œì´íŠ¸ í”„ë¡œì ì…˜(W1)ì„ ì¶”ê°€í•˜ì—¬ 3ê°œì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì‚¬ìš©\")\n",
    "print(f\"  W1(gate): {d_model}Ã—{d_ff_swiglu} = {d_model*d_ff_swiglu:,}\")\n",
    "print(f\"  W2(up):   {d_model}Ã—{d_ff_swiglu} = {d_model*d_ff_swiglu:,}\")\n",
    "print(f\"  W3(down): {d_ff_swiglu}Ã—{d_model} = {d_ff_swiglu*d_model:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SwiGLU í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Swish, GELU, ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ë¹„êµí•˜ì—¬ SwiGLUì˜ íŠ¹ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤\n",
    "\n",
    "x_range = np.linspace(-4, 4, 500)\n",
    "x_tf = tf.constant(x_range, dtype=tf.float32)\n",
    "\n",
    "activations = {\n",
    "    'Swish (SiLU)': tf.nn.silu(x_tf).numpy(),\n",
    "    'GELU': tf.nn.gelu(x_tf).numpy(),\n",
    "    'ReLU': tf.nn.relu(x_tf).numpy(),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# ì™¼ìª½: í™œì„±í™” í•¨ìˆ˜ ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "colors = ['#1E88E5', '#43A047', '#E53935']\n",
    "for (name, vals), c in zip(activations.items(), colors):\n",
    "    ax1.plot(x_range, vals, lw=2.5, label=name, color=c)\n",
    "ax1.axhline(y=0, color='gray', ls='--', lw=1)\n",
    "ax1.axvline(x=0, color='gray', ls='--', lw=1)\n",
    "ax1.set_xlabel('x', fontsize=11)\n",
    "ax1.set_ylabel('f(x)', fontsize=11)\n",
    "ax1.set_title('í™œì„±í™” í•¨ìˆ˜ ë¹„êµ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: SwiGLU ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”\n",
    "ax2 = axes[1]\n",
    "gate_signal = tf.nn.silu(x_tf).numpy()\n",
    "value_signal = np.tanh(x_range * 0.5)  # ì˜ˆì‹œ ê°’ ì‹ í˜¸\n",
    "swiglu_out = gate_signal * value_signal\n",
    "\n",
    "ax2.plot(x_range, gate_signal, 'b-', lw=2, label='Gate: Swish(xWâ‚)', alpha=0.7)\n",
    "ax2.plot(x_range, value_signal, 'g-', lw=2, label='Value: xWâ‚‚', alpha=0.7)\n",
    "ax2.plot(x_range, swiglu_out, 'r-', lw=2.5, label='SwiGLU: Gate âŠ— Value')\n",
    "ax2.axhline(y=0, color='gray', ls='--', lw=1)\n",
    "ax2.set_xlabel('x', fontsize=11)\n",
    "ax2.set_ylabel('ì¶œë ¥', fontsize=11)\n",
    "ax2.set_title('SwiGLU ê²Œì´íŒ… ë©”ì»¤ë‹ˆì¦˜', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter12_modern_llms/swiglu_activation.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/swiglu_activation.png\")\n",
    "print()\n",
    "print(\"í•µì‹¬ ê´€ì°°:\")\n",
    "print(\"  â€¢ SwishëŠ” ìŒìˆ˜ ì˜ì—­ì—ì„œ ì‘ì€ ìŒìˆ˜ê°’ì„ í—ˆìš© (ReLUì™€ ì°¨ì´)\")\n",
    "print(\"  â€¢ SwiGLUëŠ” Gate ì‹ í˜¸ë¡œ Value ì‹ í˜¸ë¥¼ 'í•„í„°ë§'í•˜ì—¬ ì¤‘ìš” ì •ë³´ë§Œ í†µê³¼\")\n",
    "print(\"  â€¢ ì´ ê²Œì´íŒ…ì´ í‘œì¤€ FFN ëŒ€ë¹„ í‘œí˜„ë ¥ì„ ë†’ì—¬ì¤Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MHA â†’ MQA â†’ GQA ë°œì „ê³¼ êµ¬í˜„ <a name='4.-GQA-êµ¬í˜„'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ MHA / MQA / GQA êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì„¸ ê°€ì§€ Attention ë°©ì‹ì„ êµ¬í˜„í•˜ì—¬ ë©”ëª¨ë¦¬ ë° íŒŒë¼ë¯¸í„° ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤\n",
    "\n",
    "class GroupedQueryAttention(tf.keras.layers.Layer):\n",
    "    # Grouped Query Attention (GQA) - MHA/MQAë¥¼ ëª¨ë‘ í¬ê´„\n",
    "    def __init__(self, d_model, n_q_heads, n_kv_heads):\n",
    "        super().__init__()\n",
    "        self.n_q_heads = n_q_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.d_head = d_model // n_q_heads\n",
    "        self.n_groups = n_q_heads // n_kv_heads  # Q í—¤ë“œë¥¼ KV ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°\n",
    "\n",
    "        # Q í”„ë¡œì ì…˜: ì „ì²´ Q í—¤ë“œ ìˆ˜ ì‚¬ìš©\n",
    "        self.wq = tf.keras.layers.Dense(n_q_heads * self.d_head, use_bias=False)\n",
    "        # K, V í”„ë¡œì ì…˜: KV í—¤ë“œ ìˆ˜ë§Œ ì‚¬ìš© (ì—¬ê¸°ê°€ í•µì‹¬!)\n",
    "        self.wk = tf.keras.layers.Dense(n_kv_heads * self.d_head, use_bias=False)\n",
    "        self.wv = tf.keras.layers.Dense(n_kv_heads * self.d_head, use_bias=False)\n",
    "        self.wo = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        B, S, _ = x.shape\n",
    "\n",
    "        # í”„ë¡œì ì…˜\n",
    "        q = tf.reshape(self.wq(x), (B, S, self.n_q_heads, self.d_head))\n",
    "        k = tf.reshape(self.wk(x), (B, S, self.n_kv_heads, self.d_head))\n",
    "        v = tf.reshape(self.wv(x), (B, S, self.n_kv_heads, self.d_head))\n",
    "\n",
    "        # KV í—¤ë“œë¥¼ Q ê·¸ë£¹ ìˆ˜ë§Œí¼ ë°˜ë³µ (repeat_kv)\n",
    "        # [B, S, n_kv, d] â†’ [B, S, n_kv, n_groups, d] â†’ [B, S, n_q, d]\n",
    "        k = tf.repeat(k, repeats=self.n_groups, axis=2)\n",
    "        v = tf.repeat(v, repeats=self.n_groups, axis=2)\n",
    "\n",
    "        # [B, n_heads, S, d_head] í˜•íƒœë¡œ ì „ì¹˜\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])  # [B, n_q, S, d]\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        scale = tf.math.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        attn = tf.matmul(q, k, transpose_b=True) / scale  # [B, n_q, S, S]\n",
    "\n",
    "        if mask is not None:\n",
    "            attn += mask\n",
    "\n",
    "        attn_weights = tf.nn.softmax(attn, axis=-1)\n",
    "        out = tf.matmul(attn_weights, v)  # [B, n_q, S, d]\n",
    "\n",
    "        # Concat heads\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, (B, S, self.n_q_heads * self.d_head))\n",
    "        return self.wo(out)\n",
    "\n",
    "\n",
    "# Llama 3 8B ìŠ¤ì¼€ì¼ (ì¶•ì†Œ ë²„ì „)\n",
    "d_model = 256  # ë°ëª¨ìš© ì¶•ì†Œ\n",
    "\n",
    "configs = {\n",
    "    'MHA (H=8, G=8)': (8, 8),\n",
    "    'MQA (H=8, G=1)': (8, 1),\n",
    "    'GQA (H=8, G=2)': (8, 2),  # Llama ìŠ¤íƒ€ì¼: G = H/4\n",
    "}\n",
    "\n",
    "x_test = tf.random.normal((1, 32, d_model))\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"MHA / MQA / GQA íŒŒë¼ë¯¸í„° ë° KV ë©”ëª¨ë¦¬ ë¹„êµ\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'ë°©ì‹':<20} | {'Q í—¤ë“œ':>7} | {'KV í—¤ë“œ':>7} | {'íŒŒë¼ë¯¸í„°':>12} | {'KV ë¹„ìœ¨':>8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "kv_sizes = {}\n",
    "for name, (n_q, n_kv) in configs.items():\n",
    "    attn = GroupedQueryAttention(d_model, n_q, n_kv)\n",
    "    _ = attn(x_test)  # ë¹Œë“œ\n",
    "    total_params = sum(tf.size(v).numpy() for v in attn.trainable_variables)\n",
    "    kv_ratio = n_kv / n_q\n",
    "    kv_sizes[name] = kv_ratio\n",
    "    print(f\"{name:<20} | {n_q:>7} | {n_kv:>7} | {total_params:>12,} | {kv_ratio:>7.0%}\")\n",
    "\n",
    "print()\n",
    "print(\"ğŸ’¡ GQAëŠ” MHAì˜ í’ˆì§ˆì„ ìœ ì§€í•˜ë©´ì„œ MQA ìˆ˜ì¤€ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë‹¬ì„±!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ MHA / MQA / GQA êµ¬ì¡° ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Q-K-V í—¤ë“œ ë§¤í•‘ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ë‹¤ì´ì–´ê·¸ë¨\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "configs_viz = [\n",
    "    ('MHA\\n(Multi-Head)', 8, 8),\n",
    "    ('MQA\\n(Multi-Query)', 8, 1),\n",
    "    ('GQA\\n(Grouped-Query)', 8, 2),\n",
    "]\n",
    "\n",
    "for ax, (title, n_q, n_kv) in zip(axes, configs_viz):\n",
    "    n_groups = n_q // n_kv\n",
    "\n",
    "    # Q í—¤ë“œ (ìƒë‹¨)\n",
    "    for i in range(n_q):\n",
    "        color_idx = i // n_groups\n",
    "        color = plt.cm.Set3(color_idx / max(n_kv, 1))\n",
    "        ax.add_patch(plt.Rectangle((i * 1.1, 2.5), 0.9, 0.8,\n",
    "                                    facecolor=color, edgecolor='black', lw=1.5))\n",
    "        ax.text(i * 1.1 + 0.45, 2.9, f'Q{i}', ha='center', va='center', fontsize=7)\n",
    "\n",
    "    # KV í—¤ë“œ (í•˜ë‹¨)\n",
    "    kv_width = (n_q * 1.1) / n_kv\n",
    "    for j in range(n_kv):\n",
    "        color = plt.cm.Set3(j / max(n_kv, 1))\n",
    "        # K\n",
    "        ax.add_patch(plt.Rectangle((j * kv_width, 1.2), kv_width - 0.2, 0.8,\n",
    "                                    facecolor=color, edgecolor='black', lw=1.5, alpha=0.7))\n",
    "        ax.text(j * kv_width + (kv_width - 0.2) / 2, 1.6, f'K{j}',\n",
    "                ha='center', va='center', fontsize=7)\n",
    "        # V\n",
    "        ax.add_patch(plt.Rectangle((j * kv_width, 0.0), kv_width - 0.2, 0.8,\n",
    "                                    facecolor=color, edgecolor='black', lw=1.5, alpha=0.5))\n",
    "        ax.text(j * kv_width + (kv_width - 0.2) / 2, 0.4, f'V{j}',\n",
    "                ha='center', va='center', fontsize=7)\n",
    "\n",
    "    # ì—°ê²°ì„ \n",
    "    for i in range(n_q):\n",
    "        kv_idx = i // n_groups\n",
    "        x_q = i * 1.1 + 0.45\n",
    "        x_kv = kv_idx * kv_width + (kv_width - 0.2) / 2\n",
    "        ax.plot([x_q, x_kv], [2.5, 2.0], 'k-', alpha=0.3, lw=0.8)\n",
    "\n",
    "    ax.set_xlim(-0.5, n_q * 1.1 + 0.5)\n",
    "    ax.set_ylim(-0.5, 4.0)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Q â†’ K,V ë§¤í•‘', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # KV ë©”ëª¨ë¦¬ ë¹„ìœ¨ í‘œì‹œ\n",
    "    ratio = n_kv / n_q * 100\n",
    "    ax.text(n_q * 1.1 / 2, -0.3, f'KV ë©”ëª¨ë¦¬: {ratio:.0f}%', ha='center',\n",
    "            fontsize=10, fontweight='bold', color='#D32F2F')\n",
    "\n",
    "plt.suptitle('Attention ë°©ì‹ë³„ Q-KV í—¤ë“œ ë§¤í•‘ ë¹„êµ', fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter12_modern_llms/gqa_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter12_modern_llms/gqa_comparison.png\")\n",
    "print()\n",
    "print(\"GQA (Llama 3 ìŠ¤íƒ€ì¼):\")\n",
    "print(f\"  â€¢ Q í—¤ë“œ 32ê°œê°€ KV í—¤ë“œ 8ê°œë¥¼ 4:1ë¡œ ê³µìœ \")\n",
    "print(f\"  â€¢ MHA ëŒ€ë¹„ KV íŒŒë¼ë¯¸í„° 75% ì ˆê°\")\n",
    "print(f\"  â€¢ MQAë³´ë‹¤ í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ì„œ MHAë³´ë‹¤ ë¹ ë¦„ â†’ ìµœì  ê· í˜•ì \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì†Œí˜• Llama Block ì¡°ë¦½ <a name='5.-Llama-Block'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì†Œí˜• Llama Decoder Block ì¡°ë¦½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# RMSNorm + GQA + SwiGLUë¥¼ ê²°í•©í•˜ì—¬ Llama 3 ìŠ¤íƒ€ì¼ ë¸”ë¡ì„ ì™„ì„±í•©ë‹ˆë‹¤\n",
    "\n",
    "class LlamaDecoderBlock(tf.keras.layers.Layer):\n",
    "    # Llama 3 ìŠ¤íƒ€ì¼ Decoder Block: Pre-Norm + GQA + SwiGLU\n",
    "    def __init__(self, d_model, n_q_heads, n_kv_heads, d_ff, eps=1e-6):\n",
    "        super().__init__()\n",
    "        # Pre-Norm: Attention ì „ RMSNorm\n",
    "        self.attn_norm = RMSNorm(d_model, eps)\n",
    "        self.attn = GroupedQueryAttention(d_model, n_q_heads, n_kv_heads)\n",
    "\n",
    "        # Pre-Norm: FFN ì „ RMSNorm\n",
    "        self.ffn_norm = RMSNorm(d_model, eps)\n",
    "        self.ffn = SwiGLUFFN(d_model, d_ff)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # Pre-Norm â†’ Attention â†’ Residual\n",
    "        h = x + self.attn(self.attn_norm(x), mask=mask)\n",
    "        # Pre-Norm â†’ SwiGLU FFN â†’ Residual\n",
    "        out = h + self.ffn(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "# ì†Œí˜• Llama 3 ì„¤ì • (ë°ëª¨ìš© ì¶•ì†Œ)\n",
    "config = {\n",
    "    'd_model': 256,\n",
    "    'n_q_heads': 8,\n",
    "    'n_kv_heads': 2,     # GQA: 4:1\n",
    "    'd_ff': 688,          # ì•½ 8/3 * 256 â†’ 256ì˜ ë°°ìˆ˜ì— ê°€ê¹ê²Œ\n",
    "}\n",
    "\n",
    "block = LlamaDecoderBlock(**config)\n",
    "\n",
    "# Causal mask ìƒì„±\n",
    "seq_len = 32\n",
    "causal_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "causal_mask = (1.0 - causal_mask) * -1e9  # ë¯¸ë˜ í† í° ë§ˆìŠ¤í‚¹\n",
    "causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # [1, 1, S, S]\n",
    "\n",
    "# í¬ì›Œë“œ íŒ¨ìŠ¤\n",
    "x_input = tf.random.normal((2, seq_len, config['d_model']))\n",
    "output = block(x_input, mask=causal_mask)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ì†Œí˜• Llama Decoder Block êµ¬ì¡°\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ì„¤ì •:\")\n",
    "print(f\"  d_model:    {config['d_model']}\")\n",
    "print(f\"  n_q_heads:  {config['n_q_heads']}\")\n",
    "print(f\"  n_kv_heads: {config['n_kv_heads']} (GQA ratio: {config['n_q_heads']//config['n_kv_heads']}:1)\")\n",
    "print(f\"  d_ff:       {config['d_ff']}\")\n",
    "print()\n",
    "print(f\"ì…ë ¥ shape:  {x_input.shape}\")\n",
    "print(f\"ì¶œë ¥ shape:  {output.shape}\")\n",
    "print(f\"shape ë³´ì¡´:  {x_input.shape == output.shape}\")\n",
    "print()\n",
    "\n",
    "# ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "total_params = sum(tf.size(v).numpy() for v in block.trainable_variables)\n",
    "print(f\"ë¸”ë¡ ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print()\n",
    "print(\"ë¸”ë¡ êµ¬ì¡° (Pre-Norm Transformer):\")\n",
    "print(\"  x â†’ RMSNorm â†’ GQA â†’ + (residual)\")\n",
    "print(\"    â†’ RMSNorm â†’ SwiGLU FFN â†’ + (residual) â†’ output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Llama 3 8B ì‹¤ì œ ê·œëª¨ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì‹¤ì œ Llama 3 8B íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ê·œëª¨ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"Llama 3 8B ì•„í‚¤í…ì²˜ ë¶„ì„\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Llama 3 8B ê³µì‹ ìŠ¤í™ (Meta AI, 2024)\n",
    "specs = {\n",
    "    'd_model': 4096,\n",
    "    'n_layers': 32,\n",
    "    'n_q_heads': 32,\n",
    "    'n_kv_heads': 8,\n",
    "    'd_head': 128,       # 4096 / 32\n",
    "    'd_ff': 14336,       # 8/3 * 4096, rounded to 256 multiple\n",
    "    'vocab_size': 128000,\n",
    "    'max_seq_len': 8192,\n",
    "    'rope_base': 500000,\n",
    "}\n",
    "\n",
    "for k, v in specs.items():\n",
    "    print(f\"  {k:<15} = {v:>10,}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "d = specs['d_model']\n",
    "L = specs['n_layers']\n",
    "Hq = specs['n_q_heads']\n",
    "Hkv = specs['n_kv_heads']\n",
    "dh = specs['d_head']\n",
    "dff = specs['d_ff']\n",
    "V = specs['vocab_size']\n",
    "\n",
    "# ê° ë¸”ë¡ íŒŒë¼ë¯¸í„°\n",
    "attn_params = (Hq * dh * d) + 2 * (Hkv * dh * d) + (d * d)  # Wq + Wk + Wv + Wo\n",
    "ffn_params = 3 * d * dff  # W1 + W2 + W3 (SwiGLU)\n",
    "norm_params = 2 * d  # 2x RMSNorm per block\n",
    "block_params = attn_params + ffn_params + norm_params\n",
    "\n",
    "# ì „ì²´ ëª¨ë¸\n",
    "embed_params = V * d  # í† í° ì„ë² ë”©\n",
    "final_norm = d\n",
    "head_params = d * V  # lm_head (ë³´í†µ ì„ë² ë”©ê³¼ ê³µìœ )\n",
    "total = L * block_params + embed_params + final_norm\n",
    "\n",
    "print(f\"{'êµ¬ì„± ìš”ì†Œ':<30} | {'íŒŒë¼ë¯¸í„° ìˆ˜':>15} | {'ë¹„ìœ¨':>8}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Attention (per block)':<30} | {attn_params:>15,} | {attn_params/block_params*100:>6.1f}%\")\n",
    "print(f\"{'SwiGLU FFN (per block)':<30} | {ffn_params:>15,} | {ffn_params/block_params*100:>6.1f}%\")\n",
    "print(f\"{'RMSNorm (per block)':<30} | {norm_params:>15,} | {norm_params/block_params*100:>6.1f}%\")\n",
    "print(f\"{'ë¸”ë¡ í•©ê³„':<30} | {block_params:>15,} |\")\n",
    "print(f\"{'32 ë¸”ë¡ í•©ê³„':<30} | {L*block_params:>15,} |\")\n",
    "print(f\"{'í† í° ì„ë² ë”©':<30} | {embed_params:>15,} |\")\n",
    "print(f\"{'Final RMSNorm':<30} | {final_norm:>15,} |\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'ì´ê³„ (lm_head ê³µìœ  ê°€ì •)':<30} | {total:>15,} |\")\n",
    "print(f\"{'â‰ˆ':<30} | {total/1e9:>14.2f}B |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| RMSNorm | í‰ê·  ì œê±° ì—†ì´ RMSë§Œìœ¼ë¡œ ì •ê·œí™” â†’ LayerNorm ëŒ€ë¹„ ~40% ì—°ì‚° ì ˆê° | â­â­â­ |\n",
    "| SwiGLU | Swish ê²Œì´íŒ…ìœ¼ë¡œ ì •ë³´ í•„í„°ë§ â†’ GELU FFN ëŒ€ë¹„ í‘œí˜„ë ¥ í–¥ìƒ | â­â­â­ |\n",
    "| GQA | KV í—¤ë“œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ê³µìœ  â†’ MHA í’ˆì§ˆ + MQA íš¨ìœ¨ì˜ ìµœì  ê· í˜• | â­â­â­ |\n",
    "| Pre-Norm | Attention/FFN ì „ì— ì •ê·œí™” â†’ Post-Norm ëŒ€ë¹„ í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ | â­â­ |\n",
    "| Residual Connection | ì…ë ¥ì„ ì¶œë ¥ì— ë”í•´ ê·¸ë˜ë””ì–¸íŠ¸ ì†Œì‹¤ ë°©ì§€ | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\text{RMSNorm}(a_i) = \\frac{a_i}{\\sqrt{\\frac{1}{n}\\sum_j a_j^2 + \\epsilon}} \\cdot g_i$$\n",
    "\n",
    "$$\\text{SwiGLU}(x) = \\text{Swish}(xW_1) \\otimes (xW_2)$$\n",
    "\n",
    "$$\\text{GQA: } H_Q = 32, \\; H_{KV} = 8 \\;\\Rightarrow\\; \\text{KV ì ˆê°ë¥ } = 1 - \\frac{8}{32} = 75\\%$$\n",
    "\n",
    "### Llama 3 8B í•µì‹¬ ìŠ¤í™\n",
    "\n",
    "| í•­ëª© | ê°’ |\n",
    "|------|-----|\n",
    "| ë ˆì´ì–´ ìˆ˜ | 32 |\n",
    "| íˆë“  ì°¨ì› | 4096 |\n",
    "| Q í—¤ë“œ | 32 |\n",
    "| KV í—¤ë“œ | 8 (GQA) |\n",
    "| FFN ì°¨ì› | 14,336 (SwiGLU) |\n",
    "| ì–´íœ˜ í¬ê¸° | 128,000 |\n",
    "| ì´ íŒŒë¼ë¯¸í„° | ~8B |\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 12-02: KV Cacheì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬** â€” Autoregressive ìƒì„±ì—ì„œ KV Cacheì˜ ë©”ëª¨ë¦¬ ê³„ì‚°, Rolling Buffer, Prefix Caching ë“± ì‹¤ì „ ì„œë¹™ì— í•„ìˆ˜ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}