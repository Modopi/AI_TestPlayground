{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03-01: 손실 함수 (Loss Functions)\n",
    "\n",
    "## 학습 목표\n",
    "- 회귀와 분류 문제에 적합한 손실 함수를 이해하고 선택할 수 있다\n",
    "- `from_logits=True`와 `False`의 차이를 이해하고 올바르게 사용할 수 있다\n",
    "- 커스텀 손실 함수를 함수형과 클래스형으로 작성할 수 있다\n",
    "\n",
    "## 목차\n",
    "1. [수학적 기초](#1.-수학적-기초)\n",
    "2. [회귀 손실 함수: MSE, MAE, Huber](#2.-회귀-손실-함수)\n",
    "3. [이진 분류: Binary Cross-Entropy](#3.-이진-분류-손실-함수)\n",
    "4. [다중 분류: Categorical vs Sparse Categorical](#4.-다중-분류-손실-함수)\n",
    "5. [커스텀 손실 함수](#5.-커스텀-손실-함수)\n",
    "6. [정리](#6.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow 버전:\", tf.__version__)\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 수학적 기초\n",
    "\n",
    "### 회귀 손실 함수\n",
    "\n",
    "**평균 제곱 오차 (MSE, Mean Squared Error)**\n",
    "\n",
    "$$L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "- 이상치(outlier)에 매우 민감 (오차를 제곱하기 때문)\n",
    "- 미분 가능하여 최적화에 유리\n",
    "\n",
    "**평균 절대 오차 (MAE, Mean Absolute Error)**\n",
    "\n",
    "$$L_{MAE} = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\hat{y}_i|$$\n",
    "\n",
    "- 이상치에 덜 민감\n",
    "- $x=0$ 에서 미분 불가능\n",
    "\n",
    "**Huber Loss**\n",
    "\n",
    "$$L_\\delta = \\begin{cases} \\frac{1}{2}x^2 & \\text{if } |x| \\leq \\delta \\\\ \\delta|x| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "- MSE와 MAE의 장점을 결합\n",
    "- $\\delta$ 범위 내에서 MSE처럼, 밖에서 MAE처럼 동작\n",
    "\n",
    "### 분류 손실 함수\n",
    "\n",
    "**이진 크로스 엔트로피 (BCE, Binary Cross-Entropy)**\n",
    "\n",
    "$$L_{BCE} = -[y\\log p + (1-y)\\log(1-p)]$$\n",
    "\n",
    "**범주형 크로스 엔트로피 (CCE, Categorical Cross-Entropy)**\n",
    "\n",
    "$$L_{CCE} = -\\sum_i y_i \\log p_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 회귀 손실 함수\n",
    "\n",
    "MSE, MAE, Huber Loss를 비교해본다. 특히 **이상치(outlier)**가 있을 때의 차이에 주목하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 회귀 손실 함수 비교: MSE, MAE, Huber\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# TensorFlow 손실 함수 객체 생성\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "huber_loss = tf.keras.losses.Huber(delta=1.0)  # delta=1.0 기본값\n",
    "\n",
    "# 이상치 없는 정상 데이터\n",
    "y_true_normal = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_normal = tf.constant([1.1, 2.1, 2.9, 4.2, 4.8])\n",
    "\n",
    "# 이상치가 포함된 데이터 (마지막 값이 이상치)\n",
    "y_true_outlier = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_outlier = tf.constant([1.1, 2.1, 2.9, 4.2, 15.0])  # 15.0: 큰 이상치\n",
    "\n",
    "print(\"=== 정상 데이터 ===\")\n",
    "print(f\"MSE:   {mse_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "print(f\"MAE:   {mae_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "print(f\"Huber: {huber_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "\n",
    "print(\"\\n=== 이상치 포함 데이터 ===\")\n",
    "print(f\"MSE:   {mse_loss(y_true_outlier, y_pred_outlier).numpy():.4f}  <- 이상치에 크게 영향받음\")\n",
    "print(f\"MAE:   {mae_loss(y_true_outlier, y_pred_outlier).numpy():.4f}\")\n",
    "print(f\"Huber: {huber_loss(y_true_outlier, y_pred_outlier).numpy():.4f}  <- 이상치에 강건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 손실 함수 곡선 시각화\n",
    "# x축: 예측 오차 (y_pred - y_true)\n",
    "# y축: 각 손실 함수 값\n",
    "# ---------------------------------------------------\n",
    "\n",
    "x = np.linspace(-4, 4, 200)  # 오차 범위\n",
    "delta = 1.0\n",
    "\n",
    "# 각 손실 함수 계산\n",
    "mse_vals = x ** 2\n",
    "mae_vals = np.abs(x)\n",
    "huber_vals = np.where(\n",
    "    np.abs(x) <= delta,\n",
    "    0.5 * x ** 2,\n",
    "    delta * np.abs(x) - 0.5 * delta ** 2\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 왼쪽: 손실 함수 곡선 비교\n",
    "ax1 = axes[0]\n",
    "ax1.plot(x, mse_vals, label='MSE', color='blue', linewidth=2)\n",
    "ax1.plot(x, mae_vals, label='MAE', color='orange', linewidth=2)\n",
    "ax1.plot(x, huber_vals, label=f'Huber (δ={delta})', color='green', linewidth=2, linestyle='--')\n",
    "ax1.axvline(x=delta, color='gray', linestyle=':', alpha=0.7, label=f'x=±δ={delta}')\n",
    "ax1.axvline(x=-delta, color='gray', linestyle=':', alpha=0.7)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_xlabel('예측 오차 (y_pred - y_true)')\n",
    "ax1.set_ylabel('손실값')\n",
    "ax1.set_title('회귀 손실 함수 비교')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 오른쪽: 이상치 영향 비교 (막대 그래프)\n",
    "ax2 = axes[1]\n",
    "categories = ['정상 데이터', '이상치 포함']\n",
    "mse_vals_bar = [\n",
    "    mse_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    mse_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "mae_vals_bar = [\n",
    "    mae_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    mae_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "huber_vals_bar = [\n",
    "    huber_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    huber_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "\n",
    "x_bar = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax2.bar(x_bar - width, mse_vals_bar, width, label='MSE', color='blue', alpha=0.7)\n",
    "ax2.bar(x_bar, mae_vals_bar, width, label='MAE', color='orange', alpha=0.7)\n",
    "ax2.bar(x_bar + width, huber_vals_bar, width, label='Huber', color='green', alpha=0.7)\n",
    "ax2.set_xticks(x_bar)\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.set_ylabel('손실값')\n",
    "ax2.set_title('이상치 존재 시 손실 함수 영향 비교')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"MSE는 이상치에 의해 손실값이 크게 증가하는 반면, Huber Loss는 상대적으로 안정적이다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 이진 분류 손실 함수\n",
    "\n",
    "### Binary Cross-Entropy와 `from_logits` 매개변수\n",
    "\n",
    "| 설정 | 모델 출력 | 내부 처리 |\n",
    "|------|-----------|----------|\n",
    "| `from_logits=False` (기본) | Sigmoid 활성화 후 확률값 (0~1) | 그대로 사용 |\n",
    "| `from_logits=True` | Sigmoid 적용 전 raw 값 (임의 범위) | 내부에서 Sigmoid 적용 후 계산 |\n",
    "\n",
    "> **권장**: `from_logits=True`를 사용하면 수치적으로 더 안정적이다 (log(0) 문제 회피)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Binary Cross-Entropy: from_logits 비교\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 실제 레이블 (이진: 0 또는 1)\n",
    "y_true_binary = tf.constant([1.0, 0.0, 1.0, 1.0, 0.0])\n",
    "\n",
    "# 방법 1: 모델 출력층에 Sigmoid 사용 -> from_logits=False\n",
    "logits = tf.constant([2.0, -1.0, 3.0, 0.5, -2.0])  # raw 출력\n",
    "probs = tf.sigmoid(logits)  # Sigmoid 적용 후 확률값\n",
    "\n",
    "bce_from_probs = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce_from_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "loss_from_probs = bce_from_probs(y_true_binary, probs)\n",
    "loss_from_logits = bce_from_logits(y_true_binary, logits)\n",
    "\n",
    "print(\"logits (raw 출력):\", logits.numpy())\n",
    "print(\"probs  (Sigmoid 후):\", probs.numpy().round(4))\n",
    "print()\n",
    "print(f\"from_logits=False (확률 입력): {loss_from_probs.numpy():.6f}\")\n",
    "print(f\"from_logits=True  (로짓 입력): {loss_from_logits.numpy():.6f}\")\n",
    "print(\"두 값은 수학적으로 동일하지만, from_logits=True가 수치적으로 더 안정적이다\")\n",
    "\n",
    "print(\"\\n=== 모델 정의 예시 ===\")\n",
    "print(\"\"\"\n",
    "# 방법 1: from_logits=True (권장)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)  # 활성화 함수 없음 (logits 출력)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "# 방법 2: from_logits=False\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid 적용\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 다중 분류 손실 함수\n",
    "\n",
    "### Categorical vs Sparse Categorical Cross-Entropy\n",
    "\n",
    "| 손실 함수 | 레이블 형식 | 사용 예 |\n",
    "|-----------|------------|--------|\n",
    "| `CategoricalCrossentropy` | One-hot 인코딩: `[0, 1, 0]` | 레이블이 이미 one-hot인 경우 |\n",
    "| `SparseCategoricalCrossentropy` | 정수 인덱스: `1` | 레이블이 클래스 번호인 경우 (일반적) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Categorical vs Sparse Categorical Cross-Entropy\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 3개 클래스 분류 예시\n",
    "# 모델의 Softmax 출력 확률\n",
    "y_pred_probs = tf.constant([\n",
    "    [0.7, 0.2, 0.1],  # 샘플 1: 클래스 0 예측\n",
    "    [0.1, 0.8, 0.1],  # 샘플 2: 클래스 1 예측\n",
    "    [0.2, 0.1, 0.7],  # 샘플 3: 클래스 2 예측\n",
    "])\n",
    "\n",
    "# One-hot 형식 레이블\n",
    "y_true_onehot = tf.constant([\n",
    "    [1, 0, 0],  # 클래스 0\n",
    "    [0, 1, 0],  # 클래스 1\n",
    "    [0, 0, 1],  # 클래스 2\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# 정수 형식 레이블 (Sparse)\n",
    "y_true_sparse = tf.constant([0, 1, 2])  # 각각 클래스 인덱스\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "loss_cce = cce(y_true_onehot, y_pred_probs)\n",
    "loss_scce = scce(y_true_sparse, y_pred_probs)\n",
    "\n",
    "print(\"레이블 형식 비교:\")\n",
    "print(f\"  One-hot 레이블:\\n{y_true_onehot.numpy()}\")\n",
    "print(f\"  Sparse 레이블: {y_true_sparse.numpy()}\")\n",
    "print()\n",
    "print(f\"CategoricalCrossentropy   (one-hot 입력): {loss_cce.numpy():.6f}\")\n",
    "print(f\"SparseCategoricalCrossentropy (정수 입력): {loss_scce.numpy():.6f}\")\n",
    "print(\"두 손실 값은 동일하다 (같은 계산을 다른 레이블 형식으로 수행)\")\n",
    "\n",
    "print(\"\\n=== from_logits 옵션도 동일하게 적용 ===\")\n",
    "y_pred_logits = tf.constant([\n",
    "    [2.0, 0.5, 0.1],\n",
    "    [0.1, 3.0, 0.2],\n",
    "    [0.3, 0.1, 2.5],\n",
    "])\n",
    "scce_logits = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(f\"SparseCCE from_logits=True: {scce_logits(y_true_sparse, y_pred_logits).numpy():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 커스텀 손실 함수\n",
    "\n",
    "TensorFlow에서 커스텀 손실 함수를 작성하는 두 가지 방법:\n",
    "\n",
    "1. **함수형**: 간단한 경우에 적합, `model.compile(loss=my_loss_fn)`\n",
    "2. **클래스형**: 매개변수가 필요하거나 상태를 가질 때 적합, `tf.keras.losses.Loss` 상속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 커스텀 손실 함수: 함수형\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"가중치가 적용된 MSE: 양수 오차에 더 큰 패널티를 부여\"\"\"\n",
    "    error = y_pred - y_true\n",
    "    # 양수 오차(과대 예측)에 2배 패널티\n",
    "    weights = tf.where(error > 0, 2.0, 1.0)\n",
    "    return tf.reduce_mean(weights * tf.square(error))\n",
    "\n",
    "# 테스트\n",
    "y_true_test = tf.constant([1.0, 2.0, 3.0])\n",
    "y_pred_under = tf.constant([0.5, 1.5, 2.5])  # 과소 예측\n",
    "y_pred_over  = tf.constant([1.5, 2.5, 3.5])  # 과대 예측 (같은 절대 오차)\n",
    "\n",
    "print(\"=== 함수형 커스텀 손실 함수 ===\")\n",
    "print(f\"과소 예측 손실: {weighted_mse_loss(y_true_test, y_pred_under).numpy():.4f}\")\n",
    "print(f\"과대 예측 손실: {weighted_mse_loss(y_true_test, y_pred_over).numpy():.4f}\")\n",
    "print(\"과대 예측에 2배 패널티가 적용되어 손실이 더 큰 것을 확인\")\n",
    "\n",
    "print(\"\\n=== 모델에 적용하는 방법 ===\")\n",
    "print(\"\"\"\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=weighted_mse_loss  # 함수를 직접 전달\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 커스텀 손실 함수: 클래스형 (tf.keras.losses.Loss 상속)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss: 불균형 데이터셋에서 어려운 샘플에 집중하는 손실 함수\n",
    "    \n",
    "    원래 논문: Lin et al., 'Focal Loss for Dense Object Detection', 2017\n",
    "    \n",
    "    수식: FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    - gamma: 포커싱 파라미터 (클수록 어려운 샘플에 더 집중)\n",
    "    - alpha: 클래스 불균형 보정 가중치\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=0.25, name='focal_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.gamma = gamma  # 포커싱 파라미터\n",
    "        self.alpha = alpha  # 클래스 가중치\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # 수치 안정성을 위해 클리핑\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # 이진 크로스 엔트로피 계산\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # 확률 p_t 계산\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        \n",
    "        # alpha 가중치 적용\n",
    "        alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        \n",
    "        # Focal Loss = alpha_t * (1 - p_t)^gamma * bce\n",
    "        focal_loss = alpha_t * tf.pow(1 - p_t, self.gamma) * bce\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"직렬화를 위한 설정 반환 (모델 저장 시 필요)\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({'gamma': self.gamma, 'alpha': self.alpha})\n",
    "        return config\n",
    "\n",
    "# 테스트\n",
    "focal_loss = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "bce_standard = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# 쉬운 샘플 (확신 있는 예측)\n",
    "y_true_easy = tf.constant([1.0, 0.0, 1.0])\n",
    "y_pred_easy = tf.constant([0.95, 0.05, 0.9])  # 높은 확신\n",
    "\n",
    "# 어려운 샘플 (불확실한 예측)\n",
    "y_true_hard = tf.constant([1.0, 0.0, 1.0])\n",
    "y_pred_hard = tf.constant([0.55, 0.45, 0.6])  # 낮은 확신\n",
    "\n",
    "print(\"=== 클래스형 커스텀 손실 함수 (Focal Loss) ===\")\n",
    "print(f\"쉬운 샘플 - BCE:   {bce_standard(y_true_easy, y_pred_easy).numpy():.4f}\")\n",
    "print(f\"쉬운 샘플 - Focal: {focal_loss(y_true_easy, y_pred_easy).numpy():.4f}  <- 크게 감소\")\n",
    "print(f\"어려운 샘플 - BCE:   {bce_standard(y_true_hard, y_pred_hard).numpy():.4f}\")\n",
    "print(f\"어려운 샘플 - Focal: {focal_loss(y_true_hard, y_pred_hard).numpy():.4f}  <- 상대적으로 덜 감소\")\n",
    "print(\"\\nFocal Loss는 쉬운 샘플의 영향을 줄여 어려운 샘플 학습에 집중하게 한다\")\n",
    "\n",
    "print(\"\\n설정 정보:\", focal_loss.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### 손실 함수 선택 가이드\n",
    "\n",
    "| 문제 유형 | 권장 손실 함수 | 비고 |\n",
    "|-----------|--------------|------|\n",
    "| 회귀 (이상치 없음) | `MeanSquaredError` | 미분 용이, 최적화 안정 |\n",
    "| 회귀 (이상치 있음) | `Huber` 또는 `MeanAbsoluteError` | Huber가 두 장점 결합 |\n",
    "| 이진 분류 | `BinaryCrossentropy(from_logits=True)` | 수치 안정성 높음 |\n",
    "| 다중 분류 (one-hot) | `CategoricalCrossentropy(from_logits=True)` | |\n",
    "| 다중 분류 (정수 레이블) | `SparseCategoricalCrossentropy(from_logits=True)` | 일반적으로 권장 |\n",
    "| 불균형 데이터 | `FocalLoss` (커스텀) | 어려운 샘플에 집중 |\n",
    "\n",
    "### 핵심 정리\n",
    "- `from_logits=True`는 모델 출력층에 Sigmoid/Softmax가 없을 때 사용하며, 수치적으로 더 안정적이다\n",
    "- Huber Loss는 `delta` 파라미터로 MSE와 MAE 사이의 균형을 조절한다\n",
    "- 커스텀 손실 함수는 함수형(간단)과 클래스형(매개변수 필요) 두 방식으로 작성한다\n",
    "\n",
    "### 다음 챕터 예고\n",
    "**Chapter 03-02: 옵티마이저 (Optimizers)** - SGD, Adam, RMSprop 등 다양한 최적화 알고리즘의 동작 원리와 선택 방법을 다룬다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
