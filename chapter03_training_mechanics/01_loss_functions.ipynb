{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03-01: ì†ì‹¤ í•¨ìˆ˜ (Loss Functions)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- íšŒê·€ì™€ ë¶„ë¥˜ ë¬¸ì œì— ì í•©í•œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì´í•´í•˜ê³  ì„ íƒí•  ìˆ˜ ìˆë‹¤\n",
    "- `from_logits=True`ì™€ `False`ì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤\n",
    "- ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ í•¨ìˆ˜í˜•ê³¼ í´ë˜ìŠ¤í˜•ìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [íšŒê·€ ì†ì‹¤ í•¨ìˆ˜: MSE, MAE, Huber](#2.-íšŒê·€-ì†ì‹¤-í•¨ìˆ˜)\n",
    "3. [ì´ì§„ ë¶„ë¥˜: Binary Cross-Entropy](#3.-ì´ì§„-ë¶„ë¥˜-ì†ì‹¤-í•¨ìˆ˜)\n",
    "4. [ë‹¤ì¤‘ ë¶„ë¥˜: Categorical vs Sparse Categorical](#4.-ë‹¤ì¤‘-ë¶„ë¥˜-ì†ì‹¤-í•¨ìˆ˜)\n",
    "5. [ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜](#5.-ì»¤ìŠ¤í…€-ì†ì‹¤-í•¨ìˆ˜)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
    "\n",
    "# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### íšŒê·€ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "**í‰ê·  ì œê³± ì˜¤ì°¨ (MSE, Mean Squared Error)**\n",
    "\n",
    "$$L_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "- ì´ìƒì¹˜(outlier)ì— ë§¤ìš° ë¯¼ê° (ì˜¤ì°¨ë¥¼ ì œê³±í•˜ê¸° ë•Œë¬¸)\n",
    "- ë¯¸ë¶„ ê°€ëŠ¥í•˜ì—¬ ìµœì í™”ì— ìœ ë¦¬\n",
    "\n",
    "**í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (MAE, Mean Absolute Error)**\n",
    "\n",
    "$$L_{MAE} = \\frac{1}{N}\\sum_{i=1}^N |y_i - \\hat{y}_i|$$\n",
    "\n",
    "- ì´ìƒì¹˜ì— ëœ ë¯¼ê°\n",
    "- $x=0$ ì—ì„œ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥\n",
    "\n",
    "**Huber Loss**\n",
    "\n",
    "$$L_\\delta = \\begin{cases} \\frac{1}{2}x^2 & \\text{if } |x| \\leq \\delta \\\\ \\delta|x| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "- MSEì™€ MAEì˜ ì¥ì ì„ ê²°í•©\n",
    "- $\\delta$ ë²”ìœ„ ë‚´ì—ì„œ MSEì²˜ëŸ¼, ë°–ì—ì„œ MAEì²˜ëŸ¼ ë™ì‘\n",
    "\n",
    "### ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "**ì´ì§„ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (BCE, Binary Cross-Entropy)**\n",
    "\n",
    "$$L_{BCE} = -[y\\log p + (1-y)\\log(1-p)]$$\n",
    "\n",
    "**ë²”ì£¼í˜• í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (CCE, Categorical Cross-Entropy)**\n",
    "\n",
    "$$L_{CCE} = -\\sum_i y_i \\log p_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ¤” ì†ì‹¤ í•¨ìˆ˜(Loss Function)ê°€ ë­ì˜ˆìš”?\n",
    "\n",
    "ì†ì‹¤ í•¨ìˆ˜ëŠ” **'AIê°€ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ë„êµ¬'**ì˜ˆìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì‹œí—˜ ì±„ì  ì„ ìƒë‹˜ì²˜ëŸ¼!\n",
    "> - ì •ë‹µ: ê³ ì–‘ì´\n",
    "> - AI ì˜ˆì¸¡: ê³ ì–‘ì´(í™•ì‹  90%) â†’ ì†ì‹¤ ë‚®ìŒ ğŸ˜Š\n",
    "> - AI ì˜ˆì¸¡: ê°•ì•„ì§€(í™•ì‹  80%) â†’ ì†ì‹¤ ë†’ìŒ ğŸ˜±\n",
    "\n",
    "í•™ìŠµì´ë€ ì´ ì†ì‹¤ ì ìˆ˜ë¥¼ **ìµœëŒ€í•œ ë‚®ì¶”ëŠ” ê³¼ì •**ì´ì—ìš”!\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ“ íšŒê·€ ì†ì‹¤ í•¨ìˆ˜: ìˆ«ìë¥¼ ì˜ˆì¸¡í•  ë•Œ\n",
    "\n",
    "**MSE (í‰ê·  ì œê³± ì˜¤ì°¨)**:\n",
    "- í‹€ë¦° ì–‘ì„ ì œê³±í•˜ì—¬ ë”í•´ìš”\n",
    "- í¬ê²Œ í‹€ë¦´ìˆ˜ë¡ **í›¨ì”¬** ë” í° ë²Œì !\n",
    "\n",
    "```\n",
    "ì˜¤ì°¨: 1 â†’ ë²Œì  1Â²  = 1\n",
    "ì˜¤ì°¨: 5 â†’ ë²Œì  5Â²  = 25   â† 5ë°° í‹€ë ¸ëŠ”ë° 25ë°° ë²Œì !\n",
    "ì˜¤ì°¨: 10 â†’ ë²Œì  10Â² = 100\n",
    "```\n",
    "\n",
    "> ğŸ’¡ ì´ìƒì¹˜(í¬ê²Œ í‹€ë¦° ë°ì´í„°)ì— ì—„ì²­ ë¯¼ê°í•´ìš”!\n",
    "\n",
    "**MAE (í‰ê·  ì ˆëŒ€ ì˜¤ì°¨)**:\n",
    "- í‹€ë¦° ì–‘ì„ ê·¸ëƒ¥ ë”í•´ìš” (ì ˆëŒ€ê°’)\n",
    "- ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•˜ì§€ë§Œ, ê²½ì‚¬(ê¸°ìš¸ê¸°)ê°€ ë¶ˆì—°ì†ì ì´ì—ìš”\n",
    "\n",
    "**Huber Loss**:\n",
    "- ì‘ì€ ì˜¤ì°¨ â†’ MSEì²˜ëŸ¼ (ë¶€ë“œëŸ½ê³  ë¯¸ë¶„ ê°€ëŠ¥)\n",
    "- í° ì˜¤ì°¨ â†’ MAEì²˜ëŸ¼ (ì´ìƒì¹˜ì— ê°•ê±´)\n",
    "- **ë‘ ì¥ì ì„ í•©ì¹œ 'ì¤‘ê°„ ë‹¤ë¦¬' ì—­í• !**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ·ï¸ ì´ì§„ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (Binary Cross-Entropy, BCE)\n",
    "\n",
    "**ì–¸ì œ ì“°ë‚˜ìš”?** ë‹µì´ **ë‘˜ ì¤‘ í•˜ë‚˜**ì¸ ë¬¸ì œ!\n",
    "- ìŠ¤íŒ¸ ë©”ì¼ vs ì •ìƒ ë©”ì¼ ğŸ“§\n",
    "- ê³ ì–‘ì´ ì‚¬ì§„ vs ê³ ì–‘ì´ ì•„ë‹Œ ì‚¬ì§„ ğŸ±\n",
    "- ë³‘ì´ ìˆëŠ”ì§€ vs ì—†ëŠ”ì§€ ğŸ¥\n",
    "\n",
    "**ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?**\n",
    "\n",
    "$$L_{BCE} = -[y \\cdot \\log p + (1-y) \\cdot \\log(1-p)]$$\n",
    "\n",
    "| ì •ë‹µ | AI ì˜ˆì¸¡ | ì†ì‹¤ |\n",
    "|------|---------|------|\n",
    "| ê³ ì–‘ì´(y=1) | ê³ ì–‘ì´ í™•ë¥  0.9 | ë‚®ìŒ ğŸ˜Š (-log(0.9) â‰ˆ 0.1) |\n",
    "| ê³ ì–‘ì´(y=1) | ê³ ì–‘ì´ í™•ë¥  0.1 | ë§¤ìš° ë†’ìŒ ğŸ˜± (-log(0.1) â‰ˆ 2.3) |\n",
    "| ì•„ë‹˜(y=0)   | ê³ ì–‘ì´ í™•ë¥  0.05 | ë‚®ìŒ ğŸ˜Š |\n",
    "\n",
    "> ğŸ’¡ **í•µì‹¬ íŠ¹ì„±**: ìì‹  ìˆê²Œ í‹€ë¦´ìˆ˜ë¡ í›¨ì”¬ í° ë²Œì !\n",
    "> log í•¨ìˆ˜ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¬´í•œëŒ€ë¡œ ì»¤ì§€ê¸° ë•Œë¬¸ì´ì—ìš”.\n",
    "\n",
    "**from_logits=True vs Falseì˜ ì°¨ì´**:\n",
    "- `False`: ëª¨ë¸ì´ ì´ë¯¸ Sigmoidë¥¼ ê±°ì³ì„œ 0~1 í™•ë¥ ì„ ì¶œë ¥\n",
    "- `True`: ëª¨ë¸ì´ Sigmoid ì—†ì´ raw ê°’ ì¶œë ¥ â†’ ë‚´ë¶€ì—ì„œ Sigmoid ì ìš©\n",
    "- â­ `True`ê°€ ë” ì•ˆì •ì ! (log(0) ë¬¸ì œë¥¼ ë‚´ë¶€ì—ì„œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì²˜ë¦¬)\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ·ï¸ ë²”ì£¼í˜• í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ (Categorical Cross-Entropy, CCE)\n",
    "\n",
    "**ì–¸ì œ ì“°ë‚˜ìš”?** ë‹µì´ **ì—¬ëŸ¬ ê°œ ì¤‘ í•˜ë‚˜**ì¸ ë¬¸ì œ!\n",
    "- ì†ê¸€ì”¨ ìˆ«ì 0~9 ì¤‘ ì–´ëŠ ê²ƒ? ğŸ”¢\n",
    "- ì´ë¯¸ì§€ê°€ ê°œ/ê³ ì–‘ì´/í† ë¼/ìƒˆ ì¤‘ ì–´ëŠ ê²ƒ? ğŸ¾\n",
    "- ë‰´ìŠ¤ê°€ ì •ì¹˜/ê²½ì œ/ìŠ¤í¬ì¸ /ë¬¸í™” ì¤‘ ì–´ëŠ ë¶„ì•¼? ğŸ“°\n",
    "\n",
    "$$L_{CCE} = -\\sum_i y_i \\log p_i$$\n",
    "\n",
    "ì •ë‹µ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í™•ë¥ ($p_i$)ë§Œ ì†ì‹¤ì— ì˜í–¥ì„ ì¤˜ìš”!\n",
    "\n",
    "**Categorical vs Sparse Categoricalì˜ ì°¨ì´**:\n",
    "\n",
    "```\n",
    "Categorical:  y = [0, 1, 0, 0]  â† One-hot í˜•ì‹ (3ê°œ ì¤‘ 1ë²ˆ í´ë˜ìŠ¤)\n",
    "Sparse:       y = 1              â† ì •ìˆ˜ ì¸ë±ìŠ¤ í˜•ì‹ (1ë²ˆ í´ë˜ìŠ¤)\n",
    "```\n",
    "\n",
    "ìˆ˜í•™ì ìœ¼ë¡œ ì™„ì „íˆ ë™ì¼í•œ ê²°ê³¼! ë ˆì´ë¸” í˜•ì‹ì— ë”°ë¼ ì„ íƒí•´ìš”.\n",
    "\n",
    "> ğŸ’¡ **ì‹¤ë¬´ íŒ**: ë³´í†µ ì •ìˆ˜ ë ˆì´ë¸”ì´ ë” í¸ë¦¬í•´ì„œ\n",
    "> `SparseCategoricalCrossentropy`ë¥¼ ë” ë§ì´ ì¨ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. íšŒê·€ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "MSE, MAE, Huber Lossë¥¼ ë¹„êµí•´ë³¸ë‹¤. íŠ¹íˆ **ì´ìƒì¹˜(outlier)**ê°€ ìˆì„ ë•Œì˜ ì°¨ì´ì— ì£¼ëª©í•˜ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# íšŒê·€ ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ: MSE, MAE, Huber\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# TensorFlow ì†ì‹¤ í•¨ìˆ˜ ê°ì²´ ìƒì„±\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "mae_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "huber_loss = tf.keras.losses.Huber(delta=1.0)  # delta=1.0 ê¸°ë³¸ê°’\n",
    "\n",
    "# ì´ìƒì¹˜ ì—†ëŠ” ì •ìƒ ë°ì´í„°\n",
    "y_true_normal = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_normal = tf.constant([1.1, 2.1, 2.9, 4.2, 4.8])\n",
    "\n",
    "# ì´ìƒì¹˜ê°€ í¬í•¨ëœ ë°ì´í„° (ë§ˆì§€ë§‰ ê°’ì´ ì´ìƒì¹˜)\n",
    "y_true_outlier = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred_outlier = tf.constant([1.1, 2.1, 2.9, 4.2, 15.0])  # 15.0: í° ì´ìƒì¹˜\n",
    "\n",
    "print(\"=== ì •ìƒ ë°ì´í„° ===\")\n",
    "print(f\"MSE:   {mse_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "print(f\"MAE:   {mae_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "print(f\"Huber: {huber_loss(y_true_normal, y_pred_normal).numpy():.4f}\")\n",
    "\n",
    "print(\"\\n=== ì´ìƒì¹˜ í¬í•¨ ë°ì´í„° ===\")\n",
    "print(f\"MSE:   {mse_loss(y_true_outlier, y_pred_outlier).numpy():.4f}  <- ì´ìƒì¹˜ì— í¬ê²Œ ì˜í–¥ë°›ìŒ\")\n",
    "print(f\"MAE:   {mae_loss(y_true_outlier, y_pred_outlier).numpy():.4f}\")\n",
    "print(f\"Huber: {huber_loss(y_true_outlier, y_pred_outlier).numpy():.4f}  <- ì´ìƒì¹˜ì— ê°•ê±´\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì†ì‹¤ í•¨ìˆ˜ ê³¡ì„  ì‹œê°í™”\n",
    "# xì¶•: ì˜ˆì¸¡ ì˜¤ì°¨ (y_pred - y_true)\n",
    "# yì¶•: ê° ì†ì‹¤ í•¨ìˆ˜ ê°’\n",
    "# ---------------------------------------------------\n",
    "\n",
    "x = np.linspace(-4, 4, 200)  # ì˜¤ì°¨ ë²”ìœ„\n",
    "delta = 1.0\n",
    "\n",
    "# ê° ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°\n",
    "mse_vals = x ** 2\n",
    "mae_vals = np.abs(x)\n",
    "huber_vals = np.where(\n",
    "    np.abs(x) <= delta,\n",
    "    0.5 * x ** 2,\n",
    "    delta * np.abs(x) - 0.5 * delta ** 2\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ì™¼ìª½: ì†ì‹¤ í•¨ìˆ˜ ê³¡ì„  ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "ax1.plot(x, mse_vals, label='MSE', color='blue', linewidth=2)\n",
    "ax1.plot(x, mae_vals, label='MAE', color='orange', linewidth=2)\n",
    "ax1.plot(x, huber_vals, label=f'Huber (Î´={delta})', color='green', linewidth=2, linestyle='--')\n",
    "ax1.axvline(x=delta, color='gray', linestyle=':', alpha=0.7, label=f'x=Â±Î´={delta}')\n",
    "ax1.axvline(x=-delta, color='gray', linestyle=':', alpha=0.7)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨ (y_pred - y_true)')\n",
    "ax1.set_ylabel('ì†ì‹¤ê°’')\n",
    "ax1.set_title('íšŒê·€ ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì˜¤ë¥¸ìª½: ì´ìƒì¹˜ ì˜í–¥ ë¹„êµ (ë§‰ëŒ€ ê·¸ë˜í”„)\n",
    "ax2 = axes[1]\n",
    "categories = ['ì •ìƒ ë°ì´í„°', 'ì´ìƒì¹˜ í¬í•¨']\n",
    "mse_vals_bar = [\n",
    "    mse_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    mse_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "mae_vals_bar = [\n",
    "    mae_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    mae_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "huber_vals_bar = [\n",
    "    huber_loss(y_true_normal, y_pred_normal).numpy(),\n",
    "    huber_loss(y_true_outlier, y_pred_outlier).numpy()\n",
    "]\n",
    "\n",
    "x_bar = np.arange(len(categories))\n",
    "width = 0.25\n",
    "ax2.bar(x_bar - width, mse_vals_bar, width, label='MSE', color='blue', alpha=0.7)\n",
    "ax2.bar(x_bar, mae_vals_bar, width, label='MAE', color='orange', alpha=0.7)\n",
    "ax2.bar(x_bar + width, huber_vals_bar, width, label='Huber', color='green', alpha=0.7)\n",
    "ax2.set_xticks(x_bar)\n",
    "ax2.set_xticklabels(categories)\n",
    "ax2.set_ylabel('ì†ì‹¤ê°’')\n",
    "ax2.set_title('ì´ìƒì¹˜ ì¡´ì¬ ì‹œ ì†ì‹¤ í•¨ìˆ˜ ì˜í–¥ ë¹„êµ')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"MSEëŠ” ì´ìƒì¹˜ì— ì˜í•´ ì†ì‹¤ê°’ì´ í¬ê²Œ ì¦ê°€í•˜ëŠ” ë°˜ë©´, Huber LossëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì ì´ë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "### Binary Cross-Entropyì™€ `from_logits` ë§¤ê°œë³€ìˆ˜\n",
    "\n",
    "| ì„¤ì • | ëª¨ë¸ ì¶œë ¥ | ë‚´ë¶€ ì²˜ë¦¬ |\n",
    "|------|-----------|----------|\n",
    "| `from_logits=False` (ê¸°ë³¸) | Sigmoid í™œì„±í™” í›„ í™•ë¥ ê°’ (0~1) | ê·¸ëŒ€ë¡œ ì‚¬ìš© |\n",
    "| `from_logits=True` | Sigmoid ì ìš© ì „ raw ê°’ (ì„ì˜ ë²”ìœ„) | ë‚´ë¶€ì—ì„œ Sigmoid ì ìš© í›„ ê³„ì‚° |\n",
    "\n",
    "> **ê¶Œì¥**: `from_logits=True`ë¥¼ ì‚¬ìš©í•˜ë©´ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë” ì•ˆì •ì ì´ë‹¤ (log(0) ë¬¸ì œ íšŒí”¼)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Binary Cross-Entropy: from_logits ë¹„êµ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ì‹¤ì œ ë ˆì´ë¸” (ì´ì§„: 0 ë˜ëŠ” 1)\n",
    "y_true_binary = tf.constant([1.0, 0.0, 1.0, 1.0, 0.0])\n",
    "\n",
    "# ë°©ë²• 1: ëª¨ë¸ ì¶œë ¥ì¸µì— Sigmoid ì‚¬ìš© -> from_logits=False\n",
    "logits = tf.constant([2.0, -1.0, 3.0, 0.5, -2.0])  # raw ì¶œë ¥\n",
    "probs = tf.sigmoid(logits)  # Sigmoid ì ìš© í›„ í™•ë¥ ê°’\n",
    "\n",
    "bce_from_probs = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "bce_from_logits = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "loss_from_probs = bce_from_probs(y_true_binary, probs)\n",
    "loss_from_logits = bce_from_logits(y_true_binary, logits)\n",
    "\n",
    "print(\"logits (raw ì¶œë ¥):\", logits.numpy())\n",
    "print(\"probs  (Sigmoid í›„):\", probs.numpy().round(4))\n",
    "print()\n",
    "print(f\"from_logits=False (í™•ë¥  ì…ë ¥): {loss_from_probs.numpy():.6f}\")\n",
    "print(f\"from_logits=True  (ë¡œì§“ ì…ë ¥): {loss_from_logits.numpy():.6f}\")\n",
    "print(\"ë‘ ê°’ì€ ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼í•˜ì§€ë§Œ, from_logits=Trueê°€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë” ì•ˆì •ì ì´ë‹¤\")\n",
    "\n",
    "print(\"\\n=== ëª¨ë¸ ì •ì˜ ì˜ˆì‹œ ===\")\n",
    "print(\"\"\"\n",
    "# ë°©ë²• 1: from_logits=True (ê¶Œì¥)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)  # í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ (logits ì¶œë ¥)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "# ë°©ë²• 2: from_logits=False\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Sigmoid ì ìš©\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë‹¤ì¤‘ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "### Categorical vs Sparse Categorical Cross-Entropy\n",
    "\n",
    "| ì†ì‹¤ í•¨ìˆ˜ | ë ˆì´ë¸” í˜•ì‹ | ì‚¬ìš© ì˜ˆ |\n",
    "|-----------|------------|--------|\n",
    "| `CategoricalCrossentropy` | One-hot ì¸ì½”ë”©: `[0, 1, 0]` | ë ˆì´ë¸”ì´ ì´ë¯¸ one-hotì¸ ê²½ìš° |\n",
    "| `SparseCategoricalCrossentropy` | ì •ìˆ˜ ì¸ë±ìŠ¤: `1` | ë ˆì´ë¸”ì´ í´ë˜ìŠ¤ ë²ˆí˜¸ì¸ ê²½ìš° (ì¼ë°˜ì ) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Categorical vs Sparse Categorical Cross-Entropy\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 3ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ ì˜ˆì‹œ\n",
    "# ëª¨ë¸ì˜ Softmax ì¶œë ¥ í™•ë¥ \n",
    "y_pred_probs = tf.constant([\n",
    "    [0.7, 0.2, 0.1],  # ìƒ˜í”Œ 1: í´ë˜ìŠ¤ 0 ì˜ˆì¸¡\n",
    "    [0.1, 0.8, 0.1],  # ìƒ˜í”Œ 2: í´ë˜ìŠ¤ 1 ì˜ˆì¸¡\n",
    "    [0.2, 0.1, 0.7],  # ìƒ˜í”Œ 3: í´ë˜ìŠ¤ 2 ì˜ˆì¸¡\n",
    "])\n",
    "\n",
    "# One-hot í˜•ì‹ ë ˆì´ë¸”\n",
    "y_true_onehot = tf.constant([\n",
    "    [1, 0, 0],  # í´ë˜ìŠ¤ 0\n",
    "    [0, 1, 0],  # í´ë˜ìŠ¤ 1\n",
    "    [0, 0, 1],  # í´ë˜ìŠ¤ 2\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# ì •ìˆ˜ í˜•ì‹ ë ˆì´ë¸” (Sparse)\n",
    "y_true_sparse = tf.constant([0, 1, 2])  # ê°ê° í´ë˜ìŠ¤ ì¸ë±ìŠ¤\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "loss_cce = cce(y_true_onehot, y_pred_probs)\n",
    "loss_scce = scce(y_true_sparse, y_pred_probs)\n",
    "\n",
    "print(\"ë ˆì´ë¸” í˜•ì‹ ë¹„êµ:\")\n",
    "print(f\"  One-hot ë ˆì´ë¸”:\\n{y_true_onehot.numpy()}\")\n",
    "print(f\"  Sparse ë ˆì´ë¸”: {y_true_sparse.numpy()}\")\n",
    "print()\n",
    "print(f\"CategoricalCrossentropy   (one-hot ì…ë ¥): {loss_cce.numpy():.6f}\")\n",
    "print(f\"SparseCategoricalCrossentropy (ì •ìˆ˜ ì…ë ¥): {loss_scce.numpy():.6f}\")\n",
    "print(\"ë‘ ì†ì‹¤ ê°’ì€ ë™ì¼í•˜ë‹¤ (ê°™ì€ ê³„ì‚°ì„ ë‹¤ë¥¸ ë ˆì´ë¸” í˜•ì‹ìœ¼ë¡œ ìˆ˜í–‰)\")\n",
    "\n",
    "print(\"\\n=== from_logits ì˜µì…˜ë„ ë™ì¼í•˜ê²Œ ì ìš© ===\")\n",
    "y_pred_logits = tf.constant([\n",
    "    [2.0, 0.5, 0.1],\n",
    "    [0.1, 3.0, 0.2],\n",
    "    [0.3, 0.1, 2.5],\n",
    "])\n",
    "scce_logits = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "print(f\"SparseCCE from_logits=True: {scce_logits(y_true_sparse, y_pred_logits).numpy():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "TensorFlowì—ì„œ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•:\n",
    "\n",
    "1. **í•¨ìˆ˜í˜•**: ê°„ë‹¨í•œ ê²½ìš°ì— ì í•©, `model.compile(loss=my_loss_fn)`\n",
    "2. **í´ë˜ìŠ¤í˜•**: ë§¤ê°œë³€ìˆ˜ê°€ í•„ìš”í•˜ê±°ë‚˜ ìƒíƒœë¥¼ ê°€ì§ˆ ë•Œ ì í•©, `tf.keras.losses.Loss` ìƒì†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜: í•¨ìˆ˜í˜•\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def weighted_mse_loss(y_true, y_pred):\n",
    "    \"\"\"ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ MSE: ì–‘ìˆ˜ ì˜¤ì°¨ì— ë” í° íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬\"\"\"\n",
    "    error = y_pred - y_true\n",
    "    # ì–‘ìˆ˜ ì˜¤ì°¨(ê³¼ëŒ€ ì˜ˆì¸¡)ì— 2ë°° íŒ¨ë„í‹°\n",
    "    weights = tf.where(error > 0, 2.0, 1.0)\n",
    "    return tf.reduce_mean(weights * tf.square(error))\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "y_true_test = tf.constant([1.0, 2.0, 3.0])\n",
    "y_pred_under = tf.constant([0.5, 1.5, 2.5])  # ê³¼ì†Œ ì˜ˆì¸¡\n",
    "y_pred_over  = tf.constant([1.5, 2.5, 3.5])  # ê³¼ëŒ€ ì˜ˆì¸¡ (ê°™ì€ ì ˆëŒ€ ì˜¤ì°¨)\n",
    "\n",
    "print(\"=== í•¨ìˆ˜í˜• ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ ===\")\n",
    "print(f\"ê³¼ì†Œ ì˜ˆì¸¡ ì†ì‹¤: {weighted_mse_loss(y_true_test, y_pred_under).numpy():.4f}\")\n",
    "print(f\"ê³¼ëŒ€ ì˜ˆì¸¡ ì†ì‹¤: {weighted_mse_loss(y_true_test, y_pred_over).numpy():.4f}\")\n",
    "print(\"ê³¼ëŒ€ ì˜ˆì¸¡ì— 2ë°° íŒ¨ë„í‹°ê°€ ì ìš©ë˜ì–´ ì†ì‹¤ì´ ë” í° ê²ƒì„ í™•ì¸\")\n",
    "\n",
    "print(\"\\n=== ëª¨ë¸ì— ì ìš©í•˜ëŠ” ë°©ë²• ===\")\n",
    "print(\"\"\"\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=weighted_mse_loss  # í•¨ìˆ˜ë¥¼ ì§ì ‘ ì „ë‹¬\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜: í´ë˜ìŠ¤í˜• (tf.keras.losses.Loss ìƒì†)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Focal Loss: ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜\n",
    "    \n",
    "    ì›ë˜ ë…¼ë¬¸: Lin et al., 'Focal Loss for Dense Object Detection', 2017\n",
    "    \n",
    "    ìˆ˜ì‹: FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    - gamma: í¬ì»¤ì‹± íŒŒë¼ë¯¸í„° (í´ìˆ˜ë¡ ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘)\n",
    "    - alpha: í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì • ê°€ì¤‘ì¹˜\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=0.25, name='focal_loss'):\n",
    "        super().__init__(name=name)\n",
    "        self.gamma = gamma  # í¬ì»¤ì‹± íŒŒë¼ë¯¸í„°\n",
    "        self.alpha = alpha  # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ í´ë¦¬í•‘\n",
    "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # ì´ì§„ í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ê³„ì‚°\n",
    "        bce = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n",
    "        \n",
    "        # í™•ë¥  p_t ê³„ì‚°\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "        \n",
    "        # alpha ê°€ì¤‘ì¹˜ ì ìš©\n",
    "        alpha_t = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)\n",
    "        \n",
    "        # Focal Loss = alpha_t * (1 - p_t)^gamma * bce\n",
    "        focal_loss = alpha_t * tf.pow(1 - p_t, self.gamma) * bce\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"ì§ë ¬í™”ë¥¼ ìœ„í•œ ì„¤ì • ë°˜í™˜ (ëª¨ë¸ ì €ì¥ ì‹œ í•„ìš”)\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({'gamma': self.gamma, 'alpha': self.alpha})\n",
    "        return config\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "focal_loss = FocalLoss(gamma=2.0, alpha=0.25)\n",
    "bce_standard = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# ì‰¬ìš´ ìƒ˜í”Œ (í™•ì‹  ìˆëŠ” ì˜ˆì¸¡)\n",
    "y_true_easy = tf.constant([1.0, 0.0, 1.0])\n",
    "y_pred_easy = tf.constant([0.95, 0.05, 0.9])  # ë†’ì€ í™•ì‹ \n",
    "\n",
    "# ì–´ë ¤ìš´ ìƒ˜í”Œ (ë¶ˆí™•ì‹¤í•œ ì˜ˆì¸¡)\n",
    "y_true_hard = tf.constant([1.0, 0.0, 1.0])\n",
    "y_pred_hard = tf.constant([0.55, 0.45, 0.6])  # ë‚®ì€ í™•ì‹ \n",
    "\n",
    "print(\"=== í´ë˜ìŠ¤í˜• ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ (Focal Loss) ===\")\n",
    "print(f\"ì‰¬ìš´ ìƒ˜í”Œ - BCE:   {bce_standard(y_true_easy, y_pred_easy).numpy():.4f}\")\n",
    "print(f\"ì‰¬ìš´ ìƒ˜í”Œ - Focal: {focal_loss(y_true_easy, y_pred_easy).numpy():.4f}  <- í¬ê²Œ ê°ì†Œ\")\n",
    "print(f\"ì–´ë ¤ìš´ ìƒ˜í”Œ - BCE:   {bce_standard(y_true_hard, y_pred_hard).numpy():.4f}\")\n",
    "print(f\"ì–´ë ¤ìš´ ìƒ˜í”Œ - Focal: {focal_loss(y_true_hard, y_pred_hard).numpy():.4f}  <- ìƒëŒ€ì ìœ¼ë¡œ ëœ ê°ì†Œ\")\n",
    "print(\"\\nFocal LossëŠ” ì‰¬ìš´ ìƒ˜í”Œì˜ ì˜í–¥ì„ ì¤„ì—¬ ì–´ë ¤ìš´ ìƒ˜í”Œ í•™ìŠµì— ì§‘ì¤‘í•˜ê²Œ í•œë‹¤\")\n",
    "\n",
    "print(\"\\nì„¤ì • ì •ë³´:\", focal_loss.get_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì •ë¦¬\n",
    "\n",
    "### ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ë¬¸ì œ ìœ í˜• | ê¶Œì¥ ì†ì‹¤ í•¨ìˆ˜ | ë¹„ê³  |\n",
    "|-----------|--------------|------|\n",
    "| íšŒê·€ (ì´ìƒì¹˜ ì—†ìŒ) | `MeanSquaredError` | ë¯¸ë¶„ ìš©ì´, ìµœì í™” ì•ˆì • |\n",
    "| íšŒê·€ (ì´ìƒì¹˜ ìˆìŒ) | `Huber` ë˜ëŠ” `MeanAbsoluteError` | Huberê°€ ë‘ ì¥ì  ê²°í•© |\n",
    "| ì´ì§„ ë¶„ë¥˜ | `BinaryCrossentropy(from_logits=True)` | ìˆ˜ì¹˜ ì•ˆì •ì„± ë†’ìŒ |\n",
    "| ë‹¤ì¤‘ ë¶„ë¥˜ (one-hot) | `CategoricalCrossentropy(from_logits=True)` | |\n",
    "| ë‹¤ì¤‘ ë¶„ë¥˜ (ì •ìˆ˜ ë ˆì´ë¸”) | `SparseCategoricalCrossentropy(from_logits=True)` | ì¼ë°˜ì ìœ¼ë¡œ ê¶Œì¥ |\n",
    "| ë¶ˆê· í˜• ë°ì´í„° | `FocalLoss` (ì»¤ìŠ¤í…€) | ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘ |\n",
    "\n",
    "### í•µì‹¬ ì •ë¦¬\n",
    "- `from_logits=True`ëŠ” ëª¨ë¸ ì¶œë ¥ì¸µì— Sigmoid/Softmaxê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ë©°, ìˆ˜ì¹˜ì ìœ¼ë¡œ ë” ì•ˆì •ì ì´ë‹¤\n",
    "- Huber LossëŠ” `delta` íŒŒë¼ë¯¸í„°ë¡œ MSEì™€ MAE ì‚¬ì´ì˜ ê· í˜•ì„ ì¡°ì ˆí•œë‹¤\n",
    "- ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ëŠ” í•¨ìˆ˜í˜•(ê°„ë‹¨)ê³¼ í´ë˜ìŠ¤í˜•(ë§¤ê°œë³€ìˆ˜ í•„ìš”) ë‘ ë°©ì‹ìœ¼ë¡œ ì‘ì„±í•œë‹¤\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 03-02: ì˜µí‹°ë§ˆì´ì € (Optimizers)** - SGD, Adam, RMSprop ë“± ë‹¤ì–‘í•œ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ì›ë¦¬ì™€ ì„ íƒ ë°©ë²•ì„ ë‹¤ë£¬ë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}