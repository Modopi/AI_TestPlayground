{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03-02: ì˜µí‹°ë§ˆì´ì € (Optimizers)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- SGD, Adam, RMSprop, AdamWì˜ ë™ì‘ ì›ë¦¬ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤\n",
    "- í•™ìŠµë¥ (Learning Rate)ì˜ ì¤‘ìš”ì„±ê³¼ ì ì ˆí•œ ì„¤ì • ë°©ë²•ì„ ì•ˆë‹¤\n",
    "- Learning Rate Schedulerë¥¼ í™œìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ í•™ìŠµë¥ ì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤\n",
    "- ë™ì¼í•œ ëª¨ë¸ì— ì—¬ëŸ¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì ìš©í•˜ê³  ìˆ˜ë ´ ì†ë„ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [ì˜µí‹°ë§ˆì´ì € ìƒì„± ë° ê¸°ë³¸ ì‚¬ìš©ë²•](#2.-ì˜µí‹°ë§ˆì´ì €-ìƒì„±-ë°-ê¸°ë³¸-ì‚¬ìš©ë²•)\n",
    "3. [í•™ìŠµë¥ ì˜ ì˜í–¥](#3.-í•™ìŠµë¥ ì˜-ì˜í–¥)\n",
    "4. [Learning Rate Scheduler](#4.-Learning-Rate-Scheduler)\n",
    "5. [ReduceLROnPlateau ì½œë°±](#5.-ReduceLROnPlateau-ì½œë°±)\n",
    "6. [ì˜µí‹°ë§ˆì´ì € ìˆ˜ë ´ ì†ë„ ë¹„êµ](#6.-ì˜µí‹°ë§ˆì´ì €-ìˆ˜ë ´-ì†ë„-ë¹„êµ)\n",
    "7. [ì •ë¦¬](#7.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "### SGD (Stochastic Gradient Descent)\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta)$$\n",
    "\n",
    "- $\\theta$: ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "- $\\eta$: í•™ìŠµë¥  (learning rate)\n",
    "- $\\nabla L(\\theta)$: ì†ì‹¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸\n",
    "\n",
    "### SGD with Momentum\n",
    "\n",
    "$$v_t = \\gamma v_{t-1} + \\eta \\nabla L(\\theta)$$\n",
    "$$\\theta \\leftarrow \\theta - v_t$$\n",
    "\n",
    "- $v_t$: ì†ë„ ë²¡í„° (ì´ì „ ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ ëˆ„ì )\n",
    "- $\\gamma$: ëª¨ë©˜í…€ ê³„ìˆ˜ (ë³´í†µ 0.9)\n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**1ì°¨ ëª¨ë©˜íŠ¸ (ê·¸ë˜ë””ì–¸íŠ¸ í‰ê· ):**\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n",
    "\n",
    "**2ì°¨ ëª¨ë©˜íŠ¸ (ê·¸ë˜ë””ì–¸íŠ¸ ì œê³± í‰ê· ):**\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n",
    "\n",
    "**í¸í–¥ ë³´ì • (ì´ˆê¸° 0ìœ¼ë¡œ ì´ˆê¸°í™”ëœ í¸í–¥ ìˆ˜ì •):**\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "**íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸:**\n",
    "$$\\theta \\leftarrow \\theta - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "- ê¸°ë³¸ê°’: $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-7}$\n",
    "- ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ì ì‘ì  í•™ìŠµë¥ ì„ ì‚¬ìš© â†’ ë¹ ë¥¸ ìˆ˜ë ´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì˜µí‹°ë§ˆì´ì € ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸƒ ì˜µí‹°ë§ˆì´ì €(Optimizer)ê°€ ë­ì˜ˆìš”?\n",
    "\n",
    "ì†ì‹¤ í•¨ìˆ˜ë¡œ 'ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€' ì•Œì•˜ìœ¼ë©´,\n",
    "ì´ì œ **ì–´ë–»ê²Œ íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì¹ ì§€** ê²°ì •í•˜ëŠ” ì—­í• ì´ ì˜µí‹°ë§ˆì´ì €ì˜ˆìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: GPS ì•ˆë‚´ì²˜ëŸ¼!\n",
    "> - ì†ì‹¤ í•¨ìˆ˜: í˜„ì¬ ìœ„ì¹˜ì™€ ëª©ì ì§€ì˜ ê±°ë¦¬ ì¸¡ì • ğŸ“\n",
    "> - ì˜µí‹°ë§ˆì´ì €: ì–´ë–¤ ê²½ë¡œë¡œ ì´ë™í• ì§€ ê²°ì • ğŸ—ºï¸\n",
    "\n",
    "#### ğŸ¢ SGD â€” ë‹¨ìˆœí•˜ì§€ë§Œ ê°•í•œ ê¸°ë³¸ê¸°\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\eta \\nabla L(\\theta)$$\n",
    "\n",
    "- **íŠ¹ì„±**: ê¸°ìš¸ê¸° ê·¸ëŒ€ë¡œ í•œ ê±¸ìŒì”© ì´ë™\n",
    "- **ì¥ì **: ë‹¨ìˆœí•´ì„œ ì´í•´í•˜ê¸° ì‰½ê³ , ì˜ tuningí•˜ë©´ ìµœì¢… ì„±ëŠ¥ì´ ì¢‹ìŒ\n",
    "- **ë‹¨ì **: í•™ìŠµë¥  ì„¤ì •ì´ ê¹Œë‹¤ë¡­ê³ , ìˆ˜ë ´ì´ ëŠë¦´ ìˆ˜ ìˆìŒ\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ë‚˜ì¹¨ë°˜ë§Œ ë³´ê³  ê±·ê¸°. ë°©í–¥ì€ ë§ì§€ë§Œ ì§€í˜•ì„ ê³ ë ¤ ëª»í•¨\n",
    "\n",
    "**Momentum ì¶”ê°€**: ì´ì „ ë°©í–¥ì˜ ê´€ì„±ì„ ìœ ì§€í•´ì„œ ëœ í”ë“¤ë ¤ìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì°ë§¤ê°€ ë‚´ë¦¬ë§‰ì—ì„œ ì†ë„ë¥¼ ìœ ì§€í•˜ë“¯ì´ ğŸ›·\n",
    "\n",
    "#### âš¡ Adam â€” ê°€ì¥ ë³´í¸ì ì¸ ì„ íƒ\n",
    "\n",
    "- **íŠ¹ì„±**: íŒŒë¼ë¯¸í„°ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ í•™ìŠµë¥ ì„ ì¡°ì ˆ\n",
    "- **ì¥ì **: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì˜ ì‘ë™, ì´ˆê¸°ê°’ì— ëœ ë¯¼ê°\n",
    "- **ë‹¨ì **: ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ SGDë³´ë‹¤ ì¼ë°˜í™” ì„±ëŠ¥ ë‚®ì„ ìˆ˜ ìˆìŒ\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìŠ¤ë§ˆíŠ¸ GPS! ì§€ì—­ë§ˆë‹¤(íŒŒë¼ë¯¸í„°ë§ˆë‹¤) ìµœì  ì†ë„ë¥¼ ë‹¬ë¦¬í•¨\n",
    "\n",
    "**Adamì˜ ë¹„ë°€**: 1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸ë¥¼ ë™ì‹œì— ìœ ì§€í•´ìš”\n",
    "- **1ì°¨ ëª¨ë©˜íŠ¸**: ê¸°ìš¸ê¸°ì˜ í‰ê·  ë°©í–¥ â†’ 'ì–´ëŠ ìª½ìœ¼ë¡œ ê°”ëŠ”ì§€'\n",
    "- **2ì°¨ ëª¨ë©˜íŠ¸**: ê¸°ìš¸ê¸° ë¶„ì‚° â†’ 'ì–¼ë§ˆë‚˜ ë¶ˆì•ˆì •í•œì§€'\n",
    "- ë¶ˆì•ˆì •í•œ íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¤„ì—¬ì¤˜ìš”!\n",
    "\n",
    "#### ğŸ“Š ì˜µí‹°ë§ˆì´ì € ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ìƒí™© | ê¶Œì¥ ì˜µí‹°ë§ˆì´ì € | ì´ìœ  |\n",
    "|------|--------------|------|\n",
    "| ì²˜ìŒ ì‹œì‘í•  ë•Œ | **Adam** | ëŒ€ë¶€ë¶„ ì˜ ì‘ë™ |\n",
    "| ì´ë¯¸ì§€ ë¶„ë¥˜ ìµœê³  ì„±ëŠ¥ | **SGD + Momentum** | ë…¼ë¬¸ ê²°ê³¼ ëŒ€ë¶€ë¶„ ì´ê±¸ë¡œ ë‹¬ì„± |\n",
    "| RNN/ì‹œê³„ì—´ | **RMSprop** | ë¹„ì •ìƒ ê¸°ìš¸ê¸°ì— ê°•ê±´ |\n",
    "| Transformer ê³„ì—´ | **AdamW** | ê°€ì¤‘ì¹˜ ê°ì‡  í¬í•¨ |\n",
    "\n",
    "#### ğŸ“‰ í•™ìŠµë¥ (Learning Rate)ì´ ì™œ ì¤‘ìš”í•´ìš”?\n",
    "\n",
    "```\n",
    "ë„ˆë¬´ í° lr: ğŸ¸ ê±´ë„ˆë›°ì–´ ì™”ë‹¤ê°”ë‹¤ â†’ ë°œì‚°!\n",
    "ë„ˆë¬´ ì‘ì€ lr: ğŸŒ ì•„ì£¼ ì²œì²œíˆ â†’ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼!\n",
    "ë”± ì¢‹ì€ lr: ğŸš¶ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´ âœ“\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **ì‹¤ë¬´ íŒ**: Adam ê¸°ë³¸ê°’ `0.001`ì´ ì¢‹ì€ ì¶œë°œì !\n",
    "> í›ˆë ¨ì´ ì˜ ì•ˆë˜ë©´ 10ë°°ì”© ì˜¬ë¦¬ê±°ë‚˜ ë‚´ë¦¬ë©´ì„œ ì°¾ì•„ë´ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ì˜µí‹°ë§ˆì´ì € ìƒì„± ë° ê¸°ë³¸ ì‚¬ìš©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ì£¼ìš” ì˜µí‹°ë§ˆì´ì € ìƒì„±\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# SGD: ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜µí‹°ë§ˆì´ì €\n",
    "sgd = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.9,       # ëª¨ë©˜í…€ ì ìš© (ê¸°ë³¸ê°’ 0.0)\n",
    "    nesterov=True       # Nesterov ëª¨ë©˜í…€ ì‚¬ìš© ì—¬ë¶€\n",
    ")\n",
    "\n",
    "# Adam: ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì˜µí‹°ë§ˆì´ì €\n",
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001,\n",
    "    beta_1=0.9,         # 1ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ìœ¨\n",
    "    beta_2=0.999,       # 2ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ìœ¨\n",
    "    epsilon=1e-7        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’\n",
    ")\n",
    "\n",
    "# RMSprop: ìˆœí™˜ ì‹ ê²½ë§ì— ìì£¼ ì‚¬ìš©\n",
    "rmsprop = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    rho=0.9,            # ì´ë™ í‰ê·  ê°ì‡ ìœ¨\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-7\n",
    ")\n",
    "\n",
    "# AdamW: Adam + Weight Decay (L2 ì •ê·œí™”ì™€ ë‹¤ë¦„, ë””ì»¤í”Œëœ ê°€ì¤‘ì¹˜ ê°ì‡ )\n",
    "adamw = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.004  # ê°€ì¤‘ì¹˜ ê°ì‡  ê³„ìˆ˜\n",
    ")\n",
    "\n",
    "print(\"ìƒì„±ëœ ì˜µí‹°ë§ˆì´ì €:\")\n",
    "for opt in [sgd, adam, rmsprop, adamw]:\n",
    "    print(f\"  {opt.__class__.__name__}: lr={opt.learning_rate.numpy():.4f}\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸ ì ìš© ì˜ˆì‹œ\n",
    "# ---------------------------------------------------\n",
    "print(\"\\n=== ìˆ˜ë™ ê·¸ë˜ë””ì–¸íŠ¸ ì ìš© ===\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ë³€ìˆ˜\n",
    "w = tf.Variable(3.0, name='w')\n",
    "b = tf.Variable(0.0, name='b')\n",
    "\n",
    "# ëª©í‘œ: w=2.0, b=1.0 ì°¾ê¸°\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = w * 2.0 + b\n",
    "    loss = (y_pred - 5.0) ** 2  # ëª©í‘œê°’ 5.0 (= 2*2 + 1)\n",
    "\n",
    "gradients = tape.gradient(loss, [w, b])\n",
    "adam.apply_gradients(zip(gradients, [w, b]))\n",
    "\n",
    "print(f\"ì—…ë°ì´íŠ¸ ì „: w=3.0, b=0.0\")\n",
    "print(f\"ì—…ë°ì´íŠ¸ í›„: w={w.numpy():.4f}, b={b.numpy():.4f}\")\n",
    "print(f\"ê·¸ë˜ë””ì–¸íŠ¸: dw={gradients[0].numpy():.4f}, db={gradients[1].numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í•™ìŠµë¥ ì˜ ì˜í–¥\n",
    "\n",
    "í•™ìŠµë¥ ì€ ëª¨ë¸ í•™ìŠµì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ì´ë‹¤.\n",
    "- **ë„ˆë¬´ í° í•™ìŠµë¥ **: ë°œì‚° (lossê°€ í­ë°œì ìœ¼ë¡œ ì¦ê°€)\n",
    "- **ë„ˆë¬´ ì‘ì€ í•™ìŠµë¥ **: ìˆ˜ë ´ ì†ë„ê°€ ëŠë¦¼ (í•™ìŠµ ì‹œê°„ ì¦ê°€)\n",
    "- **ì ì ˆí•œ í•™ìŠµë¥ **: ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ìˆ˜ë ´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# í•™ìŠµë¥  í¬ê¸°ì— ë”°ë¥¸ í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def train_with_lr(learning_rate, epochs=50):\n",
    "    \"\"\"ì£¼ì–´ì§„ í•™ìŠµë¥ ë¡œ ê°„ë‹¨í•œ íšŒê·€ ë¬¸ì œ í•™ìŠµ\"\"\"\n",
    "    # ê°„ë‹¨í•œ ë°ì´í„° ìƒì„±: y = 2x + 1\n",
    "    np.random.seed(42)\n",
    "    X = np.random.randn(100, 1).astype(np.float32)\n",
    "    y = 2 * X + 1 + 0.1 * np.random.randn(100, 1).astype(np.float32)\n",
    "    \n",
    "    # ê°„ë‹¨í•œ ì„ í˜• ëª¨ë¸\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(1, input_shape=(1,))\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "        loss='mse'\n",
    "    )\n",
    "    \n",
    "    history = model.fit(X, y, epochs=epochs, verbose=0, batch_size=32)\n",
    "    return history.history['loss']\n",
    "\n",
    "# ì„¸ ê°€ì§€ í•™ìŠµë¥  ë¹„êµ\n",
    "lr_configs = {\n",
    "    'lr=0.5 (ë„ˆë¬´ í¼)': 0.5,\n",
    "    'lr=0.01 (ì ì ˆ)': 0.01,\n",
    "    'lr=0.0001 (ë„ˆë¬´ ì‘ìŒ)': 0.0001,\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "for label, lr in lr_configs.items():\n",
    "    losses = train_with_lr(lr)\n",
    "    # NaNì´ë‚˜ inf ë°©ì§€ë¥¼ ìœ„í•´ í´ë¦¬í•‘\n",
    "    losses = [min(l, 50) if not np.isnan(l) else 50 for l in losses]\n",
    "    plt.plot(losses, label=label, linewidth=2)\n",
    "\n",
    "plt.xlabel('ì—í¬í¬')\n",
    "plt.ylabel('MSE ì†ì‹¤')\n",
    "plt.title('í•™ìŠµë¥  í¬ê¸°ì— ë”°ë¥¸ ìˆ˜ë ´ ë¹„êµ (SGD)')\n",
    "plt.legend()\n",
    "plt.ylim(0, 20)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"í•™ìŠµë¥  0.5: ë°œì‚° ê°€ëŠ¥ì„± ë†’ìŒ\")\n",
    "print(\"í•™ìŠµë¥  0.01: ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ìˆ˜ë ´\")\n",
    "print(\"í•™ìŠµë¥  0.0001: ìˆ˜ë ´í•˜ì§€ë§Œ ë§¤ìš° ëŠë¦¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Scheduler\n",
    "\n",
    "í•™ìŠµ ì´ˆê¸°ì—ëŠ” í° í•™ìŠµë¥ ë¡œ ë¹ ë¥´ê²Œ ì§„í–‰í•˜ê³ , í›„ë°˜ë¶€ì—ëŠ” ì‘ì€ í•™ìŠµë¥ ë¡œ ì„¸ë°€í•˜ê²Œ ìˆ˜ë ´ì‹œí‚¤ëŠ” ì „ëµì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Learning Rate Scheduler ì¢…ë¥˜ì™€ ì‹œê°í™”\n",
    "# ---------------------------------------------------\n",
    "\n",
    "steps = np.arange(0, 1000)\n",
    "\n",
    "# 1. ExponentialDecay: lr = initial_lr * decay_rate ^ (step / decay_steps)\n",
    "exp_decay = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=200,    # 200 ìŠ¤í…ë§ˆë‹¤ ê°ì‡ \n",
    "    decay_rate=0.5,     # 50%ì”© ê°ì†Œ\n",
    "    staircase=False     # ì—°ì†ì  ê°ì‡  (Trueë©´ ê³„ë‹¨í˜•)\n",
    ")\n",
    "\n",
    "# 2. CosineDecay: ì½”ì‚¬ì¸ í•¨ìˆ˜ í˜•íƒœë¡œ í•™ìŠµë¥  ê°ì†Œ\n",
    "cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000,\n",
    "    alpha=0.0           # ìµœì†Œ í•™ìŠµë¥  ë¹„ìœ¨ (0 = ì™„ì „íˆ 0ê¹Œì§€ ê°ì†Œ)\n",
    ")\n",
    "\n",
    "# 3. CosineDecayRestarts: ì£¼ê¸°ì ìœ¼ë¡œ í•™ìŠµë¥ ì„ ë¦¬ì…‹ (ì›œ ë¦¬ìŠ¤íƒ€íŠ¸)\n",
    "cosine_restarts = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate=0.1,\n",
    "    first_decay_steps=200,  # ì²« ë²ˆì§¸ ì‚¬ì´í´ ê¸¸ì´\n",
    "    t_mul=2.0,              # ê° ì‚¬ì´í´ë§ˆë‹¤ ê¸¸ì´ 2ë°°\n",
    "    m_mul=0.9               # ê° ì‚¬ì´í´ë§ˆë‹¤ ì´ˆê¸° lr 90%\n",
    ")\n",
    "\n",
    "# í•™ìŠµë¥  ê°’ ê³„ì‚°\n",
    "exp_lrs = [exp_decay(s).numpy() for s in steps]\n",
    "cos_lrs = [cosine_decay(s).numpy() for s in steps]\n",
    "cos_restart_lrs = [cosine_restarts(s).numpy() for s in steps]\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(steps, exp_lrs, color='blue', linewidth=2)\n",
    "axes[0].set_title('ExponentialDecay')\n",
    "axes[0].set_xlabel('í•™ìŠµ ìŠ¤í…')\n",
    "axes[0].set_ylabel('í•™ìŠµë¥ ')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(steps, cos_lrs, color='green', linewidth=2)\n",
    "axes[1].set_title('CosineDecay')\n",
    "axes[1].set_xlabel('í•™ìŠµ ìŠ¤í…')\n",
    "axes[1].set_ylabel('í•™ìŠµë¥ ')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].plot(steps, cos_restart_lrs, color='red', linewidth=2)\n",
    "axes[2].set_title('CosineDecayRestarts (Warm Restarts)')\n",
    "axes[2].set_xlabel('í•™ìŠµ ìŠ¤í…')\n",
    "axes[2].set_ylabel('í•™ìŠµë¥ ')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì €ì— ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ì˜ˆì‹œ\n",
    "print(\"=== ì˜µí‹°ë§ˆì´ì €ì— ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš© ===\")\n",
    "print(\"\"\"\n",
    "# ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì˜µí‹°ë§ˆì´ì €ì˜ learning_rateì— ì§ì ‘ ì „ë‹¬\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ReduceLROnPlateau ì½œë°±\n",
    "\n",
    "ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¤„ì´ëŠ” ì½œë°±ì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ReduceLROnPlateau ì½œë°± ì˜ˆì‹œ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# ê°„ë‹¨í•œ MNIST ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test  = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ì¼ë¶€ ë°ì´í„°ë§Œ ì‚¬ìš©\n",
    "X_small = X_train[:5000]\n",
    "y_small = y_train[:5000]\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"ê°„ë‹¨í•œ MLP ë¶„ë¥˜ ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model = build_model()\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),  # ì˜ë„ì ìœ¼ë¡œ í° lr\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ReduceLROnPlateau ì½œë°± ì„¤ì •\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # ëª¨ë‹ˆí„°í•  ì§€í‘œ\n",
    "    factor=0.5,           # í•™ìŠµë¥  ê°ì†Œ ë°°ìœ¨ (lr = lr * factor)\n",
    "    patience=3,           # ê°œì„  ì—†ì´ ê¸°ë‹¤ë¦´ ì—í¬í¬ ìˆ˜\n",
    "    min_lr=1e-6,          # ìµœì†Œ í•™ìŠµë¥  í•˜í•œì„ \n",
    "    min_delta=0.001,      # ê°œì„ ìœ¼ë¡œ ì¸ì •í•  ìµœì†Œ ë³€í™”ëŸ‰\n",
    "    verbose=1             # í•™ìŠµë¥  ë³€ê²½ ì‹œ ì¶œë ¥\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_small, y_small,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# í•™ìŠµë¥  ë³€í™” ì¶”ì  (ì½œë°± ë‚´ë¶€ì—ì„œ ê¸°ë¡)\n",
    "print(f\"\\nìµœì¢… í•™ìŠµë¥ : {model.optimizer.learning_rate.numpy():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì˜µí‹°ë§ˆì´ì € ìˆ˜ë ´ ì†ë„ ë¹„êµ\n",
    "\n",
    "ë™ì¼í•œ MNIST ëª¨ë¸ì— SGD, Adam, RMSpropì„ ì ìš©í•˜ê³  ìˆ˜ë ´ ì†ë„ë¥¼ ë¹„êµí•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# SGD / Adam / RMSprop ìˆ˜ë ´ ì†ë„ ë¹„êµ\n",
    "# ---------------------------------------------------\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# ê° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "optimizers_config = {\n",
    "    'SGD (lr=0.01)': tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    'Adam (lr=0.001)': tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    'RMSprop (lr=0.001)': tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers_config.items():\n",
    "    print(f\"\\ní•™ìŠµ ì¤‘: {name}\")\n",
    "    \n",
    "    # ë™ì¼í•œ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ ì‹œë“œ ê³ ì • í›„ ëª¨ë¸ ì¬ìƒì„±\n",
    "    tf.random.set_seed(42)\n",
    "    model = build_model()\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_small, y_small,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.2,\n",
    "        verbose=0\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    histories[name] = history.history\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"  ì™„ë£Œ - ìµœì¢… ê²€ì¦ ì •í™•ë„: {final_val_acc:.4f}, ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ\")\n",
    "\n",
    "# ë¹„êµ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for (name, history), color in zip(histories.items(), colors):\n",
    "    epochs_range = range(1, EPOCHS + 1)\n",
    "    \n",
    "    axes[0].plot(epochs_range, history['loss'], label=name, color=color, linewidth=2)\n",
    "    axes[1].plot(epochs_range, history['val_accuracy'], label=name, color=color, linewidth=2)\n",
    "\n",
    "axes[0].set_xlabel('ì—í¬í¬')\n",
    "axes[0].set_ylabel('í›ˆë ¨ ì†ì‹¤')\n",
    "axes[0].set_title('ì˜µí‹°ë§ˆì´ì €ë³„ í›ˆë ¨ ì†ì‹¤ ë¹„êµ')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('ì—í¬í¬')\n",
    "axes[1].set_ylabel('ê²€ì¦ ì •í™•ë„')\n",
    "axes[1].set_title('ì˜µí‹°ë§ˆì´ì €ë³„ ê²€ì¦ ì •í™•ë„ ë¹„êµ')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì •ë¦¬\n",
    "\n",
    "### ì˜µí‹°ë§ˆì´ì € ì„ íƒ ê°€ì´ë“œ\n",
    "\n",
    "| ì˜µí‹°ë§ˆì´ì € | ì¥ì  | ë‹¨ì  | ê¶Œì¥ ì‚¬ìš© ìƒí™© |\n",
    "|-----------|------|------|---------------|\n",
    "| SGD + Momentum | ì¼ë°˜í™” ì„±ëŠ¥ ì¢‹ìŒ | í•™ìŠµë¥  íŠœë‹ í•„ìš” | ì´ë¯¸ì§€ ë¶„ë¥˜, ìµœì¢… ì„±ëŠ¥ ì¤‘ìš” ì‹œ |\n",
    "| Adam | ë¹ ë¥¸ ìˆ˜ë ´, í•™ìŠµë¥  íŠœë‹ ëœ í•„ìš” | ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ ê³¼ì í•© ê°€ëŠ¥ | ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ê¸°ë³¸ ì„ íƒ |\n",
    "| RMSprop | ë¹„ì •ìƒ ë°ì´í„°ì— ê°•ê±´ | Adamë³´ë‹¤ ìˆ˜ë ´ ëŠë¦´ ìˆ˜ ìˆìŒ | ìˆœí™˜ ì‹ ê²½ë§(RNN) |\n",
    "| AdamW | Adam + ë” ë‚˜ì€ ì •ê·œí™” | ì¶”ê°€ í•˜ì´í¼íŒŒë¼ë¯¸í„° | Transformer ê¸°ë°˜ ëª¨ë¸ |\n",
    "\n",
    "### Learning Rate Scheduler ì„ íƒ\n",
    "\n",
    "| ìŠ¤ì¼€ì¤„ëŸ¬ | íŠ¹ì§• | ê¶Œì¥ ì‚¬ìš© ìƒí™© |\n",
    "|---------|------|---------------|\n",
    "| ExponentialDecay | ë‹¨ìˆœí•˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥ | ì¼ë°˜ì ì¸ ê²½ìš° |\n",
    "| CosineDecay | ë¶€ë“œëŸ¬ìš´ ê°ì†Œ | ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ íƒœìŠ¤í¬ |\n",
    "| CosineDecayRestarts | ì£¼ê¸°ì  ë¦¬ì…‹ìœ¼ë¡œ íƒˆì¶œ íš¨ê³¼ | ë¡œì»¬ ë¯¸ë‹ˆë©ˆ íƒˆì¶œ í•„ìš” ì‹œ |\n",
    "| ReduceLROnPlateau | ìë™ ì ì‘í˜• | ê²€ì¦ ì†ì‹¤ ì •ì²´ ì‹œ ìë™ ì¡°ì • í•„ìš” |\n",
    "\n",
    "### í•µì‹¬ ì •ë¦¬\n",
    "- í•™ìŠµë¥ ì€ ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì´ë‹¤. Adamì˜ ê¸°ë³¸ê°’ `0.001`ì´ ì¢‹ì€ ì¶œë°œì ì´ë‹¤\n",
    "- Adamì€ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ, SGD+Momentumì´ ìµœì¢… ì„±ëŠ¥ì—ì„œ ìœ ë¦¬í•  ìˆ˜ ìˆë‹¤\n",
    "- Learning Rate Schedulerë¥¼ ì‚¬ìš©í•˜ë©´ ì¶”ê°€ ë¹„ìš© ì—†ì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}