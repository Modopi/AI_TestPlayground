{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03-04: 콜백 (Callbacks)\n",
    "\n",
    "## 학습 목표\n",
    "- 콜백의 개념과 동작 시점을 이해한다\n",
    "- ModelCheckpoint, EarlyStopping, ReduceLROnPlateau 등 주요 내장 콜백을 활용할 수 있다\n",
    "- TensorBoard 콜백으로 학습 과정을 시각화할 수 있다\n",
    "- `tf.keras.callbacks.Callback`을 상속하여 커스텀 콜백을 구현할 수 있다\n",
    "\n",
    "## 목차\n",
    "1. [콜백이란?](#1.-콜백이란?)\n",
    "2. [ModelCheckpoint](#2.-ModelCheckpoint)\n",
    "3. [EarlyStopping](#3.-EarlyStopping)\n",
    "4. [ReduceLROnPlateau](#4.-ReduceLROnPlateau)\n",
    "5. [TensorBoard](#5.-TensorBoard)\n",
    "6. [커스텀 콜백](#6.-커스텀-콜백)\n",
    "7. [정리](#7.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "print(\"TensorFlow 버전:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 콜백이란?\n",
    "\n",
    "콜백(Callback)은 **학습 중 특정 시점에 자동으로 호출되는 함수(또는 객체)**이다.\n",
    "\n",
    "### 콜백이 호출되는 시점\n",
    "\n",
    "| 메서드 | 호출 시점 |\n",
    "|--------|----------|\n",
    "| `on_train_begin` | 전체 학습 시작 전 |\n",
    "| `on_train_end` | 전체 학습 종료 후 |\n",
    "| `on_epoch_begin` | 각 에포크 시작 전 |\n",
    "| `on_epoch_end` | 각 에포크 종료 후 |\n",
    "| `on_train_batch_begin` | 각 훈련 배치 시작 전 |\n",
    "| `on_train_batch_end` | 각 훈련 배치 종료 후 |\n",
    "| `on_test_begin` | 평가(evaluate) 시작 전 |\n",
    "| `on_predict_begin` | 예측(predict) 시작 전 |\n",
    "\n",
    "```python\n",
    "# 콜백 사용 방법: model.fit의 callbacks 인수에 리스트로 전달\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    callbacks=[callback1, callback2, callback3]  # 복수의 콜백을 리스트로 전달\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 공통 데이터 및 모델 준비\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test  = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# 빠른 실험용 서브셋\n",
    "X_tr = X_train[:6000]\n",
    "y_tr = y_train[:6000]\n",
    "X_val = X_train[6000:8000]\n",
    "y_val = y_train[6000:8000]\n",
    "\n",
    "def build_model(lr=0.001):\n",
    "    \"\"\"실험용 간단한 MLP 모델 생성\"\"\"\n",
    "    tf.random.set_seed(42)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"데이터 준비 완료\")\n",
    "print(f\"  훈련: {X_tr.shape}, 검증: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ModelCheckpoint\n",
    "\n",
    "학습 중 가장 좋은 모델 또는 주기적으로 체크포인트를 자동 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ModelCheckpoint: 최고 성능 모델 자동 저장\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 저장 경로 설정\n",
    "checkpoint_dir = './checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ModelCheckpoint 콜백 설정\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'best_model.keras'),  # 저장 경로\n",
    "    monitor='val_accuracy',   # 모니터할 지표\n",
    "    save_best_only=True,      # True: 최고 성능 모델만 저장 (기본: 매 에포크)\n",
    "    save_weights_only=False,  # False: 전체 모델 저장 (True: 가중치만)\n",
    "    mode='max',               # 'max': 클수록 좋음 (accuracy), 'min': 작을수록 좋음 (loss)\n",
    "    verbose=1                 # 저장 시 출력\n",
    ")\n",
    "\n",
    "# 에포크별 저장 (파일명에 에포크 번호와 지표 포함)\n",
    "checkpoint_all = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'epoch_{epoch:02d}_val_acc_{val_accuracy:.4f}.keras'),\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=False,     # 모든 에포크 저장\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== 저장된 체크포인트 ===\")\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    for f in os.listdir(checkpoint_dir):\n",
    "        print(f\"  {f}\")\n",
    "\n",
    "# 저장된 최고 모델 로드\n",
    "print(\"\\n=== 저장된 최고 모델 로드 ===\")\n",
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.keras')\n",
    "if os.path.exists(best_model_path):\n",
    "    loaded_model = tf.keras.models.load_model(best_model_path)\n",
    "    test_loss, test_acc = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"로드된 최고 모델 - 테스트 정확도: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EarlyStopping\n",
    "\n",
    "검증 성능이 개선되지 않을 때 학습을 조기 종료하여 과적합을 방지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# EarlyStopping: 조기 종료\n",
    "# ---------------------------------------------------\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',           # 모니터할 지표\n",
    "    patience=5,                   # 개선 없이 기다릴 에포크 수\n",
    "    min_delta=0.001,              # 개선으로 인정할 최소 변화량\n",
    "    restore_best_weights=True,    # 가장 좋은 가중치로 복원 (중요!)\n",
    "    mode='min',                   # 'min': 손실이 감소해야 개선\n",
    "    baseline=None,                # 기준값 (None: 없음)\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ModelCheckpoint와 함께 사용 (권장 패턴)\n",
    "checkpoint_best = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=os.path.join(checkpoint_dir, 'early_stop_best.keras'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "model_es = build_model(lr=0.001)\n",
    "print(\"EarlyStopping patience=5로 최대 30 에포크 학습 시도\")\n",
    "history_es = model_es.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=30,                          # 최대 에포크 (조기 종료 가능)\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, checkpoint_best],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "actual_epochs = len(history_es.history['loss'])\n",
    "print(f\"\\n실제 학습 에포크 수: {actual_epochs} / 30\")\n",
    "print(f\"최고 검증 손실: {min(history_es.history['val_loss']):.4f}\")\n",
    "print(f\"최종 검증 정확도: {history_es.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"restore_best_weights=True이므로 조기 종료 시점이 아닌 최고 성능 시점의 가중치 사용\")\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "epochs_range = range(1, actual_epochs + 1)\n",
    "\n",
    "axes[0].plot(epochs_range, history_es.history['loss'], label='훈련 손실')\n",
    "axes[0].plot(epochs_range, history_es.history['val_loss'], label='검증 손실')\n",
    "axes[0].axvline(x=actual_epochs, color='red', linestyle='--', label='조기 종료 시점')\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].set_ylabel('손실')\n",
    "axes[0].set_title('EarlyStopping - 손실 곡선')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs_range, history_es.history['accuracy'], label='훈련 정확도')\n",
    "axes[1].plot(epochs_range, history_es.history['val_accuracy'], label='검증 정확도')\n",
    "axes[1].set_xlabel('에포크')\n",
    "axes[1].set_ylabel('정확도')\n",
    "axes[1].set_title('EarlyStopping - 정확도 곡선')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# ReduceLROnPlateau: 학습률 자동 감소\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 학습률을 기록하는 도우미 콜백\n",
    "class LRRecorder(tf.keras.callbacks.Callback):\n",
    "    \"\"\"에포크별 학습률을 기록하는 보조 콜백\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr_history = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate)\n",
    "        self.lr_history.append(lr)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',   # 모니터 지표\n",
    "    factor=0.5,           # 학습률 감소 배율: new_lr = lr * factor\n",
    "    patience=3,           # 개선 없이 기다릴 에포크\n",
    "    min_lr=1e-6,          # 학습률 하한선\n",
    "    min_delta=0.001,      # 최소 개선량\n",
    "    cooldown=0,           # 학습률 감소 후 쉬어가는 에포크 수\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lr_recorder = LRRecorder()\n",
    "\n",
    "# 의도적으로 큰 학습률로 시작하여 감소 과정 관찰\n",
    "model_rlr = build_model(lr=0.01)\n",
    "history_rlr = model_rlr.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[reduce_lr, lr_recorder],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 학습률 변화 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "epochs_range = range(1, len(history_rlr.history['loss']) + 1)\n",
    "\n",
    "axes[0].semilogy(epochs_range, lr_recorder.lr_history, 'bo-', linewidth=2)\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].set_ylabel('학습률 (로그 스케일)')\n",
    "axes[0].set_title('ReduceLROnPlateau - 학습률 변화')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs_range, history_rlr.history['val_loss'], 'r-', linewidth=2)\n",
    "axes[1].set_xlabel('에포크')\n",
    "axes[1].set_ylabel('검증 손실')\n",
    "axes[1].set_title('검증 손실 변화')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"초기 학습률: {lr_recorder.lr_history[0]:.6f}\")\n",
    "print(f\"최종 학습률: {lr_recorder.lr_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TensorBoard\n",
    "\n",
    "TensorBoard는 학습 과정을 실시간으로 시각화하는 강력한 도구이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# TensorBoard 콜백 설정\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 타임스탬프를 포함한 로그 디렉토리 생성 (실험별 구분)\n",
    "log_dir = os.path.join(\n",
    "    './logs',\n",
    "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,              # 로그 저장 경로\n",
    "    histogram_freq=1,             # N 에포크마다 가중치/편향 히스토그램 기록 (0=비활성)\n",
    "    write_graph=True,             # 모델 그래프 기록\n",
    "    write_images=False,           # 가중치를 이미지로 기록 여부\n",
    "    update_freq='epoch',          # 'epoch' 또는 'batch' 또는 정수\n",
    "    profile_batch=0               # 성능 프로파일링 배치 (0=비활성)\n",
    ")\n",
    "\n",
    "model_tb = build_model()\n",
    "print(f\"TensorBoard 로그 경로: {log_dir}\")\n",
    "print()\n",
    "\n",
    "model_tb.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[tensorboard_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n=== TensorBoard 실행 방법 ===\")\n",
    "print(f\"터미널에서 다음 명령어를 실행:\")\n",
    "print(f\"  tensorboard --logdir=./logs\")\n",
    "print(f\"브라우저에서 http://localhost:6006 접속\")\n",
    "print()\n",
    "print(\"Jupyter Notebook에서 직접 실행:\")\n",
    "print(\"  %load_ext tensorboard\")\n",
    "print(\"  %tensorboard --logdir ./logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 커스텀 콜백\n",
    "\n",
    "`tf.keras.callbacks.Callback`을 상속하여 원하는 동작을 구현할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 커스텀 콜백: 에포크별 학습률 및 메트릭 출력\n",
    "# ---------------------------------------------------\n",
    "\n",
    "class DetailedLoggingCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"에포크 종료 시 학습률과 주요 메트릭을 상세하게 출력하는 콜백\"\"\"\n",
    "    \n",
    "    def __init__(self, print_every=1):\n",
    "        super().__init__()\n",
    "        self.print_every = print_every  # N 에포크마다 출력\n",
    "        self.lr_history = []           # 학습률 기록\n",
    "        self.metrics_history = {}      # 메트릭 기록\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"학습 시작 시 헤더 출력\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"학습 시작\")\n",
    "        print(f\"옵티마이저: {self.model.optimizer.__class__.__name__}\")\n",
    "        print(f\"초기 학습률: {float(self.model.optimizer.learning_rate):.6f}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        \"\"\"에포크 시작 전 현재 학습률 출력\"\"\"\n",
    "        current_lr = float(self.model.optimizer.learning_rate)\n",
    "        self.lr_history.append(current_lr)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"에포크 종료 후 상세 로그 출력\"\"\"\n",
    "        logs = logs or {}\n",
    "        \n",
    "        # 메트릭 기록\n",
    "        for key, value in logs.items():\n",
    "            if key not in self.metrics_history:\n",
    "                self.metrics_history[key] = []\n",
    "            self.metrics_history[key].append(value)\n",
    "        \n",
    "        # N 에포크마다 출력\n",
    "        if (epoch + 1) % self.print_every == 0:\n",
    "            current_lr = float(self.model.optimizer.learning_rate)\n",
    "            loss = logs.get('loss', 0)\n",
    "            val_loss = logs.get('val_loss', 0)\n",
    "            acc = logs.get('accuracy', 0)\n",
    "            val_acc = logs.get('val_accuracy', 0)\n",
    "            \n",
    "            print(f\"[에포크 {epoch+1:3d}] \"\n",
    "                  f\"lr={current_lr:.6f} | \"\n",
    "                  f\"loss={loss:.4f} | \"\n",
    "                  f\"val_loss={val_loss:.4f} | \"\n",
    "                  f\"acc={acc:.4f} | \"\n",
    "                  f\"val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        \"\"\"학습 완료 후 요약 출력\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"학습 완료\")\n",
    "        if 'val_accuracy' in self.metrics_history:\n",
    "            best_epoch = np.argmax(self.metrics_history['val_accuracy']) + 1\n",
    "            best_val_acc = max(self.metrics_history['val_accuracy'])\n",
    "            print(f\"최고 검증 정확도: {best_val_acc:.4f} (에포크 {best_epoch})\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# 커스텀 콜백 사용\n",
    "detailed_logger = DetailedLoggingCallback(print_every=2)\n",
    "reduce_lr2 = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=3, verbose=0\n",
    ")\n",
    "\n",
    "model_custom = build_model(lr=0.005)\n",
    "model_custom.fit(\n",
    "    X_tr, y_tr,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[detailed_logger, reduce_lr2],\n",
    "    verbose=0  # 기본 출력 비활성화\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 정리\n",
    "\n",
    "### 콜백 사용 패턴 가이드\n",
    "\n",
    "#### 기본 권장 조합\n",
    "```python\n",
    "callbacks = [\n",
    "    # 최고 모델 저장\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    # 과적합 방지 조기 종료\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    # 학습률 자동 감소\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    ),\n",
    "    # 시각화\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "```\n",
    "\n",
    "### 콜백별 핵심 파라미터 요약\n",
    "\n",
    "| 콜백 | 핵심 파라미터 | 설명 |\n",
    "|------|--------------|------|\n",
    "| ModelCheckpoint | `save_best_only`, `monitor` | 최고 모델만 저장 vs 모든 에포크 저장 |\n",
    "| EarlyStopping | `patience`, `restore_best_weights` | 조기 종료 후 최고 가중치 복원 |\n",
    "| ReduceLROnPlateau | `factor`, `patience`, `min_lr` | 학습률 감소 배율과 최소 하한선 |\n",
    "| TensorBoard | `histogram_freq`, `log_dir` | 히스토그램 기록 주기 |\n",
    "\n",
    "### 커스텀 콜백 구현 포인트\n",
    "- 적절한 메서드(`on_epoch_end`, `on_batch_end` 등)를 오버라이드\n",
    "- `self.model`로 현재 모델에 접근 가능\n",
    "- `logs` 딕셔너리에서 현재 메트릭 값 참조 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
