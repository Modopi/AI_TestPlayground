{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03 실습 1: Optimizer 비교\n",
    "\n",
    "## 목표\n",
    "동일한 Fashion MNIST 분류 모델에 SGD, Adam, RMSprop을 적용하고 수렴 속도를 비교한다.\n",
    "\n",
    "## 실습 내용\n",
    "- 동일한 가중치 초기화로 3개의 동일한 모델 생성\n",
    "- 각 옵티마이저로 동일한 에포크 동안 학습\n",
    "- 훈련 손실, 검증 손실, 검증 정확도를 한 그래프에 비교\n",
    "- 결과를 분석하고 각 옵티마이저의 특성을 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# 상위 디렉토리 경로 추가 (공통 유틸 사용 시)\n",
    "sys.path.append('..')\n",
    "\n",
    "print(\"TensorFlow 버전:\", tf.__version__)\n",
    "print(\"NumPy 버전:\", np.__version__)\n",
    "\n",
    "# 재현성을 위한 시드 고정\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실험 설계\n",
    "\n",
    "공정한 비교를 위해 다음 조건을 통일한다:\n",
    "\n",
    "| 항목 | 설정값 |\n",
    "|------|--------|\n",
    "| 데이터셋 | Fashion MNIST |\n",
    "| 모델 구조 | Dense(256, relu) -> Dropout(0.3) -> Dense(128, relu) -> Dense(10, softmax) |\n",
    "| 가중치 초기화 | 동일한 시드로 동일하게 초기화 |\n",
    "| 에포크 수 | 20 |\n",
    "| 배치 크기 | 128 |\n",
    "| 손실 함수 | SparseCategoricalCrossentropy |\n",
    "| **변수** | **옵티마이저** (SGD, Adam, RMSprop) |\n",
    "\n",
    "각 옵티마이저의 학습률:\n",
    "- SGD: lr=0.01 (Momentum=0.9)\n",
    "- Adam: lr=0.001 (기본값)\n",
    "- RMSprop: lr=0.001 (기본값)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 데이터 준비: Fashion MNIST\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Fashion MNIST 로드\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# 정규화: [0, 255] -> [0, 1]\n",
    "X_train_full = X_train_full.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# 훈련/검증 분리\n",
    "X_train, X_val = X_train_full[:50000], X_train_full[50000:]\n",
    "y_train, y_val = y_train_full[:50000], y_train_full[50000:]\n",
    "\n",
    "# 클래스 레이블\n",
    "CLASS_NAMES = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "print(\"데이터셋 정보:\")\n",
    "print(f\"  훈련 셋: {X_train.shape} | 레이블: {y_train.shape}\")\n",
    "print(f\"  검증 셋: {X_val.shape}   | 레이블: {y_val.shape}\")\n",
    "print(f\"  테스트 셋: {X_test.shape} | 레이블: {y_test.shape}\")\n",
    "print(f\"  클래스 수: {len(CLASS_NAMES)}\")\n",
    "print(f\"  클래스: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 동일 구조 모델 생성 함수 (동일한 가중치 초기화)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def create_model(optimizer, seed=42):\n",
    "    \"\"\"동일한 구조와 초기화로 모델을 생성하고 컴파일한다.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: tf.keras.optimizers 인스턴스\n",
    "        seed: 가중치 초기화 시드 (동일한 초기화 보장)\n",
    "    \n",
    "    Returns:\n",
    "        컴파일된 tf.keras.Model\n",
    "    \"\"\"\n",
    "    # 시드를 고정하여 동일한 초기 가중치 보장\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # 동일한 initializer 사용\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=seed)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(\n",
    "            256, activation='relu',\n",
    "            input_shape=(784,),\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        tf.keras.layers.Dropout(0.3, seed=seed),\n",
    "        tf.keras.layers.Dense(\n",
    "            128, activation='relu',\n",
    "            kernel_initializer=initializer\n",
    "        ),\n",
    "        tf.keras.layers.Dense(\n",
    "            10, activation='softmax',\n",
    "            kernel_initializer=initializer\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# 실험할 옵티마이저 설정\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "optimizer_configs = {\n",
    "    'SGD (lr=0.01, momentum=0.9)': tf.keras.optimizers.SGD(\n",
    "        learning_rate=0.01, momentum=0.9\n",
    "    ),\n",
    "    'Adam (lr=0.001)': tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.001\n",
    "    ),\n",
    "    'RMSprop (lr=0.001)': tf.keras.optimizers.RMSprop(\n",
    "        learning_rate=0.001\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"실험 설정:\")\n",
    "print(f\"  에포크: {EPOCHS}\")\n",
    "print(f\"  배치 크기: {BATCH_SIZE}\")\n",
    "print(f\"  비교할 옵티마이저: {list(optimizer_configs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 각 옵티마이저로 학습 실행\n",
    "# ---------------------------------------------------\n",
    "\n",
    "all_histories = {}\n",
    "training_times = {}\n",
    "\n",
    "for opt_name, optimizer in optimizer_configs.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"학습 중: {opt_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # 동일한 초기 가중치로 새 모델 생성\n",
    "    model = create_model(optimizer, seed=SEED)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=0  # 진행 상황 숨김\n",
    "    )\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    all_histories[opt_name] = history.history\n",
    "    training_times[opt_name] = elapsed\n",
    "    \n",
    "    # 결과 요약\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "    \n",
    "    print(f\"  학습 시간: {elapsed:.1f}초\")\n",
    "    print(f\"  최종 검증 정확도: {final_val_acc:.4f}\")\n",
    "    print(f\"  최고 검증 정확도: {best_val_acc:.4f} (에포크 {best_epoch})\")\n",
    "\n",
    "print(\"\\n모든 옵티마이저 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 학습 곡선 비교 시각화\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 색상 설정\n",
    "colors = {\n",
    "    'SGD (lr=0.01, momentum=0.9)': 'blue',\n",
    "    'Adam (lr=0.001)': 'red',\n",
    "    'RMSprop (lr=0.001)': 'green',\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# 1. 훈련 손실\n",
    "ax = axes[0, 0]\n",
    "for name, history in all_histories.items():\n",
    "    ax.plot(epochs_range, history['loss'], \n",
    "            color=colors[name], linewidth=2, label=name)\n",
    "ax.set_xlabel('에포크')\n",
    "ax.set_ylabel('손실')\n",
    "ax.set_title('훈련 손실 비교')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 검증 손실\n",
    "ax = axes[0, 1]\n",
    "for name, history in all_histories.items():\n",
    "    ax.plot(epochs_range, history['val_loss'],\n",
    "            color=colors[name], linewidth=2, label=name)\n",
    "ax.set_xlabel('에포크')\n",
    "ax.set_ylabel('손실')\n",
    "ax.set_title('검증 손실 비교')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 훈련 정확도\n",
    "ax = axes[1, 0]\n",
    "for name, history in all_histories.items():\n",
    "    ax.plot(epochs_range, history['accuracy'],\n",
    "            color=colors[name], linewidth=2, label=name)\n",
    "ax.set_xlabel('에포크')\n",
    "ax.set_ylabel('정확도')\n",
    "ax.set_title('훈련 정확도 비교')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 검증 정확도\n",
    "ax = axes[1, 1]\n",
    "for name, history in all_histories.items():\n",
    "    ax.plot(epochs_range, history['val_accuracy'],\n",
    "            color=colors[name], linewidth=2, label=name)\n",
    "ax.set_xlabel('에포크')\n",
    "ax.set_ylabel('정확도')\n",
    "ax.set_title('검증 정확도 비교')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Fashion MNIST - 옵티마이저별 학습 곡선 비교', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최종 결과 표\n",
    "print(\"\\n=== 최종 결과 비교 ===\")\n",
    "print(f\"{'옵티마이저':<35} {'최고 Val Acc':^15} {'최종 Val Acc':^15} {'학습 시간':^12}\")\n",
    "print(\"-\" * 80)\n",
    "for name, history in all_histories.items():\n",
    "    best_acc = max(history['val_accuracy'])\n",
    "    final_acc = history['val_accuracy'][-1]\n",
    "    t_time = training_times[name]\n",
    "    print(f\"{name:<35} {best_acc:^15.4f} {final_acc:^15.4f} {t_time:^12.1f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 해석 가이드\n",
    "\n",
    "### 일반적으로 기대되는 결과\n",
    "\n",
    "1. **Adam**: 초기 수렴이 가장 빠름. 적응적 학습률로 초기 몇 에포크 내에 빠르게 수렴한다.\n",
    "\n",
    "2. **RMSprop**: Adam과 비슷하게 빠른 수렴을 보이나, Adam보다 약간 안정성이 낮을 수 있다.\n",
    "\n",
    "3. **SGD + Momentum**: 초반에 느리지만, 많은 에포크를 학습할수록 Adam과 격차가 줄어들거나 역전될 수 있다. 잘 튜닝된 SGD가 최고 성능을 보이는 경우도 많다.\n",
    "\n",
    "### 그래프 해석 포인트\n",
    "\n",
    "- **손실 감소 속도**: 초기 에포크에서 어떤 옵티마이저가 더 빨리 수렴하는가?\n",
    "- **수렴 안정성**: 손실 곡선이 얼마나 smooth한가? 진동이 있는가?\n",
    "- **과적합**: 훈련 손실과 검증 손실의 차이가 크면 과적합 신호\n",
    "- **최종 성능**: 20 에포크 후 어떤 옵티마이저가 가장 높은 검증 정확도를 달성하는가?\n",
    "\n",
    "## 도전 과제\n",
    "\n",
    "1. **학습률 튜닝**: SGD의 학습률을 0.01, 0.05, 0.001로 변경하여 어떤 값이 최적인지 찾아보세요.\n",
    "\n",
    "2. **에포크 증가**: EPOCHS를 50으로 늘려서 더 오래 학습했을 때 결과가 어떻게 변하는지 확인하세요.\n",
    "\n",
    "3. **AdamW 추가**: `tf.keras.optimizers.AdamW`를 실험에 추가하고 Adam과 비교해보세요.\n",
    "\n",
    "4. **배치 크기 변경**: BATCH_SIZE를 32, 64, 256으로 변경하고 수렴 속도 차이를 관찰하세요.\n",
    "\n",
    "5. **Learning Rate Scheduler 적용**: ExponentialDecay를 SGD에 적용하여 Adam과의 격차를 줄여보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
