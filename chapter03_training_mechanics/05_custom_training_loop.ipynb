{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03-05: 커스텀 학습 루프 (Custom Training Loop)\n",
    "\n",
    "## 학습 목표\n",
    "- `model.fit()`이 내부적으로 수행하는 과정을 이해한다\n",
    "- `tf.GradientTape`를 사용하여 수동 학습 루프를 구현할 수 있다\n",
    "- `@tf.function` 데코레이터로 그래프 모드 컴파일을 적용하여 속도를 향상시킬 수 있다\n",
    "- `model.fit()`과 커스텀 루프의 장단점을 비교하고 적절한 상황을 선택할 수 있다\n",
    "\n",
    "## 목차\n",
    "1. [model.fit()이 내부적으로 하는 일](#1.-model.fit()이-내부적으로-하는-일)\n",
    "2. [기본 GradientTape 학습 루프](#2.-기본-GradientTape-학습-루프)\n",
    "3. [train_step / val_step 함수 분리](#3.-train_step-/-val_step-함수-분리)\n",
    "4. [@tf.function으로 성능 최적화](#4.-@tf.function으로-성능-최적화)\n",
    "5. [메트릭 초기화와 에포크 로그](#5.-메트릭-초기화와-에포크-로그)\n",
    "6. [정리](#6.-정리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(\"TensorFlow 버전:\", tf.__version__)\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. model.fit()이 내부적으로 하는 일\n",
    "\n",
    "`model.fit()`을 호출하면 내부적으로 다음 과정이 반복된다:\n",
    "\n",
    "1. **데이터를 배치로 분리** - `batch_size` 크기로 미니배치 생성\n",
    "2. **Forward Pass (순전파)** - 모델이 예측값 계산 (`y_pred = model(X, training=True)`)\n",
    "3. **Loss 계산** - 손실 함수로 예측값과 실제값 비교 (`loss = loss_fn(y_true, y_pred)`)\n",
    "4. **Backward Pass (역전파)** - 자동 미분으로 그래디언트 계산 (`tape.gradient(loss, weights)`)\n",
    "5. **가중치 업데이트** - 옵티마이저로 파라미터 갱신 (`optimizer.apply_gradients(...)`)\n",
    "6. **메트릭 갱신** - 배치 결과를 누적하여 에포크 메트릭 계산\n",
    "\n",
    "커스텀 루프는 이 과정을 직접 코드로 작성하여 더 세밀한 제어를 가능하게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 데이터 및 모델 준비\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full.reshape(-1, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(-1, 784).astype('float32') / 255.0\n",
    "\n",
    "# 빠른 실험용 서브셋\n",
    "X_train = X_train_full[:8000]\n",
    "y_train = y_train_full[:8000]\n",
    "X_val = X_train_full[8000:10000]\n",
    "y_val = y_train_full[8000:10000]\n",
    "\n",
    "# tf.data.Dataset으로 변환\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"MLP 분류 모델 생성\"\"\"\n",
    "    tf.random.set_seed(42)\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)  # 로짓 출력 (활성화 없음)\n",
    "    ])\n",
    "\n",
    "print(f\"훈련 배치 수: {len(train_dataset)}\")\n",
    "print(f\"검증 배치 수: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 기본 GradientTape 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 기본 커스텀 학습 루프 구현\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 정의\n",
    "model = build_model()\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# 에포크 및 배치 수준 메트릭\n",
    "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "val_loss_metric = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='val_acc')\n",
    "\n",
    "EPOCHS = 3\n",
    "history_custom = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "print(\"=== 커스텀 학습 루프 시작 ===\")\n",
    "total_start = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # -------- 훈련 루프 --------\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        \n",
    "        # GradientTape 컨텍스트: 이 블록 안의 연산을 기록\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 1. Forward pass: 예측값 계산 (training=True: Dropout 활성화)\n",
    "            y_pred = model(x_batch, training=True)\n",
    "            # 2. Loss 계산\n",
    "            loss = loss_fn(y_batch, y_pred)\n",
    "        \n",
    "        # 3. Backward pass: 그래디언트 계산\n",
    "        # model.trainable_variables: 학습 가능한 파라미터 (가중치 + 편향)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # 4. 가중치 업데이트\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        # 5. 메트릭 갱신\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(y_batch, y_pred)\n",
    "    \n",
    "    # -------- 검증 루프 --------\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        # 검증 시: training=False (Dropout 비활성화)\n",
    "        y_pred_val = model(x_batch_val, training=False)\n",
    "        val_loss = loss_fn(y_batch_val, y_pred_val)\n",
    "        val_loss_metric.update_state(val_loss)\n",
    "        val_acc_metric.update_state(y_batch_val, y_pred_val)\n",
    "    \n",
    "    # -------- 에포크 결과 출력 --------\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    t_loss = train_loss_metric.result().numpy()\n",
    "    t_acc = train_acc_metric.result().numpy()\n",
    "    v_loss = val_loss_metric.result().numpy()\n",
    "    v_acc = val_acc_metric.result().numpy()\n",
    "    \n",
    "    history_custom['train_loss'].append(t_loss)\n",
    "    history_custom['train_acc'].append(t_acc)\n",
    "    history_custom['val_loss'].append(v_loss)\n",
    "    history_custom['val_acc'].append(v_acc)\n",
    "    \n",
    "    print(f\"에포크 {epoch+1}/{EPOCHS} ({epoch_time:.1f}s) - \"\n",
    "          f\"loss: {t_loss:.4f} - acc: {t_acc:.4f} - \"\n",
    "          f\"val_loss: {v_loss:.4f} - val_acc: {v_acc:.4f}\")\n",
    "    \n",
    "    # -------- 메트릭 초기화 (다음 에포크를 위해) --------\n",
    "    train_loss_metric.reset_state()\n",
    "    train_acc_metric.reset_state()\n",
    "    val_loss_metric.reset_state()\n",
    "    val_acc_metric.reset_state()\n",
    "\n",
    "total_time = time.time() - total_start\n",
    "print(f\"\\n총 학습 시간: {total_time:.1f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. train_step / val_step 함수 분리\n",
    "\n",
    "배치 단위 처리를 별도 함수로 분리하면 코드 가독성이 향상되고 `@tf.function` 적용이 용이해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# train_step / val_step 함수로 분리\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# 새 모델과 옵티마이저\n",
    "model2 = build_model()\n",
    "optimizer2 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn2 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 메트릭 객체\n",
    "tr_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "tr_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "vl_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "vl_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='val_acc')\n",
    "\n",
    "\n",
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"단일 훈련 배치 처리: Forward -> Loss -> Backward -> Update\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model2(x_batch, training=True)\n",
    "        loss = loss_fn2(y_batch, predictions)\n",
    "    \n",
    "    # 그래디언트 계산 및 가중치 업데이트\n",
    "    gradients = tape.gradient(loss, model2.trainable_variables)\n",
    "    optimizer2.apply_gradients(zip(gradients, model2.trainable_variables))\n",
    "    \n",
    "    # 메트릭 갱신\n",
    "    tr_loss.update_state(loss)\n",
    "    tr_acc.update_state(y_batch, predictions)\n",
    "\n",
    "\n",
    "def val_step(x_batch, y_batch):\n",
    "    \"\"\"단일 검증 배치 처리: Forward -> Loss (그래디언트 계산 없음)\"\"\"\n",
    "    # 검증 시에는 GradientTape 불필요\n",
    "    predictions = model2(x_batch, training=False)\n",
    "    loss = loss_fn2(y_batch, predictions)\n",
    "    \n",
    "    vl_loss.update_state(loss)\n",
    "    vl_acc.update_state(y_batch, predictions)\n",
    "\n",
    "\n",
    "def run_epoch(train_ds, val_ds):\n",
    "    \"\"\"전체 에포크 실행: 훈련 + 검증\"\"\"\n",
    "    # 훈련\n",
    "    for x_b, y_b in train_ds:\n",
    "        train_step(x_b, y_b)\n",
    "    \n",
    "    # 검증\n",
    "    for x_b, y_b in val_ds:\n",
    "        val_step(x_b, y_b)\n",
    "    \n",
    "    # 결과 수집\n",
    "    results = {\n",
    "        'train_loss': tr_loss.result().numpy(),\n",
    "        'train_acc': tr_acc.result().numpy(),\n",
    "        'val_loss': vl_loss.result().numpy(),\n",
    "        'val_acc': vl_acc.result().numpy()\n",
    "    }\n",
    "    \n",
    "    # 메트릭 초기화\n",
    "    tr_loss.reset_state()\n",
    "    tr_acc.reset_state()\n",
    "    vl_loss.reset_state()\n",
    "    vl_acc.reset_state()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# 3 에포크 학습\n",
    "print(\"=== 함수 분리 버전 학습 ===\")\n",
    "start = time.time()\n",
    "for epoch in range(3):\n",
    "    results = run_epoch(train_dataset, val_dataset)\n",
    "    print(f\"에포크 {epoch+1}: \"\n",
    "          f\"loss={results['train_loss']:.4f}, \"\n",
    "          f\"acc={results['train_acc']:.4f}, \"\n",
    "          f\"val_loss={results['val_loss']:.4f}, \"\n",
    "          f\"val_acc={results['val_acc']:.4f}\")\n",
    "\n",
    "elapsed_eager = time.time() - start\n",
    "print(f\"\\n이거 모드(Eager) 소요 시간: {elapsed_eager:.1f}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. @tf.function으로 성능 최적화\n",
    "\n",
    "`@tf.function` 데코레이터는 Python 함수를 TensorFlow 그래프로 컴파일하여 실행 속도를 크게 향상시킨다.\n",
    "\n",
    "- **이거 모드(Eager mode)**: Python 코드가 즉시 실행 (디버깅 편리)\n",
    "- **그래프 모드(Graph mode)**: `@tf.function`으로 최적화된 정적 그래프 실행 (속도 빠름)\n",
    "\n",
    "첫 호출 시 **트레이싱(tracing)**이 발생하여 약간 느리지만, 이후 호출은 최적화된 그래프를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# @tf.function 적용으로 그래프 모드 컴파일\n",
    "# ---------------------------------------------------\n",
    "\n",
    "model3 = build_model()\n",
    "optimizer3 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn3 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "tr_loss3 = tf.keras.metrics.Mean()\n",
    "tr_acc3 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "vl_loss3 = tf.keras.metrics.Mean()\n",
    "vl_acc3 = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "@tf.function  # 이 데코레이터가 함수를 TF 그래프로 컴파일\n",
    "def train_step_graph(x_batch, y_batch):\n",
    "    \"\"\"그래프 모드 훈련 스텝: @tf.function으로 최적화\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model3(x_batch, training=True)\n",
    "        loss = loss_fn3(y_batch, predictions)\n",
    "    gradients = tape.gradient(loss, model3.trainable_variables)\n",
    "    optimizer3.apply_gradients(zip(gradients, model3.trainable_variables))\n",
    "    tr_loss3.update_state(loss)\n",
    "    tr_acc3.update_state(y_batch, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def val_step_graph(x_batch, y_batch):\n",
    "    \"\"\"그래프 모드 검증 스텝\"\"\"\n",
    "    predictions = model3(x_batch, training=False)\n",
    "    loss = loss_fn3(y_batch, predictions)\n",
    "    vl_loss3.update_state(loss)\n",
    "    vl_acc3.update_state(y_batch, predictions)\n",
    "\n",
    "\n",
    "# 이거 모드와 그래프 모드 속도 비교\n",
    "N_EPOCHS = 3\n",
    "\n",
    "print(\"=== @tf.function 그래프 모드 학습 ===\")\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for x_b, y_b in train_dataset:\n",
    "        train_step_graph(x_b, y_b)\n",
    "    for x_b, y_b in val_dataset:\n",
    "        val_step_graph(x_b, y_b)\n",
    "    \n",
    "    print(f\"에포크 {epoch+1}: \"\n",
    "          f\"loss={tr_loss3.result().numpy():.4f}, \"\n",
    "          f\"acc={tr_acc3.result().numpy():.4f}, \"\n",
    "          f\"val_loss={vl_loss3.result().numpy():.4f}, \"\n",
    "          f\"val_acc={vl_acc3.result().numpy():.4f}\")\n",
    "    \n",
    "    tr_loss3.reset_state()\n",
    "    tr_acc3.reset_state()\n",
    "    vl_loss3.reset_state()\n",
    "    vl_acc3.reset_state()\n",
    "\n",
    "elapsed_graph = time.time() - start\n",
    "print(f\"\\n그래프 모드 소요 시간: {elapsed_graph:.1f}초\")\n",
    "\n",
    "print()\n",
    "print(f\"이거 모드 (함수 분리 버전): {elapsed_eager:.1f}초\")\n",
    "print(f\"그래프 모드 (@tf.function):  {elapsed_graph:.1f}초\")\n",
    "if elapsed_eager > 0:\n",
    "    speedup = elapsed_eager / elapsed_graph\n",
    "    print(f\"속도 향상: {speedup:.2f}x\")\n",
    "\n",
    "print()\n",
    "print(\"주의사항:\")\n",
    "print(\"  - @tf.function 내에서 Python print() 는 트레이싱 시에만 실행됨\")\n",
    "print(\"  - 디버깅 시에는 @tf.function을 제거하고 이거 모드로 사용\")\n",
    "print(\"  - tf.print()를 사용하면 그래프 모드에서도 값 출력 가능\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 메트릭 초기화와 에포크 로그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# 완전한 커스텀 학습 루프: 메트릭 관리 + 로그 + 시각화\n",
    "# ---------------------------------------------------\n",
    "\n",
    "model4 = build_model()\n",
    "optimizer4 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn4 = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# 메트릭 딕셔너리로 관리\n",
    "metrics = {\n",
    "    'train_loss': tf.keras.metrics.Mean(),\n",
    "    'train_acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    'val_loss': tf.keras.metrics.Mean(),\n",
    "    'val_acc': tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step_final(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model4(x, training=True)\n",
    "        loss = loss_fn4(y, preds)\n",
    "    grads = tape.gradient(loss, model4.trainable_variables)\n",
    "    optimizer4.apply_gradients(zip(grads, model4.trainable_variables))\n",
    "    metrics['train_loss'].update_state(loss)\n",
    "    metrics['train_acc'].update_state(y, preds)\n",
    "\n",
    "@tf.function\n",
    "def val_step_final(x, y):\n",
    "    preds = model4(x, training=False)\n",
    "    loss = loss_fn4(y, preds)\n",
    "    metrics['val_loss'].update_state(loss)\n",
    "    metrics['val_acc'].update_state(y, preds)\n",
    "\n",
    "# 학습 기록\n",
    "history_final = {key: [] for key in metrics.keys()}\n",
    "EPOCHS_FINAL = 8\n",
    "\n",
    "print(f\"{'에포크':^8} {'Train Loss':^12} {'Train Acc':^12} {'Val Loss':^12} {'Val Acc':^12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(EPOCHS_FINAL):\n",
    "    # 훈련\n",
    "    for x_b, y_b in train_dataset:\n",
    "        train_step_final(x_b, y_b)\n",
    "    \n",
    "    # 검증\n",
    "    for x_b, y_b in val_dataset:\n",
    "        val_step_final(x_b, y_b)\n",
    "    \n",
    "    # 결과 수집 및 출력\n",
    "    epoch_results = {k: v.result().numpy() for k, v in metrics.items()}\n",
    "    for k, v in epoch_results.items():\n",
    "        history_final[k].append(v)\n",
    "    \n",
    "    print(f\"{epoch+1:^8d} \"\n",
    "          f\"{epoch_results['train_loss']:^12.4f} \"\n",
    "          f\"{epoch_results['train_acc']:^12.4f} \"\n",
    "          f\"{epoch_results['val_loss']:^12.4f} \"\n",
    "          f\"{epoch_results['val_acc']:^12.4f}\")\n",
    "    \n",
    "    # 메트릭 초기화 (중요: 다음 에포크를 위해 반드시 초기화)\n",
    "    for metric in metrics.values():\n",
    "        metric.reset_state()\n",
    "\n",
    "# 학습 곡선 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "epochs_range = range(1, EPOCHS_FINAL + 1)\n",
    "\n",
    "axes[0].plot(epochs_range, history_final['train_loss'], 'b-o', label='훈련 손실', markersize=4)\n",
    "axes[0].plot(epochs_range, history_final['val_loss'], 'r-o', label='검증 손실', markersize=4)\n",
    "axes[0].set_xlabel('에포크')\n",
    "axes[0].set_ylabel('손실')\n",
    "axes[0].set_title('커스텀 루프 - 손실 곡선')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(epochs_range, history_final['train_acc'], 'b-o', label='훈련 정확도', markersize=4)\n",
    "axes[1].plot(epochs_range, history_final['val_acc'], 'r-o', label='검증 정확도', markersize=4)\n",
    "axes[1].set_xlabel('에포크')\n",
    "axes[1].set_ylabel('정확도')\n",
    "axes[1].set_title('커스텀 루프 - 정확도 곡선')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 정리\n",
    "\n",
    "### model.fit() vs 커스텀 학습 루프 비교\n",
    "\n",
    "| 항목 | model.fit() | 커스텀 루프 |\n",
    "|------|-------------|------------|\n",
    "| **코드 양** | 적음 (간결) | 많음 (상세) |\n",
    "| **유연성** | 낮음 | 높음 |\n",
    "| **디버깅** | 어려움 | 쉬움 (이거 모드) |\n",
    "| **콜백 지원** | 풍부한 내장 콜백 | 직접 구현 필요 |\n",
    "| **분산 학습** | 내장 지원 | 추가 코드 필요 |\n",
    "| **권장 상황** | 일반적인 학습 | GAN, RL, 멀티-모델 학습 등 |\n",
    "\n",
    "### 커스텀 루프가 필요한 상황\n",
    "1. **GAN (Generative Adversarial Networks)**: 생성자와 판별자를 교대로 학습\n",
    "2. **강화학습**: 보상 기반 학습으로 표준 손실 함수 사용 불가\n",
    "3. **멀티-태스크 학습**: 여러 손실을 다르게 처리\n",
    "4. **그래디언트 클리핑/수정**: 그래디언트를 적용 전에 직접 수정\n",
    "5. **모델 앙상블**: 여러 모델을 하나의 루프에서 학습\n",
    "\n",
    "### 핵심 패턴 요약\n",
    "```python\n",
    "# 기본 패턴\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = model(x, training=True)    # Forward pass\n",
    "    loss = loss_fn(y_true, y_pred)       # Loss 계산\n",
    "\n",
    "grads = tape.gradient(loss, model.trainable_variables)  # Backward\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update\n",
    "\n",
    "metric.reset_state()  # 에포크 후 반드시 초기화!\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
