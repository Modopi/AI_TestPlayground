{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Linear Attentionê³¼ Hybrid êµ¬ì¡°\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- í‘œì¤€ Softmax Attentionê³¼ Linear Attentionì˜ ë³µì¡ë„ ì°¨ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤\n",
    "- GLA(Gated Linear Attention)ì˜ ìˆœí™˜(recurrence) êµ¬ì¡°ë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- RetNetê³¼ Mambaì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ë¹„êµ ë¶„ì„í•œë‹¤\n",
    "- Qwenì˜ SWA+Full+Linear Hybrid ì•„í‚¤í…ì²˜ë¥¼ ì´í•´í•œë‹¤\n",
    "- ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ìŠ¤ì¼€ì¼ë§ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Linear Attentionê³¼ ì»¤ë„ í•¨ìˆ˜](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [Standard vs Linear Attention ë³µì¡ë„ ë¹„êµ](#2.-ë³µì¡ë„-ë¹„êµ)\n",
    "3. [GLA ìˆœí™˜ ì‹œë®¬ë ˆì´ì…˜](#3.-GLA-ìˆœí™˜-ì‹œë®¬ë ˆì´ì…˜)\n",
    "4. [Qwen Hybrid ì•„í‚¤í…ì²˜](#4.-Qwen-Hybrid-ì•„í‚¤í…ì²˜)\n",
    "5. [ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ë¹„êµ](#5.-ë©”ëª¨ë¦¬-ì†ë„-ë¹„êµ)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### í‘œì¤€ Softmax Attention\n",
    "\n",
    "$$\\text{Attn}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$$\n",
    "\n",
    "- ì‹œê°„ ë³µì¡ë„: $O(N^2 d)$, ê³µê°„ ë³µì¡ë„: $O(N^2 + Nd)$\n",
    "- $N$: ì‹œí€€ìŠ¤ ê¸¸ì´, $d$: ì°¨ì›\n",
    "\n",
    "### Linear Attention\n",
    "\n",
    "ì»¤ë„ í•¨ìˆ˜ $\\phi$ë¥¼ ì‚¬ìš©í•˜ì—¬ softmaxë¥¼ ê·¼ì‚¬:\n",
    "\n",
    "$$O_i = \\frac{\\phi(Q_i) \\sum_{j=1}^{N} \\phi(K_j)^T V_j}{\\phi(Q_i) \\sum_{j=1}^{N} \\phi(K_j)^T}$$\n",
    "\n",
    "**í•µì‹¬ íŠ¸ë¦­**: ê²°í•© ë²•ì¹™ì„ ì´ìš©í•œ ì—°ì‚° ìˆœì„œ ë³€ê²½\n",
    "\n",
    "$$O = \\phi(Q) \\underbrace{\\left(\\phi(K)^T V\\right)}_{S \\in \\mathbb{R}^{d \\times d}} \\quad \\text{vs} \\quad O = \\underbrace{\\left(\\phi(Q) \\phi(K)^T\\right)}_{A \\in \\mathbb{R}^{N \\times N}} V$$\n",
    "\n",
    "- ì¢Œì¸¡: $O(Nd^2)$ â€” Linear Attention (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì„ í˜•)\n",
    "- ìš°ì¸¡: $O(N^2d)$ â€” Standard Attention (ì‹œí€€ìŠ¤ ê¸¸ì´ì— ì´ì°¨)\n",
    "\n",
    "### Causal Linear Attention (ìˆœí™˜ í˜•íƒœ)\n",
    "\n",
    "$$s_t = s_{t-1} + \\phi(k_t)^T v_t, \\quad o_t = \\frac{\\phi(q_t) s_t}{\\phi(q_t) z_t}$$\n",
    "\n",
    "- $s_t \\in \\mathbb{R}^{d \\times d}$: ìˆ¨ê²¨ì§„ ìƒíƒœ (ëˆ„ì  KV)\n",
    "- $z_t = z_{t-1} + \\phi(k_t)$: ì •ê·œí™” í•­\n",
    "- í† í°ë‹¹ ë©”ëª¨ë¦¬: $O(d^2)$ â€” ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€!\n",
    "\n",
    "### Gated Linear Attention (GLA)\n",
    "\n",
    "$$s_t = G_t \\odot s_{t-1} + k_t^T v_t$$\n",
    "\n",
    "$$o_t = q_t \\cdot s_t$$\n",
    "\n",
    "- $G_t \\in \\mathbb{R}^{d \\times d}$: ê²Œì´íŠ¸ í–‰ë ¬ (ë§ê° ë©”ì»¤ë‹ˆì¦˜)\n",
    "- $G_t$ê°€ ì‘ìœ¼ë©´ ê³¼ê±° ì •ë³´ ìŠê¸°, í¬ë©´ ìœ ì§€\n",
    "\n",
    "### RetNetì˜ ìˆœí™˜ ê³µì‹\n",
    "\n",
    "$$s_t = \\gamma \\cdot s_{t-1} + k_t^T v_t, \\quad o_t = q_t \\cdot s_t$$\n",
    "\n",
    "- $\\gamma \\in (0, 1)$: ê³ ì • ê°ì‡  ë¹„ìœ¨\n",
    "- GLAì™€ ìœ ì‚¬í•˜ì§€ë§Œ ê²Œì´íŠ¸ê°€ ìŠ¤ì¹¼ë¼ ìƒìˆ˜\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| ëª¨ë¸ | ìˆœí™˜ ê³µì‹ | ë³µì¡ë„ (ì¶”ë¡ ) | ì¥ì  |\n",
    "|------|-----------|---------------|------|\n",
    "| Standard Attention | $\\text{softmax}(QK^T/\\sqrt{d})V$ | $O(N^2d)$ | ë†’ì€ í‘œí˜„ë ¥ |\n",
    "| Linear Attention | $\\phi(Q)(\\phi(K)^TV)$ | $O(Nd^2)$ | ê¸´ ì‹œí€€ìŠ¤ íš¨ìœ¨ |\n",
    "| GLA | $s_t = G_t \\odot s_{t-1} + k_t^T v_t$ | $O(d^2)$/í† í° | ì„ íƒì  ë§ê° |\n",
    "| RetNet | $s_t = \\gamma s_{t-1} + k_t^T v_t$ | $O(d^2)$/í† í° | ë‹¨ìˆœí•œ ê°ì‡  |\n",
    "| Mamba | SSM ê¸°ë°˜ ì„ íƒì  ìŠ¤ìº” | $O(d)$/í† í° | ì…ë ¥ ì˜ì¡´ ê²Œì´íŒ… |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Linear Attention ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ ì™œ Linear Attentionì´ í•„ìš”í•œê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: êµì‹¤ì—ì„œ ëª¨ë“  í•™ìƒ(Nëª…)ì´ ì„œë¡œ ëŒ€í™”í•˜ë ¤ë©´ $N \\times N$ë²ˆ ëŒ€í™”í•´ì•¼ í•´ìš”. \n",
    "> í•™ìƒì´ 100ëª…ì´ë©´ 10,000ë²ˆ! í•˜ì§€ë§Œ **ë°˜ì¥ì„ í†µí•´** ëŒ€í™”í•˜ë©´ ê°ì ë°˜ì¥ê³¼ë§Œ ëŒ€í™”í•˜ë©´ ë¼ì„œ \n",
    "> 200ë²ˆì´ë©´ ì¶©ë¶„í•´ìš”!\n",
    "\n",
    "Linear Attentionì€ **ë°˜ì¥(ìˆ¨ê²¨ì§„ ìƒíƒœ $s_t$)** ì„ ë‘ëŠ” ê±°ì˜ˆìš”:\n",
    "- í‘œì¤€ Attention: ëª¨ë“  í† í°ì´ ì„œë¡œ ì§ì ‘ ëŒ€í™” â†’ $N^2$ë²ˆ ê³„ì‚°\n",
    "- Linear Attention: ê° í† í°ì´ \"ìš”ì•½ ë…¸íŠ¸($s_t$)\"ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ì½ê¸° â†’ $N$ë²ˆ ê³„ì‚°\n",
    "\n",
    "#### ğŸ§  GLAëŠ” ë­ê°€ ë‹¤ë¥¸ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: \"ìš”ì•½ ë…¸íŠ¸\"ì— **ì§€ìš°ê°œ(ê²Œì´íŠ¸ $G_t$)** ê°€ ìˆì–´ì„œ, \n",
    "> ì¤‘ìš”í•˜ì§€ ì•Šì€ ê³¼ê±° ì •ë³´ëŠ” ì§€ìš¸ ìˆ˜ ìˆì–´ìš”!\n",
    "\n",
    "ì¼ë°˜ Linear Attentionì€ ëª¨ë“  ê³¼ê±° ì •ë³´ë¥¼ ê³„ì† ìŒ“ê¸°ë§Œ í•˜ì§€ë§Œ, \n",
    "GLAëŠ” ì„ íƒì ìœ¼ë¡œ ìŠì„ ìˆ˜ ìˆì–´ì„œ **ë” ë˜‘ë˜‘í•˜ê²Œ** ì •ë³´ë¥¼ ê´€ë¦¬í•´ìš”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: ë³µì¡ë„ ë¹„êµ\n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ $N=4096$, ì°¨ì› $d=128$ì¼ ë•Œ, Standard Attentionê³¼ Linear Attentionì˜ ì—°ì‚°ëŸ‰(FLOPs)ì„ ë¹„êµí•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "**Standard Attention:**\n",
    "$$\\text{FLOPs} = O(N^2 d) = 4096^2 \\times 128 \\approx 2.15 \\times 10^9$$\n",
    "\n",
    "**Linear Attention:**\n",
    "$$\\text{FLOPs} = O(N d^2) = 4096 \\times 128^2 \\approx 6.71 \\times 10^7$$\n",
    "\n",
    "$$\\text{ë¹„ìœ¨} = \\frac{N^2 d}{N d^2} = \\frac{N}{d} = \\frac{4096}{128} = 32$$\n",
    "\n",
    "â†’ Linear Attentionì´ **32ë°°** ë” ë¹ ë¦…ë‹ˆë‹¤! ($N > d$ì¼ ë•Œ í•­ìƒ ìœ ë¦¬)\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: GLA ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "\n",
    "$s_0 = \\mathbf{0}$, $G_1 = 0.9I$, $k_1 = [1, 0]^T$, $v_1 = [2, 3]^T$ì¼ ë•Œ $s_1$ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$s_1 = G_1 \\odot s_0 + k_1^T v_1 = 0.9 \\cdot \\mathbf{0} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} [2, 3] = \\begin{bmatrix} 2 & 3 \\\\ 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "â†’ ì²« ë²ˆì§¸ í† í°ì˜ KV ì •ë³´ê°€ ìƒíƒœ í–‰ë ¬ì— ì €ì¥ë©ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 2. Standard vs Linear Attention ë³µì¡ë„ ë¹„êµ <a name='2.-ë³µì¡ë„-ë¹„êµ'></a>\n",
    "\n",
    "ë‘ ë°©ì‹ì˜ ì‹œê°„/ê³µê°„ ë³µì¡ë„ë¥¼ ì´ë¡ ì ìœ¼ë¡œ ë¹„êµí•˜ê³ , ì‹¤ì œ ì—°ì‚° ì‹œê°„ì„ ì¸¡ì •í•©ë‹ˆë‹¤.\n",
    "\n",
    "| ì¸¡ë©´ | Standard Attention | Linear Attention |\n",
    "|------|-------------------|------------------|\n",
    "| ì‹œê°„ ë³µì¡ë„ | $O(N^2 d)$ | $O(Nd^2)$ |\n",
    "| ê³µê°„ ë³µì¡ë„ | $O(N^2)$ | $O(d^2)$ |\n",
    "| ìœ ë¦¬í•œ ì¡°ê±´ | $N < d$ | $N > d$ (ê¸´ ì‹œí€€ìŠ¤) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Standard vs Linear Attention êµ¬í˜„ ë° ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def standard_attention(Q, K, V):\n",
    "    # Q, K, V: [batch, seq, dim]\n",
    "    d = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, V)\n",
    "\n",
    "def linear_attention(Q, K, V, kernel_fn=None):\n",
    "    # ì»¤ë„ í•¨ìˆ˜: elu(x) + 1 (ì–‘ìˆ˜ ë³´ì¥)\n",
    "    if kernel_fn is None:\n",
    "        kernel_fn = lambda x: tf.nn.elu(x) + 1\n",
    "\n",
    "    Q_prime = kernel_fn(Q)  # [B, N, d]\n",
    "    K_prime = kernel_fn(K)  # [B, N, d]\n",
    "\n",
    "    # S = K'^T V : [B, d, d]\n",
    "    S = tf.matmul(K_prime, V, transpose_a=True)\n",
    "\n",
    "    # O = Q' S : [B, N, d]\n",
    "    numerator = tf.matmul(Q_prime, S)\n",
    "\n",
    "    # ì •ê·œí™”\n",
    "    Z = tf.matmul(Q_prime, tf.reduce_sum(K_prime, axis=1, keepdims=True),\n",
    "                  transpose_b=True)\n",
    "    Z = tf.maximum(Z, 1e-6)\n",
    "\n",
    "    return numerator / Z\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ì—ì„œ ì—°ì‚° ì‹œê°„ ë¹„êµ\n",
    "dims = 64\n",
    "batch = 4\n",
    "seq_lengths_test = [64, 128, 256, 512, 1024, 2048]\n",
    "\n",
    "std_times = []\n",
    "lin_times = []\n",
    "\n",
    "print(f\"Standard vs Linear Attention ì‹¤í–‰ ì‹œê°„ ë¹„êµ:\")\n",
    "print(f\"  d = {dims}, batch = {batch}\")\n",
    "print(f\"{'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'Standard (ms)':>14} | {'Linear (ms)':>14} | {'ì†ë„ ë¹„ìœ¨':>10}\")\n",
    "print(\"-\" * 58)\n",
    "\n",
    "for seq_len in seq_lengths_test:\n",
    "    Q = tf.random.normal([batch, seq_len, dims])\n",
    "    K = tf.random.normal([batch, seq_len, dims])\n",
    "    V = tf.random.normal([batch, seq_len, dims])\n",
    "\n",
    "    # ì›Œë°ì—…\n",
    "    _ = standard_attention(Q, K, V)\n",
    "    _ = linear_attention(Q, K, V)\n",
    "\n",
    "    # ì‹œê°„ ì¸¡ì •\n",
    "    n_runs = 5\n",
    "    t0 = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = standard_attention(Q, K, V)\n",
    "    std_time = (time.time() - t0) / n_runs * 1000\n",
    "\n",
    "    t0 = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = linear_attention(Q, K, V)\n",
    "    lin_time = (time.time() - t0) / n_runs * 1000\n",
    "\n",
    "    std_times.append(std_time)\n",
    "    lin_times.append(lin_time)\n",
    "    ratio = std_time / max(lin_time, 1e-6)\n",
    "    print(f\"{seq_len:>12} | {std_time:>14.2f} | {lin_time:>14.2f} | {ratio:>9.2f}x\")\n",
    "\n",
    "print(f\"\\nâ†’ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ Linear Attentionì˜ ì´ì ì´ ì»¤ì§\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. GLA ìˆœí™˜ ì‹œë®¬ë ˆì´ì…˜ <a name='3.-GLA-ìˆœí™˜-ì‹œë®¬ë ˆì´ì…˜'></a>\n",
    "\n",
    "GLA(Gated Linear Attention)ì˜ ìˆœí™˜ í˜•íƒœë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$s_t = G_t \\odot s_{t-1} + k_t v_t^T$$\n",
    "\n",
    "$$o_t = s_t^T q_t$$\n",
    "\n",
    "ì—¬ê¸°ì„œ $G_t = \\sigma(W_g h_t)$ëŠ” ì…ë ¥ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ê²°ì •ë˜ëŠ” ê²Œì´íŠ¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ GLA ìˆœí™˜(Recurrence) êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class GLARecurrence(tf.keras.layers.Layer):\n",
    "    # Gated Linear Attention - ìˆœí™˜ í˜•íƒœ êµ¬í˜„\n",
    "\n",
    "    def __init__(self, d_model, d_key, d_value):\n",
    "        super().__init__()\n",
    "        self.d_key = d_key\n",
    "        self.d_value = d_value\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_key)\n",
    "        self.W_k = tf.keras.layers.Dense(d_key)\n",
    "        self.W_v = tf.keras.layers.Dense(d_value)\n",
    "        self.W_g = tf.keras.layers.Dense(d_key * d_value)  # ê²Œì´íŠ¸\n",
    "\n",
    "    def call(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        q = self.W_q(x)  # [B, N, d_k]\n",
    "        k = self.W_k(x)  # [B, N, d_k]\n",
    "        v = self.W_v(x)  # [B, N, d_v]\n",
    "        g = tf.sigmoid(self.W_g(x))  # [B, N, d_k*d_v]\n",
    "        g = tf.reshape(g, [batch, seq_len, self.d_key, self.d_value])\n",
    "\n",
    "        outputs = []\n",
    "        # ìƒíƒœ í–‰ë ¬ ì´ˆê¸°í™”\n",
    "        s = tf.zeros([batch, self.d_key, self.d_value])\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            k_t = k[:, t, :]  # [B, d_k]\n",
    "            v_t = v[:, t, :]  # [B, d_v]\n",
    "            q_t = q[:, t, :]  # [B, d_k]\n",
    "            g_t = g[:, t, :, :]  # [B, d_k, d_v]\n",
    "\n",
    "            # ìƒíƒœ ì—…ë°ì´íŠ¸: s_t = G_t * s_{t-1} + k_t^T v_t\n",
    "            kv_outer = tf.einsum('bi,bj->bij', k_t, v_t)  # [B, d_k, d_v]\n",
    "            s = g_t * s + kv_outer\n",
    "\n",
    "            # ì¶œë ¥: o_t = q_t^T s_t\n",
    "            o_t = tf.einsum('bi,bij->bj', q_t, s)  # [B, d_v]\n",
    "            outputs.append(o_t)\n",
    "\n",
    "        output = tf.stack(outputs, axis=1)  # [B, N, d_v]\n",
    "        return output, s\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "d_model = 64\n",
    "d_key = 32\n",
    "d_value = 32\n",
    "batch_size = 2\n",
    "seq_len = 20\n",
    "\n",
    "gla = GLARecurrence(d_model, d_key, d_value)\n",
    "x_test = tf.random.normal([batch_size, seq_len, d_model])\n",
    "output, final_state = gla(x_test)\n",
    "\n",
    "print(f\"GLA Recurrence ê²°ê³¼:\")\n",
    "print(f\"  ì…ë ¥: {x_test.shape}\")\n",
    "print(f\"  ì¶œë ¥: {output.shape}\")\n",
    "print(f\"  ìµœì¢… ìƒíƒœ: {final_state.shape}\")\n",
    "print(f\"  ìƒíƒœ í–‰ë ¬ í¬ê¸°: {d_key} x {d_value} = {d_key * d_value} (ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€!)\")\n",
    "\n",
    "# ê²Œì´íŠ¸ ë¶„ì„\n",
    "g_sample = tf.sigmoid(gla.W_g(x_test))\n",
    "g_mean = tf.reduce_mean(g_sample).numpy()\n",
    "g_std = tf.math.reduce_std(g_sample).numpy()\n",
    "print(f\"\\nê²Œì´íŠ¸ í†µê³„:\")\n",
    "print(f\"  í‰ê· : {g_mean:.4f} (0.5ì— ê°€ê¹Œìš°ë©´ ì¤‘ë¦½ì )\")\n",
    "print(f\"  í‘œì¤€í¸ì°¨: {g_std:.4f}\")\n",
    "print(f\"  â†’ ê²Œì´íŠ¸ê°€ 0ì— ê°€ê¹Œìš°ë©´ ê³¼ê±° ì •ë³´ ìŠê¸°, 1ì— ê°€ê¹Œìš°ë©´ ìœ ì§€\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ë¹„êµ\n",
    "std_kv_cache = seq_len * 2 * d_key  # Standard: ëª¨ë“  K,V ì €ì¥\n",
    "gla_state = d_key * d_value  # GLA: ìƒíƒœ í–‰ë ¬ë§Œ ì €ì¥\n",
    "print(f\"\\në©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ = {seq_len}):\")\n",
    "print(f\"  Standard Attention KV Cache: {std_kv_cache} ì›ì†Œ\")\n",
    "print(f\"  GLA ìƒíƒœ í–‰ë ¬: {gla_state} ì›ì†Œ\")\n",
    "print(f\"  ì ˆê°ë¥ : {(1 - gla_state/std_kv_cache)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. Qwen Hybrid ì•„í‚¤í…ì²˜ <a name='4.-Qwen-Hybrid-ì•„í‚¤í…ì²˜'></a>\n",
    "\n",
    "Qwenì€ ì„¸ ê°€ì§€ Attention ë°©ì‹ì„ ë ˆì´ì–´ë³„ë¡œ í˜¼í•©í•˜ëŠ” Hybrid êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "| ë ˆì´ì–´ ìœ í˜• | Attention ë°©ì‹ | ìš©ë„ |\n",
    "|-------------|---------------|------|\n",
    "| Full Attention | í‘œì¤€ Softmax | ì „ì—­ ì˜ì¡´ì„± í¬ì°© |\n",
    "| Sliding Window (SWA) | ìœˆë„ìš° ë‚´ Softmax | ì§€ì—­ íŒ¨í„´ í¬ì°© |\n",
    "| Linear Attention | ì»¤ë„ ê¸°ë°˜ ì„ í˜• | íš¨ìœ¨ì  ì¥ê±°ë¦¬ ìš”ì•½ |\n",
    "\n",
    "ì´ëŸ¬í•œ êµ¬ì¡°ì˜ ì¥ì :\n",
    "1. Full Attentionìœ¼ë¡œ ì¤‘ìš”í•œ ì „ì—­ ê´€ê³„ ìœ ì§€\n",
    "2. SWAë¡œ ê³„ì‚°ëŸ‰ ì ˆê° (ëŒ€ë¶€ë¶„ì˜ ë ˆì´ì–´)\n",
    "3. Linearë¡œ ì´ˆì¥ê±°ë¦¬ ì»¨í…ìŠ¤íŠ¸ íš¨ìœ¨ì  ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Qwen Hybrid ì•„í‚¤í…ì²˜ ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def sliding_window_attention(Q, K, V, window_size=256):\n",
    "    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° Attention (ê°„ì†Œí™” ë²„ì „)\n",
    "    batch, seq_len, d = Q.shape\n",
    "    d_float = tf.cast(d, tf.float32)\n",
    "\n",
    "    # ì „ì²´ attention ê³„ì‚° í›„ ìœˆë„ìš° ë§ˆìŠ¤í¬ ì ìš©\n",
    "    scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d_float)\n",
    "\n",
    "    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë§ˆìŠ¤í¬ ìƒì„±\n",
    "    positions = tf.range(seq_len)\n",
    "    row_pos = tf.expand_dims(positions, 1)  # [N, 1]\n",
    "    col_pos = tf.expand_dims(positions, 0)  # [1, N]\n",
    "    mask = tf.cast(tf.abs(row_pos - col_pos) <= window_size // 2, tf.float32)\n",
    "\n",
    "    # causal ë§ˆìŠ¤í¬ë„ ì ìš©\n",
    "    causal_mask = tf.cast(col_pos <= row_pos, tf.float32)\n",
    "    combined_mask = mask * causal_mask\n",
    "\n",
    "    scores = scores + (1.0 - combined_mask) * (-1e9)\n",
    "    weights = tf.nn.softmax(scores, axis=-1)\n",
    "    return tf.matmul(weights, V)\n",
    "\n",
    "# Hybrid êµ¬ì¡°: ë ˆì´ì–´ë³„ ë°©ì‹ ë°°ì •\n",
    "n_layers = 24\n",
    "layer_types = []\n",
    "for i in range(n_layers):\n",
    "    if i % 6 == 0:\n",
    "        layer_types.append('Full')\n",
    "    elif i % 6 == 3:\n",
    "        layer_types.append('Linear')\n",
    "    else:\n",
    "        layer_types.append('SWA')\n",
    "\n",
    "print(f\"Qwen Hybrid ì•„í‚¤í…ì²˜ ({n_layers} ë ˆì´ì–´):\")\n",
    "print(f\"{'ë ˆì´ì–´':>6} | {'ìœ í˜•':<8} | ì—­í• \")\n",
    "print(\"-\" * 40)\n",
    "type_counts = {'Full': 0, 'SWA': 0, 'Linear': 0}\n",
    "for i, lt in enumerate(layer_types):\n",
    "    type_counts[lt] += 1\n",
    "    role = {'Full': 'ì „ì—­ ì˜ì¡´ì„±', 'SWA': 'ì§€ì—­ íŒ¨í„´', 'Linear': 'ì¥ê±°ë¦¬ ìš”ì•½'}[lt]\n",
    "    if i < 12 or i >= n_layers - 2:\n",
    "        print(f\"  L{i:>3} | {lt:<8} | {role}\")\n",
    "    elif i == 12:\n",
    "        print(f\"   ... | ...      | ...\")\n",
    "\n",
    "print(f\"\\në ˆì´ì–´ ìœ í˜• ë¶„í¬:\")\n",
    "for lt, count in type_counts.items():\n",
    "    pct = count / n_layers * 100\n",
    "    bar = 'â–ˆ' * int(pct / 2)\n",
    "    print(f\"  {lt:<8}: {count:>2}ê°œ ({pct:.0f}%) {bar}\")\n",
    "\n",
    "# ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì—°ì‚° ë¹„êµ\n",
    "seq_len_test = 512\n",
    "d_test = 64\n",
    "batch_t = 2\n",
    "window = 128\n",
    "\n",
    "Q_t = tf.random.normal([batch_t, seq_len_test, d_test])\n",
    "K_t = tf.random.normal([batch_t, seq_len_test, d_test])\n",
    "V_t = tf.random.normal([batch_t, seq_len_test, d_test])\n",
    "\n",
    "out_full = standard_attention(Q_t, K_t, V_t)\n",
    "out_swa = sliding_window_attention(Q_t, K_t, V_t, window_size=window)\n",
    "out_lin = linear_attention(Q_t, K_t, V_t)\n",
    "\n",
    "print(f\"\\nì¶œë ¥ shape í™•ì¸:\")\n",
    "print(f\"  Full Attention: {out_full.shape}\")\n",
    "print(f\"  SWA (w={window}): {out_swa.shape}\")\n",
    "print(f\"  Linear Attention: {out_lin.shape}\")\n",
    "\n",
    "# Hybrid ì´ FLOPs ì¶”ì •\n",
    "N = 4096\n",
    "d = 128\n",
    "full_flops = N * N * d * type_counts['Full']\n",
    "swa_flops = N * window * d * type_counts['SWA']\n",
    "linear_flops = N * d * d * type_counts['Linear']\n",
    "total_hybrid = full_flops + swa_flops + linear_flops\n",
    "total_all_full = N * N * d * n_layers\n",
    "\n",
    "print(f\"\\nFLOPs ë¹„êµ (N={N}, d={d}):\")\n",
    "print(f\"  ì „ì²´ Full Attention: {total_all_full/1e9:.2f} GFLOPs\")\n",
    "print(f\"  Hybrid: {total_hybrid/1e9:.2f} GFLOPs\")\n",
    "print(f\"  ì ˆê°ë¥ : {(1-total_hybrid/total_all_full)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ë¹„êµ <a name='5.-ë©”ëª¨ë¦¬-ì†ë„-ë¹„êµ'></a>\n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¦ê°€í•  ë•Œ ê° Attention ë°©ì‹ì˜ ì´ë¡ ì  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ê³¼ ì—°ì‚°ëŸ‰ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ì´ë¡ ì  ë³µì¡ë„:**\n",
    "- Standard: ë©”ëª¨ë¦¬ $O(N^2)$, ì—°ì‚° $O(N^2 d)$\n",
    "- SWA ($w$): ë©”ëª¨ë¦¬ $O(Nw)$, ì—°ì‚° $O(Nwd)$\n",
    "- Linear: ë©”ëª¨ë¦¬ $O(d^2)$, ì—°ì‚° $O(Nd^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬/ì†ë„ ìŠ¤ì¼€ì¼ë§ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "seq_lengths = np.array([256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536])\n",
    "d = 128\n",
    "w = 512  # SWA ìœˆë„ìš° í¬ê¸°\n",
    "\n",
    "# ì´ë¡ ì  ë©”ëª¨ë¦¬ (Attention í–‰ë ¬ í¬ê¸°)\n",
    "mem_standard = seq_lengths ** 2  # O(N^2)\n",
    "mem_swa = seq_lengths * w  # O(Nw)\n",
    "mem_linear = np.full_like(seq_lengths, d * d, dtype=float)  # O(d^2)\n",
    "\n",
    "# ì´ë¡ ì  FLOPs\n",
    "flops_standard = seq_lengths ** 2 * d\n",
    "flops_swa = seq_lengths * w * d\n",
    "flops_linear = seq_lengths * d ** 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§\n",
    "ax1 = axes[0]\n",
    "ax1.loglog(seq_lengths / 1000, mem_standard / 1e6, 'r-o', lw=2.5, ms=7, label='Standard ($O(N^2)$)')\n",
    "ax1.loglog(seq_lengths / 1000, mem_swa / 1e6, 'b-s', lw=2, ms=7, label=f'SWA ($O(Nw), w={w}$)')\n",
    "ax1.loglog(seq_lengths / 1000, mem_linear / 1e6, 'g-^', lw=2, ms=7, label='Linear ($O(d^2)$)')\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)\n",
    "ax1.set_ylabel('Attention ë©”ëª¨ë¦¬ (M ì›ì†Œ)', fontsize=11)\n",
    "ax1.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬ ìŠ¤ì¼€ì¼ë§', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) FLOPs ìŠ¤ì¼€ì¼ë§\n",
    "ax2 = axes[1]\n",
    "ax2.loglog(seq_lengths / 1000, flops_standard / 1e9, 'r-o', lw=2.5, ms=7, label='Standard ($O(N^2d)$)')\n",
    "ax2.loglog(seq_lengths / 1000, flops_swa / 1e9, 'b-s', lw=2, ms=7, label=f'SWA ($O(Nwd)$)')\n",
    "ax2.loglog(seq_lengths / 1000, flops_linear / 1e9, 'g-^', lw=2, ms=7, label='Linear ($O(Nd^2)$)')\n",
    "ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)\n",
    "ax2.set_ylabel('FLOPs (G)', fontsize=11)\n",
    "ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ì—°ì‚°ëŸ‰ ìŠ¤ì¼€ì¼ë§', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/attention_scaling_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/attention_scaling_comparison.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ ë¹„êµ í‘œ\n",
    "print(f\"\\nì‹œí€€ìŠ¤ ê¸¸ì´ë³„ FLOPs ë¹„êµ (d={d}):\")\n",
    "print(f\"{'ê¸¸ì´':>8} | {'Standard':>12} | {'SWA':>12} | {'Linear':>12} | {'Std/Lin ë¹„ìœ¨':>12}\")\n",
    "print(\"-\" * 65)\n",
    "for i, N in enumerate(seq_lengths):\n",
    "    ratio = flops_standard[i] / flops_linear[i]\n",
    "    print(f\"{N:>8} | {flops_standard[i]/1e9:>10.2f}G | {flops_swa[i]/1e9:>10.2f}G | \"\n",
    "          f\"{flops_linear[i]/1e9:>10.2f}G | {ratio:>11.1f}x\")\n",
    "\n",
    "print(f\"\\nâ†’ N=65Kì—ì„œ LinearëŠ” Standardë³´ë‹¤ {flops_standard[-1]/flops_linear[-1]:.0f}ë°° íš¨ìœ¨ì !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| Linear Attention | $\\phi(Q)(\\phi(K)^TV)$ â€” $O(Nd^2)$ ë³µì¡ë„ | â­â­â­ |\n",
    "| GLA ìˆœí™˜ | $s_t = G_t \\odot s_{t-1} + k_t^T v_t$ â€” ì„ íƒì  ë§ê° | â­â­â­ |\n",
    "| RetNet | $\\gamma$ ê³ ì • ê°ì‡  ê¸°ë°˜ ìˆœí™˜ | â­â­ |\n",
    "| Mamba | SSM ê¸°ë°˜ ì…ë ¥ ì˜ì¡´ì  ì„ íƒ ë©”ì»¤ë‹ˆì¦˜ | â­â­ |\n",
    "| Qwen Hybrid | Full + SWA + Linear ë ˆì´ì–´ í˜¼í•© | â­â­â­ |\n",
    "| ì»¤ë„ í•¨ìˆ˜ | $\\phi(x) = \\text{elu}(x) + 1$ â€” ì–‘ìˆ˜ ë³´ì¥ | â­â­ |\n",
    "| ìƒíƒœ í–‰ë ¬ | $s_t \\in \\mathbb{R}^{d \\times d}$ â€” ì‹œí€€ìŠ¤ ê¸¸ì´ì™€ ë¬´ê´€ | â­â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$O_i = \\frac{\\phi(Q_i) \\sum_{j} \\phi(K_j)^T V_j}{\\phi(Q_i) \\sum_{j} \\phi(K_j)^T}$$\n",
    "\n",
    "$$s_t = G_t \\odot s_{t-1} + k_t^T v_t, \\quad o_t = q_t^T s_t$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**04_long_context_and_sparse_attn.ipynb** â€” YaRN ì»¨í…ìŠ¤íŠ¸ í™•ì¥, DeepSeek Sparse Attention, Sliding Window ë°©ë²•ë¡ ì„ í†µí•©í•˜ì—¬ 50% ì´ìƒì˜ ë¹„ìš© ì ˆê° ê¸°ë²•ì„ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}