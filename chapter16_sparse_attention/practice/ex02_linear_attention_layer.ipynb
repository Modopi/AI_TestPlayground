{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: Linear Attention과 GLA 레이어 구현\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: Linear Attention 커널 함수](#q1)\n",
    "- [Q2: Causal Linear Attention 순환](#q2)\n",
    "- [Q3: GLA 게이트 메커니즘](#q3)\n",
    "- [Q4: 메모리 비교 Standard vs Linear](#q4)\n",
    "- [종합 도전: Full GLA 레이어 구현 + 시퀀스 길이 스케일링 테스트](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 라이브러리 임포트 ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q1: Linear Attention 커널 함수 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Linear Attention에서 커널 함수 $\\phi$는 음수가 아닌 값을 보장해야 합니다.\n",
    "다음 세 가지 커널 함수를 구현하고 출력 범위를 비교하세요:\n",
    "\n",
    "1. $\\phi_1(x) = \\text{elu}(x) + 1$\n",
    "2. $\\phi_2(x) = \\text{ReLU}(x)$\n",
    "3. $\\phi_3(x) = 1 + x/\\sqrt{d} + x^2/(2d)$ (2차 Taylor 근사)\n",
    "\n",
    "**여러분의 예측:** 어떤 커널이 음수를 가장 잘 방지할까요? `elu+1 / ReLU / Taylor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q1 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: Linear Attention 커널 함수\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "x = tf.random.normal([1000, 64])\n",
    "\n",
    "# 커널 1: elu + 1\n",
    "phi_elu = tf.nn.elu(x) + 1.0\n",
    "\n",
    "# 커널 2: ReLU\n",
    "phi_relu = tf.nn.relu(x)\n",
    "\n",
    "# 커널 3: 2차 Taylor 근사\n",
    "d = 64.0\n",
    "phi_taylor = 1.0 + x / tf.sqrt(d) + x**2 / (2 * d)\n",
    "\n",
    "kernels = {'elu+1': phi_elu, 'ReLU': phi_relu, 'Taylor(2차)': phi_taylor}\n",
    "\n",
    "print(f\"커널 함수 비교 (입력: 표준정규분포, d={int(d)}):\")\n",
    "print(f\"{'커널':<14} | {'최솟값':>10} | {'최댓값':>10} | {'평균':>10} | {'음수 비율':>10}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for name, phi in kernels.items():\n",
    "    min_val = tf.reduce_min(phi).numpy()\n",
    "    max_val = tf.reduce_max(phi).numpy()\n",
    "    mean_val = tf.reduce_mean(phi).numpy()\n",
    "    neg_ratio = tf.reduce_mean(tf.cast(phi < 0, tf.float32)).numpy() * 100\n",
    "    print(f\"{name:<14} | {min_val:>10.4f} | {max_val:>10.4f} | {mean_val:>10.4f} | {neg_ratio:>8.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(\"  elu+1: 최솟값 > 0 보장 (elu의 최솟값 = -1, +1하면 0이상)\")\n",
    "print(\"  ReLU: 음수 = 0이 되어 정보 손실, 하지만 음수는 없음\")\n",
    "print(\"  Taylor: 이론적으로 exp의 근사이지만, 음수가 나올 수 있음\")\n",
    "print(\"  → 실전에서는 elu+1이 가장 안정적으로 사용됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q2: Causal Linear Attention 순환 <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "Causal Linear Attention의 순환 형태를 구현하세요:\n",
    "\n",
    "$$s_t = s_{t-1} + \\phi(k_t)^T v_t$$\n",
    "$$z_t = z_{t-1} + \\phi(k_t)$$\n",
    "$$o_t = \\frac{\\phi(q_t) \\cdot s_t}{\\phi(q_t) \\cdot z_t + \\epsilon}$$\n",
    "\n",
    "시퀀스 [1, 2, 3, 4]에 대해 각 스텝의 $s_t$, $z_t$, $o_t$를 추적하세요.\n",
    "\n",
    "**여러분의 예측:** $s_t$의 크기는 시간에 따라 `증가/감소/일정`할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: Causal Linear Attention 순환\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_k = 4\n",
    "d_v = 4\n",
    "seq_len = 8\n",
    "\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(seq_len, d_k).astype(np.float32)\n",
    "K = np.random.randn(seq_len, d_k).astype(np.float32)\n",
    "V = np.random.randn(seq_len, d_v).astype(np.float32)\n",
    "\n",
    "# 커널 함수: elu + 1\n",
    "def phi(x):\n",
    "    return np.where(x > 0, x + 1, np.exp(x))\n",
    "\n",
    "Q_phi = phi(Q)\n",
    "K_phi = phi(K)\n",
    "\n",
    "# 순환 계산\n",
    "s = np.zeros((d_k, d_v))  # 상태 행렬\n",
    "z = np.zeros(d_k)  # 정규화 벡터\n",
    "eps = 1e-6\n",
    "\n",
    "print(f\"Causal Linear Attention 순환 (d_k={d_k}, d_v={d_v}, seq={seq_len})\")\n",
    "print()\n",
    "\n",
    "outputs = []\n",
    "for t in range(seq_len):\n",
    "    # 상태 업데이트\n",
    "    s = s + np.outer(K_phi[t], V[t])  # [d_k, d_v]\n",
    "    z = z + K_phi[t]  # [d_k]\n",
    "\n",
    "    # 출력 계산\n",
    "    numerator = Q_phi[t] @ s  # [d_v]\n",
    "    denominator = Q_phi[t] @ z + eps  # scalar\n",
    "    o_t = numerator / denominator  # [d_v]\n",
    "    outputs.append(o_t)\n",
    "\n",
    "    s_norm = np.linalg.norm(s)\n",
    "    z_norm = np.linalg.norm(z)\n",
    "    o_norm = np.linalg.norm(o_t)\n",
    "    print(f\"  t={t}: ||s_t||={s_norm:.4f}, ||z_t||={z_norm:.4f}, ||o_t||={o_norm:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(\"  s_t의 노름은 시간에 따라 증가합니다 (누적)\")\n",
    "print(\"  이것이 Linear Attention의 한계: 과거 정보가 계속 쌓임\")\n",
    "print(\"  → GLA는 게이트로 이 문제를 해결합니다\")\n",
    "print(f\"  상태 행렬 크기: {d_k}x{d_v} = {d_k*d_v} (시퀀스 길이와 무관!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q3: GLA 게이트 메커니즘 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "GLA의 게이트가 있는 순환을 구현하고, 게이트 값에 따른 동작을 확인하세요:\n",
    "\n",
    "$$s_t = G_t \\odot s_{t-1} + k_t^T v_t$$\n",
    "\n",
    "실험:\n",
    "1. $G_t = 0.0$ (모든 과거 정보 제거)\n",
    "2. $G_t = 0.5$ (절반 유지)\n",
    "3. $G_t = 0.99$ (거의 모든 과거 유지)\n",
    "\n",
    "**여러분의 예측:** 게이트 값이 클수록 상태 노름이 `더 빠르게/더 느리게/비슷하게` 증가할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: GLA 게이트 메커니즘\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "seq_len = 50\n",
    "\n",
    "np.random.seed(42)\n",
    "K_test = np.random.randn(seq_len, d_k).astype(np.float32)\n",
    "V_test = np.random.randn(seq_len, d_v).astype(np.float32)\n",
    "Q_test = np.random.randn(seq_len, d_k).astype(np.float32)\n",
    "\n",
    "gate_values = [0.0, 0.5, 0.9, 0.99]\n",
    "all_norms = {}\n",
    "\n",
    "for g_val in gate_values:\n",
    "    s = np.zeros((d_k, d_v))\n",
    "    norms = []\n",
    "    for t in range(seq_len):\n",
    "        kv = np.outer(K_test[t], V_test[t])\n",
    "        s = g_val * s + kv\n",
    "        norms.append(np.linalg.norm(s))\n",
    "    all_norms[g_val] = norms\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"게이트 값별 최종 상태 노름 (t={seq_len-1}):\")\n",
    "print(f\"{'Gate':>6} | {'최종 ||s||':>12} | {'최대 ||s||':>12} | {'동작':>20}\")\n",
    "print(\"-\" * 58)\n",
    "for g_val in gate_values:\n",
    "    final = all_norms[g_val][-1]\n",
    "    peak = max(all_norms[g_val])\n",
    "    if g_val == 0.0:\n",
    "        behavior = \"즉시 망각 (최근만)\"\n",
    "    elif g_val < 0.9:\n",
    "        behavior = \"점진적 감쇠\"\n",
    "    elif g_val < 1.0:\n",
    "        behavior = \"느린 감쇠 (장기 기억)\"\n",
    "    else:\n",
    "        behavior = \"완전 누적\"\n",
    "    print(f\"{g_val:>6.2f} | {final:>12.4f} | {peak:>12.4f} | {behavior:>20}\")\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "for g_val in gate_values:\n",
    "    ax.plot(all_norms[g_val], lw=2, label=f'Gate = {g_val}')\n",
    "\n",
    "ax.set_xlabel('시간 스텝', fontsize=11)\n",
    "ax.set_ylabel('상태 노름 $||s_t||$', fontsize=11)\n",
    "ax.set_title('GLA 게이트 값에 따른 상태 행렬 크기 변화', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/practice_gla_gate_effect.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter16_sparse_attention/practice_gla_gate_effect.png\")\n",
    "\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(\"  Gate=0.0: 이전 상태 완전 제거 → 최근 토큰만 기억\")\n",
    "print(\"  Gate=0.5: 기하급수적 감쇠 → 짧은 범위 의존성\")\n",
    "print(\"  Gate=0.99: 느린 감쇠 → 장기 의존성 유지\")\n",
    "print(\"  GLA는 입력에 따라 Gate를 동적으로 조절하여 최적 기억 전략 학습\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q4: 메모리 비교 Standard vs Linear <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "시퀀스 길이 $N$, 차원 $d$일 때 다음을 계산하세요:\n",
    "\n",
    "| 측면 | Standard Attention | Linear Attention |\n",
    "|------|-------------------|------------------|\n",
    "| Attention 행렬 | $N \\times N$ | 없음 |\n",
    "| KV 저장 (추론) | $2Nd$ | $d^2$ |\n",
    "| FLOPs | $2N^2d$ | $2Nd^2$ |\n",
    "\n",
    "$d=128$에서 Standard보다 Linear가 메모리/연산 효율적인 $N$의 최소값은?\n",
    "\n",
    "**여러분의 예측:** $N \\geq$ `?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: 메모리 비교 Standard vs Linear\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d = 128\n",
    "\n",
    "# FLOPs 교차점: 2N^2d = 2Nd^2 → N = d\n",
    "print(f\"FLOPs 교차점 분석 (d={d}):\")\n",
    "print(f\"  Standard: 2N²d\")\n",
    "print(f\"  Linear:   2Nd²\")\n",
    "print(f\"  교차점: 2N²d = 2Nd² → N = d = {d}\")\n",
    "print(f\"  → N > {d}이면 Linear가 FLOPs 효율적\")\n",
    "\n",
    "# KV 메모리 교차점: 2Nd = d^2 → N = d/2\n",
    "print(f\"\\nKV 메모리 교차점:\")\n",
    "print(f\"  Standard: 2Nd\")\n",
    "print(f\"  Linear:   d²\")\n",
    "print(f\"  교차점: 2Nd = d² → N = d/2 = {d//2}\")\n",
    "print(f\"  → N > {d//2}이면 Linear가 KV 메모리 효율적\")\n",
    "\n",
    "# 수치 비교 표\n",
    "N_values = [32, 64, 128, 256, 512, 1024, 4096, 16384]\n",
    "print(f\"\\n상세 비교 (d={d}):\")\n",
    "print(f\"{'N':>8} | {'Std FLOPs':>12} | {'Lin FLOPs':>12} | {'비율':>8} | {'Std KV':>10} | {'Lin KV':>10}\")\n",
    "print(\"-\" * 72)\n",
    "\n",
    "for N in N_values:\n",
    "    std_flops = 2 * N * N * d\n",
    "    lin_flops = 2 * N * d * d\n",
    "    ratio = std_flops / lin_flops\n",
    "\n",
    "    std_kv = 2 * N * d\n",
    "    lin_kv = d * d\n",
    "\n",
    "    marker = \" ✅\" if ratio > 1 else \"\"\n",
    "    print(f\"{N:>8} | {std_flops:>12,} | {lin_flops:>12,} | {ratio:>7.1f}x | \"\n",
    "          f\"{std_kv:>10,} | {lin_kv:>10,}{marker}\")\n",
    "\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(f\"  N = d = {d}이 교차점: 이 이상에서 Linear가 유리\")\n",
    "print(f\"  N = 4096: Standard는 Linear보다 {4096//d}배 더 많은 FLOPs 필요\")\n",
    "print(f\"  N = 16384: {16384//d}배 차이 → 긴 시퀀스에서 압도적 우위\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 도전: Full GLA 레이어 구현 + 시퀀스 길이 스케일링 테스트 <a name='bonus'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "완전한 GLA(Gated Linear Attention) 레이어를 TF 클래스로 구현하고:\n",
    "1. Standard Attention과 출력 비교\n",
    "2. 시퀀스 길이 [64, 128, 256, 512, 1024]에서 실행 시간 비교\n",
    "3. 스케일링 그래프 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이: Full GLA 레이어 구현 ─────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"종합 도전: Full GLA 레이어 구현\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class GLALayer(tf.keras.layers.Layer):\n",
    "    # Gated Linear Attention 레이어\n",
    "\n",
    "    def __init__(self, d_model, d_key, d_value):\n",
    "        super().__init__()\n",
    "        self.d_key = d_key\n",
    "        self.d_value = d_value\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(d_key)\n",
    "        self.W_k = tf.keras.layers.Dense(d_key)\n",
    "        self.W_v = tf.keras.layers.Dense(d_value)\n",
    "        self.W_g = tf.keras.layers.Dense(d_key * d_value)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        q = self.W_q(x)\n",
    "        k = tf.nn.elu(self.W_k(x)) + 1.0  # 커널 함수 적용\n",
    "        v = self.W_v(x)\n",
    "        g = tf.sigmoid(self.W_g(x))\n",
    "        g = tf.reshape(g, [batch_size, seq_len, self.d_key, self.d_value])\n",
    "\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=seq_len)\n",
    "        s = tf.zeros([batch_size, self.d_key, self.d_value])\n",
    "\n",
    "        for t in tf.range(seq_len):\n",
    "            k_t = k[:, t, :]\n",
    "            v_t = v[:, t, :]\n",
    "            q_t = q[:, t, :]\n",
    "            g_t = g[:, t, :, :]\n",
    "\n",
    "            kv = tf.einsum('bi,bj->bij', k_t, v_t)\n",
    "            s = g_t * s + kv\n",
    "            o_t = tf.einsum('bi,bij->bj', q_t, s)\n",
    "            outputs = outputs.write(t, o_t)\n",
    "\n",
    "        output = tf.transpose(outputs.stack(), [1, 0, 2])\n",
    "        output = self.W_o(output) + residual\n",
    "        return output\n",
    "\n",
    "\n",
    "class StandardAttnLayer(tf.keras.layers.Layer):\n",
    "    # 표준 Attention 레이어 (비교 기준)\n",
    "\n",
    "    def __init__(self, d_model, d_key, d_value):\n",
    "        super().__init__()\n",
    "        self.d_key = d_key\n",
    "        self.W_q = tf.keras.layers.Dense(d_key)\n",
    "        self.W_k = tf.keras.layers.Dense(d_key)\n",
    "        self.W_v = tf.keras.layers.Dense(d_value)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        q = self.W_q(x)\n",
    "        k = self.W_k(x)\n",
    "        v = self.W_v(x)\n",
    "        d = tf.cast(self.d_key, tf.float32)\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        mask = tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
    "        scores = scores + (1.0 - mask) * (-1e9)\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        return self.W_o(output) + residual\n",
    "\n",
    "\n",
    "# 모델 생성\n",
    "d_model = 64\n",
    "d_key = 32\n",
    "d_value = 32\n",
    "\n",
    "gla_layer = GLALayer(d_model, d_key, d_value)\n",
    "std_layer = StandardAttnLayer(d_model, d_key, d_value)\n",
    "\n",
    "# 시퀀스 길이별 실행 시간 비교\n",
    "seq_lengths = [64, 128, 256, 512]\n",
    "gla_times = []\n",
    "std_times = []\n",
    "batch = 2\n",
    "\n",
    "print(f\"시퀀스 길이별 실행 시간 비교 (d_model={d_model}, d_k={d_key}):\")\n",
    "print(f\"{'시퀀스':>8} | {'Standard (ms)':>14} | {'GLA (ms)':>14} | {'비율':>8}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for sl in seq_lengths:\n",
    "    x = tf.random.normal([batch, sl, d_model])\n",
    "\n",
    "    # 워밍업\n",
    "    _ = std_layer(x)\n",
    "    _ = gla_layer(x)\n",
    "\n",
    "    n_runs = 3\n",
    "    t0 = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = std_layer(x)\n",
    "    std_time = (time.time() - t0) / n_runs * 1000\n",
    "\n",
    "    t0 = time.time()\n",
    "    for _ in range(n_runs):\n",
    "        _ = gla_layer(x)\n",
    "    gla_time = (time.time() - t0) / n_runs * 1000\n",
    "\n",
    "    std_times.append(std_time)\n",
    "    gla_times.append(gla_time)\n",
    "    ratio = std_time / max(gla_time, 0.01)\n",
    "    print(f\"{sl:>8} | {std_time:>14.2f} | {gla_time:>14.2f} | {ratio:>7.2f}x\")\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) 실행 시간\n",
    "ax1 = axes[0]\n",
    "ax1.plot(seq_lengths, std_times, 'r-o', lw=2.5, ms=8, label='Standard Attention')\n",
    "ax1.plot(seq_lengths, gla_times, 'g-s', lw=2.5, ms=8, label='GLA (순환)')\n",
    "ax1.set_xlabel('시퀀스 길이', fontsize=11)\n",
    "ax1.set_ylabel('실행 시간 (ms)', fontsize=11)\n",
    "ax1.set_title('Standard vs GLA 실행 시간', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) 이론적 메모리 비교\n",
    "ax2 = axes[1]\n",
    "std_mem = [s**2 * 4 / 1024 for s in seq_lengths]  # Attention matrix (KB)\n",
    "gla_mem = [d_key * d_value * 4 / 1024 for _ in seq_lengths]  # State matrix (KB)\n",
    "ax2.plot(seq_lengths, std_mem, 'r-o', lw=2.5, ms=8, label='Standard ($N^2$)')\n",
    "ax2.plot(seq_lengths, gla_mem, 'g-s', lw=2.5, ms=8, label='GLA ($d^2$)')\n",
    "ax2.set_xlabel('시퀀스 길이', fontsize=11)\n",
    "ax2.set_ylabel('Attention 메모리 (KB)', fontsize=11)\n",
    "ax2.set_title('메모리 사용량 비교', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/practice_gla_scaling.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter16_sparse_attention/practice_gla_scaling.png\")\n",
    "\n",
    "print(f\"\\n핵심 결론:\")\n",
    "print(f\"  GLA 상태 메모리: {d_key*d_value*4/1024:.2f} KB (시퀀스 길이와 무관!)\")\n",
    "print(f\"  Standard N=512 메모리: {512**2*4/1024:.0f} KB\")\n",
    "print(f\"  GLA는 긴 시퀀스에서 메모리 측면의 압도적 우위를 가짐\")\n",
    "print(f\"  (단, 순환 형태의 Python 루프로 인해 속도는 최적화된 Standard보다 느릴 수 있음)\")\n",
    "print(f\"  실전에서는 chunk-wise 병렬 + 하드웨어 커널 최적화를 적용합니다\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}