{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 퀴즈: Multi-head Latent Attention (MLA) 구현\n",
    "\n",
    "## 사용 방법\n",
    "- 각 문제 셀을 읽고, **직접 답을 예측한 후** 풀이 셀을 실행하세요\n",
    "- 코드 실행 전에 종이에 계산해보는 것을 권장합니다\n",
    "\n",
    "## 목차\n",
    "- [Q1: Down-projection 차원 계산](#q1)\n",
    "- [Q2: KV Cache 크기 비교 (MLA vs GQA vs MHA)](#q2)\n",
    "- [Q3: Up-projection 복원 정확도](#q3)\n",
    "- [Q4: 압축 KV로 Attention 계산](#q4)\n",
    "- [종합 도전: Full MLA 레이어 구현 + 메모리 측정](#bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 라이브러리 임포트 ──────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q1: Down-projection 차원 계산 <a name='q1'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "DeepSeek-V2 모델의 파라미터:\n",
    "- $d_{model} = 5120$\n",
    "- $H = 128$ (Attention 헤드 수)\n",
    "- $d_h = 128$ (헤드 차원)\n",
    "\n",
    "MLA에서 KV Cache를 원래의 $1/16$로 줄이려면 압축 차원 $d_c$는 얼마가 되어야 할까요?\n",
    "\n",
    "**여러분의 예측:** $d_c$ = `?`\n",
    "\n",
    "힌트: $d_c / (2 \\times H \\times d_h) = 1/16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\\n",
    "# ── Q1 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q1 풀이: Down-projection 차원 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_model = 5120\n",
    "H = 128\n",
    "d_h = 128\n",
    "\n",
    "# MHA의 KV Cache 크기 (토큰당)\n",
    "kv_mha = 2 * H * d_h\n",
    "print(f\"MHA KV Cache (토큰당): 2 × {H} × {d_h} = {kv_mha}\")\n",
    "\n",
    "# 1/16로 줄이려면\n",
    "compression_ratio = 1 / 16\n",
    "d_c = int(kv_mha * compression_ratio)\n",
    "print(f\"\\n목표 압축률: {compression_ratio} (= 1/16)\")\n",
    "print(f\"d_c = {kv_mha} × {compression_ratio} = {d_c}\")\n",
    "\n",
    "# 검증\n",
    "actual_ratio = d_c / kv_mha\n",
    "print(f\"\\n검증: {d_c} / {kv_mha} = {actual_ratio:.4f} = 1/{int(1/actual_ratio)}\")\n",
    "print(f\"→ d_c = {d_c}이면 KV Cache가 정확히 1/16로 줄어듭니다\")\n",
    "\n",
    "# 실제 DeepSeek-V2 설정과 비교\n",
    "print(f\"\\n참고: DeepSeek-V2 실제 설정 d_c = 512\")\n",
    "print(f\"  실제 비율: 512 / {kv_mha} = {512/kv_mha:.4f} = 1/{kv_mha//512}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q2: KV Cache 크기 비교 (MLA vs GQA vs MHA) <a name='q2'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "다음 모델 설정에서 시퀀스 길이 $S=4096$, 배치 크기 $B=1$, 레이어 수 $L=60$일 때 \n",
    "각 방식의 총 KV Cache 크기(MB)를 FP16 기준으로 계산하세요:\n",
    "\n",
    "| 방식 | 설정 |\n",
    "|------|------|\n",
    "| MHA | $H=128$, $d_h=128$ |\n",
    "| GQA | $H_{kv}=8$, $d_h=128$ |\n",
    "| MLA | $d_c=512$ |\n",
    "\n",
    "**여러분의 예측:** MHA = `?` MB, GQA = `?` MB, MLA = `?` MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q2 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q2 풀이: KV Cache 크기 비교\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "S = 4096\n",
    "B = 1\n",
    "L = 60\n",
    "bytes_per_elem = 2  # FP16\n",
    "\n",
    "# MHA: 2 * H * d_h per token per layer\n",
    "mha_per_token = 2 * 128 * 128\n",
    "mha_total = B * S * L * mha_per_token * bytes_per_elem\n",
    "mha_mb = mha_total / (1024 ** 2)\n",
    "\n",
    "# GQA: 2 * H_kv * d_h per token per layer\n",
    "gqa_per_token = 2 * 8 * 128\n",
    "gqa_total = B * S * L * gqa_per_token * bytes_per_elem\n",
    "gqa_mb = gqa_total / (1024 ** 2)\n",
    "\n",
    "# MLA: d_c per token per layer\n",
    "mla_per_token = 512\n",
    "mla_total = B * S * L * mla_per_token * bytes_per_elem\n",
    "mla_mb = mla_total / (1024 ** 2)\n",
    "\n",
    "print(f\"설정: S={S}, B={B}, L={L}, FP16\")\n",
    "print()\n",
    "print(f\"{'방식':<8} | {'토큰당 KV':>12} | {'총 바이트':>15} | {'크기 (MB)':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'MHA':<8} | {mha_per_token:>12} | {mha_total:>15,} | {mha_mb:>10.1f}\")\n",
    "print(f\"{'GQA':<8} | {gqa_per_token:>12} | {gqa_total:>15,} | {gqa_mb:>10.1f}\")\n",
    "print(f\"{'MLA':<8} | {mla_per_token:>12} | {mla_total:>15,} | {mla_mb:>10.1f}\")\n",
    "print()\n",
    "print(f\"[해설]\")\n",
    "print(f\"  MLA는 MHA 대비 {(1-mla_mb/mha_mb)*100:.1f}% 절감\")\n",
    "print(f\"  MLA는 GQA 대비 {(1-mla_mb/gqa_mb)*100:.1f}% 절감\")\n",
    "print(f\"  특히 H가 크고 d_c가 작을수록 MLA의 이점이 극대화됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q3: Up-projection 복원 정확도 <a name='q3'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "$d_{model}=256$, $d_c=32$, $H=4$, $d_h=64$인 MLA에서:\n",
    "1. Down-projection과 Up-projection을 수행하세요\n",
    "2. 원본 KV와 복원된 KV의 코사인 유사도를 측정하세요\n",
    "3. $d_c$를 $\\{16, 32, 64, 128\\}$로 변경하며 유사도 변화를 관찰하세요\n",
    "\n",
    "**여러분의 예측:** $d_c$가 커질수록 유사도가 `증가/감소`할까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q3 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q3 풀이: Up-projection 복원 정확도\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_model = 256\n",
    "H = 4\n",
    "d_h = 64\n",
    "batch = 4\n",
    "seq_len = 32\n",
    "\n",
    "# 원본 hidden states\n",
    "h = tf.random.normal([batch, seq_len, d_model])\n",
    "\n",
    "# 원본 KV (MHA 방식)\n",
    "W_kv_ref = tf.keras.layers.Dense(2 * H * d_h, use_bias=False)\n",
    "kv_original = W_kv_ref(h)\n",
    "\n",
    "# 다양한 d_c에서 복원 품질 측정\n",
    "d_c_values = [16, 32, 64, 128, 256]\n",
    "print(f\"원본 KV 차원: {2 * H * d_h}\")\n",
    "print()\n",
    "print(f\"{'d_c':>6} | {'코사인 유사도':>14} | {'상대 오차':>12} | {'압축률':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for d_c in d_c_values:\n",
    "    W_down = tf.keras.layers.Dense(d_c, use_bias=False)\n",
    "    W_up = tf.keras.layers.Dense(2 * H * d_h, use_bias=False)\n",
    "\n",
    "    c = W_down(h)\n",
    "    kv_restored = W_up(c)\n",
    "\n",
    "    # 코사인 유사도\n",
    "    orig_flat = tf.reshape(kv_original, [-1, kv_original.shape[-1]])\n",
    "    rest_flat = tf.reshape(kv_restored, [-1, kv_restored.shape[-1]])\n",
    "    cos_sim = tf.reduce_mean(\n",
    "        tf.reduce_sum(orig_flat * rest_flat, axis=-1) /\n",
    "        (tf.norm(orig_flat, axis=-1) * tf.norm(rest_flat, axis=-1) + 1e-8)\n",
    "    ).numpy()\n",
    "\n",
    "    # 상대 오차\n",
    "    rel_error = (tf.norm(kv_original - kv_restored) / tf.norm(kv_original)).numpy()\n",
    "\n",
    "    compression = d_c / (2 * H * d_h) * 100\n",
    "    print(f\"{d_c:>6} | {cos_sim:>14.4f} | {rel_error:>12.4f} | {compression:>8.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"[해설]\")\n",
    "print(\"  d_c가 커질수록 코사인 유사도가 증가합니다 (복원 품질 향상)\")\n",
    "print(\"  하지만 초기화 상태(미학습)이므로 유사도가 낮을 수 있습니다\")\n",
    "print(\"  학습을 통해 Down/Up projection이 최적화되면 유사도가 크게 향상됩니다\")\n",
    "print(\"  실전에서는 d_c=512 정도면 충분한 복원 품질을 달성합니다\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## Q4: 압축 KV로 Attention 계산 <a name='q4'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "MLA의 전체 attention 과정을 구현하세요:\n",
    "1. $h \\rightarrow c^{KV}$ (Down-projection)\n",
    "2. $c^{KV} \\rightarrow [K, V]$ (Up-projection)  \n",
    "3. $h \\rightarrow Q$ (Q projection)\n",
    "4. $\\text{Attention}(Q, K, V)$ 계산\n",
    "\n",
    "**여러분의 예측:** 출력 shape은 `[batch, seq_len, ?]`이 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Q4 풀이 ──────────────────────────────────────────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"Q4 풀이: 압축 KV로 Attention 계산\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "d_model = 128\n",
    "n_heads = 4\n",
    "d_head = 32\n",
    "d_c = 32\n",
    "batch = 2\n",
    "seq_len = 16\n",
    "\n",
    "# MLA 모듈 정의\n",
    "W_q = tf.keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
    "W_down = tf.keras.layers.Dense(d_c, use_bias=False)\n",
    "W_up = tf.keras.layers.Dense(2 * n_heads * d_head, use_bias=False)\n",
    "W_o = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "\n",
    "# 입력\n",
    "h = tf.random.normal([batch, seq_len, d_model])\n",
    "print(f\"입력 h: {h.shape}\")\n",
    "\n",
    "# Step 1: Q projection\n",
    "q = W_q(h)\n",
    "q = tf.reshape(q, [batch, seq_len, n_heads, d_head])\n",
    "q = tf.transpose(q, [0, 2, 1, 3])  # [B, H, S, d_h]\n",
    "print(f\"Q: {q.shape}\")\n",
    "\n",
    "# Step 2: KV down-projection (압축)\n",
    "c_kv = W_down(h)  # [B, S, d_c]\n",
    "print(f\"압축 벡터 c_kv: {c_kv.shape} (이것만 KV Cache에 저장!)\")\n",
    "\n",
    "# Step 3: KV up-projection (복원)\n",
    "kv = W_up(c_kv)\n",
    "kv = tf.reshape(kv, [batch, seq_len, 2, n_heads, d_head])\n",
    "k = tf.transpose(kv[:, :, 0], [0, 2, 1, 3])  # [B, H, S, d_h]\n",
    "v = tf.transpose(kv[:, :, 1], [0, 2, 1, 3])  # [B, H, S, d_h]\n",
    "print(f\"K (복원): {k.shape}\")\n",
    "print(f\"V (복원): {v.shape}\")\n",
    "\n",
    "# Step 4: Attention 계산\n",
    "d_float = tf.cast(d_head, tf.float32)\n",
    "scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d_float)\n",
    "\n",
    "# Causal mask\n",
    "mask = tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
    "scores = scores + (1.0 - mask) * (-1e9)\n",
    "\n",
    "weights = tf.nn.softmax(scores, axis=-1)\n",
    "attn_out = tf.matmul(weights, v)  # [B, H, S, d_h]\n",
    "print(f\"Attention 출력: {attn_out.shape}\")\n",
    "\n",
    "# 헤드 합치기 + output projection\n",
    "attn_out = tf.transpose(attn_out, [0, 2, 1, 3])  # [B, S, H, d_h]\n",
    "attn_out = tf.reshape(attn_out, [batch, seq_len, n_heads * d_head])\n",
    "output = W_o(attn_out)\n",
    "print(f\"최종 출력: {output.shape}\")\n",
    "\n",
    "print(f\"\n",
    "[해설]\")\n",
    "print(f\"  입력 shape = 출력 shape = [{batch}, {seq_len}, {d_model}]\")\n",
    "print(f\"  KV Cache에 저장되는 것: c_kv = [{batch}, {seq_len}, {d_c}]\")\n",
    "print(f\"  MHA였다면: [{batch}, {seq_len}, {2*n_heads*d_head}]를 저장해야 함\")\n",
    "print(f\"  메모리 절감: {d_c} vs {2*n_heads*d_head} = {d_c/(2*n_heads*d_head)*100:.1f}%만 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 종합 도전: Full MLA 레이어 구현 + 메모리 측정 <a name='bonus'></a>\n",
    "\n",
    "### 문제\n",
    "\n",
    "완전한 MLA 레이어를 TF 클래스로 구현하고, MHA/GQA와의 메모리 사용량을 정량적으로 비교하세요.\n",
    "\n",
    "요구사항:\n",
    "1. `MLALayer` 클래스: Down/Up projection + Multi-head Attention + Output projection\n",
    "2. `MHALayer` 클래스: 표준 Multi-Head Attention (비교 기준)\n",
    "3. 시퀀스 길이 [128, 256, 512, 1024]에서 KV Cache 메모리 비교 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 종합 도전 풀이: Full MLA vs MHA 구현 및 비교 ─────────────────\n",
    "print(\"=\" * 45)\n",
    "print(\"종합 도전: Full MLA 레이어 구현\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "class MLALayer(tf.keras.layers.Layer):\n",
    "    # Multi-head Latent Attention 레이어\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_head, d_compress):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.d_compress = d_compress\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
    "        self.W_down = tf.keras.layers.Dense(d_compress, use_bias=False)\n",
    "        self.W_up = tf.keras.layers.Dense(2 * n_heads * d_head, use_bias=False)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, return_cache=False):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # Q projection\n",
    "        q = tf.reshape(self.W_q(x), [batch, seq_len, self.n_heads, self.d_head])\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "\n",
    "        # KV compression + restoration\n",
    "        c_kv = self.W_down(x)\n",
    "        kv = tf.reshape(self.W_up(c_kv), [batch, seq_len, 2, self.n_heads, self.d_head])\n",
    "        k = tf.transpose(kv[:, :, 0], [0, 2, 1, 3])\n",
    "        v = tf.transpose(kv[:, :, 1], [0, 2, 1, 3])\n",
    "\n",
    "        # Attention\n",
    "        scale = tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / scale\n",
    "        mask = tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
    "        scores = scores + (1.0 - mask) * (-1e9)\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        out = tf.matmul(weights, v)\n",
    "\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [batch, seq_len, self.n_heads * self.d_head])\n",
    "        out = self.W_o(out) + residual\n",
    "\n",
    "        if return_cache:\n",
    "            return out, c_kv\n",
    "        return out\n",
    "\n",
    "\n",
    "class MHALayer(tf.keras.layers.Layer):\n",
    "    # 표준 Multi-Head Attention 레이어 (비교 기준)\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_head):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "\n",
    "        self.W_q = tf.keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
    "        self.W_k = tf.keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
    "        self.W_v = tf.keras.layers.Dense(n_heads * d_head, use_bias=False)\n",
    "        self.W_o = tf.keras.layers.Dense(d_model, use_bias=False)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, return_cache=False):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        q = tf.reshape(self.W_q(x), [batch, seq_len, self.n_heads, self.d_head])\n",
    "        k = tf.reshape(self.W_k(x), [batch, seq_len, self.n_heads, self.d_head])\n",
    "        v = tf.reshape(self.W_v(x), [batch, seq_len, self.n_heads, self.d_head])\n",
    "\n",
    "        q = tf.transpose(q, [0, 2, 1, 3])\n",
    "        k = tf.transpose(k, [0, 2, 1, 3])\n",
    "        v = tf.transpose(v, [0, 2, 1, 3])\n",
    "\n",
    "        scale = tf.sqrt(tf.cast(self.d_head, tf.float32))\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / scale\n",
    "        mask = tf.linalg.band_part(tf.ones([seq_len, seq_len]), -1, 0)\n",
    "        scores = scores + (1.0 - mask) * (-1e9)\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        out = tf.matmul(weights, v)\n",
    "\n",
    "        out = tf.transpose(out, [0, 2, 1, 3])\n",
    "        out = tf.reshape(out, [batch, seq_len, self.n_heads * self.d_head])\n",
    "        out = self.W_o(out) + residual\n",
    "\n",
    "        if return_cache:\n",
    "            kv_cache = tf.concat([\n",
    "                tf.reshape(k, [batch, seq_len, -1]),\n",
    "                tf.reshape(v, [batch, seq_len, -1])\n",
    "            ], axis=-1)\n",
    "            return out, kv_cache\n",
    "        return out\n",
    "\n",
    "\n",
    "# 파라미터 설정\n",
    "d_model = 256\n",
    "n_heads = 8\n",
    "d_head = 32\n",
    "d_compress = 32\n",
    "batch = 2\n",
    "\n",
    "mla = MLALayer(d_model, n_heads, d_head, d_compress)\n",
    "mha = MHALayer(d_model, n_heads, d_head)\n",
    "\n",
    "# 시퀀스 길이별 KV Cache 비교\n",
    "seq_lengths = [128, 256, 512, 1024]\n",
    "mla_cache_sizes = []\n",
    "mha_cache_sizes = []\n",
    "\n",
    "print(f\"KV Cache 크기 비교 (d_model={d_model}, H={n_heads}, d_h={d_head}, d_c={d_compress}):\")\n",
    "print(f\"{'시퀀스':>8} | {'MHA Cache':>12} | {'MLA Cache':>12} | {'절감률':>8}\")\n",
    "print(\"-\" * 48)\n",
    "\n",
    "for sl in seq_lengths:\n",
    "    x = tf.random.normal([batch, sl, d_model])\n",
    "\n",
    "    _, mla_cache = mla(x, return_cache=True)\n",
    "    _, mha_cache = mha(x, return_cache=True)\n",
    "\n",
    "    mla_size = np.prod(mla_cache.shape) * 4  # FP32 bytes\n",
    "    mha_size = np.prod(mha_cache.shape) * 4\n",
    "\n",
    "    mla_cache_sizes.append(mla_size)\n",
    "    mha_cache_sizes.append(mha_size)\n",
    "\n",
    "    saving = (1 - mla_size / mha_size) * 100\n",
    "    print(f\"{sl:>8} | {mha_size:>10,} B | {mla_size:>10,} B | {saving:>7.1f}%\")\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "x_pos = range(len(seq_lengths))\n",
    "width = 0.35\n",
    "bars1 = ax.bar([p - width/2 for p in x_pos], [s/1024 for s in mha_cache_sizes],\n",
    "               width, label='MHA', color='red', alpha=0.7)\n",
    "bars2 = ax.bar([p + width/2 for p in x_pos], [s/1024 for s in mla_cache_sizes],\n",
    "               width, label='MLA', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(s) for s in seq_lengths])\n",
    "ax.set_xlabel('시퀀스 길이', fontsize=11)\n",
    "ax.set_ylabel('KV Cache 크기 (KB)', fontsize=11)\n",
    "ax.set_title('MHA vs MLA KV Cache 크기 비교', fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('practice_mla_cache_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\n그래프 저장됨: chapter16_sparse_attention/practice_mla_cache_comparison.png\")\n",
    "\n",
    "# 총 파라미터 수 비교\n",
    "mla_params = sum(np.prod(v.shape) for v in mla.trainable_variables)\n",
    "mha_params = sum(np.prod(v.shape) for v in mha.trainable_variables)\n",
    "print(f\"\\n모델 파라미터 수:\")\n",
    "print(f\"  MHA: {mha_params:,}\")\n",
    "print(f\"  MLA: {mla_params:,}\")\n",
    "print(f\"  MLA는 Down/Up projection 추가로 파라미터가 약간 더 많지만,\")\n",
    "print(f\"  KV Cache가 {d_compress/(2*n_heads*d_head)*100:.1f}%만 필요하여 추론 메모리를 크게 절약합니다!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}