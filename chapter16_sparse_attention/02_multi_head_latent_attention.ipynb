{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Multi-head Latent Attention (MLA)\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- MLAì˜ KV ì••ì¶•(down-projection)ê³¼ ë³µì›(up-projection) ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•œë‹¤\n",
    "- MHA, MQA, GQA, MLAì˜ KV Cache í¬ê¸°ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•œë‹¤\n",
    "- MLAê°€ GQA ëŒ€ë¹„ ë‹¬ì„±í•˜ëŠ” ë©”ëª¨ë¦¬ ì ˆê°ë¥ ì„ ìˆ˜ì¹˜ë¡œ ë¶„ì„í•œë‹¤\n",
    "- ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¥¸ KV Cache ë©”ëª¨ë¦¬ ë³€í™”ë¥¼ ì‹œê°í™”í•œë‹¤\n",
    "- MLAì˜ attention quality ìœ ì§€ ë©”ì»¤ë‹ˆì¦˜ì„ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: Attentionê³¼ KV ì••ì¶•](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [MLA Down/Up Projection êµ¬í˜„](#2.-MLA-Projection-êµ¬í˜„)\n",
    "3. [GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ](#3.-ë©”ëª¨ë¦¬-ë¹„êµ)\n",
    "4. [KV Cache í¬ê¸° ê³„ì‚°](#4.-KV-Cache-í¬ê¸°-ê³„ì‚°)\n",
    "5. [Attention Quality ë¹„êµ](#5.-Attention-Quality-ë¹„êµ)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### í‘œì¤€ Multi-Head Attention (MHA) ë³µìŠµ\n",
    "\n",
    "$$\\text{Attn}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_h}}\\right)V$$\n",
    "\n",
    "KV Cache í¬ê¸° (í† í°ë‹¹):\n",
    "$$M_{KV}^{MHA} = 2 \\times H \\times d_h = 2 \\times d_{model}$$\n",
    "\n",
    "- $H$: í—¤ë“œ ìˆ˜, $d_h$: í—¤ë“œ ì°¨ì›, $d_{model} = H \\times d_h$\n",
    "\n",
    "### Grouped-Query Attention (GQA)\n",
    "\n",
    "$$M_{KV}^{GQA} = 2 \\times H_{kv} \\times d_h$$\n",
    "\n",
    "- $H_{kv} \\ll H$: KV í—¤ë“œ ìˆ˜ (Q í—¤ë“œë¥¼ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ ê³µìœ )\n",
    "- ì ˆê°ë¥ : $H_{kv} / H$ (ì˜ˆ: Llama 3 8Bì—ì„œ $H=32, H_{kv}=8$ â†’ 75% ì ˆê°)\n",
    "\n",
    "### Multi-head Latent Attention (MLA) â€” DeepSeek-V2/V3\n",
    "\n",
    "**Step 1: Down-projection (KV ì••ì¶•)**\n",
    "\n",
    "$$c_t^{KV} = W_d^{KV} h_t \\in \\mathbb{R}^{d_c}$$\n",
    "\n",
    "- $W_d^{KV} \\in \\mathbb{R}^{d_c \\times d_{model}}$: ì••ì¶• í–‰ë ¬\n",
    "- $d_c \\ll d_{model}$: ì••ì¶• ì°¨ì› (ì˜ˆ: DeepSeek-V2ì—ì„œ $d_c = 512$, $d_{model} = 5120$)\n",
    "\n",
    "**Step 2: Up-projection (KV ë³µì›)**\n",
    "\n",
    "$$[k_t^C;\\; v_t^C] = W_u^{KV} c_t^{KV} \\in \\mathbb{R}^{2 \\times H \\times d_h}$$\n",
    "\n",
    "- $W_u^{KV} \\in \\mathbb{R}^{(2Hd_h) \\times d_c}$: ë³µì› í–‰ë ¬\n",
    "- ë³µì›ëœ $k_t^C, v_t^C$ë¡œ í‘œì¤€ attention ìˆ˜í–‰\n",
    "\n",
    "**KV Cache í¬ê¸° (MLA, í† í°ë‹¹):**\n",
    "\n",
    "$$M_{KV}^{MLA} = d_c \\quad \\text{(ì˜¤ì§ ì••ì¶• ë²¡í„°ë§Œ ì €ì¥)}$$\n",
    "\n",
    "**ì ˆê°ë¥ :**\n",
    "\n",
    "$$\\text{ì ˆê°ë¥ } = \\frac{d_c}{2 \\times H_{kv} \\times d_h} \\quad \\text{(GQA ëŒ€ë¹„)}$$\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| ë°©ì‹ | KV Cache (í† í°ë‹¹) | ì˜ˆì‹œ ($d=5120, H=40, d_h=128$) |\n",
    "|------|-------------------|--------------------------------|\n",
    "| MHA | $2Hd_h = 2d_{model}$ | $10240$ |\n",
    "| GQA ($H_{kv}=8$) | $2 H_{kv} d_h$ | $2048$ |\n",
    "| MQA | $2d_h$ | $256$ |\n",
    "| MLA ($d_c=512$) | $d_c$ | $512$ |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ MLA ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ MLAê°€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ë„ì„œê´€ì— ì±…(KV Cache)ì„ ë³´ê´€í•  ë•Œë¥¼ ìƒê°í•´ ë³´ì„¸ìš”!\n",
    "\n",
    "- **MHA**: ëª¨ë“  ì±…ì„ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³´ê´€ â†’ ê³µê°„ì´ ì—„ì²­ë‚˜ê²Œ í•„ìš”í•´ìš”\n",
    "- **GQA**: ë¹„ìŠ·í•œ ì±…ë¼ë¦¬ ë¬¶ì–´ì„œ ëŒ€í‘œ í•œ ê¶Œë§Œ ë³´ê´€ â†’ ê³µê°„ ì ˆì•½!\n",
    "- **MLA**: ëª¨ë“  ì±…ì˜ **ìš”ì•½ë³¸**(ì••ì¶• ë²¡í„°)ë§Œ ë³´ê´€í•˜ê³ , í•„ìš”í•  ë•Œ ì›ë³¸ì„ ë³µì›í•´ìš” â†’ ìµœê³ ì˜ ì ˆì•½!\n",
    "\n",
    "#### ğŸ“¦ ì–´ë–»ê²Œ ì••ì¶•í•˜ë‚˜ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: 5120ê°œì˜ ìˆ«ìë¥¼ 512ê°œë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì€, \n",
    "> ê¸´ ë¬¸ì¥ì„ í•µì‹¬ í‚¤ì›Œë“œë¡œ ìš”ì•½í•˜ëŠ” ê²ƒê³¼ ê°™ì•„ìš”!\n",
    "\n",
    "1. **ì••ì¶•(Down)**: $5120 \\rightarrow 512$ (10ë°° ì¤„ì´ê¸°)\n",
    "2. **ì €ì¥**: 512ê°œì˜ ìˆ«ìë§Œ KV Cacheì— ì €ì¥\n",
    "3. **ë³µì›(Up)**: $512 \\rightarrow 5120$ (í•„ìš”í•  ë•Œ ë³µì›)\n",
    "\n",
    "í•µì‹¬ì€ **ì €ì¥í•  ë•Œë§Œ ì‘ê²Œ, ì‚¬ìš©í•  ë•ŒëŠ” ì›ë˜ í¬ê¸°ë¡œ** ëŒë¦¬ëŠ” ê±°ì˜ˆìš”!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: MLA ì••ì¶•ë¥  ê³„ì‚°\n",
    "\n",
    "DeepSeek-V2 ê¸°ì¤€: $d_{model}=5120$, $H=40$, $d_h=128$, $d_c=512$.\n",
    "MLAì˜ MHA ëŒ€ë¹„ KV Cache ì ˆê°ë¥ (%)ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$M_{KV}^{MHA} = 2 \\times H \\times d_h = 2 \\times 40 \\times 128 = 10240$$\n",
    "\n",
    "$$M_{KV}^{MLA} = d_c = 512$$\n",
    "\n",
    "$$\\text{ì ˆê°ë¥ } = 1 - \\frac{512}{10240} = 1 - 0.05 = 0.95 = 95\\%$$\n",
    "\n",
    "â†’ MLAëŠ” MHA ëŒ€ë¹„ **95%** KV Cache ì ˆê°ì„ ë‹¬ì„±í•©ë‹ˆë‹¤!\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: GQA vs MLA ë¹„êµ\n",
    "\n",
    "$H_{kv}=8$ GQAì™€ $d_c=512$ MLAì˜ KV Cache í¬ê¸°ë¥¼ ë¹„êµí•˜ì„¸ìš” ($d_h=128$).\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$M_{KV}^{GQA} = 2 \\times 8 \\times 128 = 2048$$\n",
    "\n",
    "$$M_{KV}^{MLA} = 512$$\n",
    "\n",
    "$$\\frac{M_{KV}^{MLA}}{M_{KV}^{GQA}} = \\frac{512}{2048} = 0.25$$\n",
    "\n",
    "â†’ MLAëŠ” GQA ëŒ€ë¹„ **75% ì¶”ê°€ ì ˆê°**ì„ ë‹¬ì„±í•©ë‹ˆë‹¤ (4ë°° ë” ì‘ìŒ).\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 2. MLA Down/Up Projection êµ¬í˜„ <a name='2.-MLA-Projection-êµ¬í˜„'></a>\n",
    "\n",
    "MLAì˜ í•µì‹¬ì€ KVë¥¼ ì €ì°¨ì› ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:\n",
    "\n",
    "1. **Down-projection**: $c_t^{KV} = W_d^{KV} h_t$ ($d_{model} \\rightarrow d_c$)\n",
    "2. **Up-projection**: $[k_t^C; v_t^C] = W_u^{KV} c_t^{KV}$ ($d_c \\rightarrow 2 H d_h$)\n",
    "\n",
    "ì´ë¥¼ TensorFlowë¡œ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ MLA Down/Up Projection êµ¬í˜„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class MLAProjection(tf.keras.layers.Layer):\n",
    "    # Multi-head Latent Attentionì˜ KV ì••ì¶•/ë³µì› ë ˆì´ì–´\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_head, d_compress):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        self.d_compress = d_compress\n",
    "\n",
    "        # Down-projection: d_model -> d_compress\n",
    "        self.W_down = tf.keras.layers.Dense(d_compress, use_bias=False, name='kv_down')\n",
    "\n",
    "        # Up-projection: d_compress -> 2 * n_heads * d_head (K + V)\n",
    "        self.W_up = tf.keras.layers.Dense(2 * n_heads * d_head, use_bias=False, name='kv_up')\n",
    "\n",
    "        # Q projection (ë³„ë„)\n",
    "        self.W_q = tf.keras.layers.Dense(n_heads * d_head, use_bias=False, name='q_proj')\n",
    "\n",
    "    def call(self, h):\n",
    "        batch, seq_len, _ = h.shape\n",
    "\n",
    "        # Q projection\n",
    "        q = self.W_q(h)\n",
    "        q = tf.reshape(q, [batch, seq_len, self.n_heads, self.d_head])\n",
    "\n",
    "        # KV Down-projection (ì••ì¶•)\n",
    "        c_kv = self.W_down(h)  # [B, S, d_compress]\n",
    "\n",
    "        # KV Up-projection (ë³µì›)\n",
    "        kv = self.W_up(c_kv)  # [B, S, 2*H*d_h]\n",
    "        kv = tf.reshape(kv, [batch, seq_len, 2, self.n_heads, self.d_head])\n",
    "        k, v = kv[:, :, 0], kv[:, :, 1]\n",
    "\n",
    "        return q, k, v, c_kv\n",
    "\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ì„¤ì • (DeepSeek-V2 ìŠ¤ì¼€ì¼ ë‹¤ìš´)\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_head = 64\n",
    "d_compress = 64  # 8ë°° ì••ì¶•\n",
    "\n",
    "mla = MLAProjection(d_model, n_heads, d_head, d_compress)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì…ë ¥\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "h = tf.random.normal([batch_size, seq_len, d_model])\n",
    "\n",
    "q, k, v, c_kv = mla(h)\n",
    "\n",
    "print(f\"MLA Projection ê²°ê³¼:\")\n",
    "print(f\"  ì…ë ¥ h: {h.shape}\")\n",
    "print(f\"  Q: {q.shape}\")\n",
    "print(f\"  K (ë³µì›): {k.shape}\")\n",
    "print(f\"  V (ë³µì›): {v.shape}\")\n",
    "print(f\"  ì••ì¶• ë²¡í„° c_kv: {c_kv.shape}\")\n",
    "print(f\"\\nì••ì¶•ë¥ :\")\n",
    "print(f\"  ì›ë³¸ KV í¬ê¸°: {2 * n_heads * d_head} (= 2 x {n_heads} x {d_head})\")\n",
    "print(f\"  ì••ì¶• KV í¬ê¸°: {d_compress}\")\n",
    "print(f\"  ì••ì¶•ë¥ : {d_compress / (2 * n_heads * d_head):.2%} ({(2 * n_heads * d_head) / d_compress:.0f}x ì••ì¶•)\")\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ\n",
    "mla_params = d_model * d_compress + d_compress * (2 * n_heads * d_head)\n",
    "mha_params = d_model * (2 * n_heads * d_head)  # K, V projection\n",
    "print(f\"\\níŒŒë¼ë¯¸í„° ìˆ˜:\")\n",
    "print(f\"  MLA (Down + Up): {mla_params:,}\")\n",
    "print(f\"  MHA (K + V proj): {mha_params:,}\")\n",
    "print(f\"  MLA ì˜¤ë²„í—¤ë“œ: {mla_params/mha_params:.2f}x (ì¶”ë¡  ì‹œ KV CacheëŠ” {d_compress}ë§Œ ì €ì¥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ <a name='3.-ë©”ëª¨ë¦¬-ë¹„êµ'></a>\n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¦ê°€í•  ë•Œ ê° ë°©ì‹ì˜ KV Cache ë©”ëª¨ë¦¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "| ë°©ì‹ | KV Cache í¬ê¸° (ë°”ì´íŠ¸/í† í°/ë ˆì´ì–´) |\n",
    "|------|----------------------------------|\n",
    "| MHA | $2 \\times H \\times d_h \\times \\text{bytes}$ |\n",
    "| GQA | $2 \\times H_{kv} \\times d_h \\times \\text{bytes}$ |\n",
    "| MQA | $2 \\times d_h \\times \\text{bytes}$ |\n",
    "| MLA | $d_c \\times \\text{bytes}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ GQA vs MHA vs MLA ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ ì¶•) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# DeepSeek-V2 ê¸°ì¤€ íŒŒë¼ë¯¸í„°\n",
    "d_model_real = 5120\n",
    "n_heads_real = 40\n",
    "d_head_real = 128\n",
    "n_layers = 60\n",
    "bytes_per_elem = 2  # FP16\n",
    "\n",
    "configs = {\n",
    "    'MHA': {'kv_per_token': 2 * n_heads_real * d_head_real},\n",
    "    'GQA (H_kv=8)': {'kv_per_token': 2 * 8 * d_head_real},\n",
    "    'GQA (H_kv=4)': {'kv_per_token': 2 * 4 * d_head_real},\n",
    "    'MQA': {'kv_per_token': 2 * d_head_real},\n",
    "    'MLA (d_c=512)': {'kv_per_token': 512},\n",
    "}\n",
    "\n",
    "seq_lengths = np.array([512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ì´ KV Cache ë©”ëª¨ë¦¬ (GB) â€” ì‹œí€€ìŠ¤ ê¸¸ì´ë³„\n",
    "ax1 = axes[0]\n",
    "colors = ['red', 'orange', 'blue', 'purple', 'green']\n",
    "markers = ['o', 's', '^', 'D', 'v']\n",
    "\n",
    "for (name, cfg), color, marker in zip(configs.items(), colors, markers):\n",
    "    memory_gb = seq_lengths * n_layers * cfg['kv_per_token'] * bytes_per_elem / (1024**3)\n",
    "    ax1.semilogy(seq_lengths / 1000, memory_gb, f'-{marker}',\n",
    "                 color=color, lw=2, ms=7, label=name)\n",
    "\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)\n",
    "ax1.set_ylabel('KV Cache ë©”ëª¨ë¦¬ (GB)', fontsize=11)\n",
    "ax1.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ KV Cache ë©”ëª¨ë¦¬', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=80, color='gray', ls=':', lw=1.5, alpha=0.7)\n",
    "ax1.text(seq_lengths[-1]/1000*0.5, 85, 'H100 80GB', fontsize=9, color='gray')\n",
    "\n",
    "# (2) MHA ëŒ€ë¹„ ì ˆê°ë¥ \n",
    "ax2 = axes[1]\n",
    "mha_mem = configs['MHA']['kv_per_token']\n",
    "for (name, cfg), color, marker in zip(list(configs.items())[1:], colors[1:], markers[1:]):\n",
    "    savings = (1 - cfg['kv_per_token'] / mha_mem) * 100\n",
    "    ax2.barh(name, savings, color=color, alpha=0.7, edgecolor='black')\n",
    "    ax2.text(savings + 1, name, f'{savings:.1f}%', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax2.set_xlabel('MHA ëŒ€ë¹„ KV Cache ì ˆê°ë¥  (%)', fontsize=11)\n",
    "ax2.set_title('KV Cache ì ˆê°ë¥  ë¹„êµ', fontweight='bold')\n",
    "ax2.set_xlim(0, 105)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/mha_gqa_mla_memory.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/mha_gqa_mla_memory.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ í‘œ\n",
    "print(f\"\\nKV Cache ë©”ëª¨ë¦¬ ë¹„êµ (ì‹œí€€ìŠ¤ ê¸¸ì´ = 32K, {n_layers} ë ˆì´ì–´, FP16):\")\n",
    "print(f\"{'ë°©ì‹':<16} | {'í† í°ë‹¹ KV':>12} | {'ë©”ëª¨ë¦¬ (GB)':>12} | {'MHA ëŒ€ë¹„':>10}\")\n",
    "print(\"-\" * 58)\n",
    "for name, cfg in configs.items():\n",
    "    mem_gb = 32768 * n_layers * cfg['kv_per_token'] * bytes_per_elem / (1024**3)\n",
    "    ratio = cfg['kv_per_token'] / mha_mem\n",
    "    print(f\"{name:<16} | {cfg['kv_per_token']:>12} | {mem_gb:>12.2f} | {ratio:>9.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. KV Cache í¬ê¸° ê³„ì‚° <a name='4.-KV-Cache-í¬ê¸°-ê³„ì‚°'></a>\n",
    "\n",
    "ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œ KV Cache í¬ê¸° ê³„ì‚°:\n",
    "\n",
    "$$M_{KV} = B \\times S \\times L \\times M_{per\\_token\\_per\\_layer} \\times \\text{bytes}$$\n",
    "\n",
    "- $B$: ë°°ì¹˜ í¬ê¸°\n",
    "- $S$: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "- $L$: ë ˆì´ì–´ ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì‹¤ì œ ëª¨ë¸ë³„ KV Cache í¬ê¸° ê³„ì‚°ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "models = {\n",
    "    'Llama-3-8B': {\n",
    "        'n_layers': 32, 'n_heads': 32, 'd_head': 128,\n",
    "        'n_kv_heads': 8, 'method': 'GQA',\n",
    "        'kv_per_token': lambda: 2 * 8 * 128\n",
    "    },\n",
    "    'Llama-3-70B': {\n",
    "        'n_layers': 80, 'n_heads': 64, 'd_head': 128,\n",
    "        'n_kv_heads': 8, 'method': 'GQA',\n",
    "        'kv_per_token': lambda: 2 * 8 * 128\n",
    "    },\n",
    "    'DeepSeek-V2': {\n",
    "        'n_layers': 60, 'n_heads': 128, 'd_head': 128,\n",
    "        'd_compress': 512, 'method': 'MLA',\n",
    "        'kv_per_token': lambda: 512\n",
    "    },\n",
    "    'DeepSeek-V3': {\n",
    "        'n_layers': 61, 'n_heads': 128, 'd_head': 128,\n",
    "        'd_compress': 512, 'method': 'MLA',\n",
    "        'kv_per_token': lambda: 512\n",
    "    },\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "seq_length = 4096\n",
    "fp16_bytes = 2\n",
    "\n",
    "print(f\"ëª¨ë¸ë³„ KV Cache í¬ê¸° (B={batch_size}, S={seq_length}, FP16)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'ëª¨ë¸':<18} | {'ë°©ì‹':<6} | {'ë ˆì´ì–´':>6} | {'í† í°ë‹¹ KV':>10} | {'ì´ ë©”ëª¨ë¦¬':>12} | {'ë°°ì¹˜8':>12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, cfg in models.items():\n",
    "    kv = cfg['kv_per_token']()\n",
    "    mem = batch_size * seq_length * cfg['n_layers'] * kv * fp16_bytes\n",
    "    mem_gb = mem / (1024**3)\n",
    "    mem_batch8 = mem_gb * 8\n",
    "    print(f\"{name:<18} | {cfg['method']:<6} | {cfg['n_layers']:>6} | {kv:>10} | {mem_gb:>10.3f} GB | {mem_batch8:>10.3f} GB\")\n",
    "\n",
    "# DeepSeek-V3 MLA vs ê°€ìƒì˜ GQA ë¹„êµ\n",
    "print(f\"\\nDeepSeek-V3: MLA vs ê°€ìƒì˜ GQA ë¹„êµ (S={seq_length}):\")\n",
    "mla_kv = 512 * 61 * seq_length * fp16_bytes\n",
    "gqa8_kv = (2 * 8 * 128) * 61 * seq_length * fp16_bytes\n",
    "gqa4_kv = (2 * 4 * 128) * 61 * seq_length * fp16_bytes\n",
    "print(f\"  MLA (d_c=512):     {mla_kv / (1024**3):.3f} GB\")\n",
    "print(f\"  GQA (H_kv=8):      {gqa8_kv / (1024**3):.3f} GB\")\n",
    "print(f\"  GQA (H_kv=4):      {gqa4_kv / (1024**3):.3f} GB\")\n",
    "print(f\"  MLA/GQA(8) ë¹„ìœ¨:   {mla_kv/gqa8_kv:.2%}\")\n",
    "print(f\"  MLA/GQA(4) ë¹„ìœ¨:   {mla_kv/gqa4_kv:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. Attention Quality ë¹„êµ <a name='5.-Attention-Quality-ë¹„êµ'></a>\n",
    "\n",
    "MLAì˜ KV ì••ì¶•ì´ attention í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "ì••ì¶• ì°¨ì› $d_c$ê°€ í´ìˆ˜ë¡ ë³µì› í’ˆì§ˆì´ ë†’ì§€ë§Œ, ë©”ëª¨ë¦¬ ì ˆì•½ì€ ì¤„ì–´ë“­ë‹ˆë‹¤.\n",
    "\n",
    "$$\\text{Reconstruction Error} = \\frac{\\|KV_{original} - W_u \\cdot W_d \\cdot h\\|_F}{\\|KV_{original}\\|_F}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ MLA ì••ì¶• ì°¨ì›ë³„ Attention Quality ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "d_model_test = 256\n",
    "n_heads_test = 4\n",
    "d_head_test = 64\n",
    "seq_len_test = 64\n",
    "batch_test = 4\n",
    "\n",
    "# ì›ë³¸ KV ìƒì„± (MHA)\n",
    "h_test = tf.random.normal([batch_test, seq_len_test, d_model_test])\n",
    "W_kv_original = tf.keras.layers.Dense(2 * n_heads_test * d_head_test, use_bias=False)\n",
    "kv_original = W_kv_original(h_test)\n",
    "\n",
    "# ë‹¤ì–‘í•œ ì••ì¶• ì°¨ì›ì—ì„œ ë³µì› í’ˆì§ˆ ì¸¡ì •\n",
    "compress_dims = [16, 32, 64, 128, 256, 512]\n",
    "errors = []\n",
    "savings = []\n",
    "\n",
    "print(f\"MLA ì••ì¶• ì°¨ì›ë³„ ë³µì› í’ˆì§ˆ:\")\n",
    "print(f\"{'d_c':>6} | {'ë³µì› ì˜¤ì°¨':>12} | {'KV Cache ì ˆê°ë¥ ':>16} | {'Attn ìœ ì‚¬ë„':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for d_c in compress_dims:\n",
    "    # Down-projection + Up-projection\n",
    "    W_down = tf.keras.layers.Dense(d_c, use_bias=False)\n",
    "    W_up = tf.keras.layers.Dense(2 * n_heads_test * d_head_test, use_bias=False)\n",
    "\n",
    "    c_kv = W_down(h_test)\n",
    "    kv_reconstructed = W_up(c_kv)\n",
    "\n",
    "    # ë³µì› ì˜¤ì°¨ (Frobenius norm)\n",
    "    error = tf.norm(kv_original - kv_reconstructed) / tf.norm(kv_original)\n",
    "    error_val = error.numpy()\n",
    "    errors.append(error_val)\n",
    "\n",
    "    # KV Cache ì ˆê°ë¥ \n",
    "    original_size = 2 * n_heads_test * d_head_test\n",
    "    saving = (1 - d_c / original_size) * 100\n",
    "    savings.append(saving)\n",
    "\n",
    "    # Attention ìœ ì‚¬ë„ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)\n",
    "    kv_orig_flat = tf.reshape(kv_original, [-1, kv_original.shape[-1]])\n",
    "    kv_recon_flat = tf.reshape(kv_reconstructed, [-1, kv_reconstructed.shape[-1]])\n",
    "    cos_sim = tf.reduce_mean(\n",
    "        tf.reduce_sum(kv_orig_flat * kv_recon_flat, axis=-1) /\n",
    "        (tf.norm(kv_orig_flat, axis=-1) * tf.norm(kv_recon_flat, axis=-1) + 1e-8)\n",
    "    ).numpy()\n",
    "\n",
    "    print(f\"{d_c:>6} | {error_val:>12.4f} | {saving:>14.1f}% | {cos_sim:>12.4f}\")\n",
    "\n",
    "# ì‹œê°í™”: ì••ì¶• ì°¨ì› vs ë³µì› ì˜¤ì°¨/ì ˆê°ë¥  íŠ¸ë ˆì´ë“œì˜¤í”„\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "color1 = 'tab:blue'\n",
    "ax1.set_xlabel('ì••ì¶• ì°¨ì› ($d_c$)', fontsize=11)\n",
    "ax1.set_ylabel('ë³µì› ì˜¤ì°¨ (ìƒëŒ€)', fontsize=11, color=color1)\n",
    "ax1.plot(compress_dims, errors, 'b-o', lw=2.5, ms=8, label='ë³µì› ì˜¤ì°¨')\n",
    "ax1.tick_params(axis='y', labelcolor=color1)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2_twin = ax1.twinx()\n",
    "color2 = 'tab:red'\n",
    "ax2_twin.set_ylabel('KV Cache ì ˆê°ë¥  (%)', fontsize=11, color=color2)\n",
    "ax2_twin.plot(compress_dims, savings, 'r--s', lw=2, ms=7, label='ì ˆê°ë¥ ')\n",
    "ax2_twin.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "ax1.set_title('MLA ì••ì¶• ì°¨ì›ë³„ ë³µì› í’ˆì§ˆ vs ë©”ëª¨ë¦¬ ì ˆê°', fontweight='bold')\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=9, loc='center right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/mla_quality_tradeoff.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/mla_quality_tradeoff.png\")\n",
    "print(f\"\\nâ†’ d_cê°€ ì»¤ì§ˆìˆ˜ë¡ ë³µì› ì˜¤ì°¨ ê°ì†Œ, í•˜ì§€ë§Œ ì ˆê°ë¥ ë„ ê°ì†Œ\")\n",
    "print(f\"â†’ DeepSeek-V2/V3ì€ d_c=512ë¡œ 93.75% ì ˆê°ê³¼ ë†’ì€ í’ˆì§ˆì„ ë™ì‹œì— ë‹¬ì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| MLA Down-projection | $c_t^{KV} = W_d^{KV} h_t$ â€” KVë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶• | â­â­â­ |\n",
    "| MLA Up-projection | $[k_t^C; v_t^C] = W_u^{KV} c_t^{KV}$ â€” KV ë³µì› | â­â­â­ |\n",
    "| KV Cache ì ˆê° | MLA: $d_c$ vs GQA: $2H_{kv}d_h$ | â­â­â­ |\n",
    "| ì••ì¶• ì°¨ì› ì„ íƒ | $d_c$ê°€ í´ìˆ˜ë¡ í’ˆì§ˆâ†‘, ì ˆê°ë¥ â†“ â€” íŠ¸ë ˆì´ë“œì˜¤í”„ | â­â­ |\n",
    "| MHAâ†’GQAâ†’MLA ì§„í™” | ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì˜ ë‹¨ê³„ì  ë°œì „ | â­â­â­ |\n",
    "| DeepSeek-V2/V3 ì„¤ì • | $d_c=512$, 95% MHA ëŒ€ë¹„ ì ˆê° | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$c_t^{KV} = W_d^{KV} h_t \\in \\mathbb{R}^{d_c}, \\quad d_c \\ll d_{model}$$\n",
    "\n",
    "$$[k_t^C;\\; v_t^C] = W_u^{KV} c_t^{KV} \\in \\mathbb{R}^{2Hd_h}$$\n",
    "\n",
    "$$\\text{KV Cache ì ˆê°ë¥ } = 1 - \\frac{d_c}{2 \\times H \\times d_h}$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**03_linear_attention_and_hybrids.ipynb** â€” GLA, RetNet, Mamba ë“± Linear Attention ê³„ì—´ ê¸°ë²•ê³¼ Qwenì˜ SWA+Full+Linear Hybrid êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}