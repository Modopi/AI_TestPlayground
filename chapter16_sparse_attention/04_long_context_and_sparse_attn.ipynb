{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” Long Contextì™€ Sparse Attention\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì •ì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ì°½ í™•ì¥ ì›ë¦¬ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤\n",
    "- Sliding Window Attention(SWA)ì˜ ë§ˆìŠ¤í¬ êµ¬ì¡°ì™€ ë©”ëª¨ë¦¬ ì ˆì•½ ì›ë¦¬ë¥¼ êµ¬í˜„í•œë‹¤\n",
    "- DeepSeek Sparse Attention(DSA)ì˜ ì„ íƒì  í† í° íŒ¨í„´ì„ ë¶„ì„í•œë‹¤\n",
    "- Full, Sliding Window, Sparse Attentionì˜ ë§ˆìŠ¤í¬ë¥¼ ì‹œê°í™”í•˜ì—¬ ë¹„êµí•œë‹¤\n",
    "- 50% ì´ìƒì˜ ë¹„ìš© ì ˆê°ì„ ë‹¬ì„±í•˜ëŠ” í†µí•© ë°©ë²•ë¡ ì„ ì´í•´í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ê³¼ Sparse Attention](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [Attention ë§ˆìŠ¤í¬ ì‹œê°í™”](#2.-Attention-ë§ˆìŠ¤í¬-ì‹œê°í™”)\n",
    "3. [ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ](#3.-ë©”ëª¨ë¦¬-ì ˆì•½-ë¹„êµ)\n",
    "4. [Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜](#4.-Long-Context-Perplexity)\n",
    "5. [50% ì´ìƒ ë¹„ìš© ì ˆê° ë¶„ì„](#5.-ë¹„ìš©-ì ˆê°-ë¶„ì„)\n",
    "6. [ì •ë¦¬](#6.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### YaRN (Yet another RoPE extensioN)\n",
    "\n",
    "RoPEì˜ ì£¼íŒŒìˆ˜ë¥¼ ì¬ì¡°ì •í•˜ì—¬ í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸ ì°½ì„ ë„˜ì–´ì„œëŠ” ìœ„ì¹˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\theta_i' = \\theta_i \\cdot \\begin{cases} 1 & \\text{if } \\lambda_i < 1 \\text{ (ê³ ì£¼íŒŒ â€” ë³€ê²½ ì—†ìŒ)} \\\\ 1/s & \\text{if } \\lambda_i > 1 \\text{ (ì €ì£¼íŒŒ â€” ìŠ¤ì¼€ì¼ ë‹¤ìš´)} \\\\ (1-\\gamma) \\cdot 1 + \\gamma / s & \\text{otherwise (ì„ í˜• ë³´ê°„)} \\end{cases}$$\n",
    "\n",
    "- $\\theta_i = 10000^{-2i/d}$: ì›ë˜ RoPE ì£¼íŒŒìˆ˜\n",
    "- $s$: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨ (ì˜ˆ: 4x â†’ $s=4$)\n",
    "- $\\lambda_i = 2\\pi / \\theta_i$: íŒŒì¥\n",
    "- $\\gamma$: ë³´ê°„ ë¹„ìœ¨\n",
    "\n",
    "### Sliding Window Attention (SWA)\n",
    "\n",
    "ê° í† í°ì€ ìì‹ ì˜ ìœˆë„ìš° ë‚´ í† í°ë§Œ attendí•©ë‹ˆë‹¤:\n",
    "\n",
    "$$A_{ij} = \\begin{cases} \\text{softmax}(q_i k_j^T / \\sqrt{d}) & \\text{if } |i - j| \\leq w/2 \\text{ and } j \\leq i \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "- $w$: ìœˆë„ìš° í¬ê¸°\n",
    "- ë©”ëª¨ë¦¬ ë³µì¡ë„: $O(Nw)$ vs Fullì˜ $O(N^2)$\n",
    "\n",
    "### DeepSeek Sparse Attention (DSA)\n",
    "\n",
    "í† í°ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê³ , ì¤‘ìš”í•œ ë¸”ë¡ë§Œ ì„ íƒì ìœ¼ë¡œ attend:\n",
    "\n",
    "1. **ë¸”ë¡ ë¶„í• **: ì‹œí€€ìŠ¤ë¥¼ $B$ê°œ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆ” ($b = N/B$)\n",
    "2. **ì¤‘ìš”ë„ ì ìˆ˜**: $\\text{score}_j = \\max_{k \\in \\text{block}_j} q_i^T k / \\sqrt{d}$\n",
    "3. **Top-K ë¸”ë¡ ì„ íƒ**: ìƒìœ„ $K$ê°œ ë¸”ë¡ë§Œ attend\n",
    "\n",
    "$$\\text{Sparsity} = 1 - \\frac{K}{B}, \\quad \\text{ë¹„ìš© ì ˆê°} = 1 - \\frac{Kb + w}{N}$$\n",
    "\n",
    "### ë¹„ìš© ì ˆê° ë¶„ì„\n",
    "\n",
    "| ë°©ë²• | Attention ë¹„ìš© | Full ëŒ€ë¹„ ì ˆê°ë¥  |\n",
    "|------|---------------|-----------------|\n",
    "| Full | $N^2$ | 0% |\n",
    "| SWA ($w$) | $Nw$ | $1 - w/N$ |\n",
    "| DSA ($K$ ë¸”ë¡, $b$ í¬ê¸°) | $NKb$ | $1 - Kb/N$ |\n",
    "| SWA + DSA | $N(w + Kb)$ | $1 - (w + Kb)/N$ |\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| YaRN ìŠ¤ì¼€ì¼ë§ | $\\theta_i' \\propto \\theta_i / s$ | ì €ì£¼íŒŒ ì£¼íŒŒìˆ˜ ì••ì¶• |\n",
    "| SWA ë§ˆìŠ¤í¬ | $\\|i-j\\| \\leq w/2$ | ì§€ì—­ ìœˆë„ìš° ì œí•œ |\n",
    "| DSA ë¸”ë¡ ì„ íƒ | $\\text{TopK}(\\max_{k} q^Tk)$ | ì¤‘ìš” ë¸”ë¡ë§Œ attend |\n",
    "| ë¹„ìš© ì ˆê° | $> 50\\%$ ê°€ëŠ¥ | SWA + DSA ê²°í•© |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ Long Context ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ ì™œ ê¸´ ë¬¸ì¥ì„ ì½ê¸° ì–´ë ¤ìš´ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: 1000í˜ì´ì§€ì§œë¦¬ ì±…ì„ ì½ì„ ë•Œ, ëª¨ë“  í˜ì´ì§€ì˜ ë‚´ìš©ì„ ë™ì‹œì— ê¸°ì–µí•˜ë ¤ë©´ \n",
    "> ì—„ì²­ë‚œ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•´ìš”! AIë„ ë§ˆì°¬ê°€ì§€ì˜ˆìš”.\n",
    "\n",
    "- **Full Attention**: ëª¨ë“  í˜ì´ì§€ë¥¼ ë™ì‹œì— ì°¸ì¡° â†’ í˜ì´ì§€ê°€ ë§ìœ¼ë©´ $N^2$ë°° ëŠë ¤ì§\n",
    "- **Sliding Window**: í˜„ì¬ ì½ëŠ” í˜ì´ì§€ ì£¼ë³€ 10í˜ì´ì§€ë§Œ ì°¸ì¡° â†’ ë¹ ë¥´ì§€ë§Œ ë¨¼ ë‚´ìš©ì„ ëª» ë´„\n",
    "- **Sparse Attention**: **ì¤‘ìš”í•œ í˜ì´ì§€ë§Œ** ê³¨ë¼ì„œ ì°¸ì¡° â†’ ë¹ ë¥´ë©´ì„œë„ í•µì‹¬ì€ ë†“ì¹˜ì§€ ì•ŠìŒ!\n",
    "\n",
    "#### ğŸ“ YaRNì€ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: 30cm ì(í•™ìŠµëœ ì»¨í…ìŠ¤íŠ¸)ë¡œ 1më¥¼ ì¬ê³  ì‹¶ì„ ë•Œ, \n",
    "> ìì˜ ëˆˆê¸ˆ ê°„ê²©ì„ ì¤„ì—¬ì„œ(ì£¼íŒŒìˆ˜ ì¬ì¡°ì •) ë” ê¸´ ê±°ë¦¬ë¥¼ ì´ ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê±°ì˜ˆìš”!\n",
    "\n",
    "YaRNì€ í•™ìŠµ ì‹œ 4K í† í°ê¹Œì§€ë§Œ ë°°ìš´ ëª¨ë¸ì´ 64K í† í°ê¹Œì§€ ì½ì„ ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: SWA ì ˆê°ë¥ \n",
    "\n",
    "ì‹œí€€ìŠ¤ ê¸¸ì´ $N = 8192$, ìœˆë„ìš° í¬ê¸° $w = 512$ì¼ ë•Œ SWAì˜ Full ëŒ€ë¹„ ë¹„ìš© ì ˆê°ë¥ ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\text{ì ˆê°ë¥ } = 1 - \\frac{w}{N} = 1 - \\frac{512}{8192} = 1 - 0.0625 = 93.75\\%$$\n",
    "\n",
    "â†’ ìœˆë„ìš°ë¥¼ ì œí•œí•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ **93.75%** ë¹„ìš© ì ˆê°!\n",
    "ë‹¤ë§Œ 512 í† í° ë°–ì˜ ì¥ê±°ë¦¬ ì˜ì¡´ì„±ì„ ë†“ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: DSA + SWA ê²°í•©\n",
    "\n",
    "$N=8192$, SWA $w=512$, DSA ë¸”ë¡ í¬ê¸° $b=256$, ì„ íƒ ë¸”ë¡ $K=4$ì¼ ë•Œ ì´ ë¹„ìš© ì ˆê°ë¥ ì€?\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\text{SWA ë¹„ìš©} = N \\times w = 8192 \\times 512$$\n",
    "$$\\text{DSA ë¹„ìš©} = N \\times K \\times b = 8192 \\times 4 \\times 256$$\n",
    "$$\\text{ì´ ë¹„ìš©} = N(w + Kb) = 8192 \\times (512 + 1024) = 8192 \\times 1536$$\n",
    "$$\\text{Full ë¹„ìš©} = N^2 = 8192^2 = 8192 \\times 8192$$\n",
    "\n",
    "$$\\text{ì ˆê°ë¥ } = 1 - \\frac{1536}{8192} = 1 - 0.1875 = 81.25\\%$$\n",
    "\n",
    "â†’ SWA + DSA ê²°í•©ìœ¼ë¡œ **81.25%** ë¹„ìš© ì ˆê° ë‹¬ì„±!\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 2. Attention ë§ˆìŠ¤í¬ ì‹œê°í™” <a name='2.-Attention-ë§ˆìŠ¤í¬-ì‹œê°í™”'></a>\n",
    "\n",
    "Full, Sliding Window, DeepSeek Sparse Attentionì˜ ë§ˆìŠ¤í¬ íŒ¨í„´ì„ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Attention ë§ˆìŠ¤í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "N = 64  # ì‹œê°í™”ë¥¼ ìœ„í•œ ì‘ì€ ì‹œí€€ìŠ¤\n",
    "\n",
    "def create_causal_mask(N):\n",
    "    # í‘œì¤€ causal mask (lower triangular)\n",
    "    return np.tril(np.ones((N, N)))\n",
    "\n",
    "def create_sliding_window_mask(N, window_size=16):\n",
    "    # Sliding window + causal\n",
    "    mask = np.zeros((N, N))\n",
    "    for i in range(N):\n",
    "        start = max(0, i - window_size + 1)\n",
    "        mask[i, start:i+1] = 1.0\n",
    "    return mask\n",
    "\n",
    "def create_sparse_block_mask(N, block_size=8, top_k_blocks=2, window_size=8):\n",
    "    # Sparse attention: ìœˆë„ìš° + Top-K ë¸”ë¡\n",
    "    mask = np.zeros((N, N))\n",
    "    n_blocks = N // block_size\n",
    "\n",
    "    for i in range(N):\n",
    "        # (1) ë¡œì»¬ ìœˆë„ìš°\n",
    "        start = max(0, i - window_size + 1)\n",
    "        mask[i, start:i+1] = 1.0\n",
    "\n",
    "        # (2) Top-K ë¸”ë¡ (ëœë¤ ì‹œë®¬ë ˆì´ì…˜)\n",
    "        current_block = i // block_size\n",
    "        available_blocks = list(range(0, current_block + 1))\n",
    "        if len(available_blocks) > top_k_blocks:\n",
    "            np.random.seed(i * 7 + 3)\n",
    "            selected = np.random.choice(available_blocks, top_k_blocks, replace=False)\n",
    "        else:\n",
    "            selected = available_blocks\n",
    "\n",
    "        for b in selected:\n",
    "            b_start = b * block_size\n",
    "            b_end = min(b_start + block_size, i + 1)\n",
    "            mask[i, b_start:b_end] = 1.0\n",
    "\n",
    "    return mask\n",
    "\n",
    "# ë§ˆìŠ¤í¬ ìƒì„±\n",
    "mask_full = create_causal_mask(N)\n",
    "mask_swa = create_sliding_window_mask(N, window_size=16)\n",
    "mask_sparse = create_sparse_block_mask(N, block_size=8, top_k_blocks=2, window_size=8)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "titles = ['Full Causal Attention', 'Sliding Window (w=16)', 'Sparse Block (k=2, b=8, w=8)']\n",
    "masks = [mask_full, mask_swa, mask_sparse]\n",
    "cmaps = ['Blues', 'Oranges', 'Greens']\n",
    "\n",
    "for ax, title, mask, cmap in zip(axes, titles, masks, cmaps):\n",
    "    ax.imshow(mask, cmap=cmap, aspect='equal', interpolation='nearest')\n",
    "    ax.set_xlabel('Key ìœ„ì¹˜', fontsize=11)\n",
    "    ax.set_ylabel('Query ìœ„ì¹˜', fontsize=11)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    # ë°€ë„(density) í‘œì‹œ\n",
    "    density = mask.sum() / mask.size * 100\n",
    "    ax.text(N*0.95, N*0.05, f'ë°€ë„: {density:.1f}%',\n",
    "            ha='right', va='top', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/attention_masks_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/attention_masks_comparison.png\")\n",
    "\n",
    "# ë°€ë„ ë¹„êµ\n",
    "print(f\"\\nAttention ë§ˆìŠ¤í¬ ë°€ë„ ë¹„êµ (N={N}):\")\n",
    "for title, mask in zip(titles, masks):\n",
    "    density = mask.sum() / mask.size * 100\n",
    "    active = int(mask.sum())\n",
    "    total = mask.size\n",
    "    print(f\"  {title}: {density:.1f}% ({active}/{total} ì›ì†Œ í™œì„±)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ <a name='3.-ë©”ëª¨ë¦¬-ì ˆì•½-ë¹„êµ'></a>\n",
    "\n",
    "ë‹¤ì–‘í•œ Attention ë°©ì‹ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "| ë°©ì‹ | Attention ë©”ëª¨ë¦¬ | KV Cache (ì¶”ë¡ ) |\n",
    "|------|-----------------|-----------------|\n",
    "| Full Causal | $O(N^2)$ | $O(Nd)$ |\n",
    "| SWA | $O(Nw)$ | $O(wd)$ |\n",
    "| Sparse Block | $O(NKb)$ | $O(Kbd)$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë©”ëª¨ë¦¬ ì ˆì•½ ë¹„êµ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "seq_lengths = np.array([1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072])\n",
    "window_size = 1024\n",
    "block_size = 512\n",
    "top_k_blocks = 4\n",
    "d = 128\n",
    "\n",
    "# Attention í–‰ë ¬ ë©”ëª¨ë¦¬ (ì›ì†Œ ìˆ˜)\n",
    "mem_full = seq_lengths ** 2\n",
    "mem_swa = seq_lengths * window_size\n",
    "mem_sparse = seq_lengths * (top_k_blocks * block_size + window_size)\n",
    "\n",
    "# KV Cache ë©”ëª¨ë¦¬ (ë°”ì´íŠ¸, FP16, ë‹¨ì¼ ë ˆì´ì–´)\n",
    "kv_full = seq_lengths * 2 * d * 2  # ëª¨ë“  í† í°ì˜ KV\n",
    "kv_swa = np.minimum(seq_lengths, window_size) * 2 * d * 2  # ìœˆë„ìš°ë§Œ\n",
    "kv_sparse = (top_k_blocks * block_size + window_size) * 2 * d * 2  # ë¸”ë¡ + ìœˆë„ìš°\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) Attention ì—°ì‚° ë©”ëª¨ë¦¬\n",
    "ax1 = axes[0]\n",
    "ax1.loglog(seq_lengths / 1000, mem_full / 1e9, 'r-o', lw=2.5, ms=7, label='Full ($O(N^2)$)')\n",
    "ax1.loglog(seq_lengths / 1000, mem_swa / 1e9, 'b-s', lw=2, ms=7, label=f'SWA ($w={window_size}$)')\n",
    "ax1.loglog(seq_lengths / 1000, mem_sparse / 1e9, 'g-^', lw=2, ms=7,\n",
    "           label=f'Sparse ($K={top_k_blocks}, b={block_size}$)')\n",
    "\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)\n",
    "ax1.set_ylabel('Attention ë©”ëª¨ë¦¬ (G ì›ì†Œ)', fontsize=11)\n",
    "ax1.set_title('Attention ì—°ì‚° ë©”ëª¨ë¦¬ ë¹„êµ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) ì ˆê°ë¥ \n",
    "ax2 = axes[1]\n",
    "savings_swa = (1 - mem_swa / mem_full) * 100\n",
    "savings_sparse = (1 - mem_sparse / mem_full) * 100\n",
    "\n",
    "ax2.plot(seq_lengths / 1000, savings_swa, 'b-s', lw=2.5, ms=7, label='SWA')\n",
    "ax2.plot(seq_lengths / 1000, savings_sparse, 'g-^', lw=2.5, ms=7, label='Sparse')\n",
    "ax2.axhline(y=50, color='red', ls='--', lw=1.5, label='50% ê¸°ì¤€')\n",
    "ax2.fill_between(seq_lengths / 1000, 50, 100, alpha=0.05, color='green')\n",
    "ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´ (K í† í°)', fontsize=11)\n",
    "ax2.set_ylabel('Full ëŒ€ë¹„ ì ˆê°ë¥  (%)', fontsize=11)\n",
    "ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë¹„ìš© ì ˆê°ë¥ ', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/memory_savings_comparison.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/memory_savings_comparison.png\")\n",
    "\n",
    "# ìˆ˜ì¹˜ í‘œ\n",
    "print(f\"\\në©”ëª¨ë¦¬ ì ˆê°ë¥  ìˆ˜ì¹˜ ë¹„êµ:\")\n",
    "print(f\"{'ì‹œí€€ìŠ¤ ê¸¸ì´':>12} | {'SWA ì ˆê°':>10} | {'Sparse ì ˆê°':>12} | {'>50% ë‹¬ì„±':>10}\")\n",
    "print(\"-\" * 52)\n",
    "for i, N in enumerate(seq_lengths):\n",
    "    swa_s = savings_swa[i]\n",
    "    sp_s = savings_sparse[i]\n",
    "    check = 'âœ…' if sp_s > 50 else 'âŒ'\n",
    "    print(f\"{N:>12,} | {swa_s:>9.1f}% | {sp_s:>11.1f}% | {check:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜ <a name='4.-Long-Context-Perplexity'></a>\n",
    "\n",
    "YaRNì„ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ì‹œ perplexity ë³€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\\text{PPL}(s) = \\text{PPL}_{base} \\cdot \\left(1 + \\alpha \\cdot \\max(0, s - 1)\\right)$$\n",
    "\n",
    "- $s$: ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨\n",
    "- $\\alpha$: í’ˆì§ˆ ì €í•˜ ê³„ìˆ˜ (YaRNì´ ì‘ì„ìˆ˜ë¡ ìš°ìˆ˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Long Context Perplexity ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "\n",
    "# ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë¹„ìœ¨\n",
    "extension_ratios = np.array([1, 2, 4, 8, 16, 32])\n",
    "base_ctx = 4096  # ê¸°ë³¸ í•™ìŠµ ì»¨í…ìŠ¤íŠ¸\n",
    "\n",
    "# ê° ë°©ë²•ì˜ perplexity ëª¨ë¸ë§\n",
    "ppl_base = 5.0  # ê¸°ë³¸ perplexity\n",
    "\n",
    "# NTK-aware (ë‹¨ìˆœ ìŠ¤ì¼€ì¼ë§)\n",
    "alpha_ntk = 0.15\n",
    "ppl_ntk = ppl_base * (1 + alpha_ntk * np.maximum(0, extension_ratios - 1))\n",
    "\n",
    "# YaRN (ê°œì„ ëœ ìŠ¤ì¼€ì¼ë§)\n",
    "alpha_yarn = 0.03\n",
    "ppl_yarn = ppl_base * (1 + alpha_yarn * np.maximum(0, extension_ratios - 1))\n",
    "\n",
    "# í•™ìŠµ ì—†ì´ ì§ì ‘ í™•ì¥ (PI: Position Interpolation)\n",
    "alpha_pi = 0.08\n",
    "ppl_pi = ppl_base * (1 + alpha_pi * np.maximum(0, extension_ratios - 1))\n",
    "\n",
    "# í•™ìŠµ ì—†ì´ RoPE (ì™¸ì‚½ - ê¸‰ê²©íˆ ë‚˜ë¹ ì§)\n",
    "ppl_no_ext = ppl_base * np.exp(0.2 * np.maximum(0, extension_ratios - 1))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) PPL vs í™•ì¥ ë¹„ìœ¨\n",
    "ax1 = axes[0]\n",
    "effective_ctx = base_ctx * extension_ratios\n",
    "ax1.plot(effective_ctx / 1000, ppl_no_ext, 'r-x', lw=2, ms=8, label='RoPE ì™¸ì‚½ (í•™ìŠµ ì—†ìŒ)')\n",
    "ax1.plot(effective_ctx / 1000, ppl_ntk, 'orange', lw=2, marker='D', ms=7, label='NTK-aware')\n",
    "ax1.plot(effective_ctx / 1000, ppl_pi, 'b-s', lw=2, ms=7, label='Position Interpolation')\n",
    "ax1.plot(effective_ctx / 1000, ppl_yarn, 'g-o', lw=2.5, ms=8, label='YaRN')\n",
    "\n",
    "ax1.set_xlabel('ìœ íš¨ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (K)', fontsize=11)\n",
    "ax1.set_ylabel('Perplexity', fontsize=11)\n",
    "ax1.set_title('ì»¨í…ìŠ¤íŠ¸ í™•ì¥ ë°©ë²•ë³„ Perplexity', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_ylim(4, 30)\n",
    "\n",
    "# (2) YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì •\n",
    "ax2 = axes[1]\n",
    "d_model = 128\n",
    "n_dims = d_model // 2\n",
    "freqs = 10000 ** (-2 * np.arange(n_dims) / d_model)\n",
    "wavelengths = 2 * np.pi / freqs\n",
    "\n",
    "scale = 4  # 4x í™•ì¥\n",
    "threshold_low = base_ctx\n",
    "threshold_high = base_ctx * scale\n",
    "\n",
    "freqs_yarn = np.copy(freqs)\n",
    "for i in range(n_dims):\n",
    "    wl = wavelengths[i]\n",
    "    if wl < threshold_low:\n",
    "        freqs_yarn[i] = freqs[i]  # ê³ ì£¼íŒŒ: ë³€ê²½ ì—†ìŒ\n",
    "    elif wl > threshold_high:\n",
    "        freqs_yarn[i] = freqs[i] / scale  # ì €ì£¼íŒŒ: ìŠ¤ì¼€ì¼ ë‹¤ìš´\n",
    "    else:\n",
    "        gamma = (wl - threshold_low) / (threshold_high - threshold_low)\n",
    "        freqs_yarn[i] = freqs[i] * ((1 - gamma) + gamma / scale)\n",
    "\n",
    "ax2.plot(range(n_dims), freqs, 'b-', lw=2, alpha=0.5, label='ì›ë˜ RoPE')\n",
    "ax2.plot(range(n_dims), freqs_yarn, 'g-', lw=2.5, label='YaRN (4x)')\n",
    "ax2.set_xlabel('ì°¨ì› ì¸ë±ìŠ¤', fontsize=11)\n",
    "ax2.set_ylabel('ì£¼íŒŒìˆ˜', fontsize=11)\n",
    "ax2.set_title('YaRN ì£¼íŒŒìˆ˜ ì¬ì¡°ì • (4x í™•ì¥)', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/yarn_context_extension.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/yarn_context_extension.png\")\n",
    "\n",
    "print(f\"\\nPerplexity ë¹„êµ (ê¸°ë³¸ = {ppl_base}):\")\n",
    "print(f\"{'í™•ì¥ ë¹„ìœ¨':>10} | {'ìœ íš¨ ê¸¸ì´':>10} | {'RoPE ì™¸ì‚½':>10} | {'NTK':>8} | {'PI':>8} | {'YaRN':>8}\")\n",
    "print(\"-\" * 65)\n",
    "for i, ratio in enumerate(extension_ratios):\n",
    "    ctx = base_ctx * ratio\n",
    "    print(f\"{ratio:>10}x | {ctx:>10,} | {ppl_no_ext[i]:>10.2f} | {ppl_ntk[i]:>8.2f} | \"\n",
    "          f\"{ppl_pi[i]:>8.2f} | {ppl_yarn[i]:>8.2f}\")\n",
    "print(f\"\\nâ†’ YaRNì€ 32x í™•ì¥ì—ì„œë„ PPL ì¦ê°€ê°€ {(ppl_yarn[-1]/ppl_base - 1)*100:.1f}%ì— ë¶ˆê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. 50% ì´ìƒ ë¹„ìš© ì ˆê° ë¶„ì„ <a name='5.-ë¹„ìš©-ì ˆê°-ë¶„ì„'></a>\n",
    "\n",
    "SWA + Sparse Attentionì„ ê²°í•©í•œ ì‹¤ì „ ë¹„ìš© ì ˆê° ë¶„ì„ì…ë‹ˆë‹¤.\n",
    "\n",
    "**DeepSeek-V3 ì‹¤ì œ ì„¤ì •:**\n",
    "- ì¼ë¶€ ë ˆì´ì–´: Full Attention (ì „ì—­ ì˜ì¡´ì„±)\n",
    "- ëŒ€ë¶€ë¶„ ë ˆì´ì–´: SWA ($w=4096$) + Sparse Block ($K=4, b=512$)\n",
    "- ì´ ë¹„ìš© ì ˆê°: $> 50\\%$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¹„ìš© ì ˆê° ì¢…í•© ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ì‹¤ì „ ëª¨ë¸ ì„¤ì • ê¸°ë°˜ ë¹„ìš© ë¶„ì„\n",
    "N_values = [4096, 8192, 16384, 32768, 65536, 131072]\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "n_total_layers = 61  # DeepSeek-V3\n",
    "n_full_layers = 4  # Full attention ë ˆì´ì–´\n",
    "n_swa_layers = n_total_layers - n_full_layers  # SWA + sparse ë ˆì´ì–´\n",
    "w = 4096  # SWA ìœˆë„ìš°\n",
    "K_blocks = 4  # Top-K ë¸”ë¡\n",
    "b = 512  # ë¸”ë¡ í¬ê¸°\n",
    "\n",
    "print(f\"DeepSeek-V3 ìŠ¤íƒ€ì¼ ë¹„ìš© ë¶„ì„:\")\n",
    "print(f\"  ì´ ë ˆì´ì–´: {n_total_layers} (Full: {n_full_layers}, SWA+Sparse: {n_swa_layers})\")\n",
    "print(f\"  SWA ìœˆë„ìš°: {w}, Top-K ë¸”ë¡: {K_blocks}, ë¸”ë¡ í¬ê¸°: {b}\")\n",
    "print()\n",
    "\n",
    "print(f\"{'ì‹œí€€ìŠ¤':>8} | {'Full Only':>12} | {'Hybrid':>12} | {'ì ˆê°ë¥ ':>8} | {'>50%':>5}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "savings_list = []\n",
    "\n",
    "for N in N_values:\n",
    "    # Full attention ë¹„ìš© (ëª¨ë“  ë ˆì´ì–´)\n",
    "    full_cost = n_total_layers * N * N\n",
    "\n",
    "    # Hybrid ë¹„ìš©\n",
    "    full_layer_cost = n_full_layers * N * N\n",
    "    swa_sparse_cost = n_swa_layers * N * min(w + K_blocks * b, N)\n",
    "    hybrid_cost = full_layer_cost + swa_sparse_cost\n",
    "\n",
    "    saving = (1 - hybrid_cost / full_cost) * 100\n",
    "    savings_list.append(saving)\n",
    "    check = 'âœ…' if saving > 50 else 'âŒ'\n",
    "    print(f\"{N:>8,} | {full_cost/1e12:>10.2f}T | {hybrid_cost/1e12:>10.2f}T | {saving:>7.1f}% | {check:>5}\")\n",
    "\n",
    "# ë¹„ìš© ë¶„í•´ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ë¹„ìš© êµ¬ì„± (ìŠ¤íƒ ë°” ì°¨íŠ¸)\n",
    "ax1 = axes[0]\n",
    "x_pos = range(len(N_values))\n",
    "full_attn_cost = [n_full_layers * N * N / 1e12 for N in N_values]\n",
    "swa_cost = [n_swa_layers * N * min(w, N) / 1e12 for N in N_values]\n",
    "sparse_cost = [n_swa_layers * N * min(K_blocks * b, N) / 1e12 for N in N_values]\n",
    "\n",
    "ax1.bar(x_pos, full_attn_cost, color='red', alpha=0.7, label='Full Attn ë ˆì´ì–´')\n",
    "ax1.bar(x_pos, swa_cost, bottom=full_attn_cost, color='blue', alpha=0.7, label='SWA')\n",
    "bottoms = [f + s for f, s in zip(full_attn_cost, swa_cost)]\n",
    "ax1.bar(x_pos, sparse_cost, bottom=bottoms, color='green', alpha=0.7, label='Sparse Block')\n",
    "\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([f'{N//1000}K' for N in N_values], fontsize=9)\n",
    "ax1.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´', fontsize=11)\n",
    "ax1.set_ylabel('ì—°ì‚° ë¹„ìš© (T ì—°ì‚°)', fontsize=11)\n",
    "ax1.set_title('Hybrid Attention ë¹„ìš© êµ¬ì„±', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# (2) ì ˆê°ë¥ \n",
    "ax2 = axes[1]\n",
    "ax2.bar(x_pos, savings_list, color=['orange' if s < 50 else 'green' for s in savings_list],\n",
    "        alpha=0.7, edgecolor='black')\n",
    "ax2.axhline(y=50, color='red', ls='--', lw=2, label='50% ê¸°ì¤€')\n",
    "for i, s in enumerate(savings_list):\n",
    "    ax2.text(i, s + 1, f'{s:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([f'{N//1000}K' for N in N_values], fontsize=9)\n",
    "ax2.set_xlabel('ì‹œí€€ìŠ¤ ê¸¸ì´', fontsize=11)\n",
    "ax2.set_ylabel('ë¹„ìš© ì ˆê°ë¥  (%)', fontsize=11)\n",
    "ax2.set_title('ì‹œí€€ìŠ¤ ê¸¸ì´ë³„ ë¹„ìš© ì ˆê°ë¥ ', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/cost_reduction_analysis.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"\\nê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/cost_reduction_analysis.png\")\n",
    "\n",
    "# ìµœì¢… ê²°ë¡ \n",
    "print(f\"\\nê²°ë¡ :\")\n",
    "print(f\"  ì‹œí€€ìŠ¤ ê¸¸ì´ 4K: ì ˆê°ë¥  = {savings_list[0]:.1f}% (ì§§ì€ ì‹œí€€ìŠ¤ì—ì„œëŠ” íš¨ê³¼ ì œí•œì )\")\n",
    "over_50 = sum(1 for s in savings_list if s > 50)\n",
    "print(f\"  50% ì´ìƒ ì ˆê° ë‹¬ì„±: {over_50}/{len(N_values)} ì„¤ì •\")\n",
    "print(f\"  ì‹œí€€ìŠ¤ ê¸¸ì´ 128K: ì ˆê°ë¥  = {savings_list[-1]:.1f}%\")\n",
    "print(f\"  â†’ ê¸´ ì‹œí€€ìŠ¤ì—ì„œ SWA + Sparse Attentionì˜ íš¨ê³¼ê°€ ê·¹ëŒ€í™”ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. ì •ë¦¬ <a name='6.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| YaRN | ì£¼íŒŒìˆ˜ ì¬ì¡°ì •ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ì°½ í™•ì¥ | â­â­â­ |\n",
    "| Sliding Window (SWA) | $\\|i-j\\| \\leq w/2$ ë§ˆìŠ¤í¬ë¡œ ì§€ì—­ íŒ¨í„´ í¬ì°© | â­â­â­ |\n",
    "| DeepSeek Sparse Attn | Top-K ë¸”ë¡ ì„ íƒìœ¼ë¡œ ì¤‘ìš” ì •ë³´ë§Œ attend | â­â­â­ |\n",
    "| Hybrid ì „ëµ | Full + SWA + Sparse ë ˆì´ì–´ë³„ í˜¼í•© | â­â­â­ |\n",
    "| ë¹„ìš© ì ˆê° | ê¸´ ì‹œí€€ìŠ¤ì—ì„œ $>50\\%$ ì ˆê° ë‹¬ì„± | â­â­â­ |\n",
    "| ë§ˆìŠ¤í¬ ë°€ë„ | Full(50%) â†’ SWA(~6%) â†’ Sparse(~20%) | â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$\\theta_i' = \\theta_i / s \\quad \\text{(YaRN ì €ì£¼íŒŒ ì¬ì¡°ì •)}$$\n",
    "\n",
    "$$\\text{SWA}: A_{ij} \\neq 0 \\iff |i-j| \\leq w/2 \\text{ and } j \\leq i$$\n",
    "\n",
    "$$\\text{ë¹„ìš© ì ˆê°} = 1 - \\frac{n_{full} \\cdot N + n_{swa} \\cdot (w + Kb)}{n_{total} \\cdot N}$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**Chapter 17: Diffusion Transformers** â€” DiT ì•„í‚¤í…ì²˜ì˜ adaLN-Zero ìˆ˜ì‹ê³¼ Flow Matching, HunyuanVideoì˜ 3D ë¹„ë””ì˜¤ ì¸ì½”ë”© ì•„í‚¤í…ì²˜ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}