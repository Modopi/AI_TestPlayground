{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16: ìµœì‹  ê±°ëŒ€ ëª¨ë¸ì˜ íš¨ìœ¨ì„± â€” FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ê³¼ DeepSeek-V3\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- FP8 E4M3 / E5M2 ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹ì˜ ë¹„íŠ¸ ë ˆì´ì•„ì›ƒê³¼ ìˆ˜ì¹˜ ë²”ìœ„ë¥¼ ì´í•´í•œë‹¤\n",
    "- Per-tensor / per-channel ìŠ¤ì¼€ì¼ë§ì„ í†µí•œ FP8 ì–‘ìí™” ìˆ˜ì‹ì„ ë„ì¶œí•œë‹¤\n",
    "- FP8 í›ˆë ¨ ì‹œ ë°œìƒí•˜ëŠ” ì–‘ìí™” ì˜¤ì°¨ì™€ SNRì„ ë¶„ì„í•œë‹¤\n",
    "- DeepSeek-V3ì˜ Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ê¸°ë²•ì„ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•œë‹¤\n",
    "- Multi-Token Prediction(MTP) ìˆ˜ì‹ì„ ë„ì¶œí•˜ê³  í•™ìŠµ íš¨ìœ¨ì„±ì„ ë¶„ì„í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [ìˆ˜í•™ì  ê¸°ì´ˆ: FP8 ë¶€ë™ì†Œìˆ˜ì ê³¼ ì–‘ìí™”](#1.-ìˆ˜í•™ì -ê¸°ì´ˆ)\n",
    "2. [FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ](#2.-ìˆ˜ì¹˜-ë²”ìœ„-ë¹„êµ)\n",
    "3. [FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ê³¼ ì˜¤ì°¨ ë¶„ì„](#3.-FP8-ì–‘ìí™”-ì‹œë®¬ë ˆì´ì…˜)\n",
    "4. [FP8 E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„](#4.-E4M3-vs-E5M2)\n",
    "5. [FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ë¹„êµ](#5.-í•™ìŠµ-ì•ˆì •ì„±-ë¹„êµ)\n",
    "6. [Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±](#6.-Auxiliary-Loss-Free-ë¡œë“œë°¸ëŸ°ì‹±)\n",
    "7. [Multi-Token Prediction (MTP)](#7.-Multi-Token-Prediction)\n",
    "8. [ì •ë¦¬](#8.-ì •ë¦¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 1. ìˆ˜í•™ì  ê¸°ì´ˆ <a name='1.-ìˆ˜í•™ì -ê¸°ì´ˆ'></a>\n",
    "\n",
    "### FP8 ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹\n",
    "\n",
    "FP8ì€ 8ë¹„íŠ¸ë¡œ ë¶€ë™ì†Œìˆ˜ì ì„ í‘œí˜„í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ, ë‘ ê°€ì§€ ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "**E4M3 (4-bit exponent, 3-bit mantissa):**\n",
    "\n",
    "$$x = (-1)^s \\times 2^{e - 7} \\times (1 + m \\cdot 2^{-3})$$\n",
    "\n",
    "- $s$: ë¶€í˜¸ ë¹„íŠ¸ (1ë¹„íŠ¸)\n",
    "- $e$: ì§€ìˆ˜ (4ë¹„íŠ¸, bias = 7)\n",
    "- $m$: ê°€ìˆ˜ (3ë¹„íŠ¸)\n",
    "- í‘œí˜„ ë²”ìœ„: $[-448, 448]$\n",
    "\n",
    "**E5M2 (5-bit exponent, 2-bit mantissa):**\n",
    "\n",
    "$$x = (-1)^s \\times 2^{e - 15} \\times (1 + m \\cdot 2^{-2})$$\n",
    "\n",
    "- í‘œí˜„ ë²”ìœ„: $[-57344, 57344]$\n",
    "- ë” ë„“ì€ ë²”ìœ„, ë” ë‚®ì€ ì •ë°€ë„\n",
    "\n",
    "### FP8 ìŠ¤ì¼€ì¼ë§ ì–‘ìí™”\n",
    "\n",
    "ì…ë ¥ í…ì„œ $x$ë¥¼ FP8ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ìˆ˜ì‹:\n",
    "\n",
    "$$Q(x) = \\text{clamp}\\!\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil,\\; -q_{max},\\; q_{max}\\right) \\cdot s$$\n",
    "\n",
    "- $s = \\frac{\\max(|x|)}{q_{max}}$: ìŠ¤ì¼€ì¼ íŒ©í„°\n",
    "- $q_{max} = 448$ (E4M3) ë˜ëŠ” $q_{max} = 57344$ (E5M2)\n",
    "\n",
    "**Per-tensor vs Per-channel ìŠ¤ì¼€ì¼ë§:**\n",
    "\n",
    "| ë°©ì‹ | ìŠ¤ì¼€ì¼ ê³„ì‚° | ì¥ì  | ë‹¨ì  |\n",
    "|------|-------------|------|------|\n",
    "| Per-tensor | $s = \\frac{\\max(\\|x\\|)}{q_{max}}$ | ë‹¨ìˆœ, ë¹ ë¦„ | ì•„ì›ƒë¼ì´ì–´ì— ì·¨ì•½ |\n",
    "| Per-channel | $s_c = \\frac{\\max(\\|x_c\\|)}{q_{max}}$ | ì±„ë„ë³„ ìµœì í™” | ì˜¤ë²„í—¤ë“œ ì¦ê°€ |\n",
    "\n",
    "### ì–‘ìí™” ì˜¤ì°¨ì™€ SNR\n",
    "\n",
    "$$\\text{MSE} = \\mathbb{E}\\left[(x - Q(x))^2\\right]$$\n",
    "\n",
    "$$\\text{SNR} = 10 \\log_{10}\\!\\left(\\frac{\\text{Var}(x)}{\\text{MSE}}\\right) \\propto 2^{2b}$$\n",
    "\n",
    "- $b$: ë¹„íŠ¸ ìˆ˜ (FP8 â†’ $b \\approx 3\\sim4$ ìœ íš¨ ë¹„íŠ¸)\n",
    "\n",
    "### Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±\n",
    "\n",
    "DeepSeek-V3ëŠ” ë³´ì¡° ì†ì‹¤ ì—†ì´ ì „ë¬¸ê°€ ë¶€í•˜ë¥¼ ê· í˜•ì‹œí‚µë‹ˆë‹¤:\n",
    "\n",
    "$$g_i' = g_i + b_i, \\quad b_i \\leftarrow b_i + \\alpha \\cdot \\text{sign}(\\bar{f} - f_i)$$\n",
    "\n",
    "- $g_i$: ì „ë¬¸ê°€ $i$ì˜ ê²Œì´íŠ¸ ê°’\n",
    "- $b_i$: í¸í–¥ ë³´ì • í•­ (í•™ìŠµ ê°€ëŠ¥)\n",
    "- $f_i$: ì „ë¬¸ê°€ $i$ì— í• ë‹¹ëœ í† í° ë¹„ìœ¨\n",
    "- $\\bar{f} = 1/N_E$: ëª©í‘œ ê· ë“± ë¹„ìœ¨\n",
    "- $\\alpha$: ì—…ë°ì´íŠ¸ ìŠ¤í… í¬ê¸°\n",
    "\n",
    "### Multi-Token Prediction (MTP)\n",
    "\n",
    "$$\\mathcal{L}_{MTP} = \\sum_{k=1}^{K} \\lambda_k \\cdot \\mathbb{E}\\left[-\\log p_\\theta(x_{t+k} \\mid x_{\\leq t})\\right]$$\n",
    "\n",
    "- $K$: ì˜ˆì¸¡ í† í° ìˆ˜ (DeepSeek-V3: $K = 2$)\n",
    "- $\\lambda_k$: ê° í† í° ìœ„ì¹˜ì˜ ê°€ì¤‘ì¹˜\n",
    "\n",
    "**ìš”ì•½ í‘œ:**\n",
    "\n",
    "| êµ¬ë¶„ | ìˆ˜ì‹ | ì„¤ëª… |\n",
    "|------|------|------|\n",
    "| FP8 E4M3 ë²”ìœ„ | $[-448, 448]$ | ë†’ì€ ì •ë°€ë„, ì¢ì€ ë²”ìœ„ |\n",
    "| FP8 E5M2 ë²”ìœ„ | $[-57344, 57344]$ | ë‚®ì€ ì •ë°€ë„, ë„“ì€ ë²”ìœ„ |\n",
    "| ìŠ¤ì¼€ì¼ë§ ì–‘ìí™” | $Q(x) = \\lfloor x/s \\rceil \\cdot s$ | ë™ì  ë²”ìœ„ ì¡°ì • |\n",
    "| SNR | $\\propto 2^{2b}$ | ë¹„íŠ¸ ìˆ˜ì— ì§€ìˆ˜ì  ë¹„ë¡€ |\n",
    "| í¸í–¥ ë³´ì • | $b_i \\leftarrow b_i + \\alpha \\cdot \\text{sign}(\\bar{f} - f_i)$ | ë³´ì¡° ì†ì‹¤ ì—†ëŠ” ê· í˜• |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ FP8 ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ”¢ FP8ì´ ë­”ê°€ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìˆ«ìë¥¼ ì ì„ ë•Œ, A4 ì¢…ì´ í•œ ì¥(FP32)ì— ì“°ë©´ ì•„ì£¼ ì„¸ë°€í•˜ê²Œ ì“¸ ìˆ˜ ìˆì§€ë§Œ, \n",
    "> í¬ìŠ¤íŠ¸ì‡(FP8)ì— ì“°ë©´ ê³µê°„ì´ ì‘ì•„ì„œ ëŒ€ëµì ìœ¼ë¡œë§Œ ì ì„ ìˆ˜ ìˆì–´ìš”!\n",
    "\n",
    "FP8ì€ ìˆ«ìë¥¼ **8ë¹„íŠ¸**ë¡œ í‘œí˜„í•˜ëŠ” ì•„ì£¼ ì‘ì€ í˜•ì‹ì´ì—ìš”:\n",
    "- **E4M3**: \"í° ìˆ«ìëŠ” ëª» ì“°ì§€ë§Œ, ì‘ì€ ìˆ«ìëŠ” ê½¤ ì •í™•í•´ìš”\" â†’ ê°€ì¤‘ì¹˜(weight) ì €ì¥ì— ì¢‹ì•„ìš”\n",
    "- **E5M2**: \"í° ìˆ«ìë„ ì“¸ ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ê°€ ì¢€ ë–¨ì–´ì ¸ìš”\" â†’ ê¸°ìš¸ê¸°(gradient) ì €ì¥ì— ì¢‹ì•„ìš”\n",
    "\n",
    "#### âš–ï¸ ì™œ FP8ë¡œ í›ˆë ¨í•˜ë‚˜ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ìˆ˜í•™ ì‹œí—˜ì„ ë³¼ ë•Œ, ì •ë°€í•œ ê³„ì‚°ê¸°(FP32) ëŒ€ì‹  \n",
    "> ì£¼íŒ(FP8)ìœ¼ë¡œ í’€ë©´ **2ë°° ë¹ ë¥´ê²Œ** í’€ ìˆ˜ ìˆì–´ìš”! ì•½ê°„ì˜ ì˜¤ì°¨ê°€ ìˆì§€ë§Œ, ì •ë‹µì— ê±°ì˜ ë§ì•„ìš”.\n",
    "\n",
    "DeepSeek-V3ëŠ” FP8ë¡œ í›ˆë ¨í•´ì„œ:\n",
    "- ë©”ëª¨ë¦¬ë¥¼ **ì ˆë°˜**ìœ¼ë¡œ ì¤„ì˜€ì–´ìš”\n",
    "- GPU ì—°ì‚°ì„ **2ë°°** ë¹ ë¥´ê²Œ í–ˆì–´ìš”\n",
    "- ê·¸ëŸ°ë°ë„ ì •í™•ë„ëŠ” ê±°ì˜ ë–¨ì–´ì§€ì§€ ì•Šì•˜ì–´ìš”!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "### ğŸ“ ì—°ìŠµ ë¬¸ì œ\n",
    "\n",
    "#### ë¬¸ì œ 1: FP8 E4M3 í‘œí˜„ ë²”ìœ„\n",
    "\n",
    "E4M3ì—ì„œ ê°€ìˆ˜(mantissa)ê°€ 3ë¹„íŠ¸ì´ê³  ì§€ìˆ˜(exponent)ê°€ 4ë¹„íŠ¸ì¼ ë•Œ, \n",
    "ìµœëŒ€ ì–‘ìˆ˜ ê°’ì„ ê³„ì‚°í•˜ì„¸ìš”. (bias = 7, íŠ¹ìˆ˜ê°’ ì œì™¸)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$x_{max} = 2^{(15-7)} \\times (1 + (2^3-1) \\cdot 2^{-3})$$\n",
    "$$= 2^{8} \\times (1 + 7/8) = 256 \\times 1.875 = 480$$\n",
    "\n",
    "ì‹¤ì œ FP8 E4M3 í‘œì¤€ì—ì„œëŠ” NaN í‘œí˜„ì„ ìœ„í•´ $e=15, m=7$ì„ ì˜ˆì•½í•˜ë¯€ë¡œ:\n",
    "$$x_{max} = 2^{(14-7)} \\times (1 + 7/8) = 128 \\times 1.875 = 240$$\n",
    "\n",
    "â†’ ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” $448$ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ (NVIDIA FP8 ì‚¬ì–‘ ê¸°ì¤€, $e=15$ì˜ ì¼ë¶€ë¥¼ ìœ íš¨ê°’ìœ¼ë¡œ í™œìš©).\n",
    "</details>\n",
    "\n",
    "#### ë¬¸ì œ 2: ì–‘ìí™” SNR ë¹„êµ\n",
    "\n",
    "FP32(23ë¹„íŠ¸ ê°€ìˆ˜)ì™€ FP8 E4M3(3ë¹„íŠ¸ ê°€ìˆ˜)ì˜ ì´ë¡ ì  SNR ë¹„ìœ¨ì„ ê³„ì‚°í•˜ì„¸ìš”.\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ í’€ì´ í™•ì¸</summary>\n",
    "\n",
    "$$\\frac{\\text{SNR}_{FP32}}{\\text{SNR}_{FP8}} \\approx \\frac{2^{2 \\times 23}}{2^{2 \\times 3}} = 2^{40} \\approx 10^{12}$$\n",
    "\n",
    "â†’ FP32ëŠ” FP8ë³´ë‹¤ ì•½ $10^{12}$ ë°° ë” ë†’ì€ SNRì„ ê°€ì§‘ë‹ˆë‹¤. í•˜ì§€ë§Œ ë”¥ëŸ¬ë‹ì—ì„œëŠ” ë…¸ì´ì¦ˆì— ê°•ê±´í•˜ë¯€ë¡œ FP8ë¡œë„ ì¶©ë¶„í•œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"NumPy ë²„ì „: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 2. FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ <a name='2.-ìˆ˜ì¹˜-ë²”ìœ„-ë¹„êµ'></a>\n",
    "\n",
    "ë‹¤ì–‘í•œ ë¶€ë™ì†Œìˆ˜ì  í˜•ì‹ì˜ ë¹„íŠ¸ êµ¬ì„±ê³¼ ìˆ˜ì¹˜ ë²”ìœ„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "| í˜•ì‹ | ì´ ë¹„íŠ¸ | ë¶€í˜¸ | ì§€ìˆ˜ | ê°€ìˆ˜ | ìµœëŒ€ê°’ | ìµœì†Œ ì •ê·œê°’ |\n",
    "|------|---------|------|------|------|--------|-------------|\n",
    "| FP32 | 32 | 1 | 8 | 23 | $\\sim3.4\\times10^{38}$ | $\\sim1.2\\times10^{-38}$ |\n",
    "| FP16 | 16 | 1 | 5 | 10 | $65504$ | $\\sim6.1\\times10^{-5}$ |\n",
    "| BF16 | 16 | 1 | 8 | 7 | $\\sim3.4\\times10^{38}$ | $\\sim1.2\\times10^{-38}$ |\n",
    "| FP8 E4M3 | 8 | 1 | 4 | 3 | $448$ | $\\sim0.015625$ |\n",
    "| FP8 E5M2 | 8 | 1 | 5 | 2 | $57344$ | $\\sim6.1\\times10^{-5}$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ FP8/FP16/BF16/FP32 ìˆ˜ì¹˜ ë²”ìœ„ ë¹„êµ í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "formats = {\n",
    "    'FP32':     {'bits': 32, 'sign': 1, 'exp': 8,  'man': 23, 'max': 3.4028235e+38,  'min_normal': 1.175494e-38},\n",
    "    'FP16':     {'bits': 16, 'sign': 1, 'exp': 5,  'man': 10, 'max': 65504.0,        'min_normal': 6.1035e-5},\n",
    "    'BF16':     {'bits': 16, 'sign': 1, 'exp': 8,  'man': 7,  'max': 3.3895314e+38,  'min_normal': 1.175494e-38},\n",
    "    'FP8 E4M3': {'bits': 8,  'sign': 1, 'exp': 4,  'man': 3,  'max': 448.0,          'min_normal': 0.015625},\n",
    "    'FP8 E5M2': {'bits': 8,  'sign': 1, 'exp': 5,  'man': 2,  'max': 57344.0,        'min_normal': 6.1035e-5},\n",
    "}\n",
    "\n",
    "print(\"=\" * 85)\n",
    "print(f\"{'í˜•ì‹':<12} | {'ë¹„íŠ¸':>4} | {'ë¶€í˜¸':>4} | {'ì§€ìˆ˜':>4} | {'ê°€ìˆ˜':>4} | {'ìµœëŒ€ê°’':>15} | {'ìµœì†Œ ì •ê·œê°’':>15}\")\n",
    "print(\"=\" * 85)\n",
    "for name, f in formats.items():\n",
    "    print(f\"{name:<12} | {f['bits']:>4} | {f['sign']:>4} | {f['exp']:>4} | {f['man']:>4} | \"\n",
    "          f\"{f['max']:>15.4e} | {f['min_normal']:>15.4e}\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# ë™ì  ë²”ìœ„ (dB) ê³„ì‚°\n",
    "print(f\"\\në™ì  ë²”ìœ„ ë¹„êµ (dB):\")\n",
    "print(f\"{'í˜•ì‹':<12} | {'ë™ì  ë²”ìœ„ (dB)':>15} | {'ìœ íš¨ ì†Œìˆ˜ ìë¦¿ìˆ˜':>15}\")\n",
    "print(\"-\" * 50)\n",
    "for name, f in formats.items():\n",
    "    dynamic_range_db = 20 * np.log10(f['max'] / f['min_normal'])\n",
    "    decimal_digits = np.log10(2**f['man'])\n",
    "    print(f\"{name:<12} | {dynamic_range_db:>15.1f} | {decimal_digits:>15.2f}\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì ˆì•½ë¥ \n",
    "print(f\"\\në©”ëª¨ë¦¬ ì ˆì•½ë¥  (FP32 ëŒ€ë¹„):\")\n",
    "for name, f in formats.items():\n",
    "    savings = (1 - f['bits'] / 32) * 100\n",
    "    print(f\"  {name}: {savings:.0f}% ì ˆì•½ ({f['bits']}ë¹„íŠ¸ / 32ë¹„íŠ¸)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 3. FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ê³¼ ì˜¤ì°¨ ë¶„ì„ <a name='3.-FP8-ì–‘ìí™”-ì‹œë®¬ë ˆì´ì…˜'></a>\n",
    "\n",
    "ì‹¤ì œ í…ì„œì— FP8 ì–‘ìí™”ë¥¼ ì ìš©í•˜ê³  ë°œìƒí•˜ëŠ” ì˜¤ì°¨ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$Q_{E4M3}(x) = \\text{clamp}\\!\\left(\\text{round}\\!\\left(\\frac{x}{s}\\right), -448, 448\\right) \\cdot s$$\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N}\\sum_{i=1}^{N}(x_i - Q(x_i))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def simulate_fp8_quantization(x, fmt='e4m3'):\n",
    "    # FP8 ì–‘ìí™” ì‹œë®¬ë ˆì´ì…˜ (ì†Œí”„íŠ¸ì›¨ì–´ ì—ë®¬ë ˆì´ì…˜)\n",
    "    if fmt == 'e4m3':\n",
    "        q_max = 448.0\n",
    "        mantissa_bits = 3\n",
    "    else:  # e5m2\n",
    "        q_max = 57344.0\n",
    "        mantissa_bits = 2\n",
    "\n",
    "    # per-tensor ìŠ¤ì¼€ì¼ë§\n",
    "    abs_max = np.max(np.abs(x)) + 1e-12\n",
    "    scale = abs_max / q_max\n",
    "\n",
    "    # ì–‘ìí™”: ìŠ¤ì¼€ì¼ë§ â†’ ë¼ìš´ë”© â†’ í´ë¨í•‘\n",
    "    x_scaled = x / scale\n",
    "    n_levels = 2 ** mantissa_bits\n",
    "    x_quantized = np.round(x_scaled * n_levels) / n_levels\n",
    "    x_quantized = np.clip(x_quantized, -q_max, q_max)\n",
    "\n",
    "    # ì—­ì–‘ìí™”\n",
    "    x_dequant = x_quantized * scale\n",
    "    return x_dequant, scale\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ë‹¤ì–‘í•œ ë¶„í¬ì˜ í…ì„œ\n",
    "np.random.seed(42)\n",
    "n_elements = 10000\n",
    "\n",
    "# ì •ê·œ ë¶„í¬ (ëª¨ë¸ ê°€ì¤‘ì¹˜ ê·¼ì‚¬)\n",
    "x_normal = np.random.randn(n_elements).astype(np.float32)\n",
    "# ê· ë“± ë¶„í¬\n",
    "x_uniform = np.random.uniform(-2, 2, n_elements).astype(np.float32)\n",
    "# ì•„ì›ƒë¼ì´ì–´ê°€ ìˆëŠ” ë¶„í¬\n",
    "x_outlier = np.random.randn(n_elements).astype(np.float32)\n",
    "x_outlier[np.random.choice(n_elements, 50)] *= 100  # 0.5% ì•„ì›ƒë¼ì´ì–´\n",
    "\n",
    "distributions = {'ì •ê·œ ë¶„í¬': x_normal, 'ê· ë“± ë¶„í¬': x_uniform, 'ì•„ì›ƒë¼ì´ì–´ ë¶„í¬': x_outlier}\n",
    "\n",
    "print(f\"FP8 ì–‘ìí™” ì˜¤ì°¨ ë¶„ì„ (ì›ì†Œ ìˆ˜: {n_elements})\")\n",
    "print(f\"{'ë¶„í¬':<16} | {'í˜•ì‹':<8} | {'MSE':>12} | {'SNR (dB)':>10} | {'ìµœëŒ€ ì˜¤ì°¨':>10} | {'ìŠ¤ì¼€ì¼':>10}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for dist_name, x in distributions.items():\n",
    "    for fmt in ['e4m3', 'e5m2']:\n",
    "        x_q, scale = simulate_fp8_quantization(x, fmt=fmt)\n",
    "        mse = np.mean((x - x_q) ** 2)\n",
    "        var_x = np.var(x)\n",
    "        snr = 10 * np.log10(var_x / (mse + 1e-12))\n",
    "        max_err = np.max(np.abs(x - x_q))\n",
    "        fmt_label = 'E4M3' if fmt == 'e4m3' else 'E5M2'\n",
    "        print(f\"{dist_name:<16} | {fmt_label:<8} | {mse:>12.6f} | {snr:>10.2f} | {max_err:>10.4f} | {scale:>10.6f}\")\n",
    "\n",
    "print(f\"\\nê²°ë¡ : E4M3ëŠ” ì¼ë°˜ ë¶„í¬ì—ì„œ ë” ë†’ì€ SNR (ë†’ì€ ì •ë°€ë„)\")\n",
    "print(f\"      E5M2ëŠ” ì•„ì›ƒë¼ì´ì–´ ë¶„í¬ì—ì„œ ìƒëŒ€ì ìœ¼ë¡œ ì•ˆì •ì  (ë„“ì€ ë²”ìœ„)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 4. FP8 E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„ <a name='4.-E4M3-vs-E5M2'></a>\n",
    "\n",
    "DeepSeek-V3ì—ì„œì˜ ì‚¬ìš© ì „ëµ:\n",
    "- **Forward pass (ê°€ì¤‘ì¹˜, í™œì„±í™”)**: E4M3 â€” ë†’ì€ ì •ë°€ë„ í•„ìš”\n",
    "- **Backward pass (ê¸°ìš¸ê¸°)**: E5M2 â€” í° ë™ì  ë²”ìœ„ í•„ìš”\n",
    "\n",
    "$$\\text{Forward}: W_{E4M3} \\times X_{E4M3} \\rightarrow Y_{FP32}$$\n",
    "$$\\text{Backward}: \\frac{\\partial L}{\\partial Y}_{E5M2} \\times X_{E4M3}^T \\rightarrow \\frac{\\partial L}{\\partial W}_{FP32}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ E4M3 vs E5M2 íŠ¸ë ˆì´ë“œì˜¤í”„ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# (1) ì–‘ìí™” ì˜¤ì°¨ ë¶„í¬ ë¹„êµ\n",
    "ax1 = axes[0]\n",
    "x_test = np.random.randn(5000).astype(np.float32) * 2\n",
    "x_e4m3, _ = simulate_fp8_quantization(x_test, 'e4m3')\n",
    "x_e5m2, _ = simulate_fp8_quantization(x_test, 'e5m2')\n",
    "\n",
    "err_e4m3 = x_test - x_e4m3\n",
    "err_e5m2 = x_test - x_e5m2\n",
    "\n",
    "ax1.hist(err_e4m3, bins=80, alpha=0.6, color='blue', label=f'E4M3 (std={np.std(err_e4m3):.4f})', density=True)\n",
    "ax1.hist(err_e5m2, bins=80, alpha=0.6, color='red', label=f'E5M2 (std={np.std(err_e5m2):.4f})', density=True)\n",
    "ax1.set_xlabel('ì–‘ìí™” ì˜¤ì°¨', fontsize=11)\n",
    "ax1.set_ylabel('ë°€ë„', fontsize=11)\n",
    "ax1.set_title('E4M3 vs E5M2 ì–‘ìí™” ì˜¤ì°¨ ë¶„í¬', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# (2) í¬ê¸°ë³„ SNR ë¹„êµ\n",
    "ax2 = axes[1]\n",
    "scale_factors = np.logspace(-2, 2, 20)\n",
    "snr_e4m3_list = []\n",
    "snr_e5m2_list = []\n",
    "\n",
    "for sf in scale_factors:\n",
    "    x_scaled = x_test * sf\n",
    "    for fmt, snr_list in [('e4m3', snr_e4m3_list), ('e5m2', snr_e5m2_list)]:\n",
    "        x_q, _ = simulate_fp8_quantization(x_scaled, fmt)\n",
    "        mse = np.mean((x_scaled - x_q) ** 2) + 1e-12\n",
    "        snr_val = 10 * np.log10(np.var(x_scaled) / mse)\n",
    "        snr_list.append(snr_val)\n",
    "\n",
    "ax2.semilogx(scale_factors, snr_e4m3_list, 'b-o', lw=2, ms=6, label='E4M3 (ê°€ì¤‘ì¹˜ìš©)')\n",
    "ax2.semilogx(scale_factors, snr_e5m2_list, 'r-s', lw=2, ms=6, label='E5M2 (ê¸°ìš¸ê¸°ìš©)')\n",
    "ax2.set_xlabel('í…ì„œ ìŠ¤ì¼€ì¼ íŒ©í„°', fontsize=11)\n",
    "ax2.set_ylabel('SNR (dB)', fontsize=11)\n",
    "ax2.set_title('ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ FP8 SNR ë¹„êµ', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/fp8_e4m3_vs_e5m2.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/fp8_e4m3_vs_e5m2.png\")\n",
    "print(f\"E4M3 í‰ê·  SNR: {np.mean(snr_e4m3_list):.2f} dB\")\n",
    "print(f\"E5M2 í‰ê·  SNR: {np.mean(snr_e5m2_list):.2f} dB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 5. FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ë¹„êµ <a name='5.-í•™ìŠµ-ì•ˆì •ì„±-ë¹„êµ'></a>\n",
    "\n",
    "FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ê³¼ FP16 í›ˆë ¨ì˜ í•™ìŠµ ê³¡ì„ ì„ ë¹„êµí•©ë‹ˆë‹¤.\n",
    "\n",
    "**í˜¼í•© ì •ë°€ë„ í›ˆë ¨ ì „ëµ (DeepSeek-V3):**\n",
    "1. Forward: $W_{E4M3} \\times X_{E4M3}$ (GEMM ì—°ì‚°)\n",
    "2. Backward: $\\nabla_{E5M2}$ (ê¸°ìš¸ê¸° ê³„ì‚°)\n",
    "3. Update: $W_{FP32} \\leftarrow W_{FP32} - \\eta \\nabla_{FP32}$ (ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ FP8 vs FP16 í•™ìŠµ ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# ê°„ë‹¨í•œ íšŒê·€ ëª¨ë¸ë¡œ ì •ë°€ë„ë³„ í•™ìŠµ ê³¡ì„  ë¹„êµ\n",
    "np.random.seed(42)\n",
    "\n",
    "# í•©ì„± ë°ì´í„° ìƒì„±\n",
    "n_data = 200\n",
    "X_data = np.random.randn(n_data, 10).astype(np.float32)\n",
    "true_w = np.random.randn(10, 1).astype(np.float32)\n",
    "Y_data = X_data @ true_w + np.random.randn(n_data, 1).astype(np.float32) * 0.1\n",
    "\n",
    "X_tf = tf.constant(X_data)\n",
    "Y_tf = tf.constant(Y_data)\n",
    "\n",
    "def train_with_precision(precision_name, n_epochs=100, lr=0.01):\n",
    "    # ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ëŠ” í•­ìƒ FP32\n",
    "    W = tf.Variable(tf.random.normal([10, 1], seed=42))\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            if precision_name == 'FP8':\n",
    "                # FP8 ì‹œë®¬ë ˆì´ì…˜: forwardì—ì„œ ì–‘ìí™” ë…¸ì´ì¦ˆ ì¶”ê°€\n",
    "                W_noisy = W + tf.random.normal(W.shape, stddev=0.005)\n",
    "                X_noisy = X_tf + tf.random.normal(X_tf.shape, stddev=0.002)\n",
    "                pred = tf.matmul(X_noisy, W_noisy)\n",
    "            elif precision_name == 'FP16':\n",
    "                W_fp16 = tf.cast(tf.cast(W, tf.float16), tf.float32)\n",
    "                pred = tf.matmul(X_tf, W_fp16)\n",
    "            else:  # FP32\n",
    "                pred = tf.matmul(X_tf, W)\n",
    "\n",
    "            loss = tf.reduce_mean((pred - Y_tf) ** 2)\n",
    "\n",
    "        grads = tape.gradient(loss, [W])\n",
    "        optimizer.apply_gradients(zip(grads, [W]))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "    return losses\n",
    "\n",
    "# ê° ì •ë°€ë„ë¡œ í•™ìŠµ\n",
    "losses_fp32 = train_with_precision('FP32')\n",
    "losses_fp16 = train_with_precision('FP16')\n",
    "losses_fp8 = train_with_precision('FP8')\n",
    "\n",
    "# ê²°ê³¼ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(losses_fp32, 'g-', lw=2.5, label='FP32 (ê¸°ì¤€)')\n",
    "ax1.plot(losses_fp16, 'b--', lw=2, label='FP16')\n",
    "ax1.plot(losses_fp8, 'r-.', lw=2, label='FP8 (ì‹œë®¬ë ˆì´ì…˜)')\n",
    "ax1.set_xlabel('ì—í­', fontsize=11)\n",
    "ax1.set_ylabel('MSE Loss', fontsize=11)\n",
    "ax1.set_title('ì •ë°€ë„ë³„ í•™ìŠµ ê³¡ì„ ', fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2 = axes[1]\n",
    "fp8_gap = np.array(losses_fp8) - np.array(losses_fp32)\n",
    "fp16_gap = np.array(losses_fp16) - np.array(losses_fp32)\n",
    "ax2.plot(fp16_gap, 'b-', lw=2, label='FP16 - FP32')\n",
    "ax2.plot(fp8_gap, 'r-', lw=2, label='FP8 - FP32')\n",
    "ax2.axhline(y=0, color='gray', ls='--', lw=1)\n",
    "ax2.set_xlabel('ì—í­', fontsize=11)\n",
    "ax2.set_ylabel('Loss ì°¨ì´', fontsize=11)\n",
    "ax2.set_title('FP32 ëŒ€ë¹„ Loss ê°­', fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/fp8_vs_fp16_training.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/fp8_vs_fp16_training.png\")\n",
    "print(f\"\\nìµœì¢… Loss ë¹„êµ:\")\n",
    "print(f\"  FP32: {losses_fp32[-1]:.6f}\")\n",
    "print(f\"  FP16: {losses_fp16[-1]:.6f} (FP32 ëŒ€ë¹„ ì°¨ì´: {losses_fp16[-1]-losses_fp32[-1]:+.6f})\")\n",
    "print(f\"  FP8:  {losses_fp8[-1]:.6f} (FP32 ëŒ€ë¹„ ì°¨ì´: {losses_fp8[-1]-losses_fp32[-1]:+.6f})\")\n",
    "print(f\"\\nâ†’ FP8 í˜¼í•© ì •ë°€ë„ í›ˆë ¨ì€ ì•½ê°„ì˜ ë…¸ì´ì¦ˆê°€ ì¶”ê°€ë˜ì§€ë§Œ,\")\n",
    "print(f\"  regularization íš¨ê³¼ë¡œ ìµœì¢… ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 6. Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± <a name='6.-Auxiliary-Loss-Free-ë¡œë“œë°¸ëŸ°ì‹±'></a>\n",
    "\n",
    "ê¸°ì¡´ MoEì˜ ë³´ì¡° ì†ì‹¤(Auxiliary Loss)ì€ í•™ìŠµ ëª©ì í•¨ìˆ˜ë¥¼ ì™œê³¡ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "DeepSeek-V3ëŠ” **í¸í–¥ ë³´ì •(bias correction)** ë°©ì‹ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$g_i^{(\\text{topk})} = \\begin{cases} g_i & \\text{if } g_i + b_i \\in \\text{TopK}(g_1+b_1, \\ldots, g_N+b_N) \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "í¸í–¥ ì—…ë°ì´íŠ¸ ê·œì¹™:\n",
    "\n",
    "$$b_i \\leftarrow b_i + \\alpha \\cdot \\text{sign}(\\bar{f} - f_i)$$\n",
    "\n",
    "- ê³¼ë¶€í•˜ëœ ì „ë¬¸ê°€: $f_i > \\bar{f}$ â†’ $b_i$ ê°ì†Œ â†’ ì„ íƒ í™•ë¥  â†“\n",
    "- ê³¼ì†Œ í™œìš© ì „ë¬¸ê°€: $f_i < \\bar{f}$ â†’ $b_i$ ì¦ê°€ â†’ ì„ íƒ í™•ë¥  â†‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹± ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "\n",
    "n_experts = 8\n",
    "n_tokens = 1000\n",
    "n_steps = 200\n",
    "top_k = 2\n",
    "alpha = 0.01  # í¸í–¥ ì—…ë°ì´íŠ¸ ìŠ¤í… í¬ê¸°\n",
    "target_freq = 1.0 / n_experts\n",
    "\n",
    "# ì´ˆê¸° ê²Œì´íŠ¸ ê°’: ë¶ˆê· í˜•í•˜ê²Œ ì„¤ì • (ì¼ë¶€ ì „ë¬¸ê°€ì— í¸í–¥)\n",
    "gate_bias = np.array([0.5, 0.3, 0.1, -0.1, -0.2, -0.3, 0.2, 0.0])\n",
    "\n",
    "# í¸í–¥ ë³´ì • í•­ ì´ˆê¸°í™”\n",
    "bias_correction = np.zeros(n_experts)\n",
    "\n",
    "freq_history = []\n",
    "bias_history = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    # ëœë¤ í† í° ê²Œì´íŠ¸ ê°’ ìƒì„±\n",
    "    raw_gates = np.random.randn(n_tokens, n_experts) + gate_bias\n",
    "\n",
    "    # í¸í–¥ ë³´ì • ì ìš©\n",
    "    adjusted_gates = raw_gates + bias_correction\n",
    "\n",
    "    # Top-K ì„ íƒ\n",
    "    expert_counts = np.zeros(n_experts)\n",
    "    for t in range(n_tokens):\n",
    "        top_indices = np.argsort(adjusted_gates[t])[-top_k:]\n",
    "        expert_counts[top_indices] += 1\n",
    "\n",
    "    # ë¹ˆë„ ê³„ì‚°\n",
    "    freqs = expert_counts / (n_tokens * top_k)\n",
    "    freq_history.append(freqs.copy())\n",
    "    bias_history.append(bias_correction.copy())\n",
    "\n",
    "    # í¸í–¥ ì—…ë°ì´íŠ¸: ê³¼ë¶€í•˜ â†’ ê°ì†Œ, ê³¼ì†Œí™œìš© â†’ ì¦ê°€\n",
    "    bias_correction += alpha * np.sign(target_freq - freqs)\n",
    "\n",
    "freq_history = np.array(freq_history)\n",
    "bias_history = np.array(bias_history)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "for i in range(n_experts):\n",
    "    ax1.plot(freq_history[:, i], lw=1.5, label=f'Expert {i}')\n",
    "ax1.axhline(y=target_freq, color='black', ls='--', lw=2, label=f'ëª©í‘œ ({target_freq:.3f})')\n",
    "ax1.set_xlabel('ìŠ¤í…', fontsize=11)\n",
    "ax1.set_ylabel('í† í° í• ë‹¹ ë¹„ìœ¨', fontsize=11)\n",
    "ax1.set_title('Auxiliary-Loss-Free ë¡œë“œë°¸ëŸ°ì‹±', fontweight='bold')\n",
    "ax1.legend(fontsize=8, ncol=3, loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "for i in range(n_experts):\n",
    "    ax2.plot(bias_history[:, i], lw=1.5, label=f'Expert {i}')\n",
    "ax2.axhline(y=0, color='black', ls='--', lw=1)\n",
    "ax2.set_xlabel('ìŠ¤í…', fontsize=11)\n",
    "ax2.set_ylabel('í¸í–¥ ë³´ì • ê°’ ($b_i$)', fontsize=11)\n",
    "ax2.set_title('í¸í–¥ ë³´ì • í•­ ë³€í™”', fontweight='bold')\n",
    "ax2.legend(fontsize=8, ncol=3)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chapter16_sparse_attention/aux_loss_free_balancing.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"ê·¸ë˜í”„ ì €ì¥ë¨: chapter16_sparse_attention/aux_loss_free_balancing.png\")\n",
    "\n",
    "print(f\"\\në¡œë“œë°¸ëŸ°ì‹± ê²°ê³¼:\")\n",
    "print(f\"  ì´ˆê¸° ë¶ˆê· í˜• (step 0):  std = {freq_history[0].std():.4f}\")\n",
    "print(f\"  ìµœì¢… ê· í˜• (step {n_steps-1}): std = {freq_history[-1].std():.4f}\")\n",
    "print(f\"  ê· í˜• ê°œì„  ë¹„ìœ¨: {freq_history[0].std()/freq_history[-1].std():.1f}x\")\n",
    "print(f\"\\nìµœì¢… ì „ë¬¸ê°€ë³„ í† í° ë¹„ìœ¨:\")\n",
    "for i in range(n_experts):\n",
    "    bar = 'â–ˆ' * int(freq_history[-1, i] * 200)\n",
    "    print(f\"  Expert {i}: {freq_history[-1, i]:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 7. Multi-Token Prediction (MTP) <a name='7.-Multi-Token-Prediction'></a>\n",
    "\n",
    "DeepSeek-V3ì˜ MTPëŠ” í•œ ë²ˆì˜ forward passì—ì„œ ì—¬ëŸ¬ ë¯¸ë˜ í† í°ì„ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{main} + \\sum_{k=1}^{K} \\lambda_k \\cdot \\mathcal{L}_{MTP}^{(k)}$$\n",
    "\n",
    "$$\\mathcal{L}_{MTP}^{(k)} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log p_\\theta(x_{t+k} \\mid x_{\\leq t})$$\n",
    "\n",
    "MTPì˜ ì´ì :\n",
    "1. í•™ìŠµ ë°ì´í„° í™œìš© íš¨ìœ¨ ì¦ê°€ ($K$ë°° ë” ë§ì€ ì‹ í˜¸)\n",
    "2. ì¶”ë¡  ì‹œ Speculative Decodingê³¼ ê²°í•© ê°€ëŠ¥\n",
    "3. ë‚´ë¶€ í‘œí˜„ì˜ ì˜ˆì¸¡ ëŠ¥ë ¥ ê°•í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Multi-Token Prediction (MTP) ì‹œë®¬ë ˆì´ì…˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(42)\n",
    "\n",
    "vocab_size = 100\n",
    "seq_len = 32\n",
    "batch_size = 16\n",
    "embed_dim = 64\n",
    "K_predict = 3  # ì˜ˆì¸¡ í† í° ìˆ˜\n",
    "\n",
    "# ê°„ë‹¨í•œ MTP ëª¨ë¸: ê³µìœ  ì¸ì½”ë” + Kê°œì˜ ì˜ˆì¸¡ í—¤ë“œ\n",
    "shared_encoder = tf.keras.layers.Dense(embed_dim, activation='relu')\n",
    "mtp_heads = [tf.keras.layers.Dense(vocab_size) for _ in range(K_predict)]\n",
    "\n",
    "# í•©ì„± ì‹œí€€ìŠ¤ ë°ì´í„°\n",
    "input_ids = np.random.randint(0, vocab_size, (batch_size, seq_len))\n",
    "embeddings = tf.one_hot(input_ids, vocab_size)\n",
    "embeddings = tf.cast(embeddings, tf.float32)\n",
    "\n",
    "# Forward pass\n",
    "hidden = shared_encoder(tf.reshape(embeddings, [-1, vocab_size]))\n",
    "hidden = tf.reshape(hidden, [batch_size, seq_len, embed_dim])\n",
    "\n",
    "print(f\"MTP ì„¤ì •:\")\n",
    "print(f\"  ì‹œí€€ìŠ¤ ê¸¸ì´: {seq_len}\")\n",
    "print(f\"  ì˜ˆì¸¡ í† í° ìˆ˜ (K): {K_predict}\")\n",
    "print(f\"  ì–´íœ˜ í¬ê¸°: {vocab_size}\")\n",
    "print(f\"  ì¸ì½”ë” ì¶œë ¥: {hidden.shape}\")\n",
    "\n",
    "# ê° ì˜ˆì¸¡ í—¤ë“œë³„ ì†ì‹¤ ê³„ì‚°\n",
    "total_loss = 0.0\n",
    "lambda_weights = [1.0, 0.5, 0.25]  # ê°€ê¹Œìš´ í† í°ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ\n",
    "\n",
    "print(f\"\\nê° ì˜ˆì¸¡ í—¤ë“œë³„ ì†ì‹¤:\")\n",
    "print(f\"{'í—¤ë“œ (k)':>10} | {'Lambda':>8} | {'Loss':>12} | {'ê°€ì¤‘ Loss':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for k in range(K_predict):\n",
    "    logits_k = mtp_heads[k](tf.reshape(hidden[:, :seq_len-K_predict, :], [-1, embed_dim]))\n",
    "    targets_k = input_ids[:, k+1:seq_len-K_predict+k+1].flatten()\n",
    "    targets_k = tf.constant(targets_k, dtype=tf.int32)\n",
    "\n",
    "    loss_k = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        targets_k, logits_k, from_logits=True\n",
    "    )\n",
    "    loss_k_mean = tf.reduce_mean(loss_k).numpy()\n",
    "    weighted_loss = lambda_weights[k] * loss_k_mean\n",
    "    total_loss += weighted_loss\n",
    "\n",
    "    print(f\"  k={k+1:>5} | {lambda_weights[k]:>8.2f} | {loss_k_mean:>12.4f} | {weighted_loss:>12.4f}\")\n",
    "\n",
    "print(f\"{'':>10} {'':>8}   {'í•©ê³„':>12}   {total_loss:>12.4f}\")\n",
    "\n",
    "# MTP í•™ìŠµ íš¨ìœ¨ ë¶„ì„\n",
    "print(f\"\\nMTP í•™ìŠµ íš¨ìœ¨:\")\n",
    "standard_signals = seq_len - 1  # í‘œì¤€ next-token prediction\n",
    "mtp_signals = (seq_len - K_predict) * K_predict + (seq_len - 1)\n",
    "print(f\"  í‘œì¤€ NTP í•™ìŠµ ì‹ í˜¸: {standard_signals}ê°œ/ì‹œí€€ìŠ¤\")\n",
    "print(f\"  MTP í•™ìŠµ ì‹ í˜¸: {mtp_signals}ê°œ/ì‹œí€€ìŠ¤\")\n",
    "print(f\"  íš¨ìœ¨ ì¦ê°€: {mtp_signals/standard_signals:.2f}x\")\n",
    "print(f\"\\nâ†’ MTPëŠ” ê°™ì€ ë°ì´í„°ì—ì„œ {mtp_signals/standard_signals:.1f}ë°° ë” ë§ì€ í•™ìŠµ ì‹ í˜¸ë¥¼ ì¶”ì¶œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "## 8. ì •ë¦¬ <a name='8.-ì •ë¦¬'></a>\n",
    "\n",
    "### í•µì‹¬ ê°œë… ìš”ì•½\n",
    "\n",
    "| ê°œë… | ì„¤ëª… | ì¤‘ìš”ë„ |\n",
    "|------|------|--------|\n",
    "| FP8 E4M3 | ë†’ì€ ì •ë°€ë„, $[-448, 448]$ ë²”ìœ„ â€” ê°€ì¤‘ì¹˜/í™œì„±í™”ìš© | â­â­â­ |\n",
    "| FP8 E5M2 | ë„“ì€ ë²”ìœ„, $[-57344, 57344]$ â€” ê¸°ìš¸ê¸°ìš© | â­â­â­ |\n",
    "| Per-tensor ìŠ¤ì¼€ì¼ë§ | $s = \\max(|x|)/q_{max}$ â€” ë‹¨ìˆœí•˜ê³  ë¹ ë¦„ | â­â­ |\n",
    "| SNR | $\\propto 2^{2b}$ â€” ë¹„íŠ¸ ìˆ˜ì— ë”°ë¥¸ í’ˆì§ˆ ì§€í‘œ | â­â­ |\n",
    "| Aux-Loss-Free | $b_i \\leftarrow b_i + \\alpha \\cdot \\text{sign}(\\bar{f} - f_i)$ â€” í¸í–¥ ë³´ì • | â­â­â­ |\n",
    "| MTP | $\\mathcal{L} = \\sum_k \\lambda_k \\mathcal{L}^{(k)}$ â€” ë‹¤ì¤‘ í† í° ì˜ˆì¸¡ | â­â­â­ |\n",
    "| í˜¼í•© ì •ë°€ë„ ì „ëµ | Forward(E4M3) + Backward(E5M2) + Update(FP32) | â­â­â­ |\n",
    "\n",
    "### í•µì‹¬ ìˆ˜ì‹\n",
    "\n",
    "$$Q(x) = \\text{clamp}\\!\\left(\\left\\lfloor \\frac{x}{s} \\right\\rceil, -q_{max}, q_{max}\\right) \\cdot s$$\n",
    "\n",
    "$$b_i \\leftarrow b_i + \\alpha \\cdot \\text{sign}\\!\\left(\\frac{1}{N_E} - f_i\\right)$$\n",
    "\n",
    "$$\\mathcal{L}_{MTP} = \\sum_{k=1}^{K} \\lambda_k \\cdot \\mathbb{E}\\left[-\\log p_\\theta(x_{t+k} \\mid x_{\\leq t})\\right]$$\n",
    "\n",
    "### ë‹¤ìŒ ì±•í„° ì˜ˆê³ \n",
    "**02_multi_head_latent_attention.ipynb** â€” MLA(Multi-head Latent Attention)ì˜ KV ì••ì¶• ìˆ˜ì‹ì„ ì™„ì „ ë„ì¶œí•˜ê³ , GQA ëŒ€ë¹„ KV Cache ì ˆê°ë¥ ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµí•©ë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}