{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04-01: tf.data API â€” íš¨ìœ¨ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- tf.data.Datasetì˜ ë‹¤ì–‘í•œ ìƒì„± ë°©ë²•ì„ ì´í•´í•œë‹¤\n",
    "- map/filter/batch/shuffle/prefetchì˜ ì—­í• ê³¼ ì˜¬ë°”ë¥¸ ìˆœì„œë¥¼ ì•ˆë‹¤\n",
    "- AUTOTUNEìœ¼ë¡œ ìë™ ìµœì í™”í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. Dataset ìƒì„±\n",
    "2. ë³€í™˜ ì—°ì‚°\n",
    "3. íŒŒì´í”„ë¼ì¸ ìµœì í™”\n",
    "4. ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ tf.data ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### ğŸ½ï¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì™œ í•„ìš”í•´ìš”?\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì‹ë‹¹ ì£¼ë°©ì²˜ëŸ¼!\n",
    "> - ì†ë‹˜(GPU)ì´ ë°¥(ë°ì´í„°)ì„ ë¨¹ëŠ” ì†ë„ê°€ ë§¤ìš° ë¹¨ë¼ìš”\n",
    "> - ì¬ë£Œë¥¼ í•˜ë‚˜ì”© ì†ì§ˆí•˜ë©´(ëŠë¦° ë°ì´í„° ë¡œë”©) GPUê°€ ê¸°ë‹¤ë ¤ìš”! ë‚­ë¹„!\n",
    "> - **íŒŒì´í”„ë¼ì¸** = ë¯¸ë¦¬ ì¬ë£Œ ì†ì§ˆ + ëŒ€ê¸°ì—´ ì¤€ë¹„ â†’ GPUê°€ ì‰¬ì§€ ì•Šì•„ìš”!\n",
    "\n",
    "#### ğŸ”„ tf.data í•µì‹¬ ë³€í™˜ ì—°ì‚°\n",
    "\n",
    "| ì—°ì‚° | ì—­í•  | ë¹„ìœ  |\n",
    "|------|------|------|\n",
    "| `.map()` | ê° ë°ì´í„°ì— í•¨ìˆ˜ ì ìš© | ì¬ë£Œ ì†ì§ˆ ğŸ”ª |\n",
    "| `.shuffle()` | ë°ì´í„° ë¬´ì‘ìœ„ ì„ê¸° | ì¹´ë“œ ì„ê¸° ğŸƒ |\n",
    "| `.batch()` | Nê°œì”© ë¬¶ì–´ ë°°ì¹˜ ìƒì„± | ì ‘ì‹œì— ë‹´ê¸° ğŸ½ï¸ |\n",
    "| `.cache()` | ì²« ì—í¬í¬ í›„ ë©”ëª¨ë¦¬ ì €ì¥ | ìì£¼ ì“°ëŠ” ê²ƒ ì„œëì— |\n",
    "| `.prefetch()` | ë‹¤ìŒ ë°°ì¹˜ ë¯¸ë¦¬ ì¤€ë¹„ | ë‹¤ìŒ ì†ë‹˜ ìŒì‹ ë¯¸ë¦¬ ì¡°ë¦¬ ğŸ³ |\n",
    "\n",
    "#### âš¡ ìµœì  íŒŒì´í”„ë¼ì¸ ìˆœì„œ (ê¶Œì¥)\n",
    "\n",
    "```\n",
    "from_tensor_slices()\n",
    "    â†’ .map(ì „ì²˜ë¦¬)      â† CPU ë³‘ë ¬ ì²˜ë¦¬\n",
    "    â†’ .cache()          â† ì²« ì—í¬í¬ í›„ ë©”ëª¨ë¦¬ì— ì €ì¥\n",
    "    â†’ .shuffle(buffer)  â† ëœë¤ ì…”í”Œ\n",
    "    â†’ .batch(N)         â† ë°°ì¹˜ ìƒì„±\n",
    "    â†’ .prefetch(AUTO)   â† GPU í•™ìŠµ ì¤‘ CPUê°€ ë‹¤ìŒ ë°°ì¹˜ ì¤€ë¹„\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **shuffleì˜ buffer_size**: í´ìˆ˜ë¡ ë” ì˜ ì„ì´ì§€ë§Œ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©!\n",
    "> ë³´í†µ ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸° ì •ë„ë¡œ ì„¤ì •í•´ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print('TensorFlow ë²„ì „:', tf.__version__)\n",
    "print('NumPy ë²„ì „:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜í•™ì  ê¸°ì´ˆ\n",
    "\n",
    "**Min-Max ì •ê·œí™”**\n",
    "$$x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
    "\n",
    "**Z-score í‘œì¤€í™”**\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ ë‚´ `.map()`ì—ì„œ ì´ ìˆ˜ì‹ì„ ì ìš©í•´ í”½ì…€ê°’ [0,255]ì„ [0,1]ë¡œ ë³€í™˜í•œë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ë°©ë²• 1: from_tensor_slices â€” numpy ë°°ì—´ ë˜ëŠ” í…ì„œì—ì„œ ìƒì„±\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# MNIST ë°ì´í„° ë¡œë“œ\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(f'í›ˆë ¨ ë°ì´í„° shape: {x_train.shape}, ë ˆì´ë¸” shape: {y_train.shape}')\n",
    "\n",
    "# numpy ë°°ì—´ì„ Datasetìœ¼ë¡œ ë³€í™˜\n",
    "# ê° ìŠ¬ë¼ì´ìŠ¤ëŠ” (ì´ë¯¸ì§€, ë ˆì´ë¸”) ìŒ\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "print(f'Dataset ì›ì†Œ spec: {train_dataset.element_spec}')\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë°°ì¹˜ í™•ì¸\n",
    "for img, label in train_dataset.take(1):\n",
    "    print(f'ì´ë¯¸ì§€ shape: {img.shape}, dtype: {img.dtype}')\n",
    "    print(f'ë ˆì´ë¸”: {label.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ë°©ë²• 2: from_generator â€” íŒŒì´ì¬ ì œë„ˆë ˆì´í„°ì—ì„œ ìƒì„±\n",
    "# ë©”ëª¨ë¦¬ì— ëª¨ë“  ë°ì´í„°ë¥¼ ì˜¬ë¦¬ì§€ ì•Šê³  ë™ì ìœ¼ë¡œ ìƒì„±í•  ë•Œ ìœ ìš©\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"ê°„ë‹¨í•œ ìˆ˜ì—´ ì œë„ˆë ˆì´í„°: (x, x^2) ìŒì„ ìƒì„±í•œë‹¤.\"\"\"\n",
    "    for i in range(10):\n",
    "        yield (float(i), float(i ** 2))\n",
    "\n",
    "gen_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32),  # x\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32),  # x^2\n",
    "    )\n",
    ")\n",
    "\n",
    "print('ì œë„ˆë ˆì´í„° Dataset ì›ì†Œ:')\n",
    "for x, x_sq in gen_dataset:\n",
    "    print(f'  x={x.numpy():.0f}, xÂ²={x_sq.numpy():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ë°©ë²• 3: tf.data.Dataset.range â€” ì •ìˆ˜ ë²”ìœ„ Dataset\n",
    "# ë””ë²„ê¹…ì´ë‚˜ ì¸ë±ìŠ¤ ê¸°ë°˜ íŒŒì´í”„ë¼ì¸ì—ì„œ í¸ë¦¬í•˜ë‹¤\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "range_ds = tf.data.Dataset.range(5)\n",
    "print('range Dataset:', list(range_ds.as_numpy_iterator()))\n",
    "\n",
    "# ë°©ë²• 4: from_tensors â€” ì „ì²´ë¥¼ ë‹¨ì¼ í…ì„œë¡œ ê°ì‹¸ê¸° (from_tensor_slicesì™€ ì°¨ì´)\n",
    "# from_tensors: Datasetì— ì›ì†Œê°€ 1ê°œ (ë°°ì—´ ì „ì²´)\n",
    "# from_tensor_slices: Datasetì— ì›ì†Œê°€ Nê°œ (ê° í–‰)\n",
    "ds_single = tf.data.Dataset.from_tensors([1, 2, 3])\n",
    "ds_sliced = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "\n",
    "print('from_tensors ì›ì†Œ ìˆ˜:', sum(1 for _ in ds_single))    # 1\n",
    "print('from_tensor_slices ì›ì†Œ ìˆ˜:', sum(1 for _ in ds_sliced))  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë³€í™˜ ì—°ì‚° â€” map, filter, batch, shuffle, prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .map() â€” ê° ì›ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤\n",
    "# num_parallel_calls=AUTOTUNEìœ¼ë¡œ CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ ìë™ ë³‘ë ¬í™”\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess(image, label):\n",
    "    \"\"\"Min-Max ì •ê·œí™”: [0,255] â†’ [0.0, 1.0]\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # ì±„ë„ ì°¨ì› ì¶”ê°€: (28,28) â†’ (28,28,1)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return image, label\n",
    "\n",
    "mapped_ds = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "for img, lbl in mapped_ds.take(1):\n",
    "    print(f'.map() í›„ â€” shape: {img.shape}, min: {img.numpy().min():.3f}, max: {img.numpy().max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .filter() â€” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì›ì†Œë§Œ ë‚¨ê¸´ë‹¤\n",
    "# ì˜ˆ: ë ˆì´ë¸”ì´ 0~4ì¸ í´ë˜ìŠ¤ë§Œ ì„ íƒ (MNIST ì ˆë°˜)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "filtered_ds = mapped_ds.filter(lambda img, lbl: lbl < 5)\n",
    "\n",
    "# í•„í„° ì „í›„ ì›ì†Œ ìˆ˜ ë¹„êµ\n",
    "count_before = sum(1 for _ in mapped_ds)\n",
    "count_after  = sum(1 for _ in filtered_ds)\n",
    "print(f'.filter() ì „: {count_before}ê°œ, í›„: {count_after}ê°œ')\n",
    "print(f'ë¹„ìœ¨: {count_after/count_before:.1%}  (í´ë˜ìŠ¤ 0-4ì´ë¯€ë¡œ ì•½ 50%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .shuffle() â€” ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ëŠ”ë‹¤\n",
    "# buffer_size: ì„ì‹œë¡œ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ ì…”í”Œí•  ì›ì†Œ ìˆ˜\n",
    "#   - ë„ˆë¬´ ì‘ìœ¼ë©´ ì…”í”Œ íš¨ê³¼ê°€ ì•½í•¨\n",
    "#   - ë°ì´í„°ì…‹ ì „ì²´ í¬ê¸°ì™€ ê°™ìœ¼ë©´ ì™„ì „ ì…”í”Œ (ë©”ëª¨ë¦¬ ì£¼ì˜)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "BUFFER_SIZE = 10_000  # ë©”ëª¨ë¦¬ì™€ ì…”í”Œ íš¨ê³¼ì˜ ê· í˜•ì \n",
    "SEED = 42\n",
    "\n",
    "shuffled_ds = mapped_ds.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "\n",
    "# ì…”í”Œ íš¨ê³¼ í™•ì¸: ì²« 10ê°œ ë ˆì´ë¸” ì¶œë ¥\n",
    "labels_before = [lbl.numpy() for _, lbl in mapped_ds.take(10)]\n",
    "labels_after  = [lbl.numpy() for _, lbl in shuffled_ds.take(10)]\n",
    "print(f'ì…”í”Œ ì „ ì²« 10 ë ˆì´ë¸”: {labels_before}')\n",
    "print(f'ì…”í”Œ í›„ ì²« 10 ë ˆì´ë¸”: {labels_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .batch() â€” ì›ì†Œë¥¼ Nê°œì”© ë¬¶ì–´ ë°°ì¹˜ë¥¼ ë§Œë“ ë‹¤\n",
    "# drop_remainder=True: ë§ˆì§€ë§‰ ë¶ˆì™„ì „ ë°°ì¹˜ ì œê±° (ëª¨ë¸ ì…ë ¥ shape ê³ ì • ì‹œ ìœ ìš©)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "batched_ds = shuffled_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for imgs, lbls in batched_ds.take(1):\n",
    "    print(f'.batch() í›„ â€” ì´ë¯¸ì§€ shape: {imgs.shape}, ë ˆì´ë¸” shape: {lbls.shape}')\n",
    "    # (32, 28, 28, 1), (32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .prefetch() â€” í˜„ì¬ ë°°ì¹˜ë¥¼ GPUë¡œ ë³´ë‚´ëŠ” ë™ì•ˆ\n",
    "#               CPUëŠ” ë¯¸ë¦¬ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ì¤€ë¹„í•œë‹¤ (ë¹„ë™ê¸° ì²˜ë¦¬)\n",
    "# AUTOTUNE: ì‹œìŠ¤í…œ í™˜ê²½ì— ë§ê²Œ í”„ë¦¬íŒ¨ì¹˜ ë²„í¼ í¬ê¸° ìë™ ê²°ì •\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "prefetched_ds = batched_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print('ìµœì¢… íŒŒì´í”„ë¼ì¸ spec:')\n",
    "print(prefetched_ds.element_spec)\n",
    "\n",
    "# ë°°ì¹˜ ìˆ˜ í™•ì¸\n",
    "num_batches = sum(1 for _ in prefetched_ds)\n",
    "print(f'ì´ ë°°ì¹˜ ìˆ˜: {num_batches}  (60000 // 32 = {60000 // 32})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. íŒŒì´í”„ë¼ì¸ ìµœì í™”\n",
    "\n",
    "### ì˜¬ë°”ë¥¸ íŒŒì´í”„ë¼ì¸ ìˆœì„œ\n",
    "```\n",
    "dataset\n",
    "  .cache()        # ë””ìŠ¤í¬/ë©”ëª¨ë¦¬ ìºì‹œ (shuffle ì „)\n",
    "  .shuffle()      # ì…”í”Œ\n",
    "  .batch()        # ë°°ì¹˜\n",
    "  .map()          # ë³€í™˜ (ë°°ì¹˜ ë‹¨ìœ„)\n",
    "  .prefetch(AUTOTUNE)  # ë¹„ë™ê¸° í”„ë¦¬íŒ¨ì¹˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ â€” ê¶Œì¥ ìˆœì„œë¡œ ì¡°ë¦½\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def build_pipeline(x, y, batch_size=32, buffer_size=10_000, training=True):\n",
    "    \"\"\"\n",
    "    x: numpy ì´ë¯¸ì§€ ë°°ì—´\n",
    "    y: numpy ë ˆì´ë¸” ë°°ì—´\n",
    "    training: Trueì´ë©´ ì…”í”Œ í¬í•¨\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(preprocess, num_parallel_calls=AUTOTUNE)  # ê°œë³„ ì „ì²˜ë¦¬\n",
    "    ds = ds.cache()                                        # ë©”ëª¨ë¦¬ ìºì‹œ\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=buffer_size, seed=42)  # ì…”í”Œ (í›ˆë ¨ ì‹œ)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)     # ë°°ì¹˜\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)                 # ë¹„ë™ê¸° í”„ë¦¬íŒ¨ì¹˜\n",
    "    return ds\n",
    "\n",
    "train_ds = build_pipeline(x_train, y_train, training=True)\n",
    "test_ds  = build_pipeline(x_test,  y_test,  training=False)\n",
    "\n",
    "print('í›ˆë ¨ pipeline spec:', train_ds.element_spec)\n",
    "print('í…ŒìŠ¤íŠ¸ pipeline spec:', test_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# prefetch ìœ ë¬´ì— ë”°ë¥¸ ë°°ì¹˜ ë¡œë“œ ì‹œê°„ ë¹„êµ\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    \"\"\"datasetì„ num_epochs ìˆœíšŒí•˜ëŠ” ì‹œê°„ì„ ì¸¡ì •í•œë‹¤.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in dataset:\n",
    "            pass  # ì‹¤ì œ í•™ìŠµ ëŒ€ì‹  ë¡œë“œë§Œ ì¸¡ì •\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return elapsed\n",
    "\n",
    "# prefetch ì—†ëŠ” íŒŒì´í”„ë¼ì¸\n",
    "ds_no_prefetch = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(10_000)\n",
    "    .batch(32)\n",
    ")\n",
    "\n",
    "# prefetch ìˆëŠ” íŒŒì´í”„ë¼ì¸\n",
    "ds_with_prefetch = ds_no_prefetch.prefetch(AUTOTUNE)\n",
    "\n",
    "t_no  = benchmark(ds_no_prefetch)\n",
    "t_yes = benchmark(ds_with_prefetch)\n",
    "\n",
    "print(f'prefetch ì—†ìŒ: {t_no:.3f}ì´ˆ')\n",
    "print(f'prefetch ìˆìŒ: {t_yes:.3f}ì´ˆ')\n",
    "print(f'ì†ë„ í–¥ìƒ: {t_no/t_yes:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# tf.keras.utils.image_dataset_from_directory\n",
    "# ë””ë ‰í† ë¦¬ êµ¬ì¡°:\n",
    "#   data/\n",
    "#     cats/  â† í´ë˜ìŠ¤ëª…ì´ ë ˆì´ë¸”ì´ ëœë‹¤\n",
    "#       001.jpg\n",
    "#     dogs/\n",
    "#       001.jpg\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# ì‹¤ìŠµìš© ë”ë¯¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±\n",
    "dummy_dir = pathlib.Path('/tmp/sample_images')\n",
    "for cls in ['cats', 'dogs']:\n",
    "    (dummy_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ë”ë¯¸ ì´ë¯¸ì§€(ë…¸ì´ì¦ˆ) ì €ì¥\n",
    "import cv2 as _cv2_check\n",
    "try:\n",
    "    import cv2\n",
    "    for cls_idx, cls in enumerate(['cats', 'dogs']):\n",
    "        for i in range(5):\n",
    "            img = np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8)\n",
    "            cv2.imwrite(str(dummy_dir / cls / f'{i:03d}.jpg'), img)\n",
    "    print('OpenCVë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ')\n",
    "except ImportError:\n",
    "    # OpenCVê°€ ì—†ìœ¼ë©´ matplotlibìœ¼ë¡œ ì €ì¥\n",
    "    for cls_idx, cls in enumerate(['cats', 'dogs']):\n",
    "        for i in range(5):\n",
    "            img = np.random.rand(64, 64, 3)\n",
    "            plt.imsave(str(dummy_dir / cls / f'{i:03d}.jpg'), img)\n",
    "    print('matplotlibìœ¼ë¡œ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ')\n",
    "\n",
    "# image_dataset_from_directory ì‚¬ìš©\n",
    "image_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(dummy_dir),\n",
    "    labels='inferred',         # ë””ë ‰í† ë¦¬ëª…ì„ ë ˆì´ë¸”ë¡œ ìë™ ì¶”ë¡ \n",
    "    label_mode='int',          # ì •ìˆ˜ ë ˆì´ë¸” (binary: 0/1, categorical: one-hot)\n",
    "    image_size=(64, 64),       # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ í¬ê¸°\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print('í´ë˜ìŠ¤ ì´ë¦„:', image_ds.class_names)\n",
    "print('Dataset spec:', image_ds.element_spec)\n",
    "\n",
    "# ë°°ì¹˜ ì‹œê°í™”\n",
    "for imgs, labels in image_ds.take(1):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    class_names = image_ds.class_names\n",
    "    for ax, img, lbl in zip(axes, imgs, labels):\n",
    "        ax.imshow(img.numpy().astype('uint8'))\n",
    "        ax.set_title(class_names[lbl.numpy()])\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('image_dataset_from_directory ë¡œë“œ ê²°ê³¼')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "| ì—°ì‚° | ì—­í•  |\n",
    "|------|------|\n",
    "| `.cache()` | ì²« ì—í¬í¬ í›„ ë°ì´í„° ë©”ëª¨ë¦¬ ì €ì¥ |\n",
    "| `.shuffle(buffer)` | buffer í¬ê¸°ë§Œí¼ ëœë¤ ì…”í”Œ |\n",
    "| `.batch(N)` | Nê°œì”© ë¬¶ì–´ ë°°ì¹˜ ìƒì„± |\n",
    "| `.map(fn)` | ê° ìƒ˜í”Œì— í•¨ìˆ˜ ì ìš© (ë³‘ë ¬ ê°€ëŠ¥) |\n",
    "| `.prefetch(AUTOTUNE)` | ëª¨ë¸ í•™ìŠµ ì¤‘ ë‹¤ìŒ ë°°ì¹˜ ë¯¸ë¦¬ ì¤€ë¹„ |\n",
    "\n",
    "**ë‹¤ìŒ**: 02_data_augmentation.ipynb â€” ë°ì´í„° ì¦ê°•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}