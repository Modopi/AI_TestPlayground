{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 04-01: tf.data API — 효율적인 데이터 파이프라인\n",
    "\n",
    "## 학습 목표\n",
    "- tf.data.Dataset의 다양한 생성 방법을 이해한다\n",
    "- map/filter/batch/shuffle/prefetch의 역할과 올바른 순서를 안다\n",
    "- AUTOTUNE으로 자동 최적화한다\n",
    "\n",
    "## 목차\n",
    "1. Dataset 생성\n",
    "2. 변환 연산\n",
    "3. 파이프라인 최적화\n",
    "4. 이미지 디렉토리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 한글 폰트 설정 (macOS)\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print('TensorFlow 버전:', tf.__version__)\n",
    "print('NumPy 버전:', np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학적 기초\n",
    "\n",
    "**Min-Max 정규화**\n",
    "$$x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}$$\n",
    "\n",
    "**Z-score 표준화**\n",
    "$$x' = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "파이프라인 내 `.map()`에서 이 수식을 적용해 픽셀값 [0,255]을 [0,1]로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 방법 1: from_tensor_slices — numpy 배열 또는 텐서에서 생성\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# MNIST 데이터 로드\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(f'훈련 데이터 shape: {x_train.shape}, 레이블 shape: {y_train.shape}')\n",
    "\n",
    "# numpy 배열을 Dataset으로 변환\n",
    "# 각 슬라이스는 (이미지, 레이블) 쌍\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "print(f'Dataset 원소 spec: {train_dataset.element_spec}')\n",
    "\n",
    "# 첫 번째 배치 확인\n",
    "for img, label in train_dataset.take(1):\n",
    "    print(f'이미지 shape: {img.shape}, dtype: {img.dtype}')\n",
    "    print(f'레이블: {label.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 방법 2: from_generator — 파이썬 제너레이터에서 생성\n",
    "# 메모리에 모든 데이터를 올리지 않고 동적으로 생성할 때 유용\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"간단한 수열 제너레이터: (x, x^2) 쌍을 생성한다.\"\"\"\n",
    "    for i in range(10):\n",
    "        yield (float(i), float(i ** 2))\n",
    "\n",
    "gen_dataset = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32),  # x\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32),  # x^2\n",
    "    )\n",
    ")\n",
    "\n",
    "print('제너레이터 Dataset 원소:')\n",
    "for x, x_sq in gen_dataset:\n",
    "    print(f'  x={x.numpy():.0f}, x²={x_sq.numpy():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 방법 3: tf.data.Dataset.range — 정수 범위 Dataset\n",
    "# 디버깅이나 인덱스 기반 파이프라인에서 편리하다\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "range_ds = tf.data.Dataset.range(5)\n",
    "print('range Dataset:', list(range_ds.as_numpy_iterator()))\n",
    "\n",
    "# 방법 4: from_tensors — 전체를 단일 텐서로 감싸기 (from_tensor_slices와 차이)\n",
    "# from_tensors: Dataset에 원소가 1개 (배열 전체)\n",
    "# from_tensor_slices: Dataset에 원소가 N개 (각 행)\n",
    "ds_single = tf.data.Dataset.from_tensors([1, 2, 3])\n",
    "ds_sliced = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
    "\n",
    "print('from_tensors 원소 수:', sum(1 for _ in ds_single))    # 1\n",
    "print('from_tensor_slices 원소 수:', sum(1 for _ in ds_sliced))  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 변환 연산 — map, filter, batch, shuffle, prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .map() — 각 원소에 함수를 적용한다\n",
    "# num_parallel_calls=AUTOTUNE으로 CPU 코어 수에 맞게 자동 병렬화\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def preprocess(image, label):\n",
    "    \"\"\"Min-Max 정규화: [0,255] → [0.0, 1.0]\"\"\"\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # 채널 차원 추가: (28,28) → (28,28,1)\n",
    "    image = tf.expand_dims(image, axis=-1)\n",
    "    return image, label\n",
    "\n",
    "mapped_ds = train_dataset.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "for img, lbl in mapped_ds.take(1):\n",
    "    print(f'.map() 후 — shape: {img.shape}, min: {img.numpy().min():.3f}, max: {img.numpy().max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .filter() — 조건을 만족하는 원소만 남긴다\n",
    "# 예: 레이블이 0~4인 클래스만 선택 (MNIST 절반)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "filtered_ds = mapped_ds.filter(lambda img, lbl: lbl < 5)\n",
    "\n",
    "# 필터 전후 원소 수 비교\n",
    "count_before = sum(1 for _ in mapped_ds)\n",
    "count_after  = sum(1 for _ in filtered_ds)\n",
    "print(f'.filter() 전: {count_before}개, 후: {count_after}개')\n",
    "print(f'비율: {count_after/count_before:.1%}  (클래스 0-4이므로 약 50%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .shuffle() — 데이터를 무작위로 섞는다\n",
    "# buffer_size: 임시로 메모리에 올려 셔플할 원소 수\n",
    "#   - 너무 작으면 셔플 효과가 약함\n",
    "#   - 데이터셋 전체 크기와 같으면 완전 셔플 (메모리 주의)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "BUFFER_SIZE = 10_000  # 메모리와 셔플 효과의 균형점\n",
    "SEED = 42\n",
    "\n",
    "shuffled_ds = mapped_ds.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "\n",
    "# 셔플 효과 확인: 첫 10개 레이블 출력\n",
    "labels_before = [lbl.numpy() for _, lbl in mapped_ds.take(10)]\n",
    "labels_after  = [lbl.numpy() for _, lbl in shuffled_ds.take(10)]\n",
    "print(f'셔플 전 첫 10 레이블: {labels_before}')\n",
    "print(f'셔플 후 첫 10 레이블: {labels_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .batch() — 원소를 N개씩 묶어 배치를 만든다\n",
    "# drop_remainder=True: 마지막 불완전 배치 제거 (모델 입력 shape 고정 시 유용)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "batched_ds = shuffled_ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for imgs, lbls in batched_ds.take(1):\n",
    "    print(f'.batch() 후 — 이미지 shape: {imgs.shape}, 레이블 shape: {lbls.shape}')\n",
    "    # (32, 28, 28, 1), (32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# .prefetch() — 현재 배치를 GPU로 보내는 동안\n",
    "#               CPU는 미리 다음 배치를 준비한다 (비동기 처리)\n",
    "# AUTOTUNE: 시스템 환경에 맞게 프리패치 버퍼 크기 자동 결정\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "prefetched_ds = batched_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print('최종 파이프라인 spec:')\n",
    "print(prefetched_ds.element_spec)\n",
    "\n",
    "# 배치 수 확인\n",
    "num_batches = sum(1 for _ in prefetched_ds)\n",
    "print(f'총 배치 수: {num_batches}  (60000 // 32 = {60000 // 32})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 파이프라인 최적화\n",
    "\n",
    "### 올바른 파이프라인 순서\n",
    "```\n",
    "dataset\n",
    "  .cache()        # 디스크/메모리 캐시 (shuffle 전)\n",
    "  .shuffle()      # 셔플\n",
    "  .batch()        # 배치\n",
    "  .map()          # 변환 (배치 단위)\n",
    "  .prefetch(AUTOTUNE)  # 비동기 프리패치\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 최적화된 파이프라인 — 권장 순서로 조립\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def build_pipeline(x, y, batch_size=32, buffer_size=10_000, training=True):\n",
    "    \"\"\"\n",
    "    x: numpy 이미지 배열\n",
    "    y: numpy 레이블 배열\n",
    "    training: True이면 셔플 포함\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(preprocess, num_parallel_calls=AUTOTUNE)  # 개별 전처리\n",
    "    ds = ds.cache()                                        # 메모리 캐시\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=buffer_size, seed=42)  # 셔플 (훈련 시)\n",
    "    ds = ds.batch(batch_size, drop_remainder=training)     # 배치\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)                 # 비동기 프리패치\n",
    "    return ds\n",
    "\n",
    "train_ds = build_pipeline(x_train, y_train, training=True)\n",
    "test_ds  = build_pipeline(x_test,  y_test,  training=False)\n",
    "\n",
    "print('훈련 pipeline spec:', train_ds.element_spec)\n",
    "print('테스트 pipeline spec:', test_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# prefetch 유무에 따른 배치 로드 시간 비교\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    \"\"\"dataset을 num_epochs 순회하는 시간을 측정한다.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in dataset:\n",
    "            pass  # 실제 학습 대신 로드만 측정\n",
    "    elapsed = time.perf_counter() - start\n",
    "    return elapsed\n",
    "\n",
    "# prefetch 없는 파이프라인\n",
    "ds_no_prefetch = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "    .cache()\n",
    "    .shuffle(10_000)\n",
    "    .batch(32)\n",
    ")\n",
    "\n",
    "# prefetch 있는 파이프라인\n",
    "ds_with_prefetch = ds_no_prefetch.prefetch(AUTOTUNE)\n",
    "\n",
    "t_no  = benchmark(ds_no_prefetch)\n",
    "t_yes = benchmark(ds_with_prefetch)\n",
    "\n",
    "print(f'prefetch 없음: {t_no:.3f}초')\n",
    "print(f'prefetch 있음: {t_yes:.3f}초')\n",
    "print(f'속도 향상: {t_no/t_yes:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이미지 디렉토리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# tf.keras.utils.image_dataset_from_directory\n",
    "# 디렉토리 구조:\n",
    "#   data/\n",
    "#     cats/  ← 클래스명이 레이블이 된다\n",
    "#       001.jpg\n",
    "#     dogs/\n",
    "#       001.jpg\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# 실습용 더미 디렉토리 구조 생성\n",
    "dummy_dir = pathlib.Path('/tmp/sample_images')\n",
    "for cls in ['cats', 'dogs']:\n",
    "    (dummy_dir / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 더미 이미지(노이즈) 저장\n",
    "import cv2 as _cv2_check\n",
    "try:\n",
    "    import cv2\n",
    "    for cls_idx, cls in enumerate(['cats', 'dogs']):\n",
    "        for i in range(5):\n",
    "            img = np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8)\n",
    "            cv2.imwrite(str(dummy_dir / cls / f'{i:03d}.jpg'), img)\n",
    "    print('OpenCV로 더미 이미지 생성 완료')\n",
    "except ImportError:\n",
    "    # OpenCV가 없으면 matplotlib으로 저장\n",
    "    for cls_idx, cls in enumerate(['cats', 'dogs']):\n",
    "        for i in range(5):\n",
    "            img = np.random.rand(64, 64, 3)\n",
    "            plt.imsave(str(dummy_dir / cls / f'{i:03d}.jpg'), img)\n",
    "    print('matplotlib으로 더미 이미지 생성 완료')\n",
    "\n",
    "# image_dataset_from_directory 사용\n",
    "image_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(dummy_dir),\n",
    "    labels='inferred',         # 디렉토리명을 레이블로 자동 추론\n",
    "    label_mode='int',          # 정수 레이블 (binary: 0/1, categorical: one-hot)\n",
    "    image_size=(64, 64),       # 이미지 리사이즈 크기\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print('클래스 이름:', image_ds.class_names)\n",
    "print('Dataset spec:', image_ds.element_spec)\n",
    "\n",
    "# 배치 시각화\n",
    "for imgs, labels in image_ds.take(1):\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "    class_names = image_ds.class_names\n",
    "    for ax, img, lbl in zip(axes, imgs, labels):\n",
    "        ax.imshow(img.numpy().astype('uint8'))\n",
    "        ax.set_title(class_names[lbl.numpy()])\n",
    "        ax.axis('off')\n",
    "    plt.suptitle('image_dataset_from_directory 로드 결과')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "| 연산 | 역할 |\n",
    "|------|------|\n",
    "| `.cache()` | 첫 에포크 후 데이터 메모리 저장 |\n",
    "| `.shuffle(buffer)` | buffer 크기만큼 랜덤 셔플 |\n",
    "| `.batch(N)` | N개씩 묶어 배치 생성 |\n",
    "| `.map(fn)` | 각 샘플에 함수 적용 (병렬 가능) |\n",
    "| `.prefetch(AUTOTUNE)` | 모델 학습 중 다음 배치 미리 준비 |\n",
    "\n",
    "**다음**: 02_data_augmentation.ipynb — 데이터 증강"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
