{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02-04: 레이어와 활성화 함수\n",
    "\n",
    "## 학습 목표\n",
    "- 주요 활성화 함수의 수식과 특성을 이해한다\n",
    "- Dense, Dropout, BatchNormalization 레이어를 올바르게 사용한다\n",
    "- 가중치 초기화 방법과 정규화 기법을 적용한다\n",
    "\n",
    "## 목차\n",
    "1. 수학적 기초 — 활성화 함수\n",
    "2. 활성화 함수 시각화\n",
    "3. Dense와 Dropout 사용법\n",
    "4. BatchNormalization vs LayerNormalization\n",
    "5. 가중치 초기화 비교\n",
    "6. L1/L2 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 상위 폴더의 utils 모듈 접근을 위한 경로 추가\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"TensorFlow 버전:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수학적 기초 — 활성화 함수\n",
    "\n",
    "| 함수 | 수식 | 범위 |\n",
    "|------|------|------|\n",
    "| ReLU | $f(x) = \\max(0, x)$ | $[0, +\\infty)$ |\n",
    "| Sigmoid | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | $(0, 1)$ |\n",
    "| Tanh | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$ |\n",
    "| Softmax | $\\sigma_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $(0,1)$, 합=1 |\n",
    "| ELU | $f(x) = x$ if $x>0$ else $\\alpha(e^x-1)$ | $(-\\alpha, +\\infty)$ |\n",
    "\n",
    "**BatchNorm**: $\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 활성화 함수 시각화 ────────────────────────────────────────────────────\n",
    "# utils.plot_helpers 모듈이 있으면 사용, 없으면 직접 구현\n",
    "try:\n",
    "    from utils.plot_helpers import plot_activation_functions\n",
    "    plot_activation_functions()\n",
    "except ImportError:\n",
    "    # utils 모듈이 없는 경우 직접 구현\n",
    "    x = np.linspace(-4, 4, 300)  # -4 ~ 4 범위의 300개 점\n",
    "\n",
    "    # 각 활성화 함수 계산\n",
    "    relu    = np.maximum(0, x)                                      # ReLU\n",
    "    sigmoid = 1 / (1 + np.exp(-x))                                  # Sigmoid\n",
    "    tanh    = np.tanh(x)                                            # Tanh\n",
    "    elu     = np.where(x > 0, x, 1.0 * (np.exp(x) - 1))           # ELU (alpha=1)\n",
    "    leaky   = np.where(x > 0, x, 0.01 * x)                        # Leaky ReLU\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "    configs = [\n",
    "        ('ReLU',       relu,    'steelblue'),\n",
    "        ('Sigmoid',    sigmoid, 'orangered'),\n",
    "        ('Tanh',       tanh,    'green'),\n",
    "        ('ELU',        elu,     'purple'),\n",
    "        ('Leaky ReLU', leaky,   'brown'),\n",
    "    ]\n",
    "\n",
    "    for ax, (name, y_vals, color) in zip(axes, configs):\n",
    "        ax.plot(x, y_vals, color=color, linewidth=2.5)\n",
    "        ax.axhline(0, color='k', linewidth=0.8, linestyle='--')\n",
    "        ax.axvline(0, color='k', linewidth=0.8, linestyle='--')\n",
    "        ax.set_title(name, fontsize=12)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('주요 활성화 함수 비교', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"활성화 함수 시각화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dense와 Dropout 사용법\n",
    "\n",
    "Dropout은 훈련 시(`training=True`)에만 뉴런을 무작위로 비활성화한다.\n",
    "추론 시(`training=False`)에는 모든 뉴런을 사용하되, 출력에 `(1 - drop_rate)`를 곱하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Dropout 훈련/추론 모드 비교 ──────────────────────────────────────────\n",
    "dropout_layer = tf.keras.layers.Dropout(rate=0.5)  # 50% 뉴런을 비활성화\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "x_demo = tf.ones((1, 10))  # 모두 1인 입력 (변화를 쉽게 관찰하기 위함)\n",
    "\n",
    "# 훈련 모드: 일부 값이 0으로 설정되고 나머지는 1/(1-rate)로 스케일됨\n",
    "out_train = dropout_layer(x_demo, training=True)\n",
    "print(\"훈련 모드 출력:\", out_train.numpy())\n",
    "\n",
    "# 추론 모드: 드롭아웃 없이 그대로 통과\n",
    "out_infer = dropout_layer(x_demo, training=False)\n",
    "print(\"추론 모드 출력:\", out_infer.numpy())\n",
    "\n",
    "# ── Dense 레이어 활성화 함수별 출력 분포 ────────────────────────────────\n",
    "print(\"\\n── 활성화 함수별 출력 분포 ──\")\n",
    "x_rand = tf.random.normal((1000, 64))  # 정규분포 입력\n",
    "for act_name in ['relu', 'sigmoid', 'tanh', 'elu']:\n",
    "    dense = tf.keras.layers.Dense(32, activation=act_name)\n",
    "    out   = dense(x_rand)\n",
    "    print(f\"{act_name:10s}: min={out.numpy().min():.3f}, \"\n",
    "          f\"max={out.numpy().max():.3f}, \"\n",
    "          f\"mean={out.numpy().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BatchNormalization vs LayerNormalization\n",
    "\n",
    "| 항목 | BatchNorm | LayerNorm |\n",
    "|------|-----------|----------|\n",
    "| 정규화 축 | 배치 방향 (샘플 간) | 특징 방향 (채널/차원) |\n",
    "| 배치 크기 의존 | 있음 (작으면 불안정) | 없음 |\n",
    "| 주 사용처 | CNN, 대형 배치 | RNN, Transformer |\n",
    "| 추론 시 | 이동 평균 사용 | 동일한 방식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── BatchNormalization vs LayerNormalization 비교 ─────────────────────────\n",
    "\n",
    "# 정규화를 포함한 모델 구성 비교\n",
    "def build_bn_model():\n",
    "    \"\"\"BatchNormalization을 사용한 모델\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128),                          # 활성화 함수 없이 Dense\n",
    "        tf.keras.layers.BatchNormalization(),                # 배치 정규화 후 활성화\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='bn_model')\n",
    "\n",
    "def build_ln_model():\n",
    "    \"\"\"LayerNormalization을 사용한 모델\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.LayerNormalization(),                # 레이어 정규화\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='ln_model')\n",
    "\n",
    "bn_model = build_bn_model()\n",
    "ln_model = build_ln_model()\n",
    "\n",
    "print(\"BatchNorm 모델 파라미터:\", bn_model.count_params())\n",
    "print(\"LayerNorm 모델 파라미터:\", ln_model.count_params())\n",
    "\n",
    "# BatchNorm 레이어의 내부 파라미터 확인\n",
    "bn_layer = tf.keras.layers.BatchNormalization()\n",
    "_ = bn_layer(tf.zeros((4, 32)))  # 빌드를 위해 더미 데이터 통과\n",
    "print(\"\\nBatchNorm 파라미터 목록:\")\n",
    "for w in bn_layer.weights:\n",
    "    print(f\"  {w.name:40s} shape={w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 가중치 초기화 비교\n",
    "\n",
    "| 초기화 방법 | 수식 | 권장 활성화 함수 |\n",
    "|------------|------|----------------|\n",
    "| Glorot (Xavier) | $\\text{Var}(W) = \\frac{2}{n_{in} + n_{out}}$ | Sigmoid, Tanh |\n",
    "| He (Kaiming) | $\\text{Var}(W) = \\frac{2}{n_{in}}$ | ReLU, ELU |\n",
    "| LeCun | $\\text{Var}(W) = \\frac{1}{n_{in}}$ | SELU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 초기화 방법별 가중치 분포 시각화 ─────────────────────────────────────\n",
    "initializers = {\n",
    "    'glorot_uniform': tf.keras.initializers.GlorotUniform(seed=42),  # Xavier 균등분포\n",
    "    'he_normal':      tf.keras.initializers.HeNormal(seed=42),       # Kaiming 정규분포\n",
    "    'lecun_normal':   tf.keras.initializers.LecunNormal(seed=42),    # LeCun 정규분포\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "shape = (784, 128)  # Dense(784→128)의 가중치 shape\n",
    "\n",
    "for ax, (name, init) in zip(axes, initializers.items()):\n",
    "    weights = init(shape=shape).numpy().flatten()  # 초기 가중치 생성 후 1D로 펼침\n",
    "    ax.hist(weights, bins=60, color='steelblue', alpha=0.8, edgecolor='white')\n",
    "    ax.set_title(f'{name}\\nstd={weights.std():.4f}', fontsize=11)\n",
    "    ax.set_xlabel('가중치 값')\n",
    "    ax.set_ylabel('빈도')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('가중치 초기화 방법별 분포 비교', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 초기화 방법을 Dense 레이어에 적용하는 예시\n",
    "layer_glorot = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_initializer='glorot_uniform')  # 기본값\n",
    "layer_he     = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_initializer='he_normal')        # ReLU 계열 권장\n",
    "print(\"He Normal 초기화 레이어 생성 완료 (ReLU와 함께 권장)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. L1/L2 정규화\n",
    "\n",
    "정규화는 손실함수에 패널티 항을 추가하여 가중치가 지나치게 커지는 것을 방지한다.\n",
    "\n",
    "- **L1 정규화**: $\\mathcal{L}_{reg} = \\lambda \\sum |w_i|$ → 희소 가중치 유도\n",
    "- **L2 정규화**: $\\mathcal{L}_{reg} = \\lambda \\sum w_i^2$ → 가중치 축소 (Weight Decay)\n",
    "- **L1+L2**: 두 가지를 동시에 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── kernel_regularizer로 L1/L2 정규화 적용 ──────────────────────────────\n",
    "\n",
    "lambda_val = 1e-4  # 정규화 강도 (클수록 강한 규제)\n",
    "\n",
    "# 정규화 방법별 레이어 생성\n",
    "layer_l2 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L2(lambda_val),   # L2 정규화\n",
    "    name='l2_dense'\n",
    ")\n",
    "\n",
    "layer_l1 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L1(lambda_val),   # L1 정규화\n",
    "    name='l1_dense'\n",
    ")\n",
    "\n",
    "layer_l1l2 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L1L2(l1=lambda_val, l2=lambda_val),  # L1+L2\n",
    "    name='l1l2_dense'\n",
    ")\n",
    "\n",
    "# 정규화 포함 모델 예시\n",
    "reg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(\n",
    "        128, activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(1e-4),   # 은닉층에 L2 적용\n",
    "        kernel_initializer='he_normal'                         # ReLU에 맞는 초기화\n",
    "    ),\n",
    "    tf.keras.layers.BatchNormalization(),                      # 배치 정규화\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "], name='regularized_model')\n",
    "\n",
    "reg_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "reg_model.summary()\n",
    "print(\"\\n정규화 모델 컴파일 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리\n",
    "\n",
    "| 기법 | 목적 | 적용 위치 |\n",
    "|------|------|----------|\n",
    "| Dropout | 과적합 방지 (랜덤 비활성화) | 은닉층 이후 |\n",
    "| BatchNorm | 내부 공변량 변화 감소 | Dense/Conv 이후, 활성화 이전 |\n",
    "| LayerNorm | 배치 크기 독립 정규화 | RNN, Transformer 내부 |\n",
    "| L2 정규화 | 가중치 크기 제한 | kernel_regularizer |\n",
    "| He 초기화 | ReLU 뉴런 소멸 방지 | ReLU 계열 레이어 |\n",
    "| Glorot 초기화 | 신호 분산 유지 | Sigmoid/Tanh 레이어 |\n",
    "\n",
    "**다음**: practice/ex01_build_your_first_model.ipynb — 3가지 API 통합 실습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
