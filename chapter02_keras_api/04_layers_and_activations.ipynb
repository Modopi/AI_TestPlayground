{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02-04: ë ˆì´ì–´ì™€ í™œì„±í™” í•¨ìˆ˜\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ì˜ ìˆ˜ì‹ê³¼ íŠ¹ì„±ì„ ì´í•´í•œë‹¤\n",
    "- Dense, Dropout, BatchNormalization ë ˆì´ì–´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•œë‹¤\n",
    "- ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë°©ë²•ê³¼ ì •ê·œí™” ê¸°ë²•ì„ ì ìš©í•œë‹¤\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. ìˆ˜í•™ì  ê¸°ì´ˆ â€” í™œì„±í™” í•¨ìˆ˜\n",
    "2. í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™”\n",
    "3. Denseì™€ Dropout ì‚¬ìš©ë²•\n",
    "4. BatchNormalization vs LayerNormalization\n",
    "5. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¹„êµ\n",
    "6. L1/L2 ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ğŸ£ ì´ˆë“±í•™ìƒì„ ìœ„í•œ í™œì„±í™” í•¨ìˆ˜ ì¹œì ˆ ì„¤ëª…!\n",
    "\n",
    "#### â“ ì™œ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•´ìš”?\n",
    "\n",
    "í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´, ì¸µì„ ì•„ë¬´ë¦¬ ë§ì´ ìŒ“ì•„ë„\n",
    "ê²°êµ­ **ë‹¨ìˆœí•œ ì§ì„ (1ì°¨ ë°©ì •ì‹)** ë°–ì— í‘œí˜„ ëª»í•´ìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ì•„ë¬´ë¦¬ ì§ì„ ì„ ë§ì´ ì´ì–´ ë¶™ì—¬ë„ ì§ì„ ì´ì—ìš”.\n",
    "> í™œì„±í™” í•¨ìˆ˜ëŠ” 'êµ¬ë¶€ëŸ¬ì§'ì„ ì¶”ê°€í•´ì„œ ë³µì¡í•œ í˜•íƒœë¥¼ ë§Œë“¤ì–´ì¤˜ìš”!\n",
    "\n",
    "#### ğŸ¯ ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ â€” ì–¸ì œ ë¬´ì—‡ì„ ì“°ë‚˜ìš”?\n",
    "\n",
    "| í•¨ìˆ˜ | í•µì‹¬ íŠ¹ì„± | ì–¸ì œ ì¨ìš”? | ë¹„ìœ  |\n",
    "|------|-----------|-----------|------|\n",
    "| **ReLU** | ìŒìˆ˜â†’0, ì–‘ìˆ˜â†’ê·¸ëŒ€ë¡œ | ì€ë‹‰ì¸µ ê¸°ë³¸ ì„ íƒ | ì „ë“± ìŠ¤ìœ„ì¹˜: ì–´ë‘ìš°ë©´ ë” ğŸ’¡ |\n",
    "| **Sigmoid** | 0~1 ì‚¬ì´ ì¶œë ¥ | ì´ì§„ ë¶„ë¥˜ ì¶œë ¥ì¸µ | í™•ë¥  ê³„ì‚°ê¸° ğŸ² |\n",
    "| **Softmax** | í•©ì´ 1ì¸ í™•ë¥  ë¶„í¬ | ë‹¤ì¤‘ ë¶„ë¥˜ ì¶œë ¥ì¸µ | ì—¬ëŸ¬ ì„ íƒì§€ íˆ¬í‘œ ğŸ—³ï¸ |\n",
    "| **Tanh** | -1~1 ì‚¬ì´ ì¶œë ¥ | RNN ë‚´ë¶€ | ì¤‘ì‹¬ì´ 0ì¸ Sigmoid ğŸ“Š |\n",
    "| **ELU** | ìŒìˆ˜ë„ ì™„ì „íˆ 0 ì•„ë‹˜ | ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ | ReLUì˜ ê°œì„ íŒ ğŸ”§ |\n",
    "\n",
    "#### ğŸ›‘ ReLUì˜ **ì£½ì€ ë‰´ëŸ°(Dying ReLU)** ë¬¸ì œ\n",
    "\n",
    "ReLUëŠ” ì…ë ¥ì´ ìŒìˆ˜ë©´ í•­ìƒ 0ì„ ì¶œë ¥í•´ìš”.\n",
    "í•™ìŠµ ì¤‘ì— ì–´ë–¤ ë‰´ëŸ°ì´ **í•­ìƒ ìŒìˆ˜ ì…ë ¥ë§Œ ë°›ìœ¼ë©´** ì˜ì›íˆ 0!\n",
    "ê·¸ ë‰´ëŸ°ì€ 'ì£½ì€ ê²ƒ'ì²˜ëŸ¼ í•™ìŠµì— ê¸°ì—¬ë¥¼ ëª»í•´ìš”.\n",
    "\n",
    "> **í•´ê²°ì±…**: Leaky ReLU (ìŒìˆ˜ì—ë„ ì•„ì£¼ ì‘ì€ ê¸°ìš¸ê¸° ìœ ì§€)\n",
    "> ë˜ëŠ” ELU (ìŒìˆ˜ì—ì„œ ë¶€ë“œëŸ½ê²Œ ê°ì†Œ)\n",
    "\n",
    "#### ğŸ§¹ Dropoutì´ ë­ì˜ˆìš”?\n",
    "\n",
    "í•™ìŠµ ì¤‘ì— ì¼ë¶€ ë‰´ëŸ°ì„ **ë¬´ì‘ìœ„ë¡œ ë„ëŠ”** ê¸°ë²•ì´ì—ìš”!\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: íŒ€í”Œì—ì„œ ì¼ë¶€ëŸ¬ ëª‡ ëª…ì„ ëºë‹¤ê°€ ë„£ì—ˆë‹¤ê°€!\n",
    "> íŠ¹ì • ì‚¬ëŒì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³  íŒ€ ì „ì²´ê°€ ê³¨ê³ ë£¨ ì—­í• ì„ ìµíˆê²Œ í•´ìš”.\n",
    "> â†’ ê³¼ì í•©(ì•”ê¸°ë§Œ í•˜ëŠ” í˜„ìƒ) ë°©ì§€!\n",
    "\n",
    "#### âš–ï¸ BatchNormì´ ë­ì˜ˆìš”?\n",
    "\n",
    "ì¸µì„ í†µê³¼í•  ë•Œë§ˆë‹¤ ê°’ì˜ ë¶„í¬ê°€ í¬ê²Œ ë‹¬ë¼ì§€ëŠ” ë¬¸ì œ(ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”)ë¥¼ í•´ê²°í•´ìš”.\n",
    "\n",
    "> ğŸ’¡ **ë¹„ìœ **: ê° ìˆ˜ì—… í›„ 'ì ìˆ˜ë¥¼ í‘œì¤€í™”'í•´ì„œ í•­ìƒ ë¹„ìŠ·í•œ ë¶„í¬ë¡œ ë§Œë“¤ê¸°!\n",
    "> â†’ í•™ìŠµì´ ë” ì•ˆì •ì ì´ê³  ë¹¨ë¼ì ¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # ìƒìœ„ í´ë”ì˜ utils ëª¨ë“ˆ ì ‘ê·¼ì„ ìœ„í•œ ê²½ë¡œ ì¶”ê°€\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"TensorFlow ë²„ì „:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜í•™ì  ê¸°ì´ˆ â€” í™œì„±í™” í•¨ìˆ˜\n",
    "\n",
    "| í•¨ìˆ˜ | ìˆ˜ì‹ | ë²”ìœ„ |\n",
    "|------|------|------|\n",
    "| ReLU | $f(x) = \\max(0, x)$ | $[0, +\\infty)$ |\n",
    "| Sigmoid | $\\sigma(x) = \\frac{1}{1+e^{-x}}$ | $(0, 1)$ |\n",
    "| Tanh | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $(-1, 1)$ |\n",
    "| Softmax | $\\sigma_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $(0,1)$, í•©=1 |\n",
    "| ELU | $f(x) = x$ if $x>0$ else $\\alpha(e^x-1)$ | $(-\\alpha, +\\infty)$ |\n",
    "\n",
    "**BatchNorm**: $\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# utils.plot_helpers ëª¨ë“ˆì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ êµ¬í˜„\n",
    "try:\n",
    "    from utils.plot_helpers import plot_activation_functions\n",
    "    plot_activation_functions()\n",
    "except ImportError:\n",
    "    # utils ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš° ì§ì ‘ êµ¬í˜„\n",
    "    x = np.linspace(-4, 4, 300)  # -4 ~ 4 ë²”ìœ„ì˜ 300ê°œ ì \n",
    "\n",
    "    # ê° í™œì„±í™” í•¨ìˆ˜ ê³„ì‚°\n",
    "    relu    = np.maximum(0, x)                                      # ReLU\n",
    "    sigmoid = 1 / (1 + np.exp(-x))                                  # Sigmoid\n",
    "    tanh    = np.tanh(x)                                            # Tanh\n",
    "    elu     = np.where(x > 0, x, 1.0 * (np.exp(x) - 1))           # ELU (alpha=1)\n",
    "    leaky   = np.where(x > 0, x, 0.01 * x)                        # Leaky ReLU\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "    configs = [\n",
    "        ('ReLU',       relu,    'steelblue'),\n",
    "        ('Sigmoid',    sigmoid, 'orangered'),\n",
    "        ('Tanh',       tanh,    'green'),\n",
    "        ('ELU',        elu,     'purple'),\n",
    "        ('Leaky ReLU', leaky,   'brown'),\n",
    "    ]\n",
    "\n",
    "    for ax, (name, y_vals, color) in zip(axes, configs):\n",
    "        ax.plot(x, y_vals, color=color, linewidth=2.5)\n",
    "        ax.axhline(0, color='k', linewidth=0.8, linestyle='--')\n",
    "        ax.axvline(0, color='k', linewidth=0.8, linestyle='--')\n",
    "        ax.set_title(name, fontsize=12)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('ì£¼ìš” í™œì„±í™” í•¨ìˆ˜ ë¹„êµ', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"í™œì„±í™” í•¨ìˆ˜ ì‹œê°í™” ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Denseì™€ Dropout ì‚¬ìš©ë²•\n",
    "\n",
    "Dropoutì€ í›ˆë ¨ ì‹œ(`training=True`)ì—ë§Œ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”í•œë‹¤.\n",
    "ì¶”ë¡  ì‹œ(`training=False`)ì—ëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ë˜, ì¶œë ¥ì— `(1 - drop_rate)`ë¥¼ ê³±í•˜ì§€ ì•ŠëŠ”ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Dropout í›ˆë ¨/ì¶”ë¡  ëª¨ë“œ ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dropout_layer = tf.keras.layers.Dropout(rate=0.5)  # 50% ë‰´ëŸ°ì„ ë¹„í™œì„±í™”\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "x_demo = tf.ones((1, 10))  # ëª¨ë‘ 1ì¸ ì…ë ¥ (ë³€í™”ë¥¼ ì‰½ê²Œ ê´€ì°°í•˜ê¸° ìœ„í•¨)\n",
    "\n",
    "# í›ˆë ¨ ëª¨ë“œ: ì¼ë¶€ ê°’ì´ 0ìœ¼ë¡œ ì„¤ì •ë˜ê³  ë‚˜ë¨¸ì§€ëŠ” 1/(1-rate)ë¡œ ìŠ¤ì¼€ì¼ë¨\n",
    "out_train = dropout_layer(x_demo, training=True)\n",
    "print(\"í›ˆë ¨ ëª¨ë“œ ì¶œë ¥:\", out_train.numpy())\n",
    "\n",
    "# ì¶”ë¡  ëª¨ë“œ: ë“œë¡­ì•„ì›ƒ ì—†ì´ ê·¸ëŒ€ë¡œ í†µê³¼\n",
    "out_infer = dropout_layer(x_demo, training=False)\n",
    "print(\"ì¶”ë¡  ëª¨ë“œ ì¶œë ¥:\", out_infer.numpy())\n",
    "\n",
    "# â”€â”€ Dense ë ˆì´ì–´ í™œì„±í™” í•¨ìˆ˜ë³„ ì¶œë ¥ ë¶„í¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nâ”€â”€ í™œì„±í™” í•¨ìˆ˜ë³„ ì¶œë ¥ ë¶„í¬ â”€â”€\")\n",
    "x_rand = tf.random.normal((1000, 64))  # ì •ê·œë¶„í¬ ì…ë ¥\n",
    "for act_name in ['relu', 'sigmoid', 'tanh', 'elu']:\n",
    "    dense = tf.keras.layers.Dense(32, activation=act_name)\n",
    "    out   = dense(x_rand)\n",
    "    print(f\"{act_name:10s}: min={out.numpy().min():.3f}, \"\n",
    "          f\"max={out.numpy().max():.3f}, \"\n",
    "          f\"mean={out.numpy().mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BatchNormalization vs LayerNormalization\n",
    "\n",
    "| í•­ëª© | BatchNorm | LayerNorm |\n",
    "|------|-----------|----------|\n",
    "| ì •ê·œí™” ì¶• | ë°°ì¹˜ ë°©í–¥ (ìƒ˜í”Œ ê°„) | íŠ¹ì§• ë°©í–¥ (ì±„ë„/ì°¨ì›) |\n",
    "| ë°°ì¹˜ í¬ê¸° ì˜ì¡´ | ìˆìŒ (ì‘ìœ¼ë©´ ë¶ˆì•ˆì •) | ì—†ìŒ |\n",
    "| ì£¼ ì‚¬ìš©ì²˜ | CNN, ëŒ€í˜• ë°°ì¹˜ | RNN, Transformer |\n",
    "| ì¶”ë¡  ì‹œ | ì´ë™ í‰ê·  ì‚¬ìš© | ë™ì¼í•œ ë°©ì‹ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ BatchNormalization vs LayerNormalization ë¹„êµ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# ì •ê·œí™”ë¥¼ í¬í•¨í•œ ëª¨ë¸ êµ¬ì„± ë¹„êµ\n",
    "def build_bn_model():\n",
    "    \"\"\"BatchNormalizationì„ ì‚¬ìš©í•œ ëª¨ë¸\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128),                          # í™œì„±í™” í•¨ìˆ˜ ì—†ì´ Dense\n",
    "        tf.keras.layers.BatchNormalization(),                # ë°°ì¹˜ ì •ê·œí™” í›„ í™œì„±í™”\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='bn_model')\n",
    "\n",
    "def build_ln_model():\n",
    "    \"\"\"LayerNormalizationì„ ì‚¬ìš©í•œ ëª¨ë¸\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.LayerNormalization(),                # ë ˆì´ì–´ ì •ê·œí™”\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(64),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Activation('relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ], name='ln_model')\n",
    "\n",
    "bn_model = build_bn_model()\n",
    "ln_model = build_ln_model()\n",
    "\n",
    "print(\"BatchNorm ëª¨ë¸ íŒŒë¼ë¯¸í„°:\", bn_model.count_params())\n",
    "print(\"LayerNorm ëª¨ë¸ íŒŒë¼ë¯¸í„°:\", ln_model.count_params())\n",
    "\n",
    "# BatchNorm ë ˆì´ì–´ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„° í™•ì¸\n",
    "bn_layer = tf.keras.layers.BatchNormalization()\n",
    "_ = bn_layer(tf.zeros((4, 32)))  # ë¹Œë“œë¥¼ ìœ„í•´ ë”ë¯¸ ë°ì´í„° í†µê³¼\n",
    "print(\"\\nBatchNorm íŒŒë¼ë¯¸í„° ëª©ë¡:\")\n",
    "for w in bn_layer.weights:\n",
    "    print(f\"  {w.name:40s} shape={w.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë¹„êµ\n",
    "\n",
    "| ì´ˆê¸°í™” ë°©ë²• | ìˆ˜ì‹ | ê¶Œì¥ í™œì„±í™” í•¨ìˆ˜ |\n",
    "|------------|------|----------------|\n",
    "| Glorot (Xavier) | $\\text{Var}(W) = \\frac{2}{n_{in} + n_{out}}$ | Sigmoid, Tanh |\n",
    "| He (Kaiming) | $\\text{Var}(W) = \\frac{2}{n_{in}}$ | ReLU, ELU |\n",
    "| LeCun | $\\text{Var}(W) = \\frac{1}{n_{in}}$ | SELU |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ ì´ˆê¸°í™” ë°©ë²•ë³„ ê°€ì¤‘ì¹˜ ë¶„í¬ ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "initializers = {\n",
    "    'glorot_uniform': tf.keras.initializers.GlorotUniform(seed=42),  # Xavier ê· ë“±ë¶„í¬\n",
    "    'he_normal':      tf.keras.initializers.HeNormal(seed=42),       # Kaiming ì •ê·œë¶„í¬\n",
    "    'lecun_normal':   tf.keras.initializers.LecunNormal(seed=42),    # LeCun ì •ê·œë¶„í¬\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "shape = (784, 128)  # Dense(784â†’128)ì˜ ê°€ì¤‘ì¹˜ shape\n",
    "\n",
    "for ax, (name, init) in zip(axes, initializers.items()):\n",
    "    weights = init(shape=shape).numpy().flatten()  # ì´ˆê¸° ê°€ì¤‘ì¹˜ ìƒì„± í›„ 1Dë¡œ í¼ì¹¨\n",
    "    ax.hist(weights, bins=60, color='steelblue', alpha=0.8, edgecolor='white')\n",
    "    ax.set_title(f'{name}\\nstd={weights.std():.4f}', fontsize=11)\n",
    "    ax.set_xlabel('ê°€ì¤‘ì¹˜ ê°’')\n",
    "    ax.set_ylabel('ë¹ˆë„')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ë°©ë²•ë³„ ë¶„í¬ ë¹„êµ', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì´ˆê¸°í™” ë°©ë²•ì„ Dense ë ˆì´ì–´ì— ì ìš©í•˜ëŠ” ì˜ˆì‹œ\n",
    "layer_glorot = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_initializer='glorot_uniform')  # ê¸°ë³¸ê°’\n",
    "layer_he     = tf.keras.layers.Dense(128, activation='relu',\n",
    "                                     kernel_initializer='he_normal')        # ReLU ê³„ì—´ ê¶Œì¥\n",
    "print(\"He Normal ì´ˆê¸°í™” ë ˆì´ì–´ ìƒì„± ì™„ë£Œ (ReLUì™€ í•¨ê»˜ ê¶Œì¥)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. L1/L2 ì •ê·œí™”\n",
    "\n",
    "ì •ê·œí™”ëŠ” ì†ì‹¤í•¨ìˆ˜ì— íŒ¨ë„í‹° í•­ì„ ì¶”ê°€í•˜ì—¬ ê°€ì¤‘ì¹˜ê°€ ì§€ë‚˜ì¹˜ê²Œ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤.\n",
    "\n",
    "- **L1 ì •ê·œí™”**: $\\mathcal{L}_{reg} = \\lambda \\sum |w_i|$ â†’ í¬ì†Œ ê°€ì¤‘ì¹˜ ìœ ë„\n",
    "- **L2 ì •ê·œí™”**: $\\mathcal{L}_{reg} = \\lambda \\sum w_i^2$ â†’ ê°€ì¤‘ì¹˜ ì¶•ì†Œ (Weight Decay)\n",
    "- **L1+L2**: ë‘ ê°€ì§€ë¥¼ ë™ì‹œì— ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ kernel_regularizerë¡œ L1/L2 ì •ê·œí™” ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "lambda_val = 1e-4  # ì •ê·œí™” ê°•ë„ (í´ìˆ˜ë¡ ê°•í•œ ê·œì œ)\n",
    "\n",
    "# ì •ê·œí™” ë°©ë²•ë³„ ë ˆì´ì–´ ìƒì„±\n",
    "layer_l2 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L2(lambda_val),   # L2 ì •ê·œí™”\n",
    "    name='l2_dense'\n",
    ")\n",
    "\n",
    "layer_l1 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L1(lambda_val),   # L1 ì •ê·œí™”\n",
    "    name='l1_dense'\n",
    ")\n",
    "\n",
    "layer_l1l2 = tf.keras.layers.Dense(\n",
    "    64, activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.L1L2(l1=lambda_val, l2=lambda_val),  # L1+L2\n",
    "    name='l1l2_dense'\n",
    ")\n",
    "\n",
    "# ì •ê·œí™” í¬í•¨ ëª¨ë¸ ì˜ˆì‹œ\n",
    "reg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(\n",
    "        128, activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.L2(1e-4),   # ì€ë‹‰ì¸µì— L2 ì ìš©\n",
    "        kernel_initializer='he_normal'                         # ReLUì— ë§ëŠ” ì´ˆê¸°í™”\n",
    "    ),\n",
    "    tf.keras.layers.BatchNormalization(),                      # ë°°ì¹˜ ì •ê·œí™”\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "], name='regularized_model')\n",
    "\n",
    "reg_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "reg_model.summary()\n",
    "print(\"\\nì •ê·œí™” ëª¨ë¸ ì»´íŒŒì¼ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì •ë¦¬\n",
    "\n",
    "| ê¸°ë²• | ëª©ì  | ì ìš© ìœ„ì¹˜ |\n",
    "|------|------|----------|\n",
    "| Dropout | ê³¼ì í•© ë°©ì§€ (ëœë¤ ë¹„í™œì„±í™”) | ì€ë‹‰ì¸µ ì´í›„ |\n",
    "| BatchNorm | ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™” ê°ì†Œ | Dense/Conv ì´í›„, í™œì„±í™” ì´ì „ |\n",
    "| LayerNorm | ë°°ì¹˜ í¬ê¸° ë…ë¦½ ì •ê·œí™” | RNN, Transformer ë‚´ë¶€ |\n",
    "| L2 ì •ê·œí™” | ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ | kernel_regularizer |\n",
    "| He ì´ˆê¸°í™” | ReLU ë‰´ëŸ° ì†Œë©¸ ë°©ì§€ | ReLU ê³„ì—´ ë ˆì´ì–´ |\n",
    "| Glorot ì´ˆê¸°í™” | ì‹ í˜¸ ë¶„ì‚° ìœ ì§€ | Sigmoid/Tanh ë ˆì´ì–´ |\n",
    "\n",
    "**ë‹¤ìŒ**: practice/ex01_build_your_first_model.ipynb â€” 3ê°€ì§€ API í†µí•© ì‹¤ìŠµ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}