{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 02-03: Subclassing API\n",
    "\n",
    "## 학습 목표\n",
    "- `tf.keras.Model`을 상속하여 완전 커스텀 모델을 만든다\n",
    "- `tf.keras.layers.Layer`를 상속하여 커스텀 레이어를 만든다\n",
    "- `__init__`, `build`, `call` 메서드의 역할을 이해한다\n",
    "\n",
    "## 목차\n",
    "1. Subclassing API 개요\n",
    "2. tf.keras.Model 상속 예시\n",
    "3. 커스텀 레이어 만들기\n",
    "4. add_weight로 가중치 직접 정의\n",
    "5. Subclassing으로 MNIST 분류기 완성\n",
    "6. 3가지 API 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"TensorFlow 버전:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing API\n",
    "\n",
    "`tf.keras.Model`을 상속하여 완전히 커스텀 모델을 만든다.\n",
    "\n",
    "**사용 시점**: 동적 구조, 연구용 특수 레이어, 조건부 연산이 필요할 때\n",
    "\n",
    "**핵심 메서드**:\n",
    "- `__init__`: 레이어 정의\n",
    "- `call(inputs, training=False)`: 순전파 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. tf.keras.Model 상속 예시\n",
    "\n",
    "`__init__`에서 레이어를 **인스턴스 속성**으로 선언하고,\n",
    "`call`에서 데이터 흐름을 직접 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(tf.keras.Model):\n",
    "    \"\"\"MNIST 분류를 위한 Subclassing 모델\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()  # 부모 클래스 초기화 (필수)\n",
    "\n",
    "        # 레이어를 인스턴스 속성으로 정의\n",
    "        self.flatten = tf.keras.layers.Flatten()              # 2D → 1D\n",
    "        self.dense1  = tf.keras.layers.Dense(128, activation='relu')  # 은닉층 1\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)          # 과적합 방지\n",
    "        self.dense2  = tf.keras.layers.Dense(num_classes, activation='softmax')  # 출력층\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"순전파 정의: training 인수로 Dropout 동작을 제어\"\"\"\n",
    "        x = self.flatten(inputs)               # 이미지 펼치기\n",
    "        x = self.dense1(x)                     # 은닉층 변환\n",
    "        x = self.dropout(x, training=training) # 훈련 시에만 드롭아웃 적용\n",
    "        return self.dense2(x)                  # 확률 분포 출력\n",
    "\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "subclass_model = MNISTClassifier(num_classes=10)\n",
    "\n",
    "# 더미 데이터로 빌드 (summary를 보려면 입력 shape를 알아야 함)\n",
    "dummy_input = tf.zeros((1, 28, 28))  # 배치 1개짜리 더미 이미지\n",
    "_ = subclass_model(dummy_input)       # 한 번 호출하여 가중치 초기화\n",
    "\n",
    "subclass_model.summary()\n",
    "print(f\"\\n파라미터 수: {subclass_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 커스텀 레이어 만들기\n",
    "\n",
    "`tf.keras.layers.Layer`를 상속하면 재사용 가능한 커스텀 레이어를 만들 수 있다.\n",
    "\n",
    "- `build(input_shape)`: 입력 shape가 확정된 시점에 **가중치 생성** (지연 초기화)\n",
    "- `call(inputs)`: 레이어 연산 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"커스텀 잔차 블록 레이어\n",
    "    \n",
    "    h = ReLU(Dense(Dense(x)) + x)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)  # name 등 공통 인수 전달\n",
    "        self.units = units\n",
    "\n",
    "        # 블록 내부 레이어 정의\n",
    "        self.dense1 = tf.keras.layers.Dense(units, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(units, activation=None)  # Add 전에는 활성화 없음\n",
    "        self.relu   = tf.keras.layers.Activation('relu')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"입력 shape 확정 후 가중치 초기화 (자동 호출)\"\"\"\n",
    "        # 내부 레이어들은 첫 call 때 자동으로 build됨\n",
    "        super().build(input_shape)  # built = True 플래그 설정\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"잔차 연결: h = ReLU(F(x) + x)\"\"\"\n",
    "        h = self.dense1(inputs)           # 첫 번째 변환\n",
    "        h = self.dense2(h)                # 두 번째 변환 (활성화 없음)\n",
    "        return self.relu(h + inputs)      # 스킵 연결 후 활성화\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"직렬화를 위한 설정 반환 (모델 저장 시 필요)\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "# 커스텀 레이어 테스트\n",
    "x_test = tf.random.normal((4, 64))   # 배치 4, 차원 64\n",
    "res_block = ResidualBlock(units=64, name='test_res_block')\n",
    "y_test = res_block(x_test)\n",
    "print(\"입력 shape:\", x_test.shape)\n",
    "print(\"출력 shape:\", y_test.shape)   # 잔차 연결이므로 동일해야 함\n",
    "print(\"ResidualBlock 파라미터 수:\", res_block.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. add_weight로 가중치 직접 정의\n",
    "\n",
    "`add_weight`를 사용하면 Dense 레이어 없이도 원하는 가중치를 직접 생성할 수 있다.\n",
    "이는 완전히 새로운 연산(ex. 어텐션, 커스텀 정규화)을 구현할 때 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"add_weight로 Dense 레이어를 직접 구현한 예시\"\"\"\n",
    "\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        # 활성화 함수를 문자열 또는 callable로 처리\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"입력 shape를 알아야 W의 크기를 결정할 수 있으므로 build에서 생성\"\"\"\n",
    "        n_in = int(input_shape[-1])  # 마지막 차원 = 입력 특징 수\n",
    "\n",
    "        # 가중치 행렬 W: shape (n_in, n_out)\n",
    "        self.W = self.add_weight(\n",
    "            name='kernel',\n",
    "            shape=(n_in, self.units),\n",
    "            initializer='glorot_uniform',   # Xavier 초기화\n",
    "            trainable=True                  # 역전파로 업데이트됨\n",
    "        )\n",
    "\n",
    "        # 편향 벡터 b: shape (n_out,)\n",
    "        self.b = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.units,),\n",
    "            initializer='zeros',            # 편향은 0으로 초기화\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)  # built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"y = activation(x @ W + b)\"\"\"\n",
    "        z = tf.matmul(inputs, self.W) + self.b  # 선형 변환\n",
    "        return self.activation(z)               # 활성화 함수 적용\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'units': self.units,\n",
    "            'activation': tf.keras.activations.serialize(self.activation)\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# 커스텀 Dense 레이어 테스트 및 내장 Dense와 비교\n",
    "custom_dense = CustomDenseLayer(units=32, activation='relu')\n",
    "builtin_dense = tf.keras.layers.Dense(units=32, activation='relu')\n",
    "\n",
    "x_sample = tf.random.normal((8, 64))  # 배치 8, 입력 64차원\n",
    "\n",
    "out_custom  = custom_dense(x_sample)\n",
    "out_builtin = builtin_dense(x_sample)\n",
    "\n",
    "print(\"커스텀 Dense 출력 shape:\", out_custom.shape)\n",
    "print(\"내장 Dense 출력 shape:\",   out_builtin.shape)\n",
    "print(\"커스텀 Dense 파라미터:\",   custom_dense.count_params())\n",
    "print(\"내장 Dense 파라미터:\",     builtin_dense.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Subclassing으로 MNIST 분류기 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepMNISTClassifier(tf.keras.Model):\n",
    "    \"\"\"커스텀 잔차 블록을 사용한 MNIST 분류기\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten    = tf.keras.layers.Flatten()\n",
    "        self.projection = tf.keras.layers.Dense(64, activation='relu')  # 차원 투영\n",
    "        self.res_block1 = ResidualBlock(units=64, name='res_1')          # 잔차 블록 1\n",
    "        self.res_block2 = ResidualBlock(units=64, name='res_2')          # 잔차 블록 2\n",
    "        self.dropout    = tf.keras.layers.Dropout(0.3)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.flatten(inputs)\n",
    "        x = self.projection(x)                      # 64차원으로 투영\n",
    "        x = self.res_block1(x)                      # 첫 번째 잔차 블록\n",
    "        x = self.res_block2(x)                      # 두 번째 잔차 블록\n",
    "        x = self.dropout(x, training=training)      # 훈련 시에만 드롭아웃\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# 모델 생성 및 컴파일\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # 정규화\n",
    "\n",
    "deep_model = DeepMNISTClassifier(num_classes=10)\n",
    "deep_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 더미 입력으로 빌드 후 summary 출력\n",
    "_ = deep_model(tf.zeros((1, 28, 28)))\n",
    "deep_model.summary()\n",
    "\n",
    "# 학습\n",
    "history = deep_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 테스트 평가\n",
    "test_loss, test_acc = deep_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\n테스트 정확도: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 3가지 API 비교\n",
    "\n",
    "| 항목 | Sequential | Functional | Subclassing |\n",
    "|------|-----------|------------|-------------|\n",
    "| 유연성 | 낮음 | 중간 | 높음 |\n",
    "| 모델 시각화 | 가능 | 가능 | 제한적 |\n",
    "| 동적 구조 | 불가 | 불가 | 가능 |\n",
    "| 디버깅 | 쉬움 | 쉬움 | 중간 |\n",
    "| 재사용성 | 낮음 | 중간 | 높음 |\n",
    "| 권장 상황 | 튜토리얼, 프로토타입 | 복잡한 구조 | 연구, 특수 레이어 |\n",
    "\n",
    "**다음**: 04_layers_and_activations.ipynb — 다양한 레이어와 활성화 함수"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_study)",
   "language": "python",
   "name": "tf_study"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
